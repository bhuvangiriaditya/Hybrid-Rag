{
  "description": "Evaluation questions with ground-truth answers derived from the local Wikipedia-based corpus (scraped_fixed.json).",
  "total_questions": 100,
  "sources": [
    {
      "id": "S001",
      "title": "Physics",
      "url": "https://en.wikipedia.org/wiki/Physics"
    },
    {
      "id": "S002",
      "title": "Chemistry",
      "url": "https://en.wikipedia.org/wiki/Chemistry"
    },
    {
      "id": "S003",
      "title": "Biology",
      "url": "https://en.wikipedia.org/wiki/Biology"
    },
    {
      "id": "S004",
      "title": "Mathematics",
      "url": "https://en.wikipedia.org/wiki/Mathematics"
    },
    {
      "id": "S005",
      "title": "Computer science",
      "url": "https://en.wikipedia.org/wiki/Computer_science"
    },
    {
      "id": "S006",
      "title": "Evolution",
      "url": "https://en.wikipedia.org/wiki/Evolution"
    },
    {
      "id": "S007",
      "title": "Plate tectonics",
      "url": "https://en.wikipedia.org/wiki/Plate_tectonics"
    },
    {
      "id": "S008",
      "title": "Climate change",
      "url": "https://en.wikipedia.org/wiki/Climate_change"
    },
    {
      "id": "S009",
      "title": "Quantum mechanics",
      "url": "https://en.wikipedia.org/wiki/Quantum_mechanics"
    },
    {
      "id": "S010",
      "title": "Relativity",
      "url": "https://en.wikipedia.org/wiki/Relativity"
    },
    {
      "id": "S011",
      "title": "Astronomy",
      "url": "https://en.wikipedia.org/wiki/Astronomy"
    },
    {
      "id": "S012",
      "title": "Universe",
      "url": "https://en.wikipedia.org/wiki/Universe"
    },
    {
      "id": "S013",
      "title": "Human brain",
      "url": "https://en.wikipedia.org/wiki/Human_brain"
    },
    {
      "id": "S014",
      "title": "DNA",
      "url": "https://en.wikipedia.org/wiki/DNA"
    },
    {
      "id": "S015",
      "title": "Photosynthesis",
      "url": "https://en.wikipedia.org/wiki/Photosynthesis"
    },
    {
      "id": "S016",
      "title": "Cell (biology)",
      "url": "https://en.wikipedia.org/wiki/Cell_(biology)"
    },
    {
      "id": "S017",
      "title": "History of the Internet",
      "url": "https://en.wikipedia.org/wiki/History_of_the_Internet"
    },
    {
      "id": "S018",
      "title": "World War II",
      "url": "https://en.wikipedia.org/wiki/World_War_II"
    },
    {
      "id": "S019",
      "title": "Ancient Egypt",
      "url": "https://en.wikipedia.org/wiki/Ancient_Egypt"
    },
    {
      "id": "S020",
      "title": "Renaissance",
      "url": "https://en.wikipedia.org/wiki/Renaissance"
    },
    {
      "id": "S021",
      "title": "Industrial Revolution",
      "url": "https://en.wikipedia.org/wiki/Industrial_Revolution"
    },
    {
      "id": "S022",
      "title": "French Revolution",
      "url": "https://en.wikipedia.org/wiki/French_Revolution"
    },
    {
      "id": "S023",
      "title": "Cold War",
      "url": "https://en.wikipedia.org/wiki/Cold_War"
    },
    {
      "id": "S024",
      "title": "Great Depression",
      "url": "https://en.wikipedia.org/wiki/Great_Depression"
    },
    {
      "id": "S025",
      "title": "Philosophy",
      "url": "https://en.wikipedia.org/wiki/Philosophy"
    },
    {
      "id": "S026",
      "title": "Ethics",
      "url": "https://en.wikipedia.org/wiki/Ethics"
    },
    {
      "id": "S027",
      "title": "Logic",
      "url": "https://en.wikipedia.org/wiki/Logic"
    },
    {
      "id": "S028",
      "title": "Aesthetics",
      "url": "https://en.wikipedia.org/wiki/Aesthetics"
    },
    {
      "id": "S029",
      "title": "Political science",
      "url": "https://en.wikipedia.org/wiki/Political_science"
    },
    {
      "id": "S030",
      "title": "Economics",
      "url": "https://en.wikipedia.org/wiki/Economics"
    },
    {
      "id": "S031",
      "title": "Macroeconomics",
      "url": "https://en.wikipedia.org/wiki/Macroeconomics"
    },
    {
      "id": "S032",
      "title": "Microeconomics",
      "url": "https://en.wikipedia.org/wiki/Microeconomics"
    },
    {
      "id": "S033",
      "title": "Statistics",
      "url": "https://en.wikipedia.org/wiki/Statistics"
    },
    {
      "id": "S034",
      "title": "Probability",
      "url": "https://en.wikipedia.org/wiki/Probability"
    },
    {
      "id": "S035",
      "title": "Artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence"
    },
    {
      "id": "S036",
      "title": "Machine learning",
      "url": "https://en.wikipedia.org/wiki/Machine_learning"
    },
    {
      "id": "S037",
      "title": "Neural network",
      "url": "https://en.wikipedia.org/wiki/Neural_network"
    },
    {
      "id": "S038",
      "title": "Data science",
      "url": "https://en.wikipedia.org/wiki/Data_science"
    },
    {
      "id": "S039",
      "title": "Internet of things",
      "url": "https://en.wikipedia.org/wiki/Internet_of_things"
    },
    {
      "id": "S040",
      "title": "Cryptocurrency",
      "url": "https://en.wikipedia.org/wiki/Cryptocurrency"
    },
    {
      "id": "S041",
      "title": "Blockchain",
      "url": "https://en.wikipedia.org/wiki/Blockchain"
    },
    {
      "id": "S042",
      "title": "World Wide Web",
      "url": "https://en.wikipedia.org/wiki/World_Wide_Web"
    },
    {
      "id": "S043",
      "title": "Open source",
      "url": "https://en.wikipedia.org/wiki/Open_source"
    },
    {
      "id": "S044",
      "title": "Software engineering",
      "url": "https://en.wikipedia.org/wiki/Software_engineering"
    },
    {
      "id": "S045",
      "title": "Operating system",
      "url": "https://en.wikipedia.org/wiki/Operating_system"
    },
    {
      "id": "S046",
      "title": "Database",
      "url": "https://en.wikipedia.org/wiki/Database"
    },
    {
      "id": "S047",
      "title": "Computer network",
      "url": "https://en.wikipedia.org/wiki/Computer_network"
    },
    {
      "id": "S048",
      "title": "Encryption",
      "url": "https://en.wikipedia.org/wiki/Encryption"
    },
    {
      "id": "S049",
      "title": "Cybersecurity",
      "url": "https://en.wikipedia.org/wiki/Cybersecurity"
    },
    {
      "id": "S050",
      "title": "Electricity",
      "url": "https://en.wikipedia.org/wiki/Electricity"
    },
    {
      "id": "S051",
      "title": "Magnetism",
      "url": "https://en.wikipedia.org/wiki/Magnetism"
    },
    {
      "id": "S052",
      "title": "Light",
      "url": "https://en.wikipedia.org/wiki/Light"
    },
    {
      "id": "S053",
      "title": "Sound",
      "url": "https://en.wikipedia.org/wiki/Sound"
    },
    {
      "id": "S054",
      "title": "Energy",
      "url": "https://en.wikipedia.org/wiki/Energy"
    },
    {
      "id": "S055",
      "title": "Gravity",
      "url": "https://en.wikipedia.org/wiki/Gravity"
    },
    {
      "id": "S056",
      "title": "Black hole",
      "url": "https://en.wikipedia.org/wiki/Black_hole"
    },
    {
      "id": "S057",
      "title": "Solar system",
      "url": "https://en.wikipedia.org/wiki/Solar_system"
    },
    {
      "id": "S058",
      "title": "Earth",
      "url": "https://en.wikipedia.org/wiki/Earth"
    },
    {
      "id": "S059",
      "title": "Atmosphere",
      "url": "https://en.wikipedia.org/wiki/Atmosphere"
    },
    {
      "id": "S060",
      "title": "Ocean",
      "url": "https://en.wikipedia.org/wiki/Ocean"
    },
    {
      "id": "S061",
      "title": "Biodiversity",
      "url": "https://en.wikipedia.org/wiki/Biodiversity"
    },
    {
      "id": "S062",
      "title": "Conservation",
      "url": "https://en.wikipedia.org/wiki/Conservation"
    },
    {
      "id": "S063",
      "title": "Ecology",
      "url": "https://en.wikipedia.org/wiki/Ecology"
    },
    {
      "id": "S064",
      "title": "Genetics",
      "url": "https://en.wikipedia.org/wiki/Genetics"
    },
    {
      "id": "S065",
      "title": "Immunology",
      "url": "https://en.wikipedia.org/wiki/Immunology"
    },
    {
      "id": "S066",
      "title": "Epidemiology",
      "url": "https://en.wikipedia.org/wiki/Epidemiology"
    },
    {
      "id": "S067",
      "title": "Vaccination",
      "url": "https://en.wikipedia.org/wiki/Vaccination"
    },
    {
      "id": "S068",
      "title": "Medicine",
      "url": "https://en.wikipedia.org/wiki/Medicine"
    },
    {
      "id": "S069",
      "title": "Nutrition",
      "url": "https://en.wikipedia.org/wiki/Nutrition"
    },
    {
      "id": "S070",
      "title": "Exercise",
      "url": "https://en.wikipedia.org/wiki/Exercise"
    },
    {
      "id": "S071",
      "title": "Public health",
      "url": "https://en.wikipedia.org/wiki/Public_health"
    },
    {
      "id": "S072",
      "title": "Geology",
      "url": "https://en.wikipedia.org/wiki/Geology"
    },
    {
      "id": "S073",
      "title": "Volcano",
      "url": "https://en.wikipedia.org/wiki/Volcano"
    },
    {
      "id": "S074",
      "title": "Earthquake",
      "url": "https://en.wikipedia.org/wiki/Earthquake"
    },
    {
      "id": "S075",
      "title": "Mineral",
      "url": "https://en.wikipedia.org/wiki/Mineral"
    },
    {
      "id": "S076",
      "title": "Fossil",
      "url": "https://en.wikipedia.org/wiki/Fossil"
    },
    {
      "id": "S077",
      "title": "Archaeology",
      "url": "https://en.wikipedia.org/wiki/Archaeology"
    },
    {
      "id": "S078",
      "title": "Anthropology",
      "url": "https://en.wikipedia.org/wiki/Anthropology"
    },
    {
      "id": "S079",
      "title": "Sociology",
      "url": "https://en.wikipedia.org/wiki/Sociology"
    },
    {
      "id": "S080",
      "title": "Psychology",
      "url": "https://en.wikipedia.org/wiki/Psychology"
    },
    {
      "id": "S081",
      "title": "Education",
      "url": "https://en.wikipedia.org/wiki/Education"
    },
    {
      "id": "S082",
      "title": "Linguistics",
      "url": "https://en.wikipedia.org/wiki/Linguistics"
    },
    {
      "id": "S083",
      "title": "Literature",
      "url": "https://en.wikipedia.org/wiki/Literature"
    },
    {
      "id": "S084",
      "title": "Poetry",
      "url": "https://en.wikipedia.org/wiki/Poetry"
    },
    {
      "id": "S085",
      "title": "Drama",
      "url": "https://en.wikipedia.org/wiki/Drama"
    },
    {
      "id": "S086",
      "title": "Religion",
      "url": "https://en.wikipedia.org/wiki/Religion"
    },
    {
      "id": "S087",
      "title": "Mythology",
      "url": "https://en.wikipedia.org/wiki/Mythology"
    },
    {
      "id": "S088",
      "title": "Art",
      "url": "https://en.wikipedia.org/wiki/Art"
    },
    {
      "id": "S089",
      "title": "Music",
      "url": "https://en.wikipedia.org/wiki/Music"
    },
    {
      "id": "S090",
      "title": "Painting",
      "url": "https://en.wikipedia.org/wiki/Painting"
    },
    {
      "id": "S091",
      "title": "Sculpture",
      "url": "https://en.wikipedia.org/wiki/Sculpture"
    },
    {
      "id": "S092",
      "title": "Architecture",
      "url": "https://en.wikipedia.org/wiki/Architecture"
    },
    {
      "id": "S093",
      "title": "Photography",
      "url": "https://en.wikipedia.org/wiki/Photography"
    },
    {
      "id": "S094",
      "title": "Film",
      "url": "https://en.wikipedia.org/wiki/Film"
    },
    {
      "id": "S095",
      "title": "Theatre",
      "url": "https://en.wikipedia.org/wiki/Theatre"
    },
    {
      "id": "S096",
      "title": "Dance",
      "url": "https://en.wikipedia.org/wiki/Dance"
    },
    {
      "id": "S097",
      "title": "Fashion",
      "url": "https://en.wikipedia.org/wiki/Fashion"
    },
    {
      "id": "S098",
      "title": "Cuisine",
      "url": "https://en.wikipedia.org/wiki/Cuisine"
    },
    {
      "id": "S099",
      "title": "Geography",
      "url": "https://en.wikipedia.org/wiki/Geography"
    },
    {
      "id": "S100",
      "title": "Capital city",
      "url": "https://en.wikipedia.org/wiki/Capital_city"
    },
    {
      "id": "S101",
      "title": "United States",
      "url": "https://en.wikipedia.org/wiki/United_States"
    },
    {
      "id": "S102",
      "title": "India",
      "url": "https://en.wikipedia.org/wiki/India"
    },
    {
      "id": "S103",
      "title": "China",
      "url": "https://en.wikipedia.org/wiki/China"
    },
    {
      "id": "S104",
      "title": "Brazil",
      "url": "https://en.wikipedia.org/wiki/Brazil"
    },
    {
      "id": "S105",
      "title": "Australia",
      "url": "https://en.wikipedia.org/wiki/Australia"
    },
    {
      "id": "S106",
      "title": "Russia",
      "url": "https://en.wikipedia.org/wiki/Russia"
    },
    {
      "id": "S107",
      "title": "Canada",
      "url": "https://en.wikipedia.org/wiki/Canada"
    },
    {
      "id": "S108",
      "title": "Mexico",
      "url": "https://en.wikipedia.org/wiki/Mexico"
    },
    {
      "id": "S109",
      "title": "European Union",
      "url": "https://en.wikipedia.org/wiki/European_Union"
    },
    {
      "id": "S110",
      "title": "United Nations",
      "url": "https://en.wikipedia.org/wiki/United_Nations"
    },
    {
      "id": "S111",
      "title": "Democracy",
      "url": "https://en.wikipedia.org/wiki/Democracy"
    },
    {
      "id": "S112",
      "title": "Constitution",
      "url": "https://en.wikipedia.org/wiki/Constitution"
    },
    {
      "id": "S113",
      "title": "Law",
      "url": "https://en.wikipedia.org/wiki/Law"
    },
    {
      "id": "S114",
      "title": "Justice",
      "url": "https://en.wikipedia.org/wiki/Justice"
    },
    {
      "id": "S115",
      "title": "International law",
      "url": "https://en.wikipedia.org/wiki/International_law"
    },
    {
      "id": "S116",
      "title": "Globalization",
      "url": "https://en.wikipedia.org/wiki/Globalization"
    },
    {
      "id": "S117",
      "title": "Big Bang",
      "url": "https://en.wikipedia.org/wiki/Big_Bang"
    },
    {
      "id": "S118",
      "title": "Dark matter",
      "url": "https://en.wikipedia.org/wiki/Dark_matter"
    },
    {
      "id": "S119",
      "title": "Dark energy",
      "url": "https://en.wikipedia.org/wiki/Dark_energy"
    },
    {
      "id": "S120",
      "title": "Neuroscience",
      "url": "https://en.wikipedia.org/wiki/Neuroscience"
    },
    {
      "id": "S121",
      "title": "Biotechnology",
      "url": "https://en.wikipedia.org/wiki/Biotechnology"
    },
    {
      "id": "S122",
      "title": "Genetic engineering",
      "url": "https://en.wikipedia.org/wiki/Genetic_engineering"
    },
    {
      "id": "S123",
      "title": "Deep learning",
      "url": "https://en.wikipedia.org/wiki/Deep_learning"
    },
    {
      "id": "S124",
      "title": "Natural language processing",
      "url": "https://en.wikipedia.org/wiki/Natural_language_processing"
    },
    {
      "id": "S125",
      "title": "Cloud computing",
      "url": "https://en.wikipedia.org/wiki/Cloud_computing"
    },
    {
      "id": "S126",
      "title": "Distributed computing",
      "url": "https://en.wikipedia.org/wiki/Distributed_computing"
    },
    {
      "id": "S127",
      "title": "Roman Empire",
      "url": "https://en.wikipedia.org/wiki/Roman_Empire"
    },
    {
      "id": "S128",
      "title": "Indus Valley Civilisation",
      "url": "https://en.wikipedia.org/wiki/Indus_Valley_Civilisation"
    },
    {
      "id": "S129",
      "title": "Mesopotamia",
      "url": "https://en.wikipedia.org/wiki/Mesopotamia"
    },
    {
      "id": "S130",
      "title": "Age of Enlightenment",
      "url": "https://en.wikipedia.org/wiki/Age_of_Enlightenment"
    },
    {
      "id": "S131",
      "title": "Existentialism",
      "url": "https://en.wikipedia.org/wiki/Existentialism"
    },
    {
      "id": "S132",
      "title": "Human rights",
      "url": "https://en.wikipedia.org/wiki/Human_rights"
    },
    {
      "id": "S133",
      "title": "Capitalism",
      "url": "https://en.wikipedia.org/wiki/Capitalism"
    },
    {
      "id": "S134",
      "title": "Socialism",
      "url": "https://en.wikipedia.org/wiki/Socialism"
    },
    {
      "id": "S135",
      "title": "Inflation",
      "url": "https://en.wikipedia.org/wiki/Inflation"
    },
    {
      "id": "S136",
      "title": "Monetary policy",
      "url": "https://en.wikipedia.org/wiki/Monetary_policy"
    },
    {
      "id": "S137",
      "title": "Fiscal policy",
      "url": "https://en.wikipedia.org/wiki/Fiscal_policy"
    },
    {
      "id": "S138",
      "title": "Scientific method",
      "url": "https://en.wikipedia.org/wiki/Scientific_method"
    },
    {
      "id": "S139",
      "title": "Control theory",
      "url": "https://en.wikipedia.org/wiki/Control_theory"
    },
    {
      "id": "S140",
      "title": "Systems engineering",
      "url": "https://en.wikipedia.org/wiki/Systems_engineering"
    },
    {
      "id": "S141",
      "title": "Materials science",
      "url": "https://en.wikipedia.org/wiki/Materials_science"
    },
    {
      "id": "S142",
      "title": "Nanotechnology",
      "url": "https://en.wikipedia.org/wiki/Nanotechnology"
    },
    {
      "id": "S143",
      "title": "Robotics",
      "url": "https://en.wikipedia.org/wiki/Robotics"
    },
    {
      "id": "S144",
      "title": "Mechatronics",
      "url": "https://en.wikipedia.org/wiki/Mechatronics"
    },
    {
      "id": "S145",
      "title": "Aerospace engineering",
      "url": "https://en.wikipedia.org/wiki/Aerospace_engineering"
    },
    {
      "id": "S146",
      "title": "Civil engineering",
      "url": "https://en.wikipedia.org/wiki/Civil_engineering"
    },
    {
      "id": "S147",
      "title": "Mechanical engineering",
      "url": "https://en.wikipedia.org/wiki/Mechanical_engineering"
    },
    {
      "id": "S148",
      "title": "Information theory",
      "url": "https://en.wikipedia.org/wiki/Information_theory"
    },
    {
      "id": "S149",
      "title": "Algorithm",
      "url": "https://en.wikipedia.org/wiki/Algorithm"
    },
    {
      "id": "S150",
      "title": "Computational complexity theory",
      "url": "https://en.wikipedia.org/wiki/Computational_complexity_theory"
    },
    {
      "id": "S151",
      "title": "Programming language",
      "url": "https://en.wikipedia.org/wiki/Programming_language"
    },
    {
      "id": "S152",
      "title": "Compiler",
      "url": "https://en.wikipedia.org/wiki/Compiler"
    },
    {
      "id": "S153",
      "title": "Operating system kernel",
      "url": "https://en.wikipedia.org/wiki/Operating_system_kernel"
    },
    {
      "id": "S154",
      "title": "Virtualization",
      "url": "https://en.wikipedia.org/wiki/Virtualization"
    },
    {
      "id": "S155",
      "title": "Containerization (computing)",
      "url": "https://en.wikipedia.org/wiki/Containerization_(computing)"
    },
    {
      "id": "S156",
      "title": "Big data",
      "url": "https://en.wikipedia.org/wiki/Big_data"
    },
    {
      "id": "S157",
      "title": "Information security",
      "url": "https://en.wikipedia.org/wiki/Information_security"
    },
    {
      "id": "S158",
      "title": "Cognitive science",
      "url": "https://en.wikipedia.org/wiki/Cognitive_science"
    },
    {
      "id": "S159",
      "title": "Consciousness",
      "url": "https://en.wikipedia.org/wiki/Consciousness"
    },
    {
      "id": "S160",
      "title": "Memory",
      "url": "https://en.wikipedia.org/wiki/Memory"
    },
    {
      "id": "S161",
      "title": "Attention",
      "url": "https://en.wikipedia.org/wiki/Attention"
    },
    {
      "id": "S162",
      "title": "Learning",
      "url": "https://en.wikipedia.org/wiki/Learning"
    },
    {
      "id": "S163",
      "title": "Decision theory",
      "url": "https://en.wikipedia.org/wiki/Decision_theory"
    },
    {
      "id": "S164",
      "title": "Neuroplasticity",
      "url": "https://en.wikipedia.org/wiki/Neuroplasticity"
    },
    {
      "id": "S165",
      "title": "Developmental psychology",
      "url": "https://en.wikipedia.org/wiki/Developmental_psychology"
    },
    {
      "id": "S166",
      "title": "History of humanity",
      "url": "https://en.wikipedia.org/wiki/History_of_humanity"
    },
    {
      "id": "S167",
      "title": "Prehistory",
      "url": "https://en.wikipedia.org/wiki/Prehistory"
    },
    {
      "id": "S168",
      "title": "Ancient Greece",
      "url": "https://en.wikipedia.org/wiki/Ancient_Greece"
    },
    {
      "id": "S169",
      "title": "Byzantine Empire",
      "url": "https://en.wikipedia.org/wiki/Byzantine_Empire"
    },
    {
      "id": "S170",
      "title": "Ottoman Empire",
      "url": "https://en.wikipedia.org/wiki/Ottoman_Empire"
    },
    {
      "id": "S171",
      "title": "Mughal Empire",
      "url": "https://en.wikipedia.org/wiki/Mughal_Empire"
    },
    {
      "id": "S172",
      "title": "History of India",
      "url": "https://en.wikipedia.org/wiki/History_of_India"
    },
    {
      "id": "S173",
      "title": "History of China",
      "url": "https://en.wikipedia.org/wiki/History_of_China"
    },
    {
      "id": "S174",
      "title": "Earth system science",
      "url": "https://en.wikipedia.org/wiki/Earth_system_science"
    },
    {
      "id": "S175",
      "title": "Carbon cycle",
      "url": "https://en.wikipedia.org/wiki/Carbon_cycle"
    },
    {
      "id": "S176",
      "title": "Water cycle",
      "url": "https://en.wikipedia.org/wiki/Water_cycle"
    },
    {
      "id": "S177",
      "title": "Atmospheric science",
      "url": "https://en.wikipedia.org/wiki/Atmospheric_science"
    },
    {
      "id": "S178",
      "title": "Climate model",
      "url": "https://en.wikipedia.org/wiki/Climate_model"
    },
    {
      "id": "S179",
      "title": "Environmental science",
      "url": "https://en.wikipedia.org/wiki/Environmental_science"
    },
    {
      "id": "S180",
      "title": "Environmental policy",
      "url": "https://en.wikipedia.org/wiki/Environmental_policy"
    },
    {
      "id": "S181",
      "title": "Political philosophy",
      "url": "https://en.wikipedia.org/wiki/Political_philosophy"
    },
    {
      "id": "S182",
      "title": "Public policy",
      "url": "https://en.wikipedia.org/wiki/Public_policy"
    },
    {
      "id": "S183",
      "title": "Governance",
      "url": "https://en.wikipedia.org/wiki/Governance"
    },
    {
      "id": "S184",
      "title": "International relations",
      "url": "https://en.wikipedia.org/wiki/International_relations"
    },
    {
      "id": "S185",
      "title": "Geopolitics",
      "url": "https://en.wikipedia.org/wiki/Geopolitics"
    },
    {
      "id": "S186",
      "title": "Economic development",
      "url": "https://en.wikipedia.org/wiki/Economic_development"
    },
    {
      "id": "S187",
      "title": "Sustainable economy",
      "url": "https://en.wikipedia.org/wiki/Sustainable_economy"
    },
    {
      "id": "S188",
      "title": "Mass communication",
      "url": "https://en.wikipedia.org/wiki/Mass_communication"
    },
    {
      "id": "S189",
      "title": "Journalism",
      "url": "https://en.wikipedia.org/wiki/Journalism"
    },
    {
      "id": "S190",
      "title": "Digital media",
      "url": "https://en.wikipedia.org/wiki/Digital_media"
    },
    {
      "id": "S191",
      "title": "Popular culture",
      "url": "https://en.wikipedia.org/wiki/Popular_culture"
    },
    {
      "id": "S192",
      "title": "Cultural anthropology",
      "url": "https://en.wikipedia.org/wiki/Cultural_anthropology"
    },
    {
      "id": "S193",
      "title": "Visual arts",
      "url": "https://en.wikipedia.org/wiki/Visual_arts"
    },
    {
      "id": "S194",
      "title": "Game theory",
      "url": "https://en.wikipedia.org/wiki/Game_theory"
    },
    {
      "id": "S195",
      "title": "Complex system",
      "url": "https://en.wikipedia.org/wiki/Complex_system"
    },
    {
      "id": "S196",
      "title": "Network science",
      "url": "https://en.wikipedia.org/wiki/Network_science"
    },
    {
      "id": "S197",
      "title": "Evolutionary biology",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_biology"
    },
    {
      "id": "S198",
      "title": "Urbanization",
      "url": "https://en.wikipedia.org/wiki/Urbanization"
    },
    {
      "id": "S199",
      "title": "Space exploration",
      "url": "https://en.wikipedia.org/wiki/Space_exploration"
    },
    {
      "id": "S200",
      "title": "Bengaluru",
      "url": "https://en.wikipedia.org/wiki/Bengaluru"
    },
    {
      "id": "S201",
      "title": "Kansas Department of Labor",
      "url": "https://en.wikipedia.org/wiki/Kansas_Department_of_Labor"
    },
    {
      "id": "S202",
      "title": "Fort Lily",
      "url": "https://en.wikipedia.org/wiki/Fort_Lily"
    },
    {
      "id": "S203",
      "title": "Actinidia pilosula",
      "url": "https://en.wikipedia.org/wiki/Actinidia_pilosula"
    },
    {
      "id": "S204",
      "title": "Ru Thing",
      "url": "https://en.wikipedia.org/wiki/Ru_Thing"
    },
    {
      "id": "S205",
      "title": "National Board of Review Awards 1949",
      "url": "https://en.wikipedia.org/wiki/National_Board_of_Review_Awards_1949"
    },
    {
      "id": "S206",
      "title": "XHQT-FM",
      "url": "https://en.wikipedia.org/wiki/XHQT-FM"
    },
    {
      "id": "S207",
      "title": "The Real Housewives of Dallas",
      "url": "https://en.wikipedia.org/wiki/The_Real_Housewives_of_Dallas"
    },
    {
      "id": "S208",
      "title": "Veloxis Pharmaceuticals",
      "url": "https://en.wikipedia.org/wiki/Veloxis_Pharmaceuticals"
    },
    {
      "id": "S209",
      "title": "Pacific Grove",
      "url": "https://en.wikipedia.org/wiki/Pacific_Grove"
    },
    {
      "id": "S210",
      "title": "The Konstantinos Staikos' book collection",
      "url": "https://en.wikipedia.org/wiki/The_Konstantinos_Staikos'_book_collection"
    },
    {
      "id": "S211",
      "title": "Robots (2005 film)",
      "url": "https://en.wikipedia.org/wiki/Robots_(2005_film)"
    },
    {
      "id": "S212",
      "title": "Goold v Collins",
      "url": "https://en.wikipedia.org/wiki/Goold_v_Collins"
    },
    {
      "id": "S213",
      "title": "Spectemur agendo",
      "url": "https://en.wikipedia.org/wiki/Spectemur_agendo"
    },
    {
      "id": "S214",
      "title": "Claude Doral",
      "url": "https://en.wikipedia.org/wiki/Claude_Doral"
    },
    {
      "id": "S215",
      "title": "Kibaigwa",
      "url": "https://en.wikipedia.org/wiki/Kibaigwa"
    },
    {
      "id": "S216",
      "title": "Ferrari 166 S",
      "url": "https://en.wikipedia.org/wiki/Ferrari_166_S"
    },
    {
      "id": "S217",
      "title": "Therese Lundin (footballer)",
      "url": "https://en.wikipedia.org/wiki/Therese_Lundin_(footballer)"
    },
    {
      "id": "S218",
      "title": "Bernstadt, Kentucky",
      "url": "https://en.wikipedia.org/wiki/Bernstadt,_Kentucky"
    },
    {
      "id": "S219",
      "title": "John Linaker",
      "url": "https://en.wikipedia.org/wiki/John_Linaker"
    },
    {
      "id": "S220",
      "title": "G-Files awards",
      "url": "https://en.wikipedia.org/wiki/G-Files_awards"
    },
    {
      "id": "S221",
      "title": "Isaac W. Dyer Estate",
      "url": "https://en.wikipedia.org/wiki/Isaac_W._Dyer_Estate"
    },
    {
      "id": "S222",
      "title": "2022 NCAA Division III women's basketball tournament",
      "url": "https://en.wikipedia.org/wiki/2022_NCAA_Division_III_women's_basketball_tournament"
    },
    {
      "id": "S223",
      "title": "Peter Edgar Corbett",
      "url": "https://en.wikipedia.org/wiki/Peter_Edgar_Corbett"
    },
    {
      "id": "S224",
      "title": "Tanya Plibersek",
      "url": "https://en.wikipedia.org/wiki/Tanya_Plibersek"
    },
    {
      "id": "S225",
      "title": "Charlie Chin",
      "url": "https://en.wikipedia.org/wiki/Charlie_Chin"
    },
    {
      "id": "S226",
      "title": "Mills (surname)",
      "url": "https://en.wikipedia.org/wiki/Mills_(surname)"
    },
    {
      "id": "S227",
      "title": "Arthur Stedman",
      "url": "https://en.wikipedia.org/wiki/Arthur_Stedman"
    },
    {
      "id": "S228",
      "title": "Abbas Shareef",
      "url": "https://en.wikipedia.org/wiki/Abbas_Shareef"
    },
    {
      "id": "S229",
      "title": "Lisa Carlsen (basketball)",
      "url": "https://en.wikipedia.org/wiki/Lisa_Carlsen_(basketball)"
    },
    {
      "id": "S230",
      "title": "The Story of Art",
      "url": "https://en.wikipedia.org/wiki/The_Story_of_Art"
    },
    {
      "id": "S231",
      "title": "Koozie",
      "url": "https://en.wikipedia.org/wiki/Koozie"
    },
    {
      "id": "S232",
      "title": "Light-Foot",
      "url": "https://en.wikipedia.org/wiki/Light-Foot"
    },
    {
      "id": "S233",
      "title": "Dynamo Stadium",
      "url": "https://en.wikipedia.org/wiki/Dynamo_Stadium"
    },
    {
      "id": "S234",
      "title": "List of butterflies of Bangladesh",
      "url": "https://en.wikipedia.org/wiki/List_of_butterflies_of_Bangladesh"
    },
    {
      "id": "S235",
      "title": "Three Man Army",
      "url": "https://en.wikipedia.org/wiki/Three_Man_Army"
    },
    {
      "id": "S236",
      "title": "List of acts of the Parliament of Great Britain from 1712",
      "url": "https://en.wikipedia.org/wiki/List_of_acts_of_the_Parliament_of_Great_Britain_from_1712"
    },
    {
      "id": "S237",
      "title": "Vieuille",
      "url": "https://en.wikipedia.org/wiki/Vieuille"
    },
    {
      "id": "S238",
      "title": "Lyfe Jennings",
      "url": "https://en.wikipedia.org/wiki/Lyfe_Jennings"
    },
    {
      "id": "S239",
      "title": "Motobirds",
      "url": "https://en.wikipedia.org/wiki/Motobirds"
    },
    {
      "id": "S240",
      "title": "James Campbell (of Burnbank and Boquhan)",
      "url": "https://en.wikipedia.org/wiki/James_Campbell_(of_Burnbank_and_Boquhan)"
    },
    {
      "id": "S241",
      "title": "Plowed",
      "url": "https://en.wikipedia.org/wiki/Plowed"
    },
    {
      "id": "S242",
      "title": "Sybra subunicolor",
      "url": "https://en.wikipedia.org/wiki/Sybra_subunicolor"
    },
    {
      "id": "S243",
      "title": "Milwaukee Brew",
      "url": "https://en.wikipedia.org/wiki/Milwaukee_Brew"
    },
    {
      "id": "S244",
      "title": "Kerem Aky\u00fcz",
      "url": "https://en.wikipedia.org/wiki/Kerem_Aky\u00fcz"
    },
    {
      "id": "S245",
      "title": "Glenda Ritz",
      "url": "https://en.wikipedia.org/wiki/Glenda_Ritz"
    },
    {
      "id": "S246",
      "title": "Murder of Patricia Allen",
      "url": "https://en.wikipedia.org/wiki/Murder_of_Patricia_Allen"
    },
    {
      "id": "S247",
      "title": "Axes of subordination",
      "url": "https://en.wikipedia.org/wiki/Axes_of_subordination"
    },
    {
      "id": "S248",
      "title": "Murphy's Law (band)",
      "url": "https://en.wikipedia.org/wiki/Murphy's_Law_(band)"
    },
    {
      "id": "S249",
      "title": "2000\u201301 International Baseball League of Australia team rosters",
      "url": "https://en.wikipedia.org/wiki/2000\u201301_International_Baseball_League_of_Australia_team_rosters"
    },
    {
      "id": "S250",
      "title": "Ted Scherman",
      "url": "https://en.wikipedia.org/wiki/Ted_Scherman"
    },
    {
      "id": "S251",
      "title": "Mobira Talkman 450",
      "url": "https://en.wikipedia.org/wiki/Mobira_Talkman_450"
    },
    {
      "id": "S252",
      "title": "Richelieu, Kentucky",
      "url": "https://en.wikipedia.org/wiki/Richelieu,_Kentucky"
    },
    {
      "id": "S253",
      "title": "Swimming at the 2000 Summer Olympics \u2013 Women's 100 metre backstroke",
      "url": "https://en.wikipedia.org/wiki/Swimming_at_the_2000_Summer_Olympics_\u2013_Women's_100_metre_backstroke"
    },
    {
      "id": "S254",
      "title": "Roberta Pedranzini",
      "url": "https://en.wikipedia.org/wiki/Roberta_Pedranzini"
    },
    {
      "id": "S255",
      "title": "Percy Fewtrell",
      "url": "https://en.wikipedia.org/wiki/Percy_Fewtrell"
    },
    {
      "id": "S256",
      "title": "Irfanullah Shah",
      "url": "https://en.wikipedia.org/wiki/Irfanullah_Shah"
    },
    {
      "id": "S257",
      "title": "CITCO",
      "url": "https://en.wikipedia.org/wiki/CITCO"
    },
    {
      "id": "S258",
      "title": "Cardiff Festivals",
      "url": "https://en.wikipedia.org/wiki/Cardiff_Festivals"
    },
    {
      "id": "S259",
      "title": "Sijerci",
      "url": "https://en.wikipedia.org/wiki/Sijerci"
    },
    {
      "id": "S260",
      "title": "Strathearn School",
      "url": "https://en.wikipedia.org/wiki/Strathearn_School"
    },
    {
      "id": "S261",
      "title": "Brudzew Kolonia",
      "url": "https://en.wikipedia.org/wiki/Brudzew_Kolonia"
    },
    {
      "id": "S262",
      "title": "1964 Venezuelan Primera Divisi\u00f3n season",
      "url": "https://en.wikipedia.org/wiki/1964_Venezuelan_Primera_Divisi\u00f3n_season"
    },
    {
      "id": "S263",
      "title": "John Hudson Riddick",
      "url": "https://en.wikipedia.org/wiki/John_Hudson_Riddick"
    },
    {
      "id": "S264",
      "title": "Khirbet El-Knese",
      "url": "https://en.wikipedia.org/wiki/Khirbet_El-Knese"
    },
    {
      "id": "S265",
      "title": "16ft Skiff",
      "url": "https://en.wikipedia.org/wiki/16ft_Skiff"
    },
    {
      "id": "S266",
      "title": "Farhat Square",
      "url": "https://en.wikipedia.org/wiki/Farhat_Square"
    },
    {
      "id": "S267",
      "title": "Razzberry Jazzberry Jam",
      "url": "https://en.wikipedia.org/wiki/Razzberry_Jazzberry_Jam"
    },
    {
      "id": "S268",
      "title": "Max Planck Institute for Evolutionary Anthropology",
      "url": "https://en.wikipedia.org/wiki/Max_Planck_Institute_for_Evolutionary_Anthropology"
    },
    {
      "id": "S269",
      "title": "R. Paul Smith Power Station",
      "url": "https://en.wikipedia.org/wiki/R._Paul_Smith_Power_Station"
    },
    {
      "id": "S270",
      "title": "2007 World Indoor Archery Championships",
      "url": "https://en.wikipedia.org/wiki/2007_World_Indoor_Archery_Championships"
    },
    {
      "id": "S271",
      "title": "Solans House",
      "url": "https://en.wikipedia.org/wiki/Solans_House"
    },
    {
      "id": "S272",
      "title": "Consort Yeongbi Choe",
      "url": "https://en.wikipedia.org/wiki/Consort_Yeongbi_Choe"
    },
    {
      "id": "S273",
      "title": "NA-240 Karachi South-II",
      "url": "https://en.wikipedia.org/wiki/NA-240_Karachi_South-II"
    },
    {
      "id": "S274",
      "title": "Last Evenings on Earth",
      "url": "https://en.wikipedia.org/wiki/Last_Evenings_on_Earth"
    },
    {
      "id": "S275",
      "title": "1993 Pan Pacific Swimming Championships \u2013 Men's 100 metre freestyle",
      "url": "https://en.wikipedia.org/wiki/1993_Pan_Pacific_Swimming_Championships_\u2013_Men's_100_metre_freestyle"
    },
    {
      "id": "S276",
      "title": "Kenichiro Takaki",
      "url": "https://en.wikipedia.org/wiki/Kenichiro_Takaki"
    },
    {
      "id": "S277",
      "title": "United Nations Security Council Resolution 74",
      "url": "https://en.wikipedia.org/wiki/United_Nations_Security_Council_Resolution_74"
    },
    {
      "id": "S278",
      "title": "Infrastructure debt",
      "url": "https://en.wikipedia.org/wiki/Infrastructure_debt"
    },
    {
      "id": "S279",
      "title": "Longwood Bowl",
      "url": "https://en.wikipedia.org/wiki/Longwood_Bowl"
    },
    {
      "id": "S280",
      "title": "Daniil Shishkaryov",
      "url": "https://en.wikipedia.org/wiki/Daniil_Shishkaryov"
    },
    {
      "id": "S281",
      "title": "2003\u201304 Montreal Canadiens season",
      "url": "https://en.wikipedia.org/wiki/2003\u201304_Montreal_Canadiens_season"
    },
    {
      "id": "S282",
      "title": "Decibel Festival",
      "url": "https://en.wikipedia.org/wiki/Decibel_Festival"
    },
    {
      "id": "S283",
      "title": "National Coming Out Day",
      "url": "https://en.wikipedia.org/wiki/National_Coming_Out_Day"
    },
    {
      "id": "S284",
      "title": "Brennan's",
      "url": "https://en.wikipedia.org/wiki/Brennan's"
    },
    {
      "id": "S285",
      "title": "Les Boullereaux-Champigny station",
      "url": "https://en.wikipedia.org/wiki/Les_Boullereaux-Champigny_station"
    },
    {
      "id": "S286",
      "title": "Neville Gass",
      "url": "https://en.wikipedia.org/wiki/Neville_Gass"
    },
    {
      "id": "S287",
      "title": "Guerrero, Isabela, Puerto Rico",
      "url": "https://en.wikipedia.org/wiki/Guerrero,_Isabela,_Puerto_Rico"
    },
    {
      "id": "S288",
      "title": "Black-and-white owl",
      "url": "https://en.wikipedia.org/wiki/Black-and-white_owl"
    },
    {
      "id": "S289",
      "title": "Fresh Aire",
      "url": "https://en.wikipedia.org/wiki/Fresh_Aire"
    },
    {
      "id": "S290",
      "title": "The Gray Nun of Belgium",
      "url": "https://en.wikipedia.org/wiki/The_Gray_Nun_of_Belgium"
    },
    {
      "id": "S291",
      "title": "Hartlepool by-election",
      "url": "https://en.wikipedia.org/wiki/Hartlepool_by-election"
    },
    {
      "id": "S292",
      "title": "Chokdaebong",
      "url": "https://en.wikipedia.org/wiki/Chokdaebong"
    },
    {
      "id": "S293",
      "title": "Roman Shevliakov",
      "url": "https://en.wikipedia.org/wiki/Roman_Shevliakov"
    },
    {
      "id": "S294",
      "title": "1643 in music",
      "url": "https://en.wikipedia.org/wiki/1643_in_music"
    },
    {
      "id": "S295",
      "title": "Continuance of Laws Act 1557",
      "url": "https://en.wikipedia.org/wiki/Continuance_of_Laws_Act_1557"
    },
    {
      "id": "S296",
      "title": "Seattle Wireless",
      "url": "https://en.wikipedia.org/wiki/Seattle_Wireless"
    },
    {
      "id": "S297",
      "title": "Atl\u00e9tico Santa Rosa",
      "url": "https://en.wikipedia.org/wiki/Atl\u00e9tico_Santa_Rosa"
    },
    {
      "id": "S298",
      "title": "Richmond Centre",
      "url": "https://en.wikipedia.org/wiki/Richmond_Centre"
    },
    {
      "id": "S299",
      "title": "Dehradun district",
      "url": "https://en.wikipedia.org/wiki/Dehradun_district"
    },
    {
      "id": "S300",
      "title": "Federal College of Education (Technical), Potiskum",
      "url": "https://en.wikipedia.org/wiki/Federal_College_of_Education_(Technical),_Potiskum"
    },
    {
      "id": "S301",
      "title": "Creating Masculinity in Los Angeles's Little Manila",
      "url": "https://en.wikipedia.org/wiki/Creating_Masculinity_in_Los_Angeles's_Little_Manila"
    },
    {
      "id": "S302",
      "title": "1990\u201391 Sussex County Football League",
      "url": "https://en.wikipedia.org/wiki/1990\u201391_Sussex_County_Football_League"
    },
    {
      "id": "S303",
      "title": "Siragusa",
      "url": "https://en.wikipedia.org/wiki/Siragusa"
    },
    {
      "id": "S304",
      "title": "Manzonia carboverdensis",
      "url": "https://en.wikipedia.org/wiki/Manzonia_carboverdensis"
    },
    {
      "id": "S305",
      "title": "Dasymutilla munifica",
      "url": "https://en.wikipedia.org/wiki/Dasymutilla_munifica"
    },
    {
      "id": "S306",
      "title": "Martin Motnik",
      "url": "https://en.wikipedia.org/wiki/Martin_Motnik"
    },
    {
      "id": "S307",
      "title": "Renovation Raiders",
      "url": "https://en.wikipedia.org/wiki/Renovation_Raiders"
    },
    {
      "id": "S308",
      "title": "Congo Crossing",
      "url": "https://en.wikipedia.org/wiki/Congo_Crossing"
    },
    {
      "id": "S309",
      "title": "Czech First League records and statistics",
      "url": "https://en.wikipedia.org/wiki/Czech_First_League_records_and_statistics"
    },
    {
      "id": "S310",
      "title": "1897\u201398 Burslem Port Vale F.C. season",
      "url": "https://en.wikipedia.org/wiki/1897\u201398_Burslem_Port_Vale_F.C._season"
    },
    {
      "id": "S311",
      "title": "Psycho Bitch",
      "url": "https://en.wikipedia.org/wiki/Psycho_Bitch"
    },
    {
      "id": "S312",
      "title": "Yakty-Yul",
      "url": "https://en.wikipedia.org/wiki/Yakty-Yul"
    },
    {
      "id": "S313",
      "title": "Pen-y-crug",
      "url": "https://en.wikipedia.org/wiki/Pen-y-crug"
    },
    {
      "id": "S314",
      "title": "Robert Lagomarsino",
      "url": "https://en.wikipedia.org/wiki/Robert_Lagomarsino"
    },
    {
      "id": "S315",
      "title": "Marum",
      "url": "https://en.wikipedia.org/wiki/Marum"
    },
    {
      "id": "S316",
      "title": "List of storms named Brenda",
      "url": "https://en.wikipedia.org/wiki/List_of_storms_named_Brenda"
    },
    {
      "id": "S317",
      "title": "Pavel Kovalev",
      "url": "https://en.wikipedia.org/wiki/Pavel_Kovalev"
    },
    {
      "id": "S318",
      "title": "Jaya Thyagarajan",
      "url": "https://en.wikipedia.org/wiki/Jaya_Thyagarajan"
    },
    {
      "id": "S319",
      "title": "Conospermum longifolium",
      "url": "https://en.wikipedia.org/wiki/Conospermum_longifolium"
    },
    {
      "id": "S320",
      "title": "Arielle Greenberg",
      "url": "https://en.wikipedia.org/wiki/Arielle_Greenberg"
    },
    {
      "id": "S321",
      "title": "List of Cantharis species",
      "url": "https://en.wikipedia.org/wiki/List_of_Cantharis_species"
    },
    {
      "id": "S322",
      "title": "Personz",
      "url": "https://en.wikipedia.org/wiki/Personz"
    },
    {
      "id": "S323",
      "title": "Tommy McLain",
      "url": "https://en.wikipedia.org/wiki/Tommy_McLain"
    },
    {
      "id": "S324",
      "title": "Sword of Damocles (song)",
      "url": "https://en.wikipedia.org/wiki/Sword_of_Damocles_(song)"
    },
    {
      "id": "S325",
      "title": "Anyinam",
      "url": "https://en.wikipedia.org/wiki/Anyinam"
    },
    {
      "id": "S326",
      "title": "Space Shower TV",
      "url": "https://en.wikipedia.org/wiki/Space_Shower_TV"
    },
    {
      "id": "S327",
      "title": "Bernic Lake",
      "url": "https://en.wikipedia.org/wiki/Bernic_Lake"
    },
    {
      "id": "S328",
      "title": "Ballinascarty",
      "url": "https://en.wikipedia.org/wiki/Ballinascarty"
    },
    {
      "id": "S329",
      "title": "Peter R. Huntsman",
      "url": "https://en.wikipedia.org/wiki/Peter_R._Huntsman"
    },
    {
      "id": "S330",
      "title": "Ivan More",
      "url": "https://en.wikipedia.org/wiki/Ivan_More"
    },
    {
      "id": "S331",
      "title": "James Kati",
      "url": "https://en.wikipedia.org/wiki/James_Kati"
    },
    {
      "id": "S332",
      "title": "HPGD",
      "url": "https://en.wikipedia.org/wiki/HPGD"
    },
    {
      "id": "S333",
      "title": "Boljani\u0107i",
      "url": "https://en.wikipedia.org/wiki/Boljani\u0107i"
    },
    {
      "id": "S334",
      "title": "Funeral",
      "url": "https://en.wikipedia.org/wiki/Funeral"
    },
    {
      "id": "S335",
      "title": "Claudia Reyes",
      "url": "https://en.wikipedia.org/wiki/Claudia_Reyes"
    },
    {
      "id": "S336",
      "title": "2023 Penn Quakers football team",
      "url": "https://en.wikipedia.org/wiki/2023_Penn_Quakers_football_team"
    },
    {
      "id": "S337",
      "title": "List of Rajya Sabha members from Odisha",
      "url": "https://en.wikipedia.org/wiki/List_of_Rajya_Sabha_members_from_Odisha"
    },
    {
      "id": "S338",
      "title": "List of awards and nominations received by Ian McKellen",
      "url": "https://en.wikipedia.org/wiki/List_of_awards_and_nominations_received_by_Ian_McKellen"
    },
    {
      "id": "S339",
      "title": "Ankit Mukherjee",
      "url": "https://en.wikipedia.org/wiki/Ankit_Mukherjee"
    },
    {
      "id": "S340",
      "title": "List of Italian football transfers winter 2007\u201308",
      "url": "https://en.wikipedia.org/wiki/List_of_Italian_football_transfers_winter_2007\u201308"
    },
    {
      "id": "S341",
      "title": "BREA2",
      "url": "https://en.wikipedia.org/wiki/BREA2"
    },
    {
      "id": "S342",
      "title": "Esther Summerson",
      "url": "https://en.wikipedia.org/wiki/Esther_Summerson"
    },
    {
      "id": "S343",
      "title": "James S. Warren",
      "url": "https://en.wikipedia.org/wiki/James_S._Warren"
    },
    {
      "id": "S344",
      "title": "D'Jalma Garnier",
      "url": "https://en.wikipedia.org/wiki/D'Jalma_Garnier"
    },
    {
      "id": "S345",
      "title": "Juan Smith",
      "url": "https://en.wikipedia.org/wiki/Juan_Smith"
    },
    {
      "id": "S346",
      "title": "Merv Cross",
      "url": "https://en.wikipedia.org/wiki/Merv_Cross"
    },
    {
      "id": "S347",
      "title": "Tamil Nadu Police Museum, Chennai",
      "url": "https://en.wikipedia.org/wiki/Tamil_Nadu_Police_Museum,_Chennai"
    },
    {
      "id": "S348",
      "title": "NA-68 Mandi Bahauddin-I",
      "url": "https://en.wikipedia.org/wiki/NA-68_Mandi_Bahauddin-I"
    },
    {
      "id": "S349",
      "title": "Jonathan Wordsworth",
      "url": "https://en.wikipedia.org/wiki/Jonathan_Wordsworth"
    },
    {
      "id": "S350",
      "title": "Georg M\u00fcller",
      "url": "https://en.wikipedia.org/wiki/Georg_M\u00fcller"
    },
    {
      "id": "S351",
      "title": "Gymnophthalmus vanzoi",
      "url": "https://en.wikipedia.org/wiki/Gymnophthalmus_vanzoi"
    },
    {
      "id": "S352",
      "title": "Bert Andr\u00e9",
      "url": "https://en.wikipedia.org/wiki/Bert_Andr\u00e9"
    },
    {
      "id": "S353",
      "title": "The Grey House, Highgate",
      "url": "https://en.wikipedia.org/wiki/The_Grey_House,_Highgate"
    },
    {
      "id": "S354",
      "title": "Pete Samples",
      "url": "https://en.wikipedia.org/wiki/Pete_Samples"
    },
    {
      "id": "S355",
      "title": "Bibirevo ethnographic village",
      "url": "https://en.wikipedia.org/wiki/Bibirevo_ethnographic_village"
    },
    {
      "id": "S356",
      "title": "Hunt House (Searcy, Arkansas)",
      "url": "https://en.wikipedia.org/wiki/Hunt_House_(Searcy,_Arkansas)"
    },
    {
      "id": "S357",
      "title": "Highwater Books",
      "url": "https://en.wikipedia.org/wiki/Highwater_Books"
    },
    {
      "id": "S358",
      "title": "Adiamante",
      "url": "https://en.wikipedia.org/wiki/Adiamante"
    },
    {
      "id": "S359",
      "title": "Ka\u010dinskas",
      "url": "https://en.wikipedia.org/wiki/Ka\u010dinskas"
    },
    {
      "id": "S360",
      "title": "1997 Molde FK season",
      "url": "https://en.wikipedia.org/wiki/1997_Molde_FK_season"
    },
    {
      "id": "S361",
      "title": "Ayva tatl\u0131s\u0131",
      "url": "https://en.wikipedia.org/wiki/Ayva_tatl\u0131s\u0131"
    },
    {
      "id": "S362",
      "title": "1980 World's Strongest Man",
      "url": "https://en.wikipedia.org/wiki/1980_World's_Strongest_Man"
    },
    {
      "id": "S363",
      "title": "Pol Goffaux",
      "url": "https://en.wikipedia.org/wiki/Pol_Goffaux"
    },
    {
      "id": "S364",
      "title": "Niobium\u2013tin",
      "url": "https://en.wikipedia.org/wiki/Niobium\u2013tin"
    },
    {
      "id": "S365",
      "title": "Jack Kent",
      "url": "https://en.wikipedia.org/wiki/Jack_Kent"
    },
    {
      "id": "S366",
      "title": "Adithya Patabendige",
      "url": "https://en.wikipedia.org/wiki/Adithya_Patabendige"
    },
    {
      "id": "S367",
      "title": "B. Dolan",
      "url": "https://en.wikipedia.org/wiki/B._Dolan"
    },
    {
      "id": "S368",
      "title": "1992 Internationaux de Strasbourg",
      "url": "https://en.wikipedia.org/wiki/1992_Internationaux_de_Strasbourg"
    },
    {
      "id": "S369",
      "title": "Grete Jenny",
      "url": "https://en.wikipedia.org/wiki/Grete_Jenny"
    },
    {
      "id": "S370",
      "title": "Pterostylis stricta",
      "url": "https://en.wikipedia.org/wiki/Pterostylis_stricta"
    },
    {
      "id": "S371",
      "title": "May Morning on Magdalen Tower",
      "url": "https://en.wikipedia.org/wiki/May_Morning_on_Magdalen_Tower"
    },
    {
      "id": "S372",
      "title": "Laurie Marker",
      "url": "https://en.wikipedia.org/wiki/Laurie_Marker"
    },
    {
      "id": "S373",
      "title": "Madang District",
      "url": "https://en.wikipedia.org/wiki/Madang_District"
    },
    {
      "id": "S374",
      "title": "Western Desert cultural bloc",
      "url": "https://en.wikipedia.org/wiki/Western_Desert_cultural_bloc"
    },
    {
      "id": "S375",
      "title": "Alhambra Publishing",
      "url": "https://en.wikipedia.org/wiki/Alhambra_Publishing"
    },
    {
      "id": "S376",
      "title": "Money.Net",
      "url": "https://en.wikipedia.org/wiki/Money.Net"
    },
    {
      "id": "S377",
      "title": "Porioides",
      "url": "https://en.wikipedia.org/wiki/Porioides"
    },
    {
      "id": "S378",
      "title": "Carolyn J. Stefanco",
      "url": "https://en.wikipedia.org/wiki/Carolyn_J._Stefanco"
    },
    {
      "id": "S379",
      "title": "PocketStation",
      "url": "https://en.wikipedia.org/wiki/PocketStation"
    },
    {
      "id": "S380",
      "title": "2011 Rugby World Cup \u2013 repechage qualification",
      "url": "https://en.wikipedia.org/wiki/2011_Rugby_World_Cup_\u2013_repechage_qualification"
    },
    {
      "id": "S381",
      "title": "Vanda wightii",
      "url": "https://en.wikipedia.org/wiki/Vanda_wightii"
    },
    {
      "id": "S382",
      "title": "God Loves Ugly",
      "url": "https://en.wikipedia.org/wiki/God_Loves_Ugly"
    },
    {
      "id": "S383",
      "title": "Plazishte",
      "url": "https://en.wikipedia.org/wiki/Plazishte"
    },
    {
      "id": "S384",
      "title": "Compound of five great dodecahedra",
      "url": "https://en.wikipedia.org/wiki/Compound_of_five_great_dodecahedra"
    },
    {
      "id": "S385",
      "title": "North Ottawa Township, Grant County, Minnesota",
      "url": "https://en.wikipedia.org/wiki/North_Ottawa_Township,_Grant_County,_Minnesota"
    },
    {
      "id": "S386",
      "title": "Terry Tate (Australian footballer)",
      "url": "https://en.wikipedia.org/wiki/Terry_Tate_(Australian_footballer)"
    },
    {
      "id": "S387",
      "title": "Steven Gonz\u00e1lez",
      "url": "https://en.wikipedia.org/wiki/Steven_Gonz\u00e1lez"
    },
    {
      "id": "S388",
      "title": "RTV BK Telecom",
      "url": "https://en.wikipedia.org/wiki/RTV_BK_Telecom"
    },
    {
      "id": "S389",
      "title": "Ma\u00eetresse Fran\u00e7oise",
      "url": "https://en.wikipedia.org/wiki/Ma\u00eetresse_Fran\u00e7oise"
    },
    {
      "id": "S390",
      "title": "Artur Serobyan",
      "url": "https://en.wikipedia.org/wiki/Artur_Serobyan"
    },
    {
      "id": "S391",
      "title": "Hambuch",
      "url": "https://en.wikipedia.org/wiki/Hambuch"
    },
    {
      "id": "S392",
      "title": "Lewis Arnold",
      "url": "https://en.wikipedia.org/wiki/Lewis_Arnold"
    },
    {
      "id": "S393",
      "title": "Chhankhola",
      "url": "https://en.wikipedia.org/wiki/Chhankhola"
    },
    {
      "id": "S394",
      "title": "Ar\u0131kaya, \u00c7ameli",
      "url": "https://en.wikipedia.org/wiki/Ar\u0131kaya,_\u00c7ameli"
    },
    {
      "id": "S395",
      "title": "Ultimate Hits: Rock and Roll Never Forgets",
      "url": "https://en.wikipedia.org/wiki/Ultimate_Hits:_Rock_and_Roll_Never_Forgets"
    },
    {
      "id": "S396",
      "title": "Archibald Douglas, 1st Baron Douglas",
      "url": "https://en.wikipedia.org/wiki/Archibald_Douglas,_1st_Baron_Douglas"
    },
    {
      "id": "S397",
      "title": "The Frond Files",
      "url": "https://en.wikipedia.org/wiki/The_Frond_Files"
    },
    {
      "id": "S398",
      "title": "Walchensee Forever",
      "url": "https://en.wikipedia.org/wiki/Walchensee_Forever"
    },
    {
      "id": "S399",
      "title": "Zdzis\u0142aw Kasprzak",
      "url": "https://en.wikipedia.org/wiki/Zdzis\u0142aw_Kasprzak"
    },
    {
      "id": "S400",
      "title": "Cruising",
      "url": "https://en.wikipedia.org/wiki/Cruising"
    },
    {
      "id": "S401",
      "title": "Migi Mo Hidari Mo Shihai Suru Atama Wa Kyou Mo Niku O Kui Yodare",
      "url": "https://en.wikipedia.org/wiki/Migi_Mo_Hidari_Mo_Shihai_Suru_Atama_Wa_Kyou_Mo_Niku_O_Kui_Yodare"
    },
    {
      "id": "S402",
      "title": "List of NJ Transit railroad stations",
      "url": "https://en.wikipedia.org/wiki/List_of_NJ_Transit_railroad_stations"
    },
    {
      "id": "S403",
      "title": "Red House (Guyana)",
      "url": "https://en.wikipedia.org/wiki/Red_House_(Guyana)"
    },
    {
      "id": "S404",
      "title": "Dartmouth Jack-O-Lantern",
      "url": "https://en.wikipedia.org/wiki/Dartmouth_Jack-O-Lantern"
    },
    {
      "id": "S405",
      "title": "AutoPulse",
      "url": "https://en.wikipedia.org/wiki/AutoPulse"
    },
    {
      "id": "S406",
      "title": "TV TEM Bauru",
      "url": "https://en.wikipedia.org/wiki/TV_TEM_Bauru"
    },
    {
      "id": "S407",
      "title": "2005\u201306 Greek Basket League",
      "url": "https://en.wikipedia.org/wiki/2005\u201306_Greek_Basket_League"
    },
    {
      "id": "S408",
      "title": "List of tunnels documented by the Historic American Engineering Record in Utah",
      "url": "https://en.wikipedia.org/wiki/List_of_tunnels_documented_by_the_Historic_American_Engineering_Record_in_Utah"
    },
    {
      "id": "S409",
      "title": "Dilettante",
      "url": "https://en.wikipedia.org/wiki/Dilettante"
    },
    {
      "id": "S410",
      "title": "National Aboriginal Education Committee",
      "url": "https://en.wikipedia.org/wiki/National_Aboriginal_Education_Committee"
    },
    {
      "id": "S411",
      "title": "Solar power in Thailand",
      "url": "https://en.wikipedia.org/wiki/Solar_power_in_Thailand"
    },
    {
      "id": "S412",
      "title": "Pediasia ematheudellus",
      "url": "https://en.wikipedia.org/wiki/Pediasia_ematheudellus"
    },
    {
      "id": "S413",
      "title": "Sports in Boston",
      "url": "https://en.wikipedia.org/wiki/Sports_in_Boston"
    },
    {
      "id": "S414",
      "title": "National Organization for Men Against Sexism",
      "url": "https://en.wikipedia.org/wiki/National_Organization_for_Men_Against_Sexism"
    },
    {
      "id": "S415",
      "title": "Salbia tremulalis",
      "url": "https://en.wikipedia.org/wiki/Salbia_tremulalis"
    },
    {
      "id": "S416",
      "title": "Chelsea F.C.\u2013Liverpool F.C. rivalry",
      "url": "https://en.wikipedia.org/wiki/Chelsea_F.C.\u2013Liverpool_F.C._rivalry"
    },
    {
      "id": "S417",
      "title": "Rose W\u00e9l\u00e9pa",
      "url": "https://en.wikipedia.org/wiki/Rose_W\u00e9l\u00e9pa"
    },
    {
      "id": "S418",
      "title": "Ang Soon Tong",
      "url": "https://en.wikipedia.org/wiki/Ang_Soon_Tong"
    },
    {
      "id": "S419",
      "title": "Smart.fm",
      "url": "https://en.wikipedia.org/wiki/Smart.fm"
    },
    {
      "id": "S420",
      "title": "Yazdagird (Bavandid ruler)",
      "url": "https://en.wikipedia.org/wiki/Yazdagird_(Bavandid_ruler)"
    },
    {
      "id": "S421",
      "title": "L'Enfant (film)",
      "url": "https://en.wikipedia.org/wiki/L'Enfant_(film)"
    },
    {
      "id": "S422",
      "title": "Mayiik Ayii Deng",
      "url": "https://en.wikipedia.org/wiki/Mayiik_Ayii_Deng"
    },
    {
      "id": "S423",
      "title": "Coney Reyes",
      "url": "https://en.wikipedia.org/wiki/Coney_Reyes"
    },
    {
      "id": "S424",
      "title": "B\u0142otnica, Greater Poland Voivodeship",
      "url": "https://en.wikipedia.org/wiki/B\u0142otnica,_Greater_Poland_Voivodeship"
    },
    {
      "id": "S425",
      "title": "Borgo Santa Maria Immacolata",
      "url": "https://en.wikipedia.org/wiki/Borgo_Santa_Maria_Immacolata"
    },
    {
      "id": "S426",
      "title": "Th-resa Bostick",
      "url": "https://en.wikipedia.org/wiki/Th-resa_Bostick"
    },
    {
      "id": "S427",
      "title": "The Big Ballad Jamboree",
      "url": "https://en.wikipedia.org/wiki/The_Big_Ballad_Jamboree"
    },
    {
      "id": "S428",
      "title": "National HIV Testing Day",
      "url": "https://en.wikipedia.org/wiki/National_HIV_Testing_Day"
    },
    {
      "id": "S429",
      "title": "Gamicon",
      "url": "https://en.wikipedia.org/wiki/Gamicon"
    },
    {
      "id": "S430",
      "title": "M\u00e1rcio Abreu",
      "url": "https://en.wikipedia.org/wiki/M\u00e1rcio_Abreu"
    },
    {
      "id": "S431",
      "title": "Johan Peder Basberg",
      "url": "https://en.wikipedia.org/wiki/Johan_Peder_Basberg"
    },
    {
      "id": "S432",
      "title": "Grace King",
      "url": "https://en.wikipedia.org/wiki/Grace_King"
    },
    {
      "id": "S433",
      "title": "Mr & Mrs T",
      "url": "https://en.wikipedia.org/wiki/Mr_&_Mrs_T"
    },
    {
      "id": "S434",
      "title": "Sicus abdominalis",
      "url": "https://en.wikipedia.org/wiki/Sicus_abdominalis"
    },
    {
      "id": "S435",
      "title": "List of census-designated places in West Virginia",
      "url": "https://en.wikipedia.org/wiki/List_of_census-designated_places_in_West_Virginia"
    },
    {
      "id": "S436",
      "title": "Colour My World (Chicago song)",
      "url": "https://en.wikipedia.org/wiki/Colour_My_World_(Chicago_song)"
    },
    {
      "id": "S437",
      "title": "Johnius carutta",
      "url": "https://en.wikipedia.org/wiki/Johnius_carutta"
    },
    {
      "id": "S438",
      "title": "List of county roads in Cook County, Minnesota",
      "url": "https://en.wikipedia.org/wiki/List_of_county_roads_in_Cook_County,_Minnesota"
    },
    {
      "id": "S439",
      "title": "Talladega County, Alabama",
      "url": "https://en.wikipedia.org/wiki/Talladega_County,_Alabama"
    },
    {
      "id": "S440",
      "title": "Ricardo Izar",
      "url": "https://en.wikipedia.org/wiki/Ricardo_Izar"
    },
    {
      "id": "S441",
      "title": "Hard Landing (novel)",
      "url": "https://en.wikipedia.org/wiki/Hard_Landing_(novel)"
    },
    {
      "id": "S442",
      "title": "2007 in men's road cycling",
      "url": "https://en.wikipedia.org/wiki/2007_in_men's_road_cycling"
    },
    {
      "id": "S443",
      "title": "Independent Network News (news agency)",
      "url": "https://en.wikipedia.org/wiki/Independent_Network_News_(news_agency)"
    },
    {
      "id": "S444",
      "title": "Barbara Walters",
      "url": "https://en.wikipedia.org/wiki/Barbara_Walters"
    },
    {
      "id": "S445",
      "title": "PrestonPlayz",
      "url": "https://en.wikipedia.org/wiki/PrestonPlayz"
    },
    {
      "id": "S446",
      "title": "Braddock Road (Braddock expedition)",
      "url": "https://en.wikipedia.org/wiki/Braddock_Road_(Braddock_expedition)"
    },
    {
      "id": "S447",
      "title": "La Pintada, Antioquia",
      "url": "https://en.wikipedia.org/wiki/La_Pintada,_Antioquia"
    },
    {
      "id": "S448",
      "title": "Rangawali Dam",
      "url": "https://en.wikipedia.org/wiki/Rangawali_Dam"
    },
    {
      "id": "S449",
      "title": "1984\u201385 FIRA Trophy",
      "url": "https://en.wikipedia.org/wiki/1984\u201385_FIRA_Trophy"
    },
    {
      "id": "S450",
      "title": "Vasishta N. Simha",
      "url": "https://en.wikipedia.org/wiki/Vasishta_N._Simha"
    },
    {
      "id": "S451",
      "title": "Ukraine in the Eurovision Song Contest 2017",
      "url": "https://en.wikipedia.org/wiki/Ukraine_in_the_Eurovision_Song_Contest_2017"
    },
    {
      "id": "S452",
      "title": "Kevin DeYoung",
      "url": "https://en.wikipedia.org/wiki/Kevin_DeYoung"
    },
    {
      "id": "S453",
      "title": "Suthirat Wongtewan",
      "url": "https://en.wikipedia.org/wiki/Suthirat_Wongtewan"
    },
    {
      "id": "S454",
      "title": "North Australia Railway",
      "url": "https://en.wikipedia.org/wiki/North_Australia_Railway"
    },
    {
      "id": "S455",
      "title": "Dendermonde (Chamber of Representatives constituency)",
      "url": "https://en.wikipedia.org/wiki/Dendermonde_(Chamber_of_Representatives_constituency)"
    },
    {
      "id": "S456",
      "title": "List of Marathi films of 1924",
      "url": "https://en.wikipedia.org/wiki/List_of_Marathi_films_of_1924"
    },
    {
      "id": "S457",
      "title": "CalSky",
      "url": "https://en.wikipedia.org/wiki/CalSky"
    },
    {
      "id": "S458",
      "title": "Edmund Knyvet",
      "url": "https://en.wikipedia.org/wiki/Edmund_Knyvet"
    },
    {
      "id": "S459",
      "title": "Castledawson railway station",
      "url": "https://en.wikipedia.org/wiki/Castledawson_railway_station"
    },
    {
      "id": "S460",
      "title": "Antennoseius janus",
      "url": "https://en.wikipedia.org/wiki/Antennoseius_janus"
    },
    {
      "id": "S461",
      "title": "Out of autoclave composite manufacturing",
      "url": "https://en.wikipedia.org/wiki/Out_of_autoclave_composite_manufacturing"
    },
    {
      "id": "S462",
      "title": "Aeonium sedifolium",
      "url": "https://en.wikipedia.org/wiki/Aeonium_sedifolium"
    },
    {
      "id": "S463",
      "title": "Parabothria",
      "url": "https://en.wikipedia.org/wiki/Parabothria"
    },
    {
      "id": "S464",
      "title": "Herman Fink",
      "url": "https://en.wikipedia.org/wiki/Herman_Fink"
    },
    {
      "id": "S465",
      "title": "Az Zibar",
      "url": "https://en.wikipedia.org/wiki/Az_Zibar"
    },
    {
      "id": "S466",
      "title": "2024\u201325 St Mirren F.C. season",
      "url": "https://en.wikipedia.org/wiki/2024\u201325_St_Mirren_F.C._season"
    },
    {
      "id": "S467",
      "title": "Beersheba Theater",
      "url": "https://en.wikipedia.org/wiki/Beersheba_Theater"
    },
    {
      "id": "S468",
      "title": "List of Washington Journal programs aired in January 1995",
      "url": "https://en.wikipedia.org/wiki/List_of_Washington_Journal_programs_aired_in_January_1995"
    },
    {
      "id": "S469",
      "title": "Accessible British Columbia Act",
      "url": "https://en.wikipedia.org/wiki/Accessible_British_Columbia_Act"
    },
    {
      "id": "S470",
      "title": "Tan Mui Buay",
      "url": "https://en.wikipedia.org/wiki/Tan_Mui_Buay"
    },
    {
      "id": "S471",
      "title": "Fysh",
      "url": "https://en.wikipedia.org/wiki/Fysh"
    },
    {
      "id": "S472",
      "title": "Novelda",
      "url": "https://en.wikipedia.org/wiki/Novelda"
    },
    {
      "id": "S473",
      "title": "Terai Madhes Sadbhavana Party",
      "url": "https://en.wikipedia.org/wiki/Terai_Madhes_Sadbhavana_Party"
    },
    {
      "id": "S474",
      "title": "Shane Black",
      "url": "https://en.wikipedia.org/wiki/Shane_Black"
    },
    {
      "id": "S475",
      "title": "Zhang Lu (goalkeeper)",
      "url": "https://en.wikipedia.org/wiki/Zhang_Lu_(goalkeeper)"
    },
    {
      "id": "S476",
      "title": "Viola Garfield",
      "url": "https://en.wikipedia.org/wiki/Viola_Garfield"
    },
    {
      "id": "S477",
      "title": "Theodore Y. Wu",
      "url": "https://en.wikipedia.org/wiki/Theodore_Y._Wu"
    },
    {
      "id": "S478",
      "title": "Robert Bernard Hass",
      "url": "https://en.wikipedia.org/wiki/Robert_Bernard_Hass"
    },
    {
      "id": "S479",
      "title": "Graffenrieda grandifolia",
      "url": "https://en.wikipedia.org/wiki/Graffenrieda_grandifolia"
    },
    {
      "id": "S480",
      "title": "Jack Keeney",
      "url": "https://en.wikipedia.org/wiki/Jack_Keeney"
    },
    {
      "id": "S481",
      "title": "Aloysius Sudarso",
      "url": "https://en.wikipedia.org/wiki/Aloysius_Sudarso"
    },
    {
      "id": "S482",
      "title": "Justinus",
      "url": "https://en.wikipedia.org/wiki/Justinus"
    },
    {
      "id": "S483",
      "title": "Allied Aviation XLRA",
      "url": "https://en.wikipedia.org/wiki/Allied_Aviation_XLRA"
    },
    {
      "id": "S484",
      "title": "Mailchimp",
      "url": "https://en.wikipedia.org/wiki/Mailchimp"
    },
    {
      "id": "S485",
      "title": "HUB Uitgevers",
      "url": "https://en.wikipedia.org/wiki/HUB_Uitgevers"
    },
    {
      "id": "S486",
      "title": "Tonk district",
      "url": "https://en.wikipedia.org/wiki/Tonk_district"
    },
    {
      "id": "S487",
      "title": "Na-Young Jeon",
      "url": "https://en.wikipedia.org/wiki/Na-Young_Jeon"
    },
    {
      "id": "S488",
      "title": "Rudolf the Black Cat",
      "url": "https://en.wikipedia.org/wiki/Rudolf_the_Black_Cat"
    },
    {
      "id": "S489",
      "title": "2022 California Golden Bears baseball team",
      "url": "https://en.wikipedia.org/wiki/2022_California_Golden_Bears_baseball_team"
    },
    {
      "id": "S490",
      "title": "Gbapleu",
      "url": "https://en.wikipedia.org/wiki/Gbapleu"
    },
    {
      "id": "S491",
      "title": "Frank Chermak",
      "url": "https://en.wikipedia.org/wiki/Frank_Chermak"
    },
    {
      "id": "S492",
      "title": "Dactyl Joust",
      "url": "https://en.wikipedia.org/wiki/Dactyl_Joust"
    },
    {
      "id": "S493",
      "title": "Wason selection task",
      "url": "https://en.wikipedia.org/wiki/Wason_selection_task"
    },
    {
      "id": "S494",
      "title": "Vidmantas",
      "url": "https://en.wikipedia.org/wiki/Vidmantas"
    },
    {
      "id": "S495",
      "title": "American Institute for Foreign Study",
      "url": "https://en.wikipedia.org/wiki/American_Institute_for_Foreign_Study"
    },
    {
      "id": "S496",
      "title": "Silmi-mossi",
      "url": "https://en.wikipedia.org/wiki/Silmi-mossi"
    },
    {
      "id": "S497",
      "title": "Steve Robinson (executive)",
      "url": "https://en.wikipedia.org/wiki/Steve_Robinson_(executive)"
    },
    {
      "id": "S498",
      "title": "Pelmet",
      "url": "https://en.wikipedia.org/wiki/Pelmet"
    },
    {
      "id": "S499",
      "title": "South Bolaang Mongondow Regency",
      "url": "https://en.wikipedia.org/wiki/South_Bolaang_Mongondow_Regency"
    },
    {
      "id": "S500",
      "title": "2014 CONCACAF Awards",
      "url": "https://en.wikipedia.org/wiki/2014_CONCACAF_Awards"
    }
  ],
  "questions": [
    {
      "question": "What is Developmental psychology?",
      "ground_truth": "Developmental psychology is the scientific study of how and why humans grow, change, and adapt across the course of their lives. Originally concerned with infants and children, the field has expanded to include adolescence, adult development, aging, and the entire lifespan.[1] Developmental psychologists aim to explain how thinking, feeling, and behaviors change throughout life. This field examines change[2] across three major dimensions, which are physical development, cognitive development, and social emotional development.[3][4] Within these three dimensions are a broad range of topics including motor skills, executive functions, moral understanding, language acquisition, social change, personality, emotional development, self-concept, and identity formation. Developmental psychology explores the influence of both nature and nurture on human development, as well as the processes of change that occur across different contexts over time. Many researchers are interested in the interactions among personal characteristics, the individual's behavior, and environmental factors, including the social context and the built environment. Ongoing debates in regards to developmental psychology include biological essentialism vs. neuroplasticity, and stages of development vs. dynamic systems of development. While research in developmental psychology has certain limitations, ongoing studies aim to understand how life stage transitions and biological factors influence human behavior and development.[5] Developmental psychology involves a range of fields,[2] such as educational psychology, child psychopathology, forensic developmental psychology, child development, cognitive psychology, ecological psychology, and cultural psychology. Influential developmental psychologists from the 20th century include Urie Bronfenbrenner, Erik Erikson, Sigmund Freud, Anna Freud, Jean Piaget, Barbara Rogoff, Esther Thelen, and Lev Vygotsky.[6] Jean-Jacques Rousseau and John B. Watson are typically cited as providing the foundation for modern developmental psychology.[7] In the mid-18th century, Jean Jacques Rousseau described three stages of development: infants (infancy), puer (childhood) and adolescence in Emile: Or, On Education. Rousseau's ideas were adopted and supported by educators at the time. Developmental psychology",
      "expected_answer": "Developmental psychology is the scientific study of how and why humans grow, change, and adapt across the course of their lives. Originally concerned with infants and children, the field has expanded to include adolescence, adult development, aging, and the entire lifespan.[1] Developmental psychologists aim to explain how thinking, feeling, and behaviors change throughout life. This field examines change[2] across three major dimensions, which are physical development, cognitive development, and social emotional development.[3][4] Within these three dimensions are a broad range of topics including motor skills, executive functions, moral understanding, language acquisition, social change, personality, emotional development, self-concept, and identity formation. Developmental psychology explores the influence of both nature and nurture on human development, as well as the processes of change that occur across different contexts over time. Many researchers are interested in the interactions among personal characteristics, the individual's behavior, and environmental factors, including the social context and the built environment. Ongoing debates in regards to developmental psychology include biological essentialism vs. neuroplasticity, and stages of development vs. dynamic systems of development. While research in developmental psychology has certain limitations, ongoing studies aim to understand how life stage transitions and biological factors influence human behavior and development.[5] Developmental psychology involves a range of fields,[2] such as educational psychology, child psychopathology, forensic developmental psychology, child development, cognitive psychology, ecological psychology, and cultural psychology. Influential developmental psychologists from the 20th century include Urie Bronfenbrenner, Erik Erikson, Sigmund Freud, Anna Freud, Jean Piaget, Barbara Rogoff, Esther Thelen, and Lev Vygotsky.[6] Jean-Jacques Rousseau and John B. Watson are typically cited as providing the foundation for modern developmental psychology.[7] In the mid-18th century, Jean Jacques Rousseau described three stages of development: infants (infancy), puer (childhood) and adolescence in Emile: Or, On Education. Rousseau's ideas were adopted and supported by educators at the time. Developmental psychology generally focuses on how and why certain changes (cognitive, social, intellectual, personality) occur over time in the course of a human life. Many theorists have made a profound contribution to this area of psychology. One of them is the psychologist Erik Erikson,[8] who created a model of eight phases of psychosocial development.[8] According to his theory, people go through different phases in their lives, each of which has its own developmental crisis that shapes a person's personality and behavior.[9] In the late 19th century, psychologists familiar with the evolutionary theory of Darwin began seeking an evolutionary description of psychological development;[7] prominent here was the pioneering psychologist G. Stanley Hall,[7] who attempted to correlate ages of childhood with previous ages of humanity. James Mark Baldwin, who wrote essays on topics that included Imitation: A Chapter in the Natural History of Consciousness and Mental Development in the Child and the Race: Methods and Processes, was significantly involved in the theory of developmental psychology.[7] Sigmund Freud, whose concepts were developmental, significantly affected public perceptions.[7] Sigmund Freud developed a theory that suggested that humans behave as they do because they are constantly seeking pleasure. This process of seeking pleasure changes through stages because people evolve. Each period of seeking pleasure that a person experiences is represented by a stage of psychosexual development. These stages symbolize the process of arriving at becoming a maturing adult.[10] The first is the oral stage, which begins at birth and ends around a year and a half of age. During the oral stage, the child finds pleasure in behaviors like sucking or other behaviors with the mouth. The second is the anal stage, from about a year or a year and a half to three years of age. During the anal stage, the child defecates from the anus and is often fascinated with its defecation. This period of development often occurs during the time when the child is being toilet-trained. The child becomes interested in feces and urine. Children begin to see themselves as independent from their parents. They begin to desire assertiveness and autonomy. The third is the phallic stage, which occurs from three to five years of age (most of a person's personality forms by this age). During the phallic stage, the child becomes aware of its sexual organs. Pleasure comes from finding acceptance and love from the opposite sex. The fourth is the latency stage, which occurs from age five until puberty. During the latency stage, the child's sexual interests are repressed. Stage five is the genital stage, which takes place from puberty until adulthood. During the genital stage, puberty begins to occur.[11] Children have now matured, and begin to think about other people instead of just themselves. Pleasure comes from feelings of affection from other people. Freud believed there is tension between the conscious and unconscious because the conscious tries to hold back what the unconscious tries to express. To explain this, he developed three personality structures: id, ego, and superego. The id, the most primitive of the three, functions according to the pleasure principle: seek pleasure and avoid pain.[12] The superego plays the critical and moralizing role, while the ego is the organized, realistic part that mediates between the desires of the id and the superego.[13] Jean Piaget, a Swiss theorist, posited that children learn by actively constructing knowledge through their interactions with their physical and social environments.[14] He suggested that the adult's role in helping the child learn was to provide appropriate materials. In his interview techniques with children that formed an empirical basis for his theories, he used something similar to Socratic questioning to get children to reveal their thinking. He argued that a principal source of development was through the child's inevitable generation of contradictions through their interactions with their physical and social worlds. The child's resolution of these contradictions led to more integrated and advanced forms of interaction, a developmental process that he called \"equilibration.\" Piaget argued that intellectual development takes place through a series of stages generated through the equilibration process. Each stage consists of steps the child must master before moving to the next step. He believed that these stages are not separate from one another, but rather that each stage builds on the previous one in a continuous learning process. He proposed four stages: sensorimotor, pre-operational, concrete operational, and formal operational. Though he did not believe these stages occurred at any given age, many studies have determined when these cognitive abilities should take place.[15] Piaget claimed that logic and morality develop through constructive stages.[16] Expanding on Piaget's work, Lawrence Kohlberg determined that the process of moral development was principally concerned with justice, and that it continued throughout the individual's lifetime.[17] He suggested three levels of moral reasoning: pre-conventional moral reasoning, conventional moral reasoning, and post-conventional moral reasoning. The pre-conventional moral reasoning is typical of children and is characterized by reasoning that is based on rewards and punishments associated with different courses of action. Conventional moral reasoning occurs during late childhood and early adolescence and is characterized by reasoning based on the rules and conventions of society. Lastly, post-conventional moral reasoning is a stage during which the individual sees society's rules and conventions as relative and subjective, rather than as authoritative.[18] Kohlberg used the Heinz Dilemma to apply to his stages of moral development. The Heinz Dilemma involves Heinz's wife dying from cancer and Heinz having the dilemma to save his wife by stealing a drug. Preconventional morality, conventional morality, and post-conventional morality applies to Heinz's situation.[19] German-American psychologist Erik Erikson and his collaborator and wife, Joan Erikson, posits eight stages of individual human development influenced by biological, psychological, and social factors throughout the lifespan.[8] At each stage the person must resolve a challenge, or an existential dilemma. Successful resolution of the dilemma results in the person ingraining a positive virtue, but failure to resolve the fundamental challenge of that stage reinforces negative perceptions of the person or the world around them and the person's personal development is unable to progress.[8] The first stage, \"Trust vs. Mistrust\", takes place in infancy. The positive virtue for the first stage is hope, in the infant learning whom to trust and having hope for a supportive group of people to be there for him/her. The second stage is \"Autonomy vs. Shame and Doubt\" with the positive virtue being will. This takes place in early childhood when the child learns to become more independent by discovering what they are capable of whereas if the child is overly controlled, feelings of inadequacy are reinforced, which can lead to low self-esteem and doubt. The third stage is \"Initiative vs. Guilt\". The virtue of being gained is a sense of purpose. This takes place primarily via play. This is the stage where the child will be curious and have many interactions with other kids. They will ask many questions as their curiosity grows. If too much guilt is present, the child may have a slower and harder time interacting with their world and other children in it. The fourth stage is \"Industry (competence) vs. Inferiority\". The virtue for this stage is competency and is the result of the child's early experiences in school. This stage is when the child will try to win the approval of others and understand the value of their accomplishments. The fifth stage is \"Identity vs. Role Confusion\". The virtue gained is fidelity and it takes place in adolescence. This is when the child ideally starts to identify their place in society, particularly in terms of their gender role. The sixth stage is \"Intimacy vs. Isolation\", which happens in young adults and the virtue gained is love. This is when the person starts to share his/her life with someone else intimately and emotionally. Not doing so can reinforce feelings of isolation. The seventh stage is \"Generativity vs. Stagnation\". This happens in adulthood and the virtue gained is care. A person becomes stable and starts to give back by raising a family and becoming involved in the community. The eighth stage is \"Ego Integrity vs. Despair\". When one grows old, they look back on their life and contemplate their successes and failures. If they resolve this positively, the virtue of wisdom is gained. This is also the stage when one can gain a sense of closure and accept death without regret or fear.[20] Michael Commons enhanced and simplified B\u00e4rbel Inhelder and Piaget's developmental theory and offers a standard method of examining the universal pattern of development. The Model of Hierarchical Complexity (MHC) is not based on the assessment of domain-specific information, It divides the Order of Hierarchical Complexity of tasks to be addressed from the Stage performance on those tasks. A stage is the order hierarchical complexity of the tasks the participant's successfully addresses. He expanded Piaget's original eight stage (counting the half stages) to seventeen stages. The stages are: The order of hierarchical complexity of tasks predicts how difficult the performance is with an R ranging from 0.9 to 0.98. In the MHC, there are three main axioms for an order to meet in order for the higher order task to coordinate the next lower order task. Axioms are rules that are followed to determine how the MHC orders actions to form a hierarchy. These axioms are: a) defined in terms of tasks at the next lower order of hierarchical complexity task action; b) defined as the higher order task action that organizes two or more less complex actions; that is, the more complex action specifies the way in which the less complex actions combine; c) defined as the lower order task actions have to be carried out non-arbitrarily.Commons, Michael L.; Gane-McCalla, Rebecca; Barker, Christopher D.; Li, Ellen Y. (2014). \"The model of hierarchical complexity as a measurement system\". Behavioral Development Bulletin. 19 (3). American Psychological Association: 9\u201368. doi:10.1037/h0100589. Ecological systems theory, originally formulated by Urie Bronfenbrenner, specifies four types of nested environmental systems, with bi-directional influences within and between the systems. The four systems are microsystem, mesosystem, exosystem, and macrosystem. Each system contains roles, norms and rules that can powerfully shape development. The microsystem is the direct environment in our lives such as our home and school. Mesosystem is how relationships connect to the microsystem. Exosystem is a larger social system where the child plays no role. Macrosystem refers to the cultural values, customs and laws of society.[21] The microsystem is the immediate environment surrounding and influencing the individual (example: school or the home setting). The mesosystem is the combination of two microsystems and how they influence each other (example: sibling relationships at home vs. peer relationships at school). The exosystem is the interaction among two or more settings that are indirectly linked (example: a father's job requiring more overtime ends up influencing his daughter's performance in school because he can no longer help with her homework). The macrosystem is broader taking into account social economic status, culture, beliefs, customs and morals (example: a child from a wealthier family sees a peer from a less wealthy family as inferior for that reason). Lastly, the chronosystem refers to the chronological nature of life events and how they interact and change the individual and their circumstances through transition (example: a mother losing her own mother to illness and no longer having that support in her life).[15] Since its publication in 1979, Bronfenbrenner's major statement of this theory, The Ecology of Human Development,[22] has had widespread influence on the way psychologists and others approach the study of human beings and their environments. As a result of this conceptualization of development, these environments\u2014from the family to economic and political structures\u2014have come to be viewed as part of the life course from childhood through to adulthood.[23] Lev Vygotsky was a Russian theorist from the Soviet era, who posited that children learn through hands-on experience and social interactions with members of their culture.[24] Vygotsky believed that a child's development should be examined during problem-solving activities.[25] Unlike Piaget, he claimed that timely and sensitive intervention by adults when a child is on the edge of learning a new task (called the \"zone of proximal development\") could help children learn new tasks. Zone of proximal development is a tool used to explain the learning of children and collaborating problem solving activities with an adult or peer.[25] This adult role is often referred to as the skilled \"master\", whereas the child is considered the learning apprentice through an educational process often termed \"cognitive apprenticeship\" Martin Hill stated that \"The world of reality does not apply to the mind of a child.\" This technique is called \"scaffolding\", because it builds upon knowledge children already have with new knowledge that adults can help the child learn.[26] Vygotsky was strongly focused on the role of culture in determining the child's pattern of development, arguing that development moves from the social level to the individual level.[26] In other words, Vygotsky claimed that psychology should focus on the progress of human consciousness through the relationship of an individual and their environment.[27] He felt that if scholars continued to disregard this connection, then this disregard would inhibit the full comprehension of the human consciousness.[27] Constructivism is a paradigm in psychology that characterizes learning as a process of actively constructing knowledge. Individuals create meaning for themselves or make sense of new information by selecting, organizing, and integrating information with other knowledge, often in the context of social interactions. Constructivism can occur in two ways: individual and social. Individual constructivism is when a person constructs knowledge through cognitive processes of their own experiences rather than by memorizing facts provided by others. Social constructivism is when individuals construct knowledge through an interaction between the knowledge they bring to a situation and social or cultural exchanges within that content.[15] A foundational concept of constructivism is that the purpose of cognition is to organize one's experiential world, instead of the ontological world around them.[28] Jean Piaget, a Swiss developmental psychologist, proposed that learning is an active process because children learn through experience and make mistakes and solve problems. Piaget proposed that learning should be whole by helping students understand that meaning is constructed.[29] Evolutionary developmental psychology is a research paradigm that applies the basic principles of Darwinian evolution, particularly natural selection, to understand the development of human behavior and cognition. It involves the study of both the genetic and environmental mechanisms that underlie the development of social and cognitive competencies, as well as the epigenetic (gene-environment interactions) processes that adapt these competencies to local conditions.[30] EDP considers both the reliably developing, species-typical features of ontogeny (developmental adaptations), as well as individual differences in behavior, from an evolutionary perspective. While evolutionary views tend to regard most individual differences as the result of either random genetic noise (evolutionary byproducts)[31] and/or idiosyncrasies (for example, peer groups, education, neighborhoods, and chance encounters)[32] rather than products of natural selection, EDP asserts that natural selection can favor the emergence of individual differences via \"adaptive developmental plasticity\".[30][33] From this perspective, human development follows alternative life-history strategies in response to environmental variability, rather than following one species-typical pattern of development.[30] EDP is closely linked to the theoretical framework of evolutionary psychology (EP), but is also distinct from EP in several domains, including research emphasis (EDP focuses on adaptations of ontogeny, as opposed to adaptations of adulthood) and consideration of proximate ontogenetic and environmental factors (i.e., how development happens) in addition to more ultimate factors (i.e., why development happens), which are the focus of mainstream evolutionary psychology.[34] Attachment theory, originally developed by John Bowlby, focuses on the importance of open, intimate, emotionally meaningful relationships.[35] Attachment is described as a biological system or powerful survival impulse that evolved to ensure the survival of the infant. A threatened or stressed child will move toward caregivers who create a sense of physical, emotional, and psychological safety for the individual. Attachment feeds on body contact and familiarity. Psychologist Harry Harlow's research with infant rhesus monkeys in the mid-20th century provided pivotal experimental support for attachment theory. His studies found that infant monkeys consistently preferred cloth surrogate mothers that provided comfort over wire ones that offered only food. These results demonstrated that emotional security and physical comfort are more critical to attachment than nourishment alone. Harlow's findings reinforced Bowlby's view that early caregiving relationships are biologically essential for healthy emotional development and social bonding later in life.[36] Later Mary Ainsworth developed the Strange Situation protocol and the concept of the secure base. This tool has been found to help understand attachment, such as the Strange Situation Test and the Adult Attachment Interview. Both of which help determine factors to certain attachment styles. The Strange Situation Test helps find \"disturbances in attachment\" and whether certain attributes are found to contribute to a certain attachment issue.[37] The Adult Attachment Interview is a tool that is similar to the Strange Situation Test but instead focuses attachment issues found in adults.[37] Both tests have helped many researchers gain more information on the risks and how to identify them.[37] Theorists have proposed four types of attachment styles:[38] secure, anxious-avoidant, anxious-resistant,[18] and disorganized.[38] Secure attachment is a healthy attachment between the infant and the caregiver. It is characterized by trust. Anxious-avoidant is an insecure attachment between an infant and a caregiver. This is characterized by the infant's indifference toward the caregiver. Anxious-resistant is an insecure attachment between the infant and the caregiver characterized by distress from the infant when separated and anger when reunited.[18] Disorganized is an attachment style without a consistent pattern of responses upon return of the parent.[38] It is possible to prevent a child's innate propensity to develop bonds. Some infants are kept in isolation or subjected to severe neglect or abuse, or they are raised without the stimulation and care of a regular caregiver. This deprivation may cause short-term consequences such as separation, rage, despair, and a brief lag in cerebral growth. Increased aggression, clinging behavior, alienation, psychosomatic illnesses, and an elevated risk of adult depression are among the long-term consequences.[39][page\u00a0needed][40][page\u00a0needed]\\ According to attachment theory, which is a psychological concept, people's capacity to develop healthy social and emotional ties later in life is greatly impacted by their early relationships with their primary caregivers, especially during infancy. This suggests that humans have an inbuilt need to develop strong bonds with caregivers in order to survive and be healthy. Childhood attachment styles can have an impact on how people behave in adult social situations, including romantic partnerships.[41] A significant concern of developmental psychology is the relationship between innateness and environmental influences on development. This is often referred to as \"nature and nurture\" or nativism versus empiricism. A nativist account of development would argue that the processes in question are innate, that is, they are specified by the organism's genes.[42] What makes a person who they are? Is it their environment or their genetics? This is the debate of nature vs nurture.[43] According to an empiricist viewpoint, those processes are learned through interaction with the environment. Today most developmental psychologists take a more holistic approach, emphasizing the interaction between genetic and environmental influences. One of the ways this relationship has been explored in recent years is through the emerging field of evolutionary developmental psychology. The dispute over innateness has been well represented in the field of language acquisition studies. A major question in this area is whether or not certain properties of human language are specified genetically or can be acquired through learning. The empiricist position on the issue of language acquisition suggests that the language input provides the necessary information required for learning the structure of language and that infants acquire language through a process of statistical learning. From this perspective, language can be acquired via general learning methods that also apply to other aspects of development, such as perceptual learning.[44] The nativist position argues that the input from language is too impoverished for infants and children to acquire the structure of language. Linguist Noam Chomsky asserts that, evidenced by the lack of sufficient information in the language input, there is a universal grammar that applies to all human languages and is pre-specified. This has led to the idea that there is a special cognitive module suited for learning language, often called the language acquisition device. Chomsky's critique of the behaviorist model of language acquisition is regarded by many as a key turning point in the decline in the prominence of the theory of behaviorism generally.[45] But Skinner's conception of \"Verbal Behavior\" has not died, perhaps in part because it has generated successful practical applications.[45] Maybe there could be \"strong interactions of both nature and nurture\".[46] Many researchers now emphasize that development results from a continuous, dynamic interaction between genetic predispositions and environmental influences. Rather than acting independently, nature and nurture are seen as intertwined forces, where genetic factors can shape sensitivity to environmental inputs, and environmental conditions can influence how genes are expressed across development.[47] One of the major discussions in developmental psychology includes whether development is discontinuous or continuous. Continuous development is quantifiable and quantitative, whereas discontinuous development is qualitative. Quantitative estimations of development can be measuring the stature of a child, and measuring their memory or consideration span. \"Particularly dramatic examples of qualitative changes are metamorphoses, such as the emergence of a caterpillar into a butterfly.\"[48] Those psychologists who bolster the continuous view of improvement propose that improvement includes slow and progressing changes all through the life span, with behavior within the prior stages of advancement giving the premise of abilities and capacities required for the other stages. \"To many, the concept of continuous, quantifiable measurement seems to be the essence of science\".[48] However, not all psychologists concur that advancement could be a continuous process. A few see advancement as a discontinuous process. They accept advancement includes unmistakable and partitioned stages with diverse sorts of behavior happening in each organization. This proposes that the development of certain capacities in each arrange, such as particular feelings or ways of considering, has a definite beginning and ending point. Nevertheless, there is no exact moment when a capacity suddenly appears or disappears. Although some sorts of considering, feeling or carrying on could seem to seem abruptly, it is more than likely that this has been developing gradually for some time.[49] Stage theories of development rest on the suspicion that development may be a discontinuous process including particular stages which are characterized by subjective contrasts in behavior. They moreover assume that the structure of the stages is not variable concurring to each person, in any case, the time of each arrangement may shift separately. Stage theories can be differentiated with ceaseless hypotheses, which set that development is an incremental process.[50] This issue involves the degree to which one becomes older renditions of their early experience or whether they develop into something different from who they were at an earlier point in development.[51] It considers the extent to which early experiences (especially infancy) or later experiences are the key determinants of a person's development. Stability is defined as the consistent ordering of individual differences with respect to some attribute.[52] Change is altering someone/something. Most human development lifespan developmentalists recognize that extreme positions are unwise. Therefore, the key to a comprehensive understanding of development at any stage requires the interaction of different factors and not only one.[53] Theory of mind is the ability to attribute mental states to ourselves and others.[54] It is a complex but vital process in which children begin to understand the emotions, motives, and feelings of not only themselves but also others. Theory of mind allows individuals to understand that others have unique beliefs and desires different from their own. This ability enables successful social interactions by recognizing and interpreting the mental states of others. If a child does not fully develop theory of mind within this crucial 5-year period, they can suffer from communication barriers that follow them into adolescence and adulthood.[55] Exposure to more people and the availability of stimuli that encourages social-cognitive growth is a factor that relies heavily on family.[56] Developmental psychology is concerned not only with describing the characteristics of psychological change over time but also seeks to explain the principles and internal workings underlying these changes. Psychologists have attempted to better understand these factors by using models. A model must simply account for the means by which a process takes place. This is sometimes done in reference to changes in the brain that may correspond to changes in behavior over the course of the development. Mathematical modeling is useful in developmental psychology for implementing theory in a precise and easy-to-study manner, allowing generation, explanation, integration, and prediction of diverse phenomena. Several modeling techniques are applied to development: symbolic, connectionist (neural network), or dynamical systems models. Dynamic systems models illustrate how many different features of a complex system may interact to yield emergent behaviors and abilities. Nonlinear dynamics has been applied to human systems specifically to address issues that require attention to temporality such as life transitions, human development, and behavioral or emotional change over time. Nonlinear dynamic systems is currently being explored as a way to explain discrete phenomena of human development such as affect,[57] second language acquisition,[58] and locomotion.[59] One critical aspect of developmental psychology is the study of neural development, which investigates how the brain changes and develops during different stages of life. Neural development focuses on how the brain changes and develops during different stages of life. Studies have shown that the human brain undergoes rapid changes during prenatal and early postnatal periods. These changes include the formation of neurons, the development of neural networks, and the establishment of synaptic connections.[60] The formation of neurons and the establishment of basic neural circuits in the developing brain are crucial for laying the foundation of the brain's structure and function, and disruptions during this period can have long-term effects on cognitive and emotional development.[61] Experiences and environmental factors play a crucial role in shaping neural development. Early sensory experiences, such as exposure to language and visual stimuli, can influence the development of neural pathways related to perception and language processing.[62] Genetic factors play a huge roll in neural development. Genetic factors can influence the timing and pattern of neural development, as well as the susceptibility to certain developmental disorders, such as autism spectrum disorder and attention-deficit/hyperactivity disorder.[63] Research finds that the adolescent brain undergoes significant changes in neural connectivity and plasticity. During this period, there is a pruning process where certain neural connections are strengthened while others are eliminated, resulting in more efficient neural networks and increased cognitive abilities, such as decision-making and impulse control.[64] The study of neural development provides crucial insights into the complex interplay between genetics, environment, and experiences in shaping the developing brain. By understanding the neural processes underlying developmental changes, researchers gain a better understanding of cognitive, emotional, and social development in humans. Cognitive development is primarily concerned with how infants and children acquire, develop, and use internal mental capabilities such as: problem-solving, memory, and language. Major topics in cognitive development are the study of language acquisition and the development of perceptual and motor skills. Piaget was one of the influential early psychologists to study the development of cognitive abilities. His theory suggests that development proceeds through a set of stages from infancy to adulthood and that there is an end point or goal. Other accounts, such as that of Lev Vygotsky, have suggested that development does not progress through stages, but rather that the developmental process that begins at birth and continues until death is too complex for such structure and finality. Rather, from this viewpoint, developmental processes proceed more continuously. Thus, development should be analyzed, instead of treated as a product to obtain. K. Warner Schaie has expanded the study of cognitive development into adulthood. Rather than being stable from adolescence, Schaie sees adults as progressing in the application of their cognitive abilities.[65] Modern cognitive development has integrated the considerations of cognitive psychology and the psychology of individual differences into the interpretation and modeling of development.[66] Specifically, the neo-Piagetian theories of cognitive development showed that the successive levels or stages of cognitive development are associated with increasing processing efficiency and working memory capacity. These increases explain differences between stages, progression to higher stages, and individual differences of children who are the same-age and of the same grade-level. However, other theories have moved away from Piagetian stage theories, and are influenced by accounts of domain-specific information processing, which posit that development is guided by innate evolutionarily-specified and content-specific information processing mechanisms. Developmental psychologists who are interested in social development examine how individuals develop social and emotional competencies. For example, they study how children form friendships, how they understand and deal with emotions, and how identity develops. Research in this area may involve study of the relationship between cognition or cognitive development and social behavior. Emotional regulation or ER refers to an individual's ability to modulate emotional responses across a variety of contexts. In young children, this modulation is in part controlled externally, by parents and other authority figures. As children develop, they take on more and more responsibility for their internal state. Studies have shown that the development of ER is affected by the emotional regulation children observe in parents and caretakers, the emotional climate in the home, and the reaction of parents and caretakers to the child's emotions.[67] Music also has an influence on stimulating and enhancing the senses of a child through self-expression.[68] A child's social and emotional development can be disrupted by motor coordination problems, evidenced by the environmental stress hypothesis. The environmental hypothesis explains how children with coordination problems and developmental coordination disorder are exposed to several psychosocial consequences which act as secondary stressors, leading to an increase in internalizing symptoms such as depression and anxiety.[69] Motor coordination problems affect fine and gross motor movement as well as perceptual-motor skills. Secondary stressors commonly identified include the tendency for children with poor motor skills to be less likely to participate in organized play with other children and more likely to feel socially isolated.[69] Social and emotional development focuses on five keys areas: Self-Awareness, Self Management, Social Awareness, Relationship Skills and Responsible Decision Making.[70] Physical development concerns the physical maturation of an individual's body until it reaches the adult stature. Although physical growth is a highly regular process, all children differ tremendously in the timing of their growth spurts.[71] Studies are being done to analyze how the differences in these timings affect and are related to other variables of developmental psychology such as information processing speed. Traditional measures of physical maturity using x-rays are less in practice nowadays, compared to simple measurements of body parts such as height, weight, head circumference, and arm span.[71] A few other studies and practices with physical developmental psychology are the phonological abilities of mature 5- to 11-year-olds, and the controversial hypotheses of left-handers being maturationally delayed compared to right-handers. A study by Eaton, Chipperfield, Ritchot, and Kostiuk in 1996 found in three different samples that there was no difference between right- and left-handers.[71] Researchers interested in memory development look at the way our memory develops from childhood and onward. According to fuzzy-trace theory, a theory of cognition originally proposed by Valerie F. Reyna and Charles Brainerd, people have two separate memory processes: verbatim and gist. These two traces begin to develop at different times as well as at a different pace. Children as young as four years old have verbatim memory, memory for surface information, which increases up to early adulthood, at which point it begins to decline. On the other hand, our capacity for gist memory, memory for semantic information, increases up to early adulthood, at which point it is consistent through old age. Furthermore, one's reliance on gist memory traces increases as one ages.[72] Neuroscientific research has contributed to understanding the biological mechanisms behind memory development. A study using diffusion MRI in children aged four to twelve found that greater maturity in white matter tracts, specifically the uncinate fasciculus and dorsal cingulum bundle, was associated with stronger episodic memory recall. These findings suggest that the structural development of white matter pathways plays a significant role in memory function during childhood.[73] Developmental psychology employs many of the research methods used in other areas of psychology. However, infants and children cannot be tested in the same ways as adults, so different methods are often used to study their development. Developmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation.[74] These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study.[75] Every developmental investigation can be characterized in terms of whether its underlying strategy involves the experimental, correlational, or case study approach.[76][77] The experimental method involves \"actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed;[77] the experimental design points to cause-and-effect relationships.[78] This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable.[77] The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables.[78] On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability.[78] The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention.[77][78] The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment;[78] however, the limitation is that it does not permit determination of cause-and-effect relationships among variables.[78] The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores.[78] Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.[77] Most developmental studies, regardless of whether they employ the experimental, correlational, or case study method, can also be constructed using research designs.[75] Research designs are logical frameworks used to make key comparisons within research studies such as: In a longitudinal study, a researcher observes many individuals born at or around the same time (a cohort) and carries out new observations as members of the cohort age. This method can be used to draw conclusions about which types of development are universal (or normative) and occur in most members of a cohort. As an example a longitudinal study of early literacy development examined in detail the early literacy experiences of one child in each of 30 families.[79] Researchers may also observe ways that development varies between individuals, and hypothesize about the causes of variation in their data. Longitudinal studies often require large amounts of time and funding, making them unfeasible in some situations. Also, because members of a cohort all experience historical events unique to their generation, apparently normative developmental trends may, in fact, be universal only to their cohort.[80] In a cross-sectional study, a researcher observes differences between individuals of different ages at the same time. This generally requires fewer resources than the longitudinal method, and because the individuals come from different cohorts, shared historical events are not so much of a confounding factor. By the same token, however, cross-sectional research may not be the most effective way to study differences between participants, as these differences may result not from their different ages but from their exposure to different historical events.[81] A third study design, the sequential design, combines both methodologies. Here, a researcher observes members of different birth cohorts at the same time, and then tracks all participants over time, charting changes in the groups. While much more resource-intensive, the format aids in a clearer distinction between what changes can be attributed to an individual or historical environment from those that are truly universal.[82] Because every method has some weaknesses, developmental psychologists rarely rely on one study or even one method to reach conclusions by finding consistent evidence from as many converging sources as possible.[77] Prenatal development is of interest to psychologists investigating the context of early psychological development. The whole prenatal development involves three main stages: germinal stage, embryonic stage and fetal stage. Germinal stage begins at conception until 2 weeks; embryonic stage means the development from 2 weeks to 8 weeks; fetal stage represents 9 weeks until birth of the baby.[83] The senses develop in the womb itself: a fetus can both see and hear by the second trimester (13 to 24 weeks of age). The sense of touch develops in the embryonic stage (5 to 8 weeks).[84] Most of the brain's billions of neurons also are developed by the second trimester.[85] Babies are hence born with some odor, taste and sound preferences, largely related to the mother's environment.[86] Some primitive reflexes too arise before birth and are still present in newborns. One hypothesis is that these reflexes are vestigial and have limited use in early human life. Piaget's theory of cognitive development suggested that some early reflexes are building blocks for infant sensorimotor development. For example, the tonic neck reflex may help development by bringing objects into the infant's field of view.[87] Other reflexes, such as the walking reflex, appear to be replaced by more sophisticated voluntary control later in infancy. This may be because the infant gains too much weight after birth to be strong enough to use the reflex, or because the reflex and subsequent development are functionally different.[88] It has also been suggested that some reflexes (for example the moro and walking reflexes) are predominantly adaptations to life in the womb with little connection to early infant development.[87] Primitive reflexes reappear in adults under certain conditions, such as neurological conditions like dementia or traumatic lesions. Ultrasounds have shown that infants are capable of a range of movements in the womb, many of which appear to be more than simple reflexes.[88] By the time they are born, infants can recognize and have a preference for their mother's voice suggesting some prenatal development of auditory perception.[88] Prenatal development and birth complications may also be connected to neurodevelopmental disorders, for example in schizophrenia. With the advent of cognitive neuroscience, embryology and the neuroscience of prenatal development is of increasing interest to developmental psychology research. Several environmental agents\u2014teratogens\u2014can cause damage during the prenatal period. These include prescription and nonprescription drugs, illegal drugs, tobacco, alcohol, environmental pollutants, infectious disease agents such as the rubella virus and the toxoplasmosis parasite, maternal malnutrition, maternal emotional stress, and Rh factor blood incompatibility between mother and child.[89] There are many statistics which prove the effects of the aforementioned substances. A leading example of this would be that at least 100,000 \"cocaine babies\" were born in the United States annually in the late 1980s. \"Cocaine babies\" are proven to have quite severe and lasting difficulties which persist throughout infancy and right throughout childhood. The drug also encourages behavioural problems in the affected children and defects of various vital organs.[90] From birth until the first year, children are referred to as infants. As they grow, children respond to their environment in unique ways.[91] Developmental psychologists vary widely in their assessment of infant psychology, and the influence the outside world has upon it. The majority of a newborn infant's time is spent sleeping.[92] At first, their sleep cycles are evenly spread throughout the day and night, but after a couple of months, infants generally become diurnal.[93] In human or rodent infants, there is always the observation of a diurnal cortisol rhythm, which is sometimes entrained with a maternal substance.[94] Nevertheless, the circadian rhythm starts to take shape, and a 24-hour rhythm is observed in just some few months after birth.[93][94] Infants can be seen to have six states, grouped into pairs: Infant perception is what a newborn can see, hear, smell, taste, and touch. These five features are considered as the \"five senses\".[97] Because of these different senses, infants respond to stimuli differently.[88] Babies are born with the ability to discriminate virtually all sounds of all human languages.[105] Infants of around six months can differentiate between phonemes in their own language, but not between similar phonemes in another language. Notably, infants are able to differentiate between various durations and sound levels and can easily differentiate all the languages they have encountered, hence easy for infants to understand a certain language compared to an adult.[106] At this stage infants also start to babble, whereby they start making vowel consonant sound as they try to understand the true meaning of language and copy whatever they are hearing in their surrounding producing their own phonemes. In various cultures, a distinct form of speech called \"babytalk\" is used when communicating with newborns and young children. This register consists of simplified terms for common topics such as family members, food, hygiene, and familiar animals. It also exhibits specific phonological patterns, such as substituting alveolar sounds with initial velar sounds, especially in languages like English. Furthermore, babytalk often involves morphological simplifications, such as regularizing verb conjugations (for instance, saying \"corned\" instead of \"cornered\" or \"goed\" instead of \"went\"). This language is typically taught to children and is perceived as their natural way of communication. Interestingly, in mythology and popular culture, certain characters, such as the \"Hausa trickster\" or the Warner Bros cartoon character \"Tweety Pie\", are portrayed as speaking in a babytalk-like manner.[107] Piaget suggested that an infant's perception and understanding of the world depended on their motor development, which was required for the infant to link visual, tactile and motor representations of objects.[108] The concept of object permanence refers to the knowledge that an object exists even when it is not directly perceived or visible; in other words, something is still there even if it is not visible. This is a crucial developmental milestone for infants, who learn that something is not necessarily lost forever just because it is hidden. When a child displays object permanence, they will look for a toy that is hidden, showing that they are aware that the item is still there even when it is covered by a blanket. Most babies start to exhibit symptoms of object permanence around the age of eight months. According to this theory, infants develop object permanence through touching and handling objects.[88] Piaget's sensorimotor stage comprised six sub-stages (see sensorimotor stages for more detail). In the early stages, development arises out of movements caused by primitive reflexes.[109] Discovery of new behaviors results from classical and operant conditioning, and the formation of habits.[109] From eight months the infant is able to uncover a hidden object but will persevere when the object is moved. Piaget concluded that infants lacked object permanence before 18 months when infants' before this age failed to look for an object where it had last been seen. Instead, infants continued to look for an object where it was first seen, committing the \"A-not-B error\". Some researchers have suggested that before the age of 8\u20139 months, infants' inability to understand object permanence extends to people, which explains why infants at this age do not cry when their mothers are gone (\"Out of sight, out of mind\"). In the 1980s and 1990s, researchers developed new methods of assessing infants' understanding of the world with far more precision and subtlety than Piaget was able to do in his time. Since then, many studies based on these methods suggest that young infants understand far more about the world than first thought. Based on recent findings, some researchers (such as Elizabeth Spelke and Renee Baillargeon) have proposed that an understanding of object permanence is not learned at all, but rather comprises part of the innate cognitive capacities of our species. According to Jean Piaget's developmental psychology, object permanence, or the awareness that objects exist even when they are no longer visible, was thought to emerge gradually between the ages of 8 and 12 months. However, experts such as Elizabeth Spelke and Renee Baillargeon have questioned this notion. They studied infants' comprehension of object permanence at a young age using novel experimental approaches such as violation-of-expectation paradigms. These findings imply that children as young as 3 to 4 months old may have an innate awareness of object permanence. Baillargeon's \"drawbridge\" experiment, for example, showed that infants were surprised when they saw occurrences that contradicted object permanence expectations. This proposition has important consequences for our understanding of infant cognition, implying that infants may be born with core cognitive abilities rather than developing them via experience and learning.[110] Other research has suggested that young infants in their first six months of life may possess an understanding of numerous aspects of the world around them, including: There are critical periods in infancy and childhood during which development of certain perceptual, sensorimotor, social and language systems depends crucially on environmental stimulation.[114] Feral children such as Genie, deprived of adequate stimulation, fail to acquire important skills and are unable to learn in later childhood. In this case, Genie is used to represent the case of a feral child because she was socially neglected and abused while she was just a young girl. She underwent abnormal child psychology which involved problems with her linguistics. This happened because she was neglected while she was very young with no one to care about her and had less human contact. The concept of critical periods is also well-established in neurophysiology, from the work of Hubel and Wiesel among others. Neurophysiology in infants generally provides correlating details that exists between neurophysiological details and clinical features and also focuses on vital information on rare and common neurological disorders that affect infants. Studies have been done to look at the differences in children who have developmental delays versus typical development. Normally when being compared to one another, mental age (MA) is not taken into consideration. There still may be differences in developmentally delayed (DD) children vs. typical development (TD) behavioral, emotional and other mental disorders. When compared to MA children there is a bigger difference between normal developmental behaviors overall. DDs can cause lower MA, so comparing DDs with TDs may not be as accurate. Pairing DDs specifically with TD children at similar MA can be more accurate. There are levels of behavioral differences that are considered as normal at certain ages. When evaluating DDs and MA in children, consider whether those with DDs have a larger amount of behavior that is not typical for their MA group. Developmental delays tend to contribute to other disorders or difficulties than their TD counterparts.[115] Infants shift between ages of one and two to a developmental stage known as toddlerhood. In this stage, an infant's transition into toddlerhood is highlighted through self-awareness, developing maturity in language use, and presence of memory and imagination. During toddlerhood, babies begin learning how to walk, talk, and make decisions for themselves. An important characteristic of this age period is the development of language, where children are learning how to communicate and express their emotions and desires through the use of vocal sounds, babbling, and eventually words.[116] Self-control also begins to develop. At this age, children take initiative to explore, experiment and learn from making mistakes. Caretakers who encourage toddlers to try new things and test their limits, help the child become autonomous, self-reliant, and confident.[117] If the caretaker is overprotective or disapproving of independent actions, the toddler may begin to doubt their abilities and feel ashamed of the desire for independence. The child's autonomic development is inhibited, leaving them less prepared to deal with the world in the future. Toddlers also begin to identify themselves in gender roles, acting according to their perception of what a man or woman should do.[118] Socially, the period of toddler-hood is commonly called the \"terrible twos\".[119] Toddlers often use their new-found language abilities to voice their desires, but are often misunderstood by parents due to their language skills just beginning to develop. A person at this stage testing their independence is another reason behind the stage's infamous label. Tantrums in a fit of frustration are also common. Erik Erikson divides childhood into four stages, each with its distinct social crisis:[120] As stated, the psychosocial crisis for Erikson is Trust versus Mistrust. Needs are the foundation for gaining or losing trust in the infant. If the needs are met, trust in the guardian and the world forms. If the needs are not met, or the infant is neglected, mistrust forms alongside feelings of anxiety and fear.[122] Autonomy versus shame follows trust in infancy. The child begins to explore their world in this stage and discovers preferences in what they like. If autonomy is allowed, the child grows in independence and their abilities. If freedom of exploration is hindered, it leads to feelings of shame and low self-esteem.[122] In the earliest years, children are \"completely dependent on the care of others\". Therefore, they develop a \"social relationship\" with their care givers and, later, with family members. During their preschool years (3\u20135), they \"enlarge their social horizons\" to include people outside the family.[123] Preoperational and then operational thinking develops, which means actions are reversible, and egocentric thought diminishes.[124] The motor skills of preschoolers increase so they can do more things for themselves. They become more independent. No longer completely dependent on the care of others, the world of this age group expands. More people have a role in shaping their individual personalities. Preschoolers explore and question their world.[125] For Jean Piaget, the child is \"a little scientist exploring and reflecting on these explorations to increase competence\" and this is done in \"a very independent way\".[126] Play is a major activity for ages 3\u20135. For Piaget, through play \"a child reaches higher levels of cognitive development.\"[127] In their expanded world, children in the 3\u20135 age group attempt to find their own way. If this is done in a socially acceptable way, the child develops the initiative. If not, the child develops guilt.[128] Children who develop \"guilt\" rather than \"initiative\" have failed Erikson's psychosocial crisis for the 3\u20135 age group. For Erik Erikson, the psychosocial crisis during middle childhood is Industry vs. Inferiority which, if successfully met, instills a sense of Competency in the child.[120] In all cultures, middle childhood is a time for developing \"skills that will be needed in their society.\"[129] School offers an arena in which children can gain a view of themselves as \"industrious (and worthy)\". They are \"graded for their school work and often for their industry\". They can also develop industry outside of school in sports, games, and doing volunteer work.[130] Children who achieve \"success in school or games might develop a feeling of competence.\" The \"peril during this period is that feelings of inadequacy and inferiority will develop.[129] Parents and teachers can \"undermine\" a child's development by failing to recognize accomplishments or being overly critical of a child's efforts.[130]\nChildren who are \"encouraged and praised\" develop a belief in their competence. Lack of encouragement or ability to excel lead to \"feelings of inadequacy and inferiority\".[131] The Centers for Disease Control (CDC) divides Middle Childhood into two stages, 6\u20138 years and 9\u201311 years, and gives \"developmental milestones for each stage\".[132][133] Entering elementary school, children in this age group begin to thinks about the future and their \"place in the world\". Working with other students and wanting their friendship and acceptance become more important. This leads to \"more independence from parents and family\". As students, they develop the mental and verbal skills \"to describe experiences and talk about thoughts and feelings\". They become less self-centered and show \"more concern for others\".[132] For children ages 9\u201311 \"friendships and peer relationships\" increase in strength, complexity, and importance. This results in greater \"peer pressure\". They grow even less dependent on their families and they are challenged academically. To meet this challenge, they increase their attention span and learn to see other points of view.[133] Adolescence is the period of life between the onset of puberty and the full commitment to an adult social role, such as worker, parent, and/or citizen. It is the period known for the formation of personal and social identity (see Erik Erikson) and the discovery of moral purpose (see William Damon). Intelligence is demonstrated through the logical use of symbols related to abstract concepts and formal reasoning. A return to egocentric thought often occurs early in the period. Only 35% develop the capacity to reason formally during adolescence or adulthood. (Huitt, W. and Hummel, J. January 1998)[134] Erik Erikson labels this stage identity versus role confusion. Erikson emphasizes the importance of developing a sense of identity in adolescence because it affects the individual throughout their life. Identity is a lifelong process and is related with curiosity and active engagement. Role confusion is often considered the current state of identity of the individual. Identity exploration is the process of changing from role confusion to resolution.[135] During Erik Erikson's identity versus role uncertainty stage, which occurs in adolescence, people struggle to form a cohesive sense of self while exploring many social roles and prospective life routes. This time is characterized by deep introspection, self-examination, and the pursuit of self-understanding. Adolescents are confronted with questions regarding their identity, beliefs, and future goals. The major problem is building a strong sense of identity in the face of society standards, peer pressure, and personal preferences. Adolescents participate in identity exploration, commitment, and synthesis, actively seeking out new experiences, embracing ideals and aspirations, and merging their changing sense of self into a coherent identity. Successfully navigating this stage builds the groundwork for good psychological development in adulthood, allowing people to pursue meaningful relationships, make positive contributions to society, and handle life's adversities with perseverance and purpose.[9] It is divided into three parts, namely: The adolescent unconsciously explores questions such as \"Who am I? Who do I want to be?\" Like toddlers, adolescents must explore, test limits, become autonomous, and commit to an identity, or sense of self. Different roles, behaviors and ideologies must be tried out to select an identity. Role confusion and inability to choose vocation can result from a failure to achieve a sense of identity through, for example, friends.[136] Early adulthood generally refers to the period between ages 18 to 39,[137] and according to theorists such as Erik Erikson, is a stage where development is mainly focused on maintaining relationships.[138] Erikson shows the importance of relationships by labeling this stage intimacy vs isolation. Intimacy suggests a process of becoming part of something larger than oneself by sacrificing in romantic relationships and working for both life and career goals.[139] Other examples include creating bonds of intimacy, sustaining friendships, and starting a family. Some theorists state that development of intimacy skills rely on the resolution of previous developmental stages. A sense of identity gained in the previous stages is also necessary for intimacy to develop. If this skill is not learned the alternative is alienation, isolation, a fear of commitment, and the inability to depend on others. Isolation, on the other hand, suggests something different than most might expect. Erikson defined it as a delay of commitment in order to maintain freedom. Yet, this decision does not come without consequences. Erikson explained that choosing isolation may affect one's chances of getting married, progressing in a career, and overall development.[139] A related framework for studying this part of the lifespan is that of emerging adulthood. Scholars of emerging adulthood, such as Jeffrey Arnett, are not necessarily interested in relationship development. Instead, this concept suggests that people transition after their teenage years into a period, not characterized as relationship building and an overall sense of constancy with life, but with years of living with parents, phases of self-discovery, and experimentation.[140] Middle adulthood generally refers to the period between ages 40 to 64. During this period, middle-aged adults experience a conflict between generativity and stagnation. Generativity is the sense of contributing to society, the next generation, or their immediate community. On the other hand, stagnation results in a lack of purpose.[141] The adult's identity continues to develop in middle-adulthood. Middle-aged adults often adopt opposite gender characteristics. The adult realizes they are half-way through their life and often reevaluate vocational and social roles. Life circumstances can also cause a reexamination of identity.[142] Physically, the middle-aged experience a decline in muscular strength, reaction time, sensory keenness, and cardiac output. Also, women experience menopause at an average age of 48.8 and a sharp drop in the hormone estrogen.[143] Men experience an equivalent endocrine system event to menopause. Andropause in males is a hormone fluctuation with physical and psychological effects that can be similar to those seen in menopausal females. As men age lowered testosterone levels can contribute to mood swings and a decline in sperm count. Sexual responsiveness can also be affected, including delays in erection and longer periods of penile stimulation required to achieve ejaculation. The important influence of biological and social changes experienced by women and men in middle adulthood is reflected in the fact that depression is highest at age 48.5 around the world.[144] The World Health Organization finds \"no general agreement on the age at which a person becomes old.\" Most \"developed countries\" set the age as 65 or 70. However, in developing countries inability to make \"active contribution\" to society, not chronological age, marks the beginning of old age.[145][146] According to Erikson's stages of psychosocial development, old age is the stage in which individuals assess the quality of their lives.[147] Erikson labels this stage as integrity versus despair. For integrated persons, there is a sense of fulfillment in life. They have become self-aware and optimistic due to life's commitments and connection to others. While reflecting on life, people in this stage develop feelings of contentment with their experiences. If a person falls into despair, they are often disappointed about failures or missed chances in life. They may feel that the time left in life is an insufficient amount to turn things around.[148] Physically, older people experience a decline in muscular strength, reaction time, stamina, hearing, distance perception, and the sense of smell.[149] They also are more susceptible to diseases such as cancer and pneumonia due to a weakened immune system.[150] Programs aimed at balance, muscle strength, and mobility have been shown to reduce disability among mildly (but not more severely) disabled elderly.[151] Sexual expression depends in large part upon the emotional and physical health of the individual. Many older adults continue to be sexually active and satisfied with their sexual activity.[152] Mental disintegration may also occur, leading to dementia or ailments such as Alzheimer's disease. The average age of onset for dementia in males is 78.8 and 81.9 for women.[153] It is generally believed that crystallized intelligence increases up to old age, while fluid intelligence decreases with age.[154] Whether or not normal intelligence increases or decreases with age depends on the measure and study. Longitudinal studies show that perceptual speed, inductive reasoning, and spatial orientation decline.[155] An article on adult cognitive development reports that cross-sectional studies show that \"some abilities remained stable into early old age\".[155] Parenting variables alone have typically accounted for 20 to 50 percent of the variance in child outcomes.[156] All parents have their own parenting styles. Parenting styles, according to Kimberly Kopko, are \"based upon two aspects of parenting behavior; control and warmth. Parental control refers to the degree to which parents manage their children's behavior. Parental warmth refers to the degree to which parents are accepting and responsive to their children's behavior.\"[157] The following parenting styles have been described in the child development literature: Parenting research has traditionally focused on mothers, but recent studies highlight the important role of fathers in child development. Children as young as 15 months benefit significantly from substantial engagement with their father.[162][163] In particular, a study in the U.S. and New Zealand found the presence of the natural father was the most significant factor in reducing rates of early sexual activity and rates of teenage pregnancy in girls.[164] However, neither a mother nor a father is actually essential in successful parenting, and both single parents as well as homosexual couples can support positive child outcomes.[165] Children need at least one consistently responsible adult with whom they can form a positive emotional bond. Having multiple such figures further increases the likelihood of positive outcomes.[165] Recent research also suggests that the way parents interact with infants can influence early brain development. Parents who guide their baby's attention during play by shifting their gaze between a toy and the child tend to have infants with more complex brain activity. This attention-guiding behavior helps infants process social cues more effectively.[166] Another parental factor often debated in terms of its effects on child development is divorce. Divorce in itself is not a determining factor of negative child outcomes. In fact, the majority of children from divorcing families fall into the normal range on measures of psychological and cognitive functioning.[167] A number of mediating factors play a role in determining the effects divorce has on a child, for example, divorcing families with young children often face harsher consequences in terms of demographic, social, and economic changes than do families with older children.[167] Positive coparenting after divorce is part of a pattern associated with positive child coping, while hostile parenting behaviors lead to a destructive pattern leaving children at risk.[167] Additionally, direct parental relationship with the child also affects the development of a child after a divorce. Overall, protective factors facilitating positive child development after a divorce are maternal warmth, positive father-child relationship, and cooperation between parents.[167] A way to improve developmental psychology is a representation of cross-cultural studies. The psychology field in general assumes that \"basic\" human developments are represented in any population, specifically the Western-Educated-Industrialized-Rich and Democratic (W.E.I.R.D.) subjects that are relied on for a majority of their studies. Previous research generalizes the findings done with W.E.I.R.D. samples because many in the Psychological field assume certain aspects of development are exempted from or are not affected by life experiences. However, many of the assumptions have been proven incorrect or are not supported by empirical research. For example, according to Kohlberg, moral reasoning is dependent on cognitive abilities. While both analytical and holistic cognitive systems do have the potential to develop in any adult, the West is still on the extreme end of analytical thinking, and the non-West tend to use holistic processes. Furthermore, moral reasoning in the West only considers aspects that support autonomy and the individual, whereas non-Western adults emphasize moral behaviors supporting the community and maintaining an image of holiness or divinity. Not all aspects of human development are universal and we can learn a lot from observing different regions and subjects.[168] An example of a non-Western model for development stages is the Indian model, focusing a large amount of its psychological research on morality and interpersonal progress. The developmental stages in Indian models are founded by Hinduism, which primarily teaches stages of life in the process of someone discovering their fate or Dharma.[169] This cross-cultural model can add another perspective to psychological development in which the West behavioral sciences have not emphasized kinship, ethnicity, or religion.[168] Indian psychologists study the relevance of attentive families during the early stages of life. The early life stages conceptualize a different parenting style from the West because it does not try to rush children out of dependency. The family is meant to help the child grow into the next developmental stage at a particular age. This way, when children finally integrate into society, they are interconnected with those around them and reach renunciation when they are older. Children are raised in joint families so that in early childhood (ages 6 months to 2 years) the other family members help gradually wean the child from its mother. During ages 2 to 5, the parents do not rush toilet training. Instead of training the child to perform this behavior, the child learns to do it as they mature at their own pace. This model of early human development encourages dependency, unlike Western models that value autonomy and independence. By being attentive and not forcing the child to become independent, they are confident and have a sense of belonging by late childhood and adolescence. This stage in life (5\u201315 years) is also when children start education and increase their knowledge of Dharma.[170] It is within early and middle adulthood that we see moral development progress. Early, middle, and late adulthood are all concerned with caring for others and fulfilling Dharma. The main distinction between early adulthood to middle or late adulthood is how far their influence reaches. Early adulthood emphasizes the importance of fulfilling the immediate family needs, until later adulthood when they broaden their responsibilities to the general public. The old-age life stage development reaches renunciation or a complete understanding of Dharma.[169] The current mainstream views in the psychological field are against the Indian model for human development. The criticism against such models is that the parenting style is overly protective and encourages too much dependency. It focuses on interpersonal instead of individual goals. Also, there are some overlaps and similarities between Erikson's stages of human development and the Indian model but both of them still have major differences. The West prefers Erickson's ideas over the Indian model because they are supported by scientific studies. The life cycles based on Hinduism are not as favored, because it is not supported with research and it focuses on the ideal human development.[169] [1][2]",
      "ground_truth_chunk_ids": [
        "165_fixed_chunk1"
      ],
      "source_ids": [
        "S165"
      ],
      "category": "factual",
      "id": 1
    },
    {
      "question": "What is Compound of five great dodecahedra?",
      "ground_truth": "This uniform polyhedron compound is a composition of 5 great dodecahedra, in the same arrangement as in the compound of 5 icosahedra. It is one of only five polyhedral compounds (along with the compound of six tetrahedra, the compound of two great dodecahedra, the compound of two small stellated dodecahedra, and the compound of five small stellated dodecahedra) which is vertex-transitive and face-transitive but not edge-transitive. This polyhedron-related article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "This uniform polyhedron compound is a composition of 5 great dodecahedra, in the same arrangement as in the compound of 5 icosahedra. It is one of only five polyhedral compounds (along with the compound of six tetrahedra, the compound of two great dodecahedra, the compound of two small stellated dodecahedra, and the compound of five small stellated dodecahedra) which is vertex-transitive and face-transitive but not edge-transitive. This polyhedron-related article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "184_random_chunk1"
      ],
      "source_ids": [
        "S384"
      ],
      "category": "factual",
      "id": 2
    },
    {
      "question": "What is Database?",
      "ground_truth": "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data became widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file. Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use",
      "expected_answer": "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data became widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file. Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages. Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. Because of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it. Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1] Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups: Both a database and its DBMS conform to the principles of a particular database model.[5] \"Database system\" refers collectively to the database model, database management system, and database.[6] Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed] Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.[7] Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security. The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational. The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another. The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed] Object databases were developed in the 1980s to overcome the inconvenience of object\u2013relational impedance mismatch, which led to the coining of the term \"post-relational\" and also the development of hybrid object\u2013relational databases. The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key\u2013value stores and document-oriented databases. A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs. The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense.[10] As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market. The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods: Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications. IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11] Edgar F. Codd worked at IBM in San Jose, California, in an office primarily involved in the development of hard disk systems.[12] He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[13] The paper described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of \"tables\", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each \"fact\" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated. Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based. The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit. In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys. For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided. As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic. Codd's paper inspired teams at various universities to research the subject, including one at University of California, Berkeley[12] led by Eugene Wong and Michael Stonebraker, who started INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard. IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs. In 1970, the University of Michigan began development of the MICRO Information Management System[14] based on D.L. Childs' Set-Theoretic Data model.[15][16][17] The university in 1974 hosted a debate between Codd and Bachman which Bruce Lindsay of IBM later described as \"throwing lightning bolts at each other!\".[12] MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System.[18] The system remained in production until 1998. In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine. Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata). IBM formed a team led by Codd that started working on a prototype system, System R despite opposition from others at the company.[12] The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language \u2013 SQL[citation needed] \u2013 had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2). Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.[19] Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions). In Sweden, Codd's paper was also read and Mimer SQL was developed in the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise. Another data model, the entity\u2013relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity\u2013relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.[citation needed] Besides IBM and various software companies such as Sybase and Informix Corporation, most large computer hardware vendors by the 1980s had their own database systems such as DEC's VAX Rdb/VMS.[20] The decade ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\"[21] dBASE was one of the top selling software titles in the 1980s and early 1990s. By the start of the decade databases had become a billion-dollar industry in about ten years.[20] The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields.[22] The term \"object\u2013relational impedance mismatch\" described the inconvenience of translating between programmed objects and database tables. Object databases and object\u2013relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object\u2013relational mappings (ORMs) attempt to solve the same problem. Database sales grew rapidly during the dotcom bubble and, after its end, the rise of ecommerce. The popularity of open source databases such as MySQL has grown since 2000, to the extent that Ken Jacobs of Oracle said in 2005 that perhaps \"these guys are doing to us what we did to IBM\".[20] XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records. NoSQL databases are often very fast,[23][24] do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally. In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency. NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system. Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software). Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database. One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases. Connolly and Begg define database management system (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database.\"[28] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access. The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object\u2013relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems. The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:[29] It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[30] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine. Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount. The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime.[a] Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client\u2013server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.[32] A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email. External interaction with the database will be via an application program that interfaces with the DBMS.[33] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information. A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET. Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages: Database languages are specific to a particular data model. Notable examples include: A database language may also incorporate features like: Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, \"data about the data\", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[37] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database). Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database. Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases. Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy. Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated. With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[38] Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program). Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces. This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases. Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption). Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.[39] Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability. A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs. After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.). When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation. After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc. Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state. Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[40] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc. Other DBMS features might include: Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as \"DevOps for database\".[41] The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity\u2013relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes. Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data. Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design). The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency. The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS. Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself. A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format. Common logical data models for databases include: An object\u2013relational database combines the two related structures. Physical data models include: Other models include: Specialized models are optimized for particular types of data: A database management system provides three views of the database data: While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database. The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[43] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance. The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types. Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more. The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).",
      "ground_truth_chunk_ids": [
        "46_fixed_chunk1"
      ],
      "source_ids": [
        "S046"
      ],
      "category": "factual",
      "id": 3
    },
    {
      "question": "What is Information security?",
      "ground_truth": "Information security (infosec) is the practice of protecting information by mitigating information risks. It is part of information risk management.[1] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[2][3] Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (known as the CIA triad, unrelated to the US government organization)[4] while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[5] This is largely achieved through a structured risk management process.[6] To standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on passwords, antivirus software, firewalls, encryption software, legal liability, security awareness and training, and so forth.[7] This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred, and destroyed.[8] While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized,[9][10] with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses.[11] They are responsible for keeping all of the technology within the company secure from malicious attacks that often attempt to acquire critical private information or gain control of the internal systems.[12][13] There are many specialist roles in Information Security including securing networks and",
      "expected_answer": "Information security (infosec) is the practice of protecting information by mitigating information risks. It is part of information risk management.[1] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[2][3] Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (known as the CIA triad, unrelated to the US government organization)[4] while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[5] This is largely achieved through a structured risk management process.[6] To standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on passwords, antivirus software, firewalls, encryption software, legal liability, security awareness and training, and so forth.[7] This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred, and destroyed.[8] While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized,[9][10] with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses.[11] They are responsible for keeping all of the technology within the company secure from malicious attacks that often attempt to acquire critical private information or gain control of the internal systems.[12][13] There are many specialist roles in Information Security including securing networks and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning, electronic record discovery, and digital forensics.[14] Information security standards are techniques generally outlined in published materials that attempt to protect the information of a user or organization.[15] This environment includes users themselves, networks, devices, all software, processes, information in storage or transit, applications, services, and systems that can be connected directly or indirectly to networks. The principal objective is to reduce the risks, including preventing or mitigating attacks. These published materials consist of tools, policies, security concepts, security safeguards, guidelines, risk management approaches, actions, training, best practices, assurance and technologies. Various definitions of information security are suggested below, summarized from different sources: Information security threats come in many different forms.[26] Some of the most common threats today are software attacks, theft of intellectual property, theft of identity, theft of equipment or information, sabotage, and information extortion.[27][28] Viruses,[29] worms, phishing attacks, and Trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses.[30] Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information through social engineering.[31][32] Sabotage usually consists of the destruction of an organization's website in an attempt to cause loss of confidence on the part of its customers.[33] Information extortion consists of theft of a company's property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner, as with ransomware.[34] One of the most functional precautions against these attacks is to conduct periodical user awareness.[35] Governments, military, corporations, financial institutions, hospitals, non-profit organizations, and private businesses amass a great deal of confidential information about their employees, customers, products, research, and financial status.[36] Should confidential information about a business's customers or finances or new product line fall into the hands of a competitor or hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation.[37] From a business perspective, information security must be balanced against cost; the Gordon-Loeb Model provides a mathematical economic approach for addressing this concern.[38] For the individual, information security has a significant effect on privacy, which is viewed very differently in various cultures.[39] Since the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering.[40] Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands.[41] However, for the most part protection was achieved through the application of procedural handling controls.[42][43] Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box.[44] As postal services expanded, governments created official organizations to intercept, decipher, read, and reseal letters (e.g., the U.K.'s Secret Office, founded in 1653[45]). In the mid-nineteenth century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity.[46] For example, the British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889.[47] Section 1 of the law concerned espionage and unlawful disclosures of information, while Section 2 dealt with breaches of official trust.[48] A public interest defense was soon added to defend disclosures in the interest of the state.[49] A similar law was passed in India in 1889, The Indian Official Secrets Act, which was associated with the British colonial era and used to crack down on newspapers that opposed the Raj's policies.[50] A newer version was passed in 1923 that extended to all matters of confidential or secret information for governance.[51]  By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters.[52] Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information.[53] The establishment of computer security inaugurated the history of information security. The need for such appeared during World War II.[54] The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls.[55] An arcane range of markings evolved to indicate who could handle documents (usually officers rather than enlisted troops) and where they should be stored as increasingly complex safes and storage facilities were developed.[56] The Enigma Machine, which was employed by the Germans to encrypt the data of warfare and was successfully decrypted by Alan Turing, can be regarded as a striking example of creating and using secured information.[57] Procedures evolved to ensure documents were destroyed properly, and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g., the capture of U-570[57]). Various mainframe computers were connected online during the Cold War to complete more sophisticated tasks, in a communication process easier than mailing magnetic tapes back and forth by computer centers. As such, the Advanced Research Projects Agency (ARPA), of the United States Department of Defense, started researching the feasibility of a networked system of communication to trade information within the United States Armed Forces. In 1968, the ARPANET project was formulated by Larry Roberts, which would later evolve into what is known as the internet.[58] In 1973, important elements of ARPANET security were found by internet pioneer Robert Metcalfe to have many flaws such as the: \"vulnerability of password structure and formats; lack of safety procedures for dial-up connections; and nonexistent user identification and authorizations\", aside from the lack of controls and safeguards to keep data safe from unauthorized access. Hackers had effortless access to ARPANET, as phone numbers were known by the public.[59] Due to these problems, coupled with the constant violation of computer security, as well as the exponential increase in the number of hosts and users of the system, \"network security\" was often alluded to as \"network insecurity\".[59] The end of the twentieth century and the early years of the twenty-first century saw rapid advancements in telecommunications, computing hardware and software, and data encryption.[60] The availability of smaller, more powerful, and less expensive computing equipment made electronic data processing within the reach of small business and home users.[61] The establishment of Transfer Control Protocol/Internetwork Protocol (TCP/IP) in the early 1980s enabled different types of computers to communicate.[62] These computers quickly became interconnected through the internet.[63] The rapid growth and widespread use of electronic data processing and electronic business conducted through the internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process, and transmit.[64] The academic disciplines of computer security and information assurance emerged along with numerous professional organizations, all sharing the common goals of ensuring the security and reliability of information systems.[65] The \"CIA triad\" of confidentiality, integrity, and availability is at the heart of information security.[66] The concept was introduced in the Anderson Report in 1972 and later repeated in The Protection of Information in Computer Systems. The abbreviation was coined by Steve Lipner around 1986.[67] Debate continues about whether or not this triad is sufficient to address rapidly changing technology and business requirements, with recommendations to consider expanding on the intersections between availability and confidentiality, as well as the relationship between security and privacy.[4] Other principles such as \"accountability\" have sometimes been proposed; it has been pointed out that issues such as non-repudiation do not fit well within the three core concepts.[68] In information security, confidentiality \"is the property, that information is not made available or disclosed to unauthorized individuals, entities, or processes.\"[69] While similar to \"privacy\", the two words are not interchangeable. Rather, confidentiality is a component of privacy that implements to protect our data from unauthorized viewers.[70] Examples of confidentiality of electronic data being compromised include laptop theft, password theft, or sensitive emails being sent to the incorrect individuals.[71] In IT security, data integrity means maintaining and assuring the accuracy and completeness of data over its entire lifecycle.[72] This means that data cannot be modified in an unauthorized or undetected manner.[73] This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing.[74] Information security systems typically incorporate controls to ensure their own integrity, in particular protecting the kernel or core functions against both deliberate and accidental threats.[75] Multi-purpose and multi-user computer systems aim to compartmentalize the data and processing such that no user or process can adversely impact another: the controls may not succeed however, as we see in incidents such as malware infections, hacks, data theft, fraud, and privacy breaches.[76] More broadly, integrity is an information security principle that involves human/social, process, and commercial integrity, as well as data integrity. As such it touches on aspects such as credibility, consistency, truthfulness, completeness, accuracy, timeliness, and assurance.[77] For any information system to serve its purpose, the information must be available when it is needed.[78] This means the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly.[79] High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades.[80] Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system, essentially forcing it to shut down.[81] In the realm of information security, availability can often be viewed as one of the most important parts of a successful information security program.[citation needed] Ultimately end-users need to be able to perform job functions; by ensuring availability an organization is able to perform to the standards that an organization's stakeholders expect.[82] This can involve topics such as proxy configurations, outside web access, the ability to access shared drives and the ability to send emails.[83] Executives oftentimes do not understand the technical side of information security and look at availability as an easy fix, but this often requires collaboration from many different organizational teams, such as network operations, development operations, incident response, and policy/change management.[84] A successful information security team involves many different key roles to mesh and align for the \"CIA\" triad to be provided effectively.[85] In addition to the classic CIA triad of security goals, some organisations may want to include security goals like authenticity, accountability, non-repudiation, and reliability. In law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction, nor can the other party deny having sent a transaction.[86] It is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology.[87] It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message, and nobody else could have altered it in transit (data integrity).[88] The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised.[89] The fault for these violations may or may not lie with the sender, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity. As such, the sender may repudiate the message (because authenticity and integrity are pre-requisites for non-repudiation).[90] In 1992 and revised in 2002, the OECD's Guidelines for the Security of Information Systems and Networks[91] proposed the nine generally accepted principles: awareness, responsibility, response, ethics, democracy, risk assessment, security design and implementation, security management, and reassessment.[92] Building upon those, in 2004 the NIST's Engineering Principles for Information Technology Security[68] proposed 33 principles. In 1998, Donn Parker proposed an alternative model for the classic \"CIA\" triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian Hexad are a subject of debate amongst security professionals.[93] In 2011, The Open Group published the information security management standard O-ISM3.[94] This standard proposed an operational definition of the key concepts of security, with elements called \"security objectives\", related to access control (9), availability (3), data quality (1), compliance, and technical (4). Risk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset).[95] A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm.[96] The likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact.[97] In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property).[98] The Certified Information Systems Auditor (CISA) Review Manual 2006 defines risk management as \"the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures,[99] if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization.\"[100] There are two things in this definition that may need some clarification. First, the process of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day.[101] Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.[102] Furthermore, these processes have limitations as security breaches are generally rare and emerge in a specific context which may not be easily duplicated.[103] Thus, any process and countermeasure should itself be evaluated for vulnerabilities.[104] It is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called \"residual risk\".[105] A risk assessment is carried out by a team of people who have knowledge of specific areas of the business.[106] Membership of the team may vary over time as different parts of the business are assessed.[107] The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis. Research has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human.[108] The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment: In broad terms, the risk management process consists of:[109][110] For any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business.[117] Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business.[118] The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.[119] Selecting and implementing proper security controls will initially help an organization bring down risk to acceptable levels.[120] Control selection should follow and should be based on the risk assessment.[121] Controls can vary in nature, but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. ISO/IEC 27001 has defined controls in different areas.[122] Organizations can implement additional controls according to requirement of the organization.[123] ISO/IEC 27002 offers a guideline for organizational information security standards.[124] Defense in depth is a fundamental security philosophy that relies on overlapping security systems designed to maintain protection even if individual components fail. Rather than depending on a single security measure, it combines multiple layers of security controls both in the cloud and at network endpoints. This approach includes combinations like firewalls with intrusion-detection systems, email filtering services with desktop anti-virus, and cloud-based security alongside traditional network defenses.[125]\nThe concept can be implemented through three distinct layers of administrative, logical, and physical controls,[126] or visualized as an onion model with data at the core, surrounded by people, network security, host-based security, and application security layers.[127] The strategy emphasizes that security involves not just technology, but also people and processes working together, with real-time monitoring and response being crucial components.[125] An important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information.[128] Not all information is equal and so not all information requires the same degree of protection.[129] This requires information to be assigned a security classification.[130] The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy.[131] The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.[132] Some factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete.[133] Laws and other regulatory requirements are also important considerations when classifying information.[134] The Information Systems Audit and Control Association (ISACA) and its Business Model for Information Security also serves as a tool for security professionals to examine security from a systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.[135] The type of information security classification labels selected and used will depend on the nature of the organization, with examples being:[132] All employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification.[138] The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.[139] Access to protected information must be restricted to people who are authorized to access the information.[140] The computer programs, and in many cases the computers that process the information, must also be authorized.[141] This requires that mechanisms be in place to control the access to protected information.[141] The sophistication of the access control mechanisms should be in parity with the value of the information being protected; the more sensitive or valuable the information the stronger the control mechanisms need to be.[142] The foundation on which access control mechanisms are built start with identification and authentication.[143] Access control is generally considered in three steps: identification, authentication, and authorization.[144][71] Identification is an assertion of who someone is or what something is. If a person makes the statement \"Hello, my name is John Doe\" they are making a claim of who they are.[145] However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe.[146] Typically the claim is in the form of a username. By entering that username you are claiming \"I am the person the username belongs to\".[147] Authentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe, a claim of identity.[148] The bank teller asks to see a photo ID, so he hands the teller his driver's license.[149] The bank teller checks the license to make sure it has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe.[150] If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly, by entering the correct password, the user is providing evidence that he/she is the person the username belongs to.[151] There are three different types of information that can be used for authentication:[152][153] Strong authentication requires providing more than one type of authentication information (two-factor authentication).[159] The username is the most common form of identification on computer systems today and the password is the most common form of authentication.[160] Usernames and passwords have served their purpose, but they are increasingly inadequate.[161] Usernames and passwords are slowly being replaced or supplemented with more sophisticated authentication mechanisms such as time-based one-time password algorithms.[162] After a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change).[163] This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures.[164] The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies.[165] Different computing systems are equipped with different kinds of access control mechanisms. Some may even offer a choice of different access control mechanisms.[166] The access control mechanism a system offers will be based upon one of three approaches to access control, or it may be derived from a combination of the three approaches.[71] The non-discretionary approach consolidates all access control under a centralized administration.[167] The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform.[168][169] The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources.[167] In the mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.[140] Examples of common access control mechanisms in use today include role-based access control, available in many advanced database management systems; simple file permissions provided in the UNIX and Windows operating systems;[170] Group Policy Objects provided in Windows network systems; and Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.[171] To be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions.[172] The U.S. Treasury's guidelines for systems processing sensitive or proprietary information, for example, states that all failed and successful authentication and access attempts must be logged, and all access to information must leave some type of audit trail.[173] Also, the need-to-know principle needs to be in effect when talking about access control. This principle gives access rights to a person to perform their job functions.[174] This principle is used in the government when dealing with difference clearances.[175] Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee the least amount of privilege to prevent employees from accessing more than what they are supposed to.[176] Need-to-know helps to enforce the confidentiality-integrity-availability triad. Need-to-know directly impacts the confidential area of the triad.[177] Information security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption.[178] Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user who possesses the cryptographic key, through the process of decryption.[179] Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.[71] Cryptography provides information security with other useful applications as well, including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications.[180] Older, less secure applications such as Telnet and File Transfer Protocol (FTP) are slowly being replaced with more secure applications such as Secure Shell (SSH) that use encrypted network communications.[181] Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU\u2011T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange.[182] Software applications such as GnuPG or PGP can be used to encrypt data files and email.[183] Cryptography can introduce security problems when it is not implemented correctly.[184] Cryptographic solutions need to be implemented using industry-accepted solutions that have undergone rigorous peer review by independent experts in cryptography.[185] The length and strength of the encryption key is also an important consideration.[186] A key that is weak or too short will produce weak encryption.[186] The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information.[187] They must be protected from unauthorized disclosure and destruction, and they must be available when needed.[citation needed] Public key infrastructure (PKI) solutions address many of the problems that surround key management.[71] U.S. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.[188] In the field of information security, Harris[189]\noffers the following definitions of due care and due diligence: \"Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees[190].\" And, [Due diligence are the] \"continual activities that make sure the protection mechanisms are continually maintained and operational.\"[191] Attention should be made to two important points in these definitions.[192][193] First, in due care, steps are taken to show; this means that the steps can be verified, measured, or even produce tangible artifacts.[194][195] Second, in due diligence, there are continual activities; this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.[196] Organizations have a responsibility with practicing duty of care when applying information security. The Duty of Care Risk Analysis Standard (DoCRA)[197] provides principles and practices for evaluating risk.[198] It considers all parties that could be affected by those risks.[199] DoCRA helps evaluate safeguards if they are appropriate in protecting others from harm while presenting a reasonable burden.[200] With increased data breach litigation, companies must balance security controls, compliance, and its mission.[201] Computer security incident management is a specialized form of incident management focused on monitoring, detecting, and responding to security events on computers and networks in a predictable way.[202] Organizations implement this through incident response plans (IRPs) that are activated when security breaches are detected.[203] These plans typically involve an incident response team (IRT) with specialized skills in areas like penetration testing, computer forensics, and network security.[204] Change management is a formal process for directing and controlling alterations to the information processing environment.[205][206] This includes alterations to desktop computers, the network, servers, and software.[207] The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the processing environment as changes are made.[208] It is not the objective of change management to prevent or hinder necessary changes from being implemented.[209][210] Any change to the information processing environment introduces an element of risk.[211] Even apparently simple changes can have unexpected effects.[212] One of management's many responsibilities is the management of risk.[213][214] Change management is a tool for managing the risks introduced by changes to the information processing environment.[215] Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.[216] Not every change needs to be managed.[217][218] Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment.[219] Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management.[220] However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity.[221] The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.[222] Change management is usually overseen by a change review board composed of representatives from key business areas,[223] security, networking, systems administrators, database administration, application developers, desktop support, and the help desk.[224] The tasks of the change review board can be facilitated with the use of automated work flow application.[225] The responsibility of the change review board is to ensure the organization's documented change management procedures are followed.[226] The change management process is as follows[227] Change management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment.[259] Good change management procedures improve the overall quality and success of changes as they are implemented.[260] This is accomplished through planning, peer review, documentation, and communication.[261] ISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps[262] (Full book summary),[263] and ITIL all provide valuable guidance on implementing an efficient and effective change management program information security.[264] Business continuity management (BCM) concerns arrangements aiming to protect an organization's critical business functions from interruption due to incidents, or at least minimize the effects.[265][266] BCM is essential to any organization to keep technology and business in line with current threats to the continuation of business as usual.[267] The BCM should be included in an organizations risk analysis plan to ensure that all of the necessary business functions have what they need to keep going in the event of any type of threat to any business function.[268] It encompasses: Whereas BCM takes a broad approach to minimizing disaster-related risks by reducing both the probability and the severity of incidents, a disaster recovery plan (DRP) focuses specifically on resuming business operations as quickly as possible after a disaster.[278] A disaster recovery plan, invoked soon after a disaster occurs, lays out the steps necessary to recover critical information and communications technology (ICT) infrastructure.[279] Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.[280] Below is a partial listing of governmental laws and regulations in various parts of the world that have, had, or will have, a significant effect on data processing and information security.[281][282] Important industry sector regulations have also been included when they have a significant impact on information security.[281] The US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.[317] Describing more than simply how security aware employees are, information security culture is the ideas, customs, and social behaviors of an organization that impact information security in both positive and negative ways.[318] Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization. The way employees think and feel about security and the actions they take can have a big impact on information security in organizations. Roer & Petric (2017) identify seven core dimensions of information security culture in organizations:[319] Andersson and Reimers (2014) found that employees often do not see themselves as part of the organization Information Security \"effort\" and often take actions that ignore organizational information security best interests.[321] Research shows information security culture needs to be improved continuously. In Information Security Culture from Analysis to Change, authors commented, \"It's a never ending process, a cycle of evaluation and change or maintenance.\" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.[322]",
      "ground_truth_chunk_ids": [
        "157_fixed_chunk1"
      ],
      "source_ids": [
        "S157"
      ],
      "category": "factual",
      "id": 4
    },
    {
      "question": "What is Energy?",
      "ground_truth": "Energy (from Ancient Greek \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 (en\u00e9rgeia) 'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J). Forms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass. These are not mutually exclusive. All living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the Sun.[6] The total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the object's components \u2013 while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself.[7] While these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to",
      "expected_answer": "Energy (from Ancient Greek  \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 (en\u00e9rgeia)\u00a0'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J). Forms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass. These are not mutually exclusive. All living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the Sun.[6] The total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the object's components \u2013 while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself.[7] While these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to the combined potentials within an atomic nucleus from either the nuclear force or the weak force, among other examples.[8] The word energy derives from the Ancient Greek: \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1, romanized:\u00a0energeia, lit.\u2009'activity, operation',[11] which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.[12] In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the motions of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two.[13] Writing in the early 18th century, \u00c9milie du Ch\u00e2telet proposed the concept of conservation of energy in the marginalia of her French language translation of Newton's Principia Mathematica, which represented the first formulation of a conserved measurable quantity that was distinct from momentum, and which would later be called \"energy\".[14] In 1807, Thomas Young was possibly the first to use the term \"energy\" instead of vis viva, in its modern sense.[15] Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense,[16] and in 1853, William Rankine coined the term \"potential energy\".[17] The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system.[18] It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.[19] These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics.[20] Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, Walther Nernst, and others.[21] It also led to a mathematical formulation of the concept of entropy by Clausius[22] and to the introduction of laws of radiant energy by Jo\u017eef Stefan.[23] According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time.[24] Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.[25] Albert Einstein's 1905 theory of special relativity showed that rest mass corresponds to an equivalent amount of rest energy. This means that rest mass can be converted to or from equivalent amounts of (non-material) forms of energy, for example, kinetic energy, potential energy, and electromagnetic radiant energy. When this happens, rest mass is not conserved, unlike the total mass or total energy. All forms of energy contribute to the total mass and total energy. Thus, conservation of energy (total, including material or rest energy) and conservation of mass (total, not just rest) are one (equivalent) law. In the 18th century, these had appeared as two seemingly-distinct laws.[26][27] The first evidence of quantization in atoms was the observation of spectral lines in light from the sun in the early 1800s by Joseph von Fraunhofer and William Hyde Wollaston. The notion of quantized energy levels was proposed in 1913 by Danish physicist Niels Bohr in the Bohr theory of the atom. The modern quantum mechanical theory giving an explanation of these energy levels in terms of the Schr\u00f6dinger equation was advanced by Erwin Schr\u00f6dinger and Werner Heisenberg in 1926.[28] Noether's theorem shows that the symmetry of this equation is equivalent to a conservation of probability.[29] At the quantum level, mass-energy interactions are all subject to this principle.[30] During wave function collapse, the conservation of energy does not hold at the local level, although statistically the principle holds on average for sufficiently large numbers of collapses.[31] Conservation of energy does apply during wave function collapse in H. Everett's many-worlds interpretation of quantum mechanics.[32] In dimensional analysis, the base units of energy are given by: Work = Force \u00d7 Distance = M L2 T\u22122, with the fundamental dimensions of Mass M, Length L, and time T.[5] In the International System of Units (SI), the unit of energy is the joule. It is a derived unit that is equal to the energy expended, or work done, in applying a force of one newton through a distance of one metre.[1] The SI unit of power, defined as energy per unit of time, is the watt, which is one joule per second.[3] Thus, a kilowatt-hour (kWh), which can be realized as the energy delivered by one kilowatt of power for an hour, is equal to 3.6 million joules.[33] The CGS energy unit is the erg and the imperial and US customary unit is the foot-pound.[34] Other energy units such as the electronvolt, food calorie, thermodynamic kilocalorie and BTU are used in specific areas of science and commerce.[35][2] In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept. Work, a function of energy, is force times distance.[36] This says that the work (\n\n\n\nW\n\n\n{\\displaystyle W}\n\n) is equal to the line integral of the force F along a path C; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.[37] The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems.[38] These classical equations have direct analogs in nonrelativistic quantum mechanics.[39] Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).[40] Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian;[41] for example, dissipative systems with continuous symmetries need not have a corresponding conservation law. In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular, or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is usually accompanied by a decrease, and sometimes an increase, of the total energy of the substances involved. Some energy may be transferred between the surroundings and the reactants in the form of heat or light; thus the products of a reaction have sometimes more but usually less energy than the reactants. A reaction is said to be exothermic or exergonic if the final state is lower on the energy scale than the initial state; in the less common case of endothermic reactions the situation is the reverse.[42] Chemical reactions are usually not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at a given temperature\u00a0T) is related to the activation energy\u00a0E by the Boltzmann population factor\u00a0e\u2212E/kT; that is, the probability of a molecule to have energy greater than or equal to\u00a0E at a given temperature\u00a0T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction can be provided in the form of thermal energy.[43] In biology, energy is an attribute of all biological systems, from the biosphere to the smallest living organism. It enables the growth, development, and functioning of a biological cell or organelle in an organism. All living creatures rely on an external source of energy to be able to grow and reproduce \u2013 radiant energy from the Sun in the case of green plants and chemical energy (in some form) in the case of animals. Energy provided through cellular respiration is stored in nutrients such as carbohydrates (including sugars), lipids, and proteins by cells.[44] Sunlight's radiant energy is captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into carbohydrates, lipids, proteins, and oxygen.[45] Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark in a forest fire, or it may be made available more slowly for animal or human metabolism when organic molecules are ingested and catabolism is triggered by enzyme action.[46] The basal metabolism rate measures the food energy expenditure per unit time by endothermic animals at rest.[47] In other words it is the energy required by body organs to perform normally. For humans, metabolic equivalent of task (MET) compares the energy expenditure per unit mass while performing a physical activity, relative to a baseline. By convention, this baseline is 3.5\u00a0mL of oxygen consumed per kg per minute, which is the energy consumed by a typical individual when sitting quietly.[48] In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, using as a standard an average human energy expenditure of 6,900\u00a0kJ per day and a basal metabolic rate of 80 watts.[citation needed] For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 \u00f7 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum.[49] The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.[50] The daily 1,600\u20133,000 calories (7\u201313\u00a0MJ) recommended for a human adult are taken as food molecules,[51] mostly carbohydrates and fats. Only a tiny fraction of the original chemical energy is used for work:[note 1] It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy); most machines manage higher efficiencies.[citation needed] In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism's tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\").[note 2] Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology. As an example, to take just the first step in the food chain: of the estimated 124.7\u00a0Pg/a of carbon that is fixed by photosynthesis, 64.3\u00a0Pg/a (52%) are used for the metabolism of green plants,[52] i.e. reconverted into carbon dioxide and heat. Multicellular organisms such as humans have cell forms that are classified as Eukaryote. These cells include an organelle called the mitochondria that generates chemical energy for the rest of the hosting cell. Ninety percent of the oxygen intake by humans is utilized by the mitochondria, especially for nutrient processing.[53] The molecule adenosine triphosphate (ATP) is the primary energy transporter in living cells, providing an energy source for cellular processes. It is continually being broken down and synthesized as a component of cellular respiration.[54] Two examples of nutrients consumed by animals are glucose (C6H12O6) and stearin (C57H110O6). These food molecules are oxidized to carbon dioxide and water in the mitochondria:[55]\n\n\n\n\n\n\nC\n\n6\n\n\n\n\n\n\nH\n\n12\n\n\n\n\n\n\nO\n\n6\n\n\n\n\n\n+\n6\n\n\nO\n\n2\n\n\n\n\n\n\u27f6\n6\n\n\nCO\n\n2\n\n\n\n\n\n+\n6\n\n\nH\n\n2\n\n\n\n\n\nO\n\n\n\n{\\displaystyle {\\ce {C6H12O6 + 6O2 -> 6CO2 + 6H2O}}}\n\n\n\n\n\n\n\n\nC\n\n57\n\n\n\n\n\n\nH\n\n110\n\n\n\n\n\n\nO\n\n6\n\n\n\n\n\n+\n\n(\n81\n\n\n\n1\n2\n\n\n)\n\n\nO\n\n2\n\n\n\n\n\n\u27f6\n57\n\n\nCO\n\n2\n\n\n\n\n\n+\n55\n\n\nH\n\n2\n\n\n\n\n\nO\n\n\n\n{\\displaystyle {\\ce {C57H110O6 + (81 1/2) O2 -> 57CO2 + 55H2O}}}\n\n\nand some of the energy is used to convert ADP into ATP:[56][53] The rest of the chemical energy of the nutrients are converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior,[57] while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes, and hurricanes are all a result of energy transformations in our atmosphere brought about by solar energy. Sunlight is the main input to Earth's energy budget which accounts for its temperature and climate stability, after accounting for interaction with the atmosphere.[58] Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example when) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity).[59]  An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, suddenly give up some of their thermal energy to power a few days of violent air movement.[60] In a slower process, radioactive decay of atoms in the core of the Earth releases heat, which supplies more than half of the planet's internal heat budget.[61] In the present day, this radiogenic heat production was primarily driven by the decay of Uranium-235, Potassium-40, and Thorium-232 some time in the past.[62] This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may later be transformed into active kinetic energy during landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks.[63] Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars (which created these atoms).[64] Early in a planet's history, the accretion process provides impact energy that can partially or completely melt the body. This allows a planet to become differentiated by chemical element. Chemical phase changes of minerals during formation provide additional internal heating. Over time the internal heat is brought to the surface then radiated away into space, cooling the body. Accreted radiogenic heat sources settle toward the core, providing thermal energy to the planet on a geologic time scale.[65] Ongoing sedimentation provides a persistent internal energy source for gas giant planets like Jupiter and Saturn.[66] In cosmology and astronomy the phenomena of stars, nova, supernova, quasars, and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen).[67] The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.[68] The accretion of matter onto a compact object is a very efficient means of generating energy from gravitational potential. This behavior is responsible for some of the universe's brightest persistent energy sources.[69] The Penrose process is a theoretical method by which energy could be extracted from a rotating black hole.[70] Hawking radiation is the emission of black-body radiation from a black hole, which results in a steady loss of mass and rotational energy. As the object evaporates, the temperature of this radiation is predicted to increase, speeding up the process.[71] In quantum mechanics, energy is defined in terms of the energy operator\n(Hamiltonian) as a time derivative of the wave function. The Schr\u00f6dinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schr\u00f6dinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta.[72] In the solution of the Schr\u00f6dinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by the Planck relation: \n\n\n\nE\n=\nh\n\u03bd\n\n\n{\\displaystyle E=h\\nu }\n\n, where \n\n\n\nh\n\n\n{\\displaystyle h}\n\n is the Planck constant and \n\n\n\n\u03bd\n\n\n{\\displaystyle \\nu }\n\n the frequency. In the case of an electromagnetic wave these energy states are called quanta of light or photons. For matter waves, the de Broglie relation yields \n\n\n\np\n=\nh\n\u03bd\n\n\n{\\displaystyle p=h\\nu }\n\n, where \n\n\n\np\n\n\n{\\displaystyle p}\n\n is the momentum.[73] When calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically \u2013 using Lorentz transformations instead of Newtonian mechanics \u2013 Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:[74] E\n\n0\n\n\n=\n\nm\n\n0\n\n\n\nc\n\n2\n\n\n,\n\n\n{\\displaystyle E_{0}=m_{0}c^{2},}\n\n\nwhere For example, consider electron\u2013positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process \u2013 the inverse process is called pair creation \u2013 in which the rest mass of the particles is created from a sufficiently energetic photon near a nucleus.[75] In general relativity, the stress\u2013energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.[76][page\u00a0needed] Energy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"),[77] and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws. In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy\u2013momentum 4-vector).[76][page\u00a0needed] In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of spacetime (= boosts). Energy may be transformed between different forms at various efficiencies. Devices that usefully transform between these forms are called transducers. Examples of transducers include a battery (from chemical energy to electric energy), a dam (from gravitational potential energy to the kinetic energy of water spinning the blades of a turbine, and ultimately to electric energy through an electric generator), and a heat engine (from heat to work).[78][79] Examples of energy transformation include generating electric energy from heat energy via a steam turbine,[79] or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground.[80] The Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that itself (since it still contains the same total energy even in different forms) but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.[81] There are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics.[82] However, some energy transformations can be quite efficient.[83] The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a sufficiently small scale, but certain larger transformations are highly improbable because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.[84] Energy transformations in the universe over time are characterized by various kinds of potential energy, that has been available since the Big Bang, being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available.[85] Familiar examples of such processes include nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae to \"store\" energy in the creation of heavy isotopes (such as uranium and thorium), and nuclear decay, a process in which energy is released that was originally stored in these heavy elements, before they were incorporated into the Solar System and the Earth.[86] This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic and thermal energy in a very short time.[87] Yet another example of energy transformation is that of a simple gravity pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at its maximum. At its lowest point the kinetic energy is at its maximum and is equal to the decrease in potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever. Energy is transferred from potential energy (\n\n\n\n\nE\n\np\n\n\n\n\n{\\displaystyle E_{p}}\n\n) to kinetic energy (\n\n\n\n\nE\n\nk\n\n\n\n\n{\\displaystyle E_{k}}\n\n) and then back to potential energy constantly. This is referred to as conservation of energy. In this isolated system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following: The equation can then be simplified further since \n\n\n\n\nE\n\np\n\n\n=\nm\ng\nh\n\n\n{\\displaystyle E_{p}=mgh}\n\n (mass times acceleration due to gravity times the height) and \n\n\n\n\nE\n\nk\n\n\n=\n\n\n1\n2\n\n\nm\n\nv\n\n2\n\n\n\n\n{\\textstyle E_{k}={\\frac {1}{2}}mv^{2}}\n\n (half\u00a0mass times velocity squared). Then the total amount of energy can be found by adding \n\n\n\n\nE\n\np\n\n\n+\n\nE\n\nk\n\n\n=\n\nE\n\ntotal\n\n\n\n\n{\\displaystyle E_{p}+E_{k}=E_{\\text{total}}}\n\n.[88] Within a gravitational field, both mass and energy give rise to a measureable weight when trapped in a system with zero momentum. The formula E\u00a0=\u00a0mc2, derived by Albert Einstein (1905) quantifies this mass\u2013energy equivalence between relativistic mass and energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincar\u00e9 (1900), Friedrich Hasen\u00f6hrl (1904), and others (see Mass\u2013energy equivalence#History for further information). Part of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since \n\n\n\n\nc\n\n2\n\n\n\n\n{\\displaystyle c^{2}}\n\n is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy, as can be seen in nuclear reactors and nuclear weapons.[89] For example, 1\u00a0kg of rest mass equals 9\u00d71016\u00a0joules, equivalent to 21.5 megatonnes of TNT.[90] Conversely, the mass equivalent of an everyday amount energy is minuscule. Examples of large-scale transformations between the rest energy of matter and other forms of energy are found in nuclear physics and particle physics. The complete conversion of matter, such as atoms, to non-matter, such as photons, occurs during interaction with antimatter.[91] Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another is reversible, as in the pendulum system described above.[92] At the atomic scale, thermal energy is present in the form of motion and vibrations of individual atoms and molecules. When heat is generated, radiation excites lower energy states of these atoms and their surrounding fields. This heating process acts as a reservoir for part of the applied energy, from which it cannot be converted with 100% efficiency into other forms of energy.[93] According to the second law of thermodynamics, this heat can only be completely recovered as usable energy at the price of an increase in some other kind of heat-like disorder in quantum states. As the universe evolves with time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or as other kinds of increases in disorder). This has led to the hypothesis of the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), continues to decrease.[94] The fact that energy can be neither created nor destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out as work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.[95] While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations.[96] The total energy of a system can be calculated by adding up all forms of energy in the system. Richard Feynman said during a 1961 lecture:[97] There is a fact, or if you wish, a law, governing all natural phenomena that are known to date. There is no known exception to this law \u2013 it is exact so far as we know. The law is called the conservation of energy. It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same. \u2014\u200aThe Feynman Lectures on Physics Most kinds of energy (with gravitational energy being a notable exception)[98] are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.[96][97] This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time,[99] a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle \u2013 it is impossible to define the exact amount of energy during any definite time interval (though this is practically significant only for very short time intervals). The uncertainty principle should not be confused with energy conservation \u2013 rather it provides mathematical limits to which energy can in principle be defined and measured. Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appear as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.[100] In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scale, the uncertainty in the energy is given by which is similar in form to the Heisenberg Uncertainty Principle,[101] but not really mathematically equivalent thereto, since E and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics.[102] In particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum.[102] The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions).[103]:\u200a101\u200a Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law),[103]:\u200a336\u200a for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force,[104] for the Van der Waals force,[105] and some other observable phenomena.[106] Energy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat.[note 3] Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy,[note 4] tidal interactions,[107] and the conductive transfer of thermal energy.[108] Energy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:[note 5][108] where \n\n\n\nE\n\n\n{\\displaystyle E}\n\n is the amount of energy transferred, \n\n\n\nW\n\n\n{\\displaystyle W}\n\n\u00a0 represents the work done on or by the system, and \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n represents the heat flow into or out of the system. As a simplification, the heat term, \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n, can sometimes be ignored, especially for fast processes involving gases, which are poor conductors of heat, or when the thermal efficiency of the transfer is high. For such adiabatic processes, This simplified equation is the one used to define the joule, for example. Beyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (this process is illustrated by injection of an air-fuel mixture into a car engine, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by \n\n\n\n\nE\n\nmatter\n\n\n\n\n{\\displaystyle E_{\\text{matter}}}\n\n, one may write:[109] Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.[110] The first law of thermodynamics asserts that the total energy of a system and its surroundings (but not necessarily thermodynamic free energy) is always conserved[111] and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as:[112] where the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and its change dS is positive when heat is added to the system), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system). This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and PV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form by:[108] where \n\n\n\n\u03b4\nQ\n\n\n{\\displaystyle \\delta Q}\n\n is the heat supplied to the system and \n\n\n\n\u03b4\nW\n\n\n{\\displaystyle \\delta W}\n\n is the work applied to the system. The energy of a mechanical harmonic oscillator (a mass on a spring) is alternately kinetic and potential energy. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential.[88] Over a whole cycle, or over many cycles, average energy is equally split between kinetic and potential. This is an example of the equipartition principle: the total energy of a system with many degrees of freedom is equally split among all available degrees of freedom, on average.[113] This principle is vitally important to understanding the behavior of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is part of the second law of thermodynamics. The second law of thermodynamics is simple only for systems which are near or in a physical equilibrium state. For non-equilibrium systems, the laws governing the systems' behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production.[114][115] It states that nonequilibrium systems behave in such a way as to maximize their entropy production.[116]",
      "ground_truth_chunk_ids": [
        "54_fixed_chunk1"
      ],
      "source_ids": [
        "S054"
      ],
      "category": "factual",
      "id": 5
    },
    {
      "question": "What is Volcano?",
      "ground_truth": "A volcano is commonly defined as a vent or fissure in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface.[1] On Earth, volcanoes are most often found where tectonic plates are diverging or converging, and because most of Earth's plate boundaries are underwater, most volcanoes are found underwater. For example, a mid-ocean ridge, such as the Mid-Atlantic Ridge, has volcanoes caused by divergent tectonic plates whereas the Pacific Ring of Fire has volcanoes caused by convergent tectonic plates. Volcanoes resulting from divergent tectonic activity are usually non-explosive whereas those resulting from convergent tectonic activity cause violent eruptions.[2][3] Volcanoes can also form where there is stretching and thinning of the crust's plates, such as in the East African Rift, the Wells Gray-Clearwater volcanic field, and the Rio Grande rift in North America. Volcanism away from plate boundaries most likely arises from upwelling diapirs from the core\u2013mantle boundary called mantle plumes, 3,000 kilometres (1,900 mi) deep within Earth. This results in hotspot volcanism or intraplate volcanism, in which the plume may cause thinning of the crust and result in a volcanic island chain due to the continuous movement of the tectonic plate, of which the Hawaiian hotspot is an example.[4] Volcanoes are usually not created at transform tectonic boundaries where two tectonic plates slide past one another. Volcanoes, based on their frequency of eruption or volcanism, are referred to as either active, dormant, or extinct.[5] Active volcanoes have a history of volcanism and are likely to erupt again, while extinct ones are not capable of eruption at all as they have no magma source. \"Dormant\" volcanoes have not erupted in a long time \u2013 generally accepted as since the start of the Holocene, about 12,000",
      "expected_answer": "A volcano is commonly defined as a vent or fissure in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface.[1] On Earth, volcanoes are most often found where tectonic plates are diverging or converging, and because most of Earth's plate boundaries are underwater, most volcanoes are found underwater. For example, a mid-ocean ridge, such as the Mid-Atlantic Ridge, has volcanoes caused by divergent tectonic plates whereas the Pacific Ring of Fire has volcanoes caused by convergent tectonic plates. Volcanoes resulting from divergent tectonic activity are usually non-explosive whereas those resulting from convergent tectonic activity cause violent eruptions.[2][3] Volcanoes can also form where there is stretching and thinning of the crust's plates, such as in the East African Rift, the Wells Gray-Clearwater volcanic field, and the Rio Grande rift in North America. Volcanism away from plate boundaries most likely arises from upwelling diapirs from the core\u2013mantle boundary called mantle plumes, 3,000 kilometres (1,900\u00a0mi) deep within Earth. This results in hotspot volcanism or intraplate volcanism, in which the plume may cause thinning of the crust and result in a volcanic island chain due to the continuous movement of the tectonic plate, of which the Hawaiian hotspot is an example.[4] Volcanoes are usually not created at transform tectonic boundaries where two tectonic plates slide past one another. Volcanoes, based on their frequency of eruption or volcanism, are referred to as either active, dormant, or extinct.[5] Active volcanoes have a history of volcanism and are likely to erupt again, while extinct ones are not capable of eruption at all as they have no magma source. \"Dormant\" volcanoes have not erupted in a long time \u2013 generally accepted as since the start of the Holocene, about 12,000 years ago \u2013 but may erupt again. However, dormant volcanoes are technically considered to be seismically \"active\".[5] These categories aren't entirely uniform; they may overlap for certain examples.[2][6][7] Large eruptions can affect atmospheric temperature as ash and droplets of sulfuric acid obscure the Sun and cool Earth's troposphere. Historically, large volcanic eruptions have been followed by volcanic winters which have caused catastrophic famines.[8] Other planets besides Earth have volcanoes. For example, volcanoes are very numerous on Venus.[9] Mars has significant volcanoes.[10] In 2009, a paper was published suggesting a new definition for the word 'volcano' that includes processes such as cryovolcanism. It suggested that a volcano be defined as 'an opening on a planet or moon's surface from which magma, as defined for that body, and/or magmatic gas is erupted.'[11] This article mainly covers volcanoes on Earth. See \u00a7\u00a0Volcanoes on other celestial bodies and cryovolcano for more information. The word volcano (UK: /v\u0252l\u02c8ke\u026an\u0259\u028a/; US: /v\u0251\u02d0l\u02c8ke\u026ano\u028a/) originates from the early 17th century, derived from the Italian name Vulcano, a volcanic island in the Aeolian Islands of Italy, which in turn comes from the Latin name Volc\u0101nus or Vulc\u0101nus, referring to Vulcan, the god of fire in Roman mythology.[12][13] The set of processes and phenomena involved in volcanic activity is called volcanism [early 19th century: from volcano + -ism]. The study of volcanism and volcanoes is called volcanology [mid-19th century: from volcano + -logy], sometimes spelled vulcanology.[12] According to the theory of plate tectonics, Earth's lithosphere, its rigid outer shell, is broken into sixteen larger and several smaller plates. These move continuously at a slow pace, due to convection in the underlying ductile mantle, and most volcanic activity on Earth takes place along plate boundaries, where plates are converging (and lithosphere is being destroyed) or are diverging (and new lithosphere is being created).[14] During the development of geological theory, certain concepts that allowed the grouping of volcanoes in time, place, structure and composition have developed that ultimately have had to be explained in the theory of plate tectonics. For example, some volcanoes are polygenetic with more than one period of activity during their history; other volcanoes that become extinct after erupting exactly once are monogenetic (meaning \"one life\") and such volcanoes are often grouped together in a geographical region.[15] At the mid-ocean ridges, two tectonic plates diverge from one another as hot mantle rock creeps upwards beneath the thinned oceanic crust. The decrease of pressure in the rising mantle rock leads to adiabatic expansion and the partial melting of the rock, causing volcanism and creating new oceanic crust. Most divergent plate boundaries are at the bottom of the oceans, and so most volcanic activity on Earth is submarine, forming new seafloor. Black smokers (also known as deep sea vents) are evidence of this kind of volcanic activity. Where the mid-oceanic ridge is above sea level, volcanic islands are formed, such as Iceland.[16][3] Subduction zones are places where two plates, usually an oceanic plate and a continental plate, collide. The oceanic plate subducts (dives beneath the continental plate), forming a deep ocean trench just offshore. In a process called flux melting, water released from the subducting plate lowers the melting temperature of the overlying mantle wedge, thus creating magma. This magma tends to be extremely viscous because of its high silica content, so it often does not reach the surface but cools and solidifies at depth. When it does reach the surface, however, a volcano is formed. Thus subduction zones are bordered by chains of volcanoes called volcanic arcs. Typical examples are the volcanoes in the Pacific Ring of Fire, such as the Cascade Volcanoes or the Japanese Archipelago, or the eastern islands of Indonesia.[17][2] Hotspots are volcanic areas thought to be formed by mantle plumes, which are hypothesized to be columns of hot material rising from the core-mantle boundary. As with mid-ocean ridges, the rising mantle rock experiences decompression melting which generates large volumes of magma. Because tectonic plates move across mantle plumes, each volcano becomes inactive as it drifts off the plume, and new volcanoes are created where the plate advances over the plume. The Hawaiian Islands are thought to have been formed in such a manner, as has the Snake River Plain, with the Yellowstone Caldera being part of the North American plate currently above the Yellowstone hotspot.[18][4] However, the mantle plume hypothesis has been questioned.[19] Sustained upwelling of hot mantle rock can develop under the interior of a continent and lead to rifting. Early stages of rifting are characterized by flood basalts and may progress to the point where a tectonic plate is completely split.[20][21] A divergent plate boundary then develops between the two halves of the split plate. However, rifting often fails to completely split the continental lithosphere (such as in an aulacogen), and failed rifts are characterized by volcanoes that erupt unusual alkali lava or carbonatites. Examples include the volcanoes of the East African Rift.[22] A volcano needs a reservoir of molten magma (e.g. a magma chamber), a conduit to allow magma to rise through the crust, and a vent to allow the magma to escape above the surface as lava. The erupted volcanic material (lava and tephra) that is deposited around the vent is known as a volcanic edifice, typically a volcanic cone or mountain.[2][23] The most common perception of a volcano is of a conical mountain, spewing lava and poisonous gases from a crater at its summit; however, this describes just one of the many types of volcano. The features of volcanoes are varied. The structure and behaviour of volcanoes depend on several factors. Some volcanoes have rugged peaks formed by lava domes rather than a summit crater while others have landscape features such as massive plateaus. Vents that issue volcanic material (including lava and ash) and gases (mainly steam and magmatic gases) can develop anywhere on the landform and may give rise to smaller cones such as Pu\u02bbu \u02bb\u014c\u02bb\u014d on a flank of K\u012blauea in Hawaii. Volcanic craters are not always at the top of a mountain or hill and may be filled with lakes such as with Lake Taup\u014d in New Zealand. Some volcanoes can be low-relief landform features, with the potential to be hard to recognize as such and be obscured by geological processes.[2][24][25] Other types of volcano include mud volcanoes, which are structures often not associated with known magmatic activity; and cryovolcanoes (or ice volcanoes), particularly on some moons of Jupiter, Saturn, and Neptune. Active mud volcanoes tend to involve temperatures much lower than those of igneous volcanoes except when the mud volcano is actually a vent of an igneous volcano. Volcanic fissure vents are generally found at diverging plate boundaries, they are flat, linear fractures through which basaltic lava emerges. These kinds of volcanoes are non-explosive and the basaltic lava tends to have a low viscosity and solidifies slowly leading to a gentle sloping basaltic lava plateau. They often relate or constitute shield volcanoes[2][26] Shield volcanoes, so named for their broad, shield-like profiles, are formed by the eruption of low-viscosity basaltic or andesitic lava that can flow a great distance from a vent. They generally do not explode catastrophically but are characterized by relatively gentle effusive eruptions.[2] Since low-viscosity magma is typically low in silica, shield volcanoes are more common in oceanic than continental settings. The Hawaiian volcanic chain is a series of shield cones, and they are common in Iceland, as well.[26] Olympus Mons, an extinct martian shield volcano is the largest known volcano in the Solar System.[27] Lava domes, also called dome volcanoes, have steep convex sides built by slow eruptions of highly viscous lava, for example, rhyolite.[2] They are sometimes formed within the crater of a previous volcanic eruption, as in the case of Mount St. Helens, but can also form independently, as in the case of Lassen Peak. Like stratovolcanoes, they can produce violent, explosive eruptions, but the lava generally does not flow far from the originating vent. Cryptodomes are formed when viscous lava is forced upward causing the surface to bulge. The 1980 eruption of Mount St. Helens was an example; lava beneath the surface of the mountain created an upward bulge, which later collapsed down the north side of the mountain. Cinder cones result from eruptions of mostly small pieces of scoria and pyroclastics (both resemble cinders, hence the name of this volcano type) that build up around the vent. These can be relatively short-lived eruptions that produce a cone-shaped hill perhaps 30 to 400 metres (100 to 1,300\u00a0ft) high. Most cinder cones erupt only once and some may be found in monogenetic volcanic fields that may include other features that form when magma comes into contact with water such as maar explosion craters and tuff rings.[28] Cinder cones may form as flank vents on larger volcanoes, or occur on their own. Par\u00edcutin in Mexico and Sunset Crater in Arizona are examples of cinder cones. In New Mexico, Caja del Rio is a volcanic field of over 60 cinder cones. Based on satellite images, it has been suggested that cinder cones might occur on other terrestrial bodies in the Solar system too; on the surface of Mars and the Moon.[29][30][31][32] Stratovolcanoes are tall conical mountains composed of lava flows and tephra in alternate layers, the strata that gives rise to the name. They are also known as composite volcanoes because they are created from multiple structures during different kinds of eruptions; the main conduit bringing magma to the surface branches into multiple secondary conduits and occasional laccoliths or sills, the branching conduits may form parasitic cones on the flanks of the main cone.[2] Classic examples include Mount Fuji in Japan, Mayon Volcano in the Philippines, and Mount Vesuvius and Stromboli in Italy. Ash produced by the explosive eruption of stratovolcanoes has historically posed the greatest volcanic hazard to civilizations. The lavas of stratovolcanoes are higher in silica, and therefore much more viscous, than lavas from shield volcanoes. High-silica lavas also tend to contain more dissolved gas. The combination is deadly, promoting explosive eruptions that produce great quantities of ash, as well as pyroclastic surges like the one that destroyed the city of Saint-Pierre in Martinique in 1902. They are also steeper than shield volcanoes, with slopes of 30\u201335\u00b0 compared to slopes of generally 5\u201310\u00b0, and their loose tephra are material for dangerous lahars.[33] Large pieces of tephra are called volcanic bombs. Big bombs can measure more than 1.2 metres (4\u00a0ft) across and weigh several tons.[34] A supervolcano is defined as a volcano that has experienced one or more eruptions that produced over 1,000 cubic kilometres (240\u00a0cu\u00a0mi) of volcanic deposits in a single explosive event.[35] Such eruptions occur when a very large magma chamber full of gas-rich, silicic magma is emptied in a catastrophic caldera-forming eruption. Ash flow tuffs emplaced by such eruptions are the only volcanic product with volumes rivalling those of flood basalts.[36] Supervolcano eruptions, while the most dangerous type, are very rare; four are known from the last million years, and about 60 historical VEI 8 eruptions have been identified in the geologic record over millions of years. A supervolcano can produce devastation on a continental scale, and severely cool global temperatures for many years after the eruption due to the huge volumes of sulfur and ash released into the atmosphere. Because of the enormous area they cover, and subsequent concealment under vegetation and glacial deposits, supervolcanoes can be difficult to identify in the geologic record without careful geological mapping.[37] Known examples include Yellowstone Caldera in Yellowstone National Park and Valles Caldera in New Mexico (both western United States); Lake Taup\u014d in New Zealand; Lake Toba in Sumatra, Indonesia; and Ngorongoro Crater in Tanzania. Volcanoes that, though large, are not large enough to be called supervolcanoes, may also form calderas (collapsed crater) in the same way. There may be active or dormant cones inside of the caldera or even a lake, such lakes are called Volcanogenic lakes, or simply, volcanic lakes.[38][2] Submarine volcanoes are common features of the ocean floor. Volcanic activity during the Holocene Epoch has been documented at only 119 submarine volcanoes, but there may be more than one million geologically young submarine volcanoes on the ocean floor.[39][40] In shallow water, active volcanoes disclose their presence by blasting steam and rocky debris high above the ocean's surface. In the deep ocean basins, the tremendous weight of the water prevents the explosive release of steam and gases; however, submarine eruptions can be detected by hydrophones and by the discoloration of water because of volcanic gases. Pillow lava is a common eruptive product of submarine volcanoes and is characterized by thick sequences of discontinuous pillow-shaped masses which form underwater. Even large submarine eruptions may not disturb the ocean surface, due to the rapid cooling effect and increased buoyancy in water (as compared to air), which often causes volcanic vents to form steep pillars on the ocean floor. Hydrothermal vents are common near these volcanoes, and some support peculiar ecosystems based on chemotrophs feeding on dissolved minerals. Over time, the formations created by submarine volcanoes may become so large that they break the ocean surface as new islands or floating pumice rafts. In May and June 2018, a multitude of seismic signals were detected by earthquake monitoring agencies all over the world. They took the form of unusual humming sounds, and some of the signals detected in November of that year had a duration of up to 20 minutes. An oceanographic research campaign in May 2019 showed that the previously mysterious humming noises were caused by the formation of a submarine volcano off the coast of Mayotte.[41] Subglacial volcanoes develop underneath ice caps. They are made up of lava plateaus capping extensive pillow lavas and palagonite. These volcanoes are also called table mountains, tuyas,[42] or (in Iceland) mobergs.[43] Very good examples of this type of volcano can be seen in Iceland and in British Columbia. The origin of the term comes from Tuya Butte, which is one of the several tuyas in the area of the Tuya River and Tuya Range in northern British Columbia. Tuya Butte was the first such landform analysed and so its name has entered the geological literature for this kind of volcanic formation.[44] The Tuya Mountains Provincial Park was recently established to protect this unusual landscape, which lies north of Tuya Lake and south of the Jennings River near the boundary with the Yukon Territory. Hydrothermal features, for example geysers, fumaroles, mud pools, mud volcanoes, hot springs and acidic hot springs involve water as well as geothermal or magmatic activity. Such features are common around volcanoes and are often indicative of volcanism.[2][45] Mud volcanoes or mud domes are conical structures created by eruption of liquids and gases, particularly mud (slurries), water and gases, although several activities may contribute. The largest mud volcanoes are 10 kilometres (6.2\u00a0mi) in diameter and reach 700 metres (2,300\u00a0ft) high.[46][47] Mud volcanoes can be seen off the shore of Indonesia, on the island of Baratang, in Balochistan and in central Asia. Fumaroles are vents on the surface from which hot steam and volcanic gases erupt due to the presence of superheated groundwater, these may indicate volcanic activity. Fumaroles erupting sulfurous gases are also often called solfataras.[48][2] Geysers are springs which will occasionally erupt and discharge hot water and steam. Geysers may indicate ongoing magmatism, water underground is heated by hot rocks and steam pressure builds up before being released along with a jet of hot water. Almost half of all active geysers are present in Yellowstone National Park, US.[2][49] The material that is expelled in a volcanic eruption can be classified into three types: The concentrations of different volcanic gases can vary considerably from one volcano to the next. Water vapour is typically the most abundant volcanic gas, followed by carbon dioxide[53] and sulfur dioxide. Other principal volcanic gases include hydrogen sulfide, hydrogen chloride, and hydrogen fluoride. A large number of minor and trace gases are also found in volcanic emissions, for example hydrogen, carbon monoxide, halocarbons, organic compounds, and volatile metal chlorides. The form and style of an eruption of a volcano is largely determined by the composition of the lava it erupts. The viscosity (how fluid the lava is) and the amount of dissolved gas are the most important characteristics of magma, and both are largely determined by the amount of silica in the magma. Magma rich in silica is much more viscous than silica-poor magma, and silica-rich magma also tends to contain more dissolved gases. Lava can be broadly classified into four different compositions:[54] Mafic lava flows show two varieties of surface texture: \u02bbA\u02bba (pronounced [\u02c8\u0294a\u0294a]) and p\u0101hoehoe ([pa\u02d0\u02c8ho.e\u02c8ho.e]), both Hawaiian words. \u02bbA\u02bba is characterized by a rough, clinkery surface and is the typical texture of cooler basalt lava flows. P\u0101hoehoe is characterized by its smooth and often ropey or wrinkly surface and is generally formed from more fluid lava flows. P\u0101hoehoe flows are sometimes observed to transition to \u02bba\u02bba flows as they move away from the vent, but never the reverse.[68] More silicic lava flows take the form of block lava, where the flow is covered with angular, vesicle-poor blocks. Rhyolitic flows typically consist largely of obsidian.[69] Tephra is made when magma inside the volcano is blown apart by the rapid expansion of hot volcanic gases. Magma commonly explodes as the gas dissolved in it comes out of solution as the pressure decreases when it flows to the surface. These violent explosions produce particles of material that can then fly from the volcano. Solid particles smaller than 2\u00a0mm in diameter (sand-sized or smaller) are called volcanic ash.[51][52] Tephra and other volcaniclastics (shattered volcanic material) make up more of the volume of many volcanoes than do lava flows. Volcaniclastics may have contributed as much as a third of all sedimentation in the geologic record. The production of large volumes of tephra is characteristic of explosive volcanism.[70] Through natural processes, mainly erosion, so much of the solidified erupted material that makes up the mantle of a volcano may be stripped away that its inner anatomy becomes apparent. Using the metaphor of biological anatomy, such a process is called \"dissection\".[71] When the volcano is extinct, a plug forms on its vent, over time due to erosion, the volcanic cone slowly erodes away leaving the resistant lava plug intact.[2] Cinder Hill, a feature of Mount Bird on Ross Island, Antarctica, is a prominent example of a dissected volcano. Volcanoes that were, on a geological timescale, recently active, such as for example Mount Kaimon in southern Ky\u016bsh\u016b, Japan, tend to be undissected. Devils Tower in Wyoming is a famous example of exposed volcanic plug. As of December\u00a02022[update], the Smithsonian Institution's Global Volcanism Program database of volcanic eruptions in the Holocene Epoch (the last 11,700 years) lists 9,901 confirmed eruptions from 859 volcanoes. The database also lists 1,113 uncertain eruptions and 168 discredited eruptions for the same time interval.[72][73] Eruption styles are broadly divided into magmatic, phreatomagmatic (hydrovolcanic), and phreatic eruptions.[74] The intensity of explosive volcanism is expressed using the volcanic explosivity index (VEI), which ranges from 0 for Hawaiian-type eruptions to 8 for supervolcanic eruptions:[75][76] Volcanoes vary greatly in their level of activity, with individual volcanic systems having an eruption recurrence ranging from several times a year to once in tens of thousands of years.[77] Volcanoes are informally described as erupting, active, dormant, or extinct, but the definitions of these terms are not entirely uniform among volcanologists. The level of activity of most volcanoes falls upon a graduated spectrum, with much overlap between categories, and does not always fit neatly into only one of these three separate categories.[6] The USGS defines a volcano as \"erupting\" whenever the ejection of magma from any point on the volcano is visible, including visible magma still contained within the walls of the summit crater. While there is no international consensus among volcanologists on how to define an active volcano, the USGS defines a volcano as active whenever subterranean indicators, such as earthquake swarms, ground inflation, or unusually high levels of carbon dioxide or sulfur dioxide are present.[78][79] The USGS defines a dormant volcano as any volcano that is not showing any signs of unrest such as earthquake swarms, ground swelling, or excessive noxious gas emissions, but which shows signs that it could yet become active again.[79] Many dormant volcanoes have not erupted for thousands of years, but have still shown signs that they may be likely to erupt again in the future.[80][81] Technically, any volcano that is dormant is also considered to be geologically \"active\".[5] In an article justifying the re-classification of Alaska's Mount Edgecumbe volcano from \"dormant\" to \"active\", volcanologists at the Alaska Volcano Observatory pointed out that the term \"dormant\" in reference to volcanoes has been deprecated over the past few decades and that \"[t]he term \"dormant volcano\" is so little used and undefined in modern volcanology that the Encyclopedia of Volcanoes (2000) does not contain it in the glossaries or index\",[82] however the USGS still widely employs the term. Previously a volcano was often considered to be extinct if there were no written records of its activity. Such a generalization is inconsistent with observation and deeper study, as has occurred recently with the unexpected eruption of the Chait\u00e9n volcano in 2008.[83] Modern volcanic activity monitoring techniques, and improvements in the modelling of the factors that produce eruptions, have helped the understanding of why volcanoes may remain dormant for a long time, and then become unexpectedly active again. The potential for eruptions, and their style, depend mainly upon the state of the magma storage system under the volcano, the eruption trigger mechanism and its timescale.[84]:\u200a95\u200a For example, the Yellowstone volcano has a repose/recharge period of around 700,000 years, and Toba of around 380,000 years.[85] Vesuvius was described by Roman writers as having been covered with gardens and vineyards before its unexpected eruption of 79 CE, which destroyed the towns of Herculaneum and Pompeii. Accordingly, it can sometimes be difficult to distinguish between an extinct volcano and a dormant (inactive) one. Long volcano dormancy is known to decrease awareness.[84]:\u200a96\u200a Pinatubo was an inconspicuous volcano, unknown to most people in the surrounding areas, and initially not seismically monitored before its unanticipated and catastrophic eruption of 1991. Two other examples of volcanoes that were once thought to be extinct, before springing back into eruptive activity were the long-dormant Soufri\u00e8re Hills volcano on the island of Montserrat, thought to be extinct until activity resumed in 1995 (turning its capital Plymouth into a ghost town) and Fourpeaked Mountain in Alaska, which, before its September 2006 eruption, had not erupted since before 8000 BCE. Another example is the Taftan volcano in southwestern Iran. This volcano was long thought by volcanologists to be extinct, with its last eruption having occurred an estimated 710,000 years ago. However, beginning around June 2023, the volcano began experiencing uplifting near its summit, suggesting that the volcano was dormant.[86] Extinct volcanoes are those that scientists consider unlikely to erupt again because the volcano no longer has a magma supply. Examples of extinct volcanoes are many volcanoes on the Hawaiian\u2013Emperor seamount chain in the Pacific Ocean (although some volcanoes at the eastern end of the chain are active), Hohentwiel in Germany, Shiprock in New Mexico, U.S., Capulin in New Mexico, U.S., Zuidwal volcano in the Netherlands, and many volcanoes in Italy such as Monte Vulture. Edinburgh Castle in Scotland is located atop an extinct volcano, which forms Castle Rock. Whether a volcano is truly extinct is often difficult to determine. Since \"supervolcano\" calderas can have eruptive lifespans sometimes measured in millions of years, a caldera that has not produced an eruption in tens of thousands of years may be considered dormant instead of extinct. An individual volcano in a monogenetic volcanic field can be extinct, but that does not mean a completely new volcano might not erupt close by with little or no warning, as its field may have an active magma supply. The three common popular classifications of volcanoes can be subjective and some volcanoes thought to have been extinct have erupted again. To help prevent people from falsely believing they are not at risk when living on or near a volcano, countries have adopted new classifications to describe the various levels and stages of volcanic activity.[87] Some alert systems use different numbers or colours to designate the different stages. Other systems use colours and words. Some systems use a combination of both. The Decade Volcanoes are 16 volcanoes identified by the International Association of Volcanology and Chemistry of the Earth's Interior (IAVCEI) as being worthy of particular study in light of their history of large, destructive eruptions and proximity to populated areas. They are named Decade Volcanoes because the project was initiated as part of the United Nations-sponsored International Decade for Natural Disaster Reduction (the 1990s). The 16 current Decade Volcanoes are: The Deep Earth Carbon Degassing Project, an initiative of the Deep Carbon Observatory, monitors nine volcanoes, two of which are Decade volcanoes. The focus of the Deep Earth Carbon Degassing Project is to use Multi-Component Gas Analyzer System instruments to measure CO2/SO2 ratios in real-time and in high-resolution to allow detection of the pre-eruptive degassing of rising magmas, improving prediction of volcanic activity.[88] Volcanic eruptions pose a significant threat to human civilization. However, volcanic activity has also provided humans with important resources. There are many different types of volcanic eruptions and associated activity: phreatic eruptions (steam-generated eruptions), explosive eruptions of high-silica lava (e.g., rhyolite), effusive eruptions of low-silica lava (e.g., basalt), sector collapses, pyroclastic flows, lahars (debris flows) and volcanic gas emissions. These can pose a hazard to humans. Earthquakes, hot springs, fumaroles, mud pots and geysers often accompany volcanic activity. Volcanic gases can reach the stratosphere, where they form sulfuric acid aerosols that can reflect solar radiation and lower surface temperatures significantly.[89] Sulfur dioxide from the eruption of Huaynaputina may have caused the Russian famine of 1601\u20131603.[90] Chemical reactions of sulfate aerosols in the stratosphere can also damage the ozone layer, and acids such as hydrogen chloride (HCl) and hydrogen fluoride (HF) can fall to the ground as acid rain. Excessive fluoride salts from eruptions have poisoned livestock in Iceland on multiple occasions.[91]:\u200a39\u201358\u200a Explosive volcanic eruptions release the greenhouse gas carbon dioxide and thus provide a deep source of carbon for biogeochemical cycles.[92] Ash thrown into the air by eruptions can present a hazard to aircraft, especially jet aircraft where the particles can be melted by the high operating temperature; the melted particles then adhere to the turbine blades and alter their shape, disrupting the operation of the turbine. This can cause major disruptions to air travel. A volcanic winter is thought to have taken place around 70,000 years ago after the supereruption of Lake Toba on Sumatra island in Indonesia.[93] This may have created a population bottleneck that affected the genetic inheritance of all humans today.[94] Volcanic eruptions may have contributed to major extinction events, such as the End-Ordovician, Permian-Triassic, and Late Devonian mass extinctions.[95] The 1815 eruption of Mount Tambora created global climate anomalies that became known as the \"Year Without a Summer\" because of the effect on North American and European weather.[96] The freezing winter of 1740\u201341, which led to widespread famine in northern Europe, may also owe its origins to a volcanic eruption.[97] Although volcanic eruptions pose considerable hazards to humans, past volcanic activity has created important economic resources. Tuff formed from volcanic ash is a relatively soft rock, and it has been used for construction since ancient times.[98][99] The Romans often used tuff, which is abundant in Italy, for construction.[100] The Rapa Nui people used tuff to make most of the moai statues in Easter Island.[101] Volcanic ash and weathered basalt produce some of the most fertile soil in the world, rich in nutrients such as iron, magnesium, potassium, calcium, and phosphorus.[102] Volcanic activity is responsible for emplacing valuable mineral resources, such as metal ores.[102] It is accompanied by high rates of heat flow from Earth's interior. These can be tapped as geothermal power.[102] Tourism associated with volcanoes is also a worldwide industry.[103] Many volcanoes near human settlements are heavily monitored with the aim of providing adequate advance warnings of imminent eruptions to nearby populations. Also, a better modern-day understanding of volcanology has led to some better informed governmental and public responses to unanticipated volcanic activities. While the science of volcanology may not yet be capable of predicting the exact times and dates of eruptions far into the future, on suitably monitored volcanoes the monitoring of ongoing volcanic indicators is often capable of predicting imminent eruptions with advance warnings minimally of hours, and usually of days prior to any eruptions.[104] The diversity of volcanoes and their complexities mean that eruption forecasts for the foreseeable future will be based on probability, and the application of risk management. Even then, some eruptions will have no useful warning. An example of this occurred in March 2017, when a tourist group was witnessing a presumed to be predictable Mount Etna eruption and the flowing lava came in contact with a snow accumulation causing a situational phreatic explosion causing injury to ten persons.[103] Other types of significant eruptions are known to give useful warnings of only hours at the most by seismic monitoring.[83] The recent demonstration of a magma chamber with repose times of tens of thousands of years, with potential for rapid recharge so potentially decreasing warning times, under the youngest volcano in central Europe,[84] does not tell us if more careful monitoring will be useful. Scientists are known to perceive risk, with its social elements, differently from local populations and those that undertake social risk assessments on their behalf, so that both disruptive false alarms and retrospective blame, when disasters occur, will continue to happen.[105]:\u200a1\u20133 Thus in many cases, while volcanic eruptions may still cause major property destruction, the periodic large-scale loss of human life that was once associated with many volcanic eruptions, has recently been significantly reduced in areas where volcanoes are adequately monitored. This life-saving ability is derived via such volcanic-activity monitoring programs, through the greater abilities of local officials to facilitate timely evacuations based upon the greater modern-day knowledge of volcanism that is now available, and upon improved communications technologies such as cell phones. Such operations tend to provide enough time for humans to escape at least with their lives before a pending eruption. One example of such a recent successful volcanic evacuation was the Mount Pinatubo evacuation of 1991. This evacuation is believed to have saved 20,000 lives.[106] In the case of Mount Etna, a 2021 review found 77 deaths due to eruptions since 1536 but none since 1987.[103] Citizens who may be concerned about their own exposure to risk from nearby volcanic activity should familiarize themselves with the types of, and quality of, volcano monitoring and public notification procedures being employed by governmental authorities in their areas.[107] Earth's Moon has no large volcanoes and no current volcanic activity, although recent evidence suggests it may still possess a partially molten core.[108] However, the Moon does have many volcanic features such as maria[109] (the darker patches seen on the Moon), rilles[110] and domes.[111] The planet Venus has a surface that is 90% basalt, indicating that volcanism played a major role in shaping its surface. The planet may have had a major global resurfacing event about 500 million years ago,[112] from what scientists can tell from the density of impact craters on the surface. Lava flows are widespread and forms of volcanism not present on Earth occur as well. Changes in the planet's atmosphere and observations of lightning have been attributed to ongoing volcanic eruptions, although there is no confirmation of whether or not Venus is still volcanically active. However, radar sounding by the Magellan probe revealed evidence for comparatively recent volcanic activity at Venus's highest volcano Maat Mons, in the form of ash flows near the summit and on the northern flank.[113] However, the interpretation of the flows as ash flows has been questioned.[114] There are several extinct volcanoes on Mars, four of which are vast shield volcanoes far bigger than any on Earth. They include Arsia Mons, Ascraeus Mons, Hecates Tholus, Olympus Mons, and Pavonis Mons. These volcanoes have been extinct for many millions of years,[115] but the European Mars Express spacecraft has found evidence that volcanic activity may have occurred on Mars in the recent past as well.[115] Jupiter's moon Io is the most volcanically active object in the Solar System because of tidal interaction with Jupiter. It is covered with volcanoes that erupt sulfur, sulfur dioxide and silicate rock, and as a result, Io is constantly being resurfaced. Its lavas are the hottest known anywhere in the Solar System, with temperatures exceeding 1,800 K (1,500\u00a0\u00b0C). In February 2001, the largest recorded volcanic eruptions in the Solar System occurred on Io.[116] Europa, the smallest of Jupiter's Galilean moons, also appears to have an active volcanic system, except that its volcanic activity is entirely in the form of water, which freezes into ice on the frigid surface. This process is known as cryovolcanism, and is apparently most common on the moons of the outer planets of the Solar System.[117] In 1989, the Voyager 2 spacecraft observed cryovolcanoes (ice volcanoes) on Triton, a moon of Neptune, and in 2005 the Cassini\u2013Huygens probe photographed fountains of frozen particles erupting from Enceladus, a moon of Saturn.[118][119] The ejecta may be composed of water, liquid nitrogen, ammonia, dust, or methane compounds. Cassini\u2013Huygens also found evidence of a methane-spewing cryovolcano on the Saturnian moon Titan, which is believed to be a significant source of the methane found in its atmosphere.[120] It is theorized that cryovolcanism may also be present on the Kuiper Belt Object Quaoar. A 2010 study of the exoplanet COROT-7b, which was detected by transit in 2009, suggested that tidal heating from the host star very close to the planet and neighbouring planets could generate intense volcanic activity similar to that found on Io.[121] Volcanoes are not distributed evenly over the Earth's surface but active ones with significant impact were encountered early in human history, evidenced by footprints of hominina found in East African volcanic ash dated at 3.66 million years old.[122]:\u200a104\u200a The association of volcanoes with fire and disaster is found in many oral traditions and had religious and thus social significance before the first written record of concepts related to volcanoes. Examples are: (1) the stories in the Athabascan subcultures about humans living inside mountains and a woman who uses fire to escape from a mountain,[123]:\u200a135\u200a (2) Pele's migration through the Hawarian island chain, ability to destroy forests and manifestations of the god's temper,[124] and (3) the association in Javanese folklore of a king resident in Mount Merapi volcano and a queen resident at a beach 50\u00a0km (31\u00a0mi) away on what is now known to be an earthquake fault that interacts with that volcano.[125] Many ancient accounts ascribe volcanic eruptions to supernatural causes, such as the actions of gods or demigods. The earliest known such example is a neolithic goddess at \u00c7atalh\u00f6y\u00fck.[126]:\u200a203\u200a The Ancient Greek god Hephaistos and the concepts of the underworld are aligned to volcanoes in that Greek culture.[103] However, others proposed more natural (but still incorrect) causes of volcanic activity. In the fifth century BC, Anaxagoras proposed eruptions were caused by a great wind.[127] By 65\u00a0CE, Seneca the Younger proposed combustion as the cause,[127] an idea also adopted by the Jesuit Athanasius Kircher (1602\u20131680), who witnessed eruptions of Mount Etna and Stromboli, then visited the crater of Vesuvius and published his view of an Earth in Mundus Subterraneus with a central fire connected to numerous others depicting volcanoes as a type of safety valve.[128] Edward Jorden, in his work on mineral waters, challenged this view; in 1632 he proposed sulfur \"fermentation\" as a heat source within Earth,[127] Astronomer Johannes Kepler (1571\u20131630) believed volcanoes were ducts for Earth's tears.[129][better\u00a0source\u00a0needed] In 1650, Ren\u00e9 Descartes proposed the core of Earth was incandescent and, by 1785, the works of Decartes and others were synthesized into geology by James Hutton in his writings about igneous intrusions of magma.[127] Lazzaro Spallanzani had demonstrated by 1794 that steam explosions could cause explosive eruptions and many geologists held this as the universal cause of explosive eruptions up to the 1886 eruption of Mount Tarawera which allowed in one event differentiation of the concurrent phreatomagmatic and hydrothermal eruptions from dry explosive eruption, of, as it turned out, a basalt dyke.[130]:\u200a16\u201318\u200a[131]:\u200a4\u200a Alfred Lacroix built upon his other knowledge with his studies on the 1902 eruption of Mount Pel\u00e9e,[127] and by 1928 Arthur Holmes work had brought together the concepts of radioactive generation of heat, Earth's mantle structure, partial decompression melting of magma, and magma convection.[127] This eventually led to the acceptance of plate tectonics.[132]",
      "ground_truth_chunk_ids": [
        "73_fixed_chunk1"
      ],
      "source_ids": [
        "S073"
      ],
      "category": "factual",
      "id": 6
    },
    {
      "question": "What is List of storms named Brenda?",
      "ground_truth": "The name Brenda has been used for nine tropical cyclones worldwide, including five in the Atlantic Ocean. In the Atlantic: In the Western Pacific Ocean: In the South-West Indian: In the Australian region:",
      "expected_answer": "The name Brenda has been used for nine tropical cyclones worldwide, including five in the Atlantic Ocean. In the Atlantic: In the Western Pacific Ocean: In the South-West Indian: In the Australian region:",
      "ground_truth_chunk_ids": [
        "116_random_chunk1"
      ],
      "source_ids": [
        "S316"
      ],
      "category": "factual",
      "id": 7
    },
    {
      "question": "What is The Konstantinos Staikos' book collection?",
      "ground_truth": "The book collection of Konstantinos Staikos is now part of the Alexander S. Onassis Public Benefit Foundation Library[1][2] It is centered on the intellectual, printing and publishing activity of the Greeks from the Fall of Constantinople in 1453 to the late 19th century. The aim of its creation was to collect and present relevant material from that time period. The genesis of the book collection dates from the 1970s. The bibliophilic interests of Konstantinos Staikos changed radically. In those years also, the Hellenic Bibliophile Society was established [3] under the Honorary Presidency of Constantinos Tsatsos. The exhibitions of books of the Society (1975) with travellers' accounts: 'Travellers in Greece from the fifteenth century to 1821', or with printed material regarding the chronicle of Greek typography: 'Outset of Greek typography' (1976) radically altered Konstantinos Staikos interests as collector and from then on he consciously turned to the study and research of the pioneers of Greek printing and the relations they cultivated with the world of books in Venice and elsewhere. His acquaintance with Georgios Ladas, who was profoundly conscious of the role played by printed books during the Ottoman domination and who collected and documented the bibliographic identity of an enormous number of books that came into his hands, empowered Konstantinos Staikos' intention to explore the chronicle of Greek typography in greater depth. The initial approach was to record printers' marks and emblems characterizing printed Greek books, resulting in the planning of the Charta of Greek Printing. At the same time the collection began to take shape, with the purchase of books entirely compatible with the terms regulating the Hellenic Bibliography as recorded by \u00c9. Legrand, printed material, that is to say, testifying to the pains and labours of the printing workshops. From 1986 the most representative body of the Konstantinos",
      "expected_answer": "The book collection of Konstantinos Staikos is now part of the Alexander S. Onassis Public Benefit Foundation Library[1][2] It is centered on the intellectual, printing and publishing activity of the Greeks from the Fall of Constantinople in 1453 to the late 19th century. The aim of its creation was to collect and present relevant material from that time period. The genesis of the book collection dates from the 1970s. The bibliophilic interests of Konstantinos Staikos changed radically. In those years also, the Hellenic Bibliophile Society was established [3] under the Honorary Presidency of Constantinos Tsatsos. The exhibitions of books of the Society (1975) with travellers' accounts: 'Travellers in Greece from the fifteenth century to 1821', or with printed material regarding the chronicle of Greek typography: 'Outset of Greek typography' (1976) radically altered Konstantinos Staikos interests as collector and from then on he consciously turned to the study and research of the pioneers of Greek printing and the relations they cultivated with the world of books in Venice and elsewhere. His acquaintance with Georgios Ladas, who was profoundly conscious of the role played by printed books during the Ottoman domination and who collected and documented the bibliographic identity of an enormous number of books that came into his hands, empowered Konstantinos Staikos' intention to explore the chronicle of Greek typography in greater depth. The initial approach was to record printers' marks and emblems characterizing printed Greek books, resulting in the planning of the Charta of Greek Printing. At the same time the collection began to take shape, with the purchase of books entirely compatible with the terms regulating the Hellenic Bibliography as recorded by \u00c9. Legrand, printed material, that is to say, testifying to the pains and labours of the printing workshops. From 1986 the most representative body of the Konstantinos Staikos collection, covering the works and the days of Greek scholars and printers active in the period of the Italian Renaissance (late fourteenth \u2013 mid-sixteenth centuries) became the object of exhibitions for the promotion of their work. First editions by Manuel Chrysoloras, George of Trebizond, Cardinal Bessarion, Theodoros Gazis, Zacharias Kallierges, Nikolaos Vlastos and numerous others were presented successively in Florence (1986); the Benaki Museum (1987); Geneva University (1988); Strasburg (1989) and elsewhere. These exhibitions were accompanied by detailed bilingual catalogues, compiled in collaboration with M.I. Manoussakas, with introductory notes and extensive commentaries for each book. The ultimate goal of these exhibitions was the promotion of the inestimable and decisive contribution of the Greek scholars of the period to the diffusion of Greek letters and to demonstrate: the relations they cultivated with the supreme Humanists of Italy, many of whom had been their pupils. Examples from the collection were exhibited at the Hellenic Institute of Byzantine and Post-Byzantine Studies in Venice in 1993, with landmark editions by Aldus Manutius, the products of literary editors by renowned Greek scholars such as Marcus Musurus and Ioannes Gregoropoulos. In Austria, at Vienna's Imperial Library nearly all the Greek books published/printed there (1749\u20131800) were exhibited, which were the most significant examples of the Neohellenic Enlightenment. In celebration of the Five Hundred Years since the establishment of the first Greek printing press (Venice 1499), the Greek Parliament Foundation assigned to Triantafyllos Sklavenitis and Konstantinos Staikos the organization of an exhibition of the most important material of the whole period: a considerable number of incunables and printed material deriving for the greater part from his library. In 2010 the Collection was acquired by the Onassis Foundation in order to be preserved as perpetual property of the Greek Nation.",
      "ground_truth_chunk_ids": [
        "10_random_chunk1"
      ],
      "source_ids": [
        "S210"
      ],
      "category": "factual",
      "id": 8
    },
    {
      "question": "What is Mineral?",
      "ground_truth": "In geology and mineralogy, a mineral or mineral species is, broadly speaking, a solid substance with a fairly well-defined chemical composition and a specific crystal structure that occurs naturally in pure form.[1][2] The geological definition of mineral normally excludes compounds that occur only in living organisms. However, some minerals are often biogenic (such as calcite) or chemically organic compounds (such as mellite). Moreover, living organisms often synthesize inorganic minerals (such as hydroxylapatite) that also occur in rocks. The concept of mineral is distinct from rock, which is any bulk solid geologic material that is relatively homogeneous at a large enough scale. A rock may consist of one type of mineral or may be an aggregate of two or more different types of minerals, spacially segregated into distinct phases.[3] Some natural solid substances without a definite crystalline structure, such as opal or obsidian, are more properly called mineraloids.[4] If a chemical compound occurs naturally with different crystal structures, each structure is considered a different mineral species. Thus, for example, quartz and stishovite are two different minerals consisting of the same compound, silicon dioxide. The International Mineralogical Association (IMA) is the generally recognized standard body for the definition and nomenclature of mineral species. As of May 2025[update], the IMA recognizes 6,145 official mineral species.[5] The chemical composition of a named mineral species may vary somewhat because the inclusion of small amounts of impurities. Specific varieties of a species sometimes have conventional or official names of their own.[6] For example, amethyst is a purple variety of the mineral species quartz. Some mineral species can have variable proportions of two or more chemical elements that occupy equivalent positions in the mineral's structure; for example, the formula of mackinawite is given as (Fe,Ni)9S8, meaning FexNi9-xS8, where x is a variable number between 0 and 9.",
      "expected_answer": "In geology and mineralogy, a mineral or mineral species is, broadly speaking, a solid substance with a fairly well-defined chemical composition and a specific crystal structure that occurs naturally in pure form.[1][2] The geological definition of mineral normally excludes compounds that occur only in living organisms. However, some minerals are often biogenic (such as calcite) or chemically organic compounds (such as mellite). Moreover, living organisms often synthesize inorganic minerals (such as hydroxylapatite) that also occur in rocks. The concept of mineral is distinct from rock, which is any bulk solid geologic material that is relatively homogeneous at a large enough scale. A rock may consist of one type of mineral or may be an aggregate of two or more different types of minerals, spacially segregated into distinct phases.[3] Some natural solid substances without a definite crystalline structure, such as opal or obsidian, are more properly called mineraloids.[4] If a chemical compound occurs naturally with different crystal structures, each structure is considered a different mineral species. Thus, for example, quartz and stishovite are two different minerals consisting of the same compound, silicon dioxide. The International Mineralogical Association (IMA) is the generally recognized standard body for the definition and nomenclature of mineral species. As of May\u00a02025[update], the IMA recognizes 6,145 official mineral species.[5] The chemical composition of a named mineral species may vary somewhat because the inclusion of small amounts of impurities.  Specific varieties of a species sometimes have conventional or official names of their own.[6] For example, amethyst is a purple variety of the mineral species quartz.  Some mineral species can have variable proportions of two or more chemical elements that occupy equivalent positions in the mineral's structure; for example, the formula of mackinawite is given as (Fe,Ni)9S8, meaning FexNi9-xS8, where x is a variable number between 0 and 9.  Sometimes a mineral with variable composition is split into separate species, more or less arbitrarily, forming a mineral group; that is the case of the silicates CaxMgyFe2-x-ySiO4, the olivine group. Besides the essential chemical composition and crystal structure, the description of a mineral species usually includes its common physical properties such as  habit, hardness, lustre, diaphaneity, colour, streak, tenacity, cleavage, fracture, system, zoning, parting, specific gravity, magnetism, fluorescence, radioactivity, as well as its taste or smell and its reaction to acid.[7][8] Minerals are classified by key chemical constituents; the two dominant systems are the Dana classification and the Strunz classification. Silicate minerals comprise approximately 90% of the Earth's crust.[9][10] Other important mineral groups include the native elements (made up of a single pure element) and compounds (combinations of multiple elements) namely sulfides (e.g. Galena PbS), oxides (e.g. quartz SiO2), halides (e.g. rock salt NaCl), carbonates (e.g. calcite CaCO3), sulfates (e.g. gypsum CaSO4\u00b72H2O), silicates (e.g. orthoclase KAlSi3O8), molybdates (e.g. wulfenite PbMoO4) and phosphates (e.g. pyromorphite Pb5(PO4)3Cl).[7] The International Mineralogical Association has established the following requirements for a substance to be considered a distinct mineral:[11][12] The details of these rules are somewhat controversial.[15] For instance, there have been several recent proposals to classify amorphous substances as minerals, but they have not been accepted by the IMA. The IMA is also reluctant to accept minerals that occur naturally only in the form of nanoparticles a few hundred atoms across, but has not defined a minimum crystal size.[11] Some authors require the material to be a stable or metastable solid at room temperature (25\u00a0\u00b0C).[15]  However, the IMA only requires that the substance be stable enough for its structure and composition to be well-determined. For example, it recognizes meridianiite (a naturally occurring hydrate of magnesium sulfate) as a mineral, even though it is formed and stable only below 2\u00a0\u00b0C. As of May\u00a02025[update], 6,145 mineral species are approved by the IMA.[5] They are most commonly named after a person, followed by discovery location; names based on chemical composition or physical properties are the two other major groups of mineral name etymologies.[18][19]  Most names end in \"-ite\"; the exceptions are usually names that were well-established before the organization of mineralogy as a discipline, for example galena and diamond. A topic of contention among geologists and mineralogists has been the IMA's decision to exclude biogenic crystalline substances. For example, Lowenstam (1981) stated that \"organisms are capable of forming a diverse array of minerals, some of which cannot be formed inorganically in the biosphere.\"[20] Skinner (2005) views all solids as potential minerals and includes biominerals in the mineral kingdom, which are those that are created by the metabolic activities of organisms. Skinner expanded the previous definition of a mineral to classify \"element or compound, amorphous or crystalline, formed through biogeochemical  processes,\" as a mineral.[21] Recent advances in high-resolution genetics and X-ray absorption spectroscopy are providing revelations on the biogeochemical relations between microorganisms and minerals that may shed new light on this question.[12][21] For example, the IMA-commissioned \"Working Group on Environmental Mineralogy and Geochemistry \" deals with minerals in the hydrosphere, atmosphere, and biosphere.[22] The group's scope includes mineral-forming microorganisms, which exist on nearly every rock, soil, and particle surface spanning the globe to depths of at least 1600 metres below the sea floor and 70 kilometres into the stratosphere (possibly entering the mesosphere).[23][24][25] Biogeochemical cycles have contributed to the formation of minerals for billions of years. Microorganisms can precipitate metals from solution, contributing to the formation of ore deposits. They can also catalyze the dissolution of minerals.[26][27][28] Prior to the International Mineralogical Association's listing, over 60 biominerals had been discovered, named, and published.[29] These minerals (a sub-set tabulated in Lowenstam (1981)[20]) are considered minerals proper according to Skinner's (2005) definition.[21] These biominerals are not listed in the International Mineral Association official list of mineral names;[30]  however, many of these biomineral representatives are distributed amongst the 78 mineral classes listed in the Dana classification scheme.[21] Skinner's (2005) definition of a mineral takes this matter into account by stating that a mineral can be crystalline or amorphous.[21] Although biominerals are not the most common form of minerals,[31] they help to define the limits of what constitutes a mineral proper. Nickel's (1995) formal definition explicitly mentioned crystallinity as a key to defining a substance as a mineral. A 2011 article defined icosahedrite, an aluminium-iron-copper alloy, as a mineral; named for its unique natural icosahedral symmetry, it is a quasicrystal. Unlike a true crystal, quasicrystals are ordered but not periodic.[32][33] A mineral assemblage is defined by Mindat.org as \"Any set of minerals in a rock, whether in [chemical] equilibrium or not\",[34] while Encyclopaedia Britannica says \"The term assemblage is frequently applied to all minerals included in a rock but more appropriately should be used for those minerals that are in equilibrium (and are known more specifically as the equilibrium assemblage)\".[35] The term is often prefixed by other terms that describe its formation.[34] A rock is an aggregate of one or more minerals[36] or mineraloids. Some rocks, such as limestone or quartzite, are composed primarily of one mineral\u00a0\u2013 calcite or aragonite in the case of limestone, and quartz in the latter case.[37][38] Other rocks can be defined by relative abundances of key (essential) minerals; a granite is defined by proportions of quartz, alkali feldspar, and plagioclase feldspar.[39] The other minerals in the rock are termed accessory minerals, and do not greatly affect the bulk composition of the rock. Rocks can also be composed entirely of non-mineral material; coal is a sedimentary rock composed primarily of organically derived carbon.[36][40] In rocks, some mineral species and groups are much more abundant than others; these are termed the rock-forming minerals. The major examples of these are quartz, the feldspars, the micas, the amphiboles, the pyroxenes, the olivines, and calcite; except for the last one, all of these minerals are silicates.[41] Overall, around 150 minerals are considered particularly important, whether in terms of their abundance or aesthetic value in terms of collecting.[42] Commercially valuable minerals and rocks, other than gemstones, metal ores, or mineral fuels, are referred to as industrial minerals.[43] For example, muscovite, a white mica, can be used for windows (sometimes referred to as isinglass), as a filler, or as an insulator.[44] Ores are minerals that have a high concentration of a certain element, typically a metal. Examples are cinnabar (HgS), an ore of mercury; sphalerite (ZnS), an ore of zinc; cassiterite (SnO2), an ore of tin; and colemanite, an ore of boron. Gems are minerals with an ornamental value, and are distinguished from non-gems by their beauty, durability, and usually, rarity. There are about 20 mineral species that qualify as gem minerals, which constitute about 35 of the most common gemstones. Gem minerals are often present in several varieties, and so one mineral can account for several different gemstones; for example, ruby and sapphire are both corundum, Al2O3.[45] The first known use of the word \"mineral\" in the English language (Middle English) was the 15th century.  The word came from Medieval Latin: minerale, from minera, mine, ore.[46] The word \"species\" comes from the Latin species, \"a particular sort, kind, or type with distinct look, or appearance\".[47] The abundance and diversity of minerals is controlled directly by their chemistry, in turn dependent on elemental abundances in the Earth. The majority of minerals observed are derived from the Earth's crust. Eight elements account for most of the key components of minerals, due to their abundance in the crust. These eight elements, summing to over 98% of the crust by weight, are, in order of decreasing abundance: oxygen, silicon, aluminium, iron, magnesium, calcium, sodium and potassium. Oxygen and silicon are by far the two most important\u00a0\u2013 oxygen composes 47% of the crust by weight, and silicon accounts for 28%.[48] The minerals that form are those that are most stable at the temperature and pressure of formation, within the limits imposed by the bulk chemistry of the parent body.[49] For example, in most igneous rocks, the aluminium and alkali metals (sodium and potassium) that are present are  primarily found in combination with oxygen, silicon, and calcium as feldspar minerals. However, if the rock is unusually rich in alkali metals, there will not be enough aluminium to combine with all the sodium as feldspar, and the excess sodium will form sodic amphiboles such as riebeckite. If the aluminium abundance is unusually high, the excess aluminium will form muscovite or other aluminium-rich minerals.[50] If silicon is deficient, part of the feldspar will be replaced by feldspathoid minerals.[51] Precise predictions of which minerals will be present in a rock of a particular composition formed at a particular temperature and pressure requires complex thermodynamic calculations. However, approximate estimates may be made using relatively simple rules of thumb, such as the CIPW norm, which gives reasonable estimates for volcanic rock formed from dry magma.[52] The chemical composition may vary between end member species of a solid solution series. For example, the plagioclase feldspars comprise a continuous series from sodium-rich end member albite (NaAlSi3O8) to calcium-rich anorthite (CaAl2Si2O8) with four recognized intermediate varieties between them (given in order from sodium- to calcium-rich): oligoclase, andesine, labradorite, and bytownite.[53] Other examples of series include the olivine series of magnesium-rich forsterite and iron-rich fayalite, and the wolframite series of manganese-rich h\u00fcbnerite and iron-rich ferberite.[54] Chemical substitution and coordination polyhedra explain this common feature of minerals. In nature, minerals are not pure substances, and are contaminated by whatever other elements are present in the given chemical system. As a result, it is possible for one element to be substituted for another.[55] Chemical substitution will occur between ions of a similar size and charge; for example, K+ will not substitute for Si4+ because of chemical and structural incompatibilities caused by a big difference in size and charge. A common example of chemical substitution is that of Si4+ by Al3+, which are close in charge, size, and abundance in the crust. In the example of plagioclase, there are three cases of substitution. Feldspars are all framework silicates, which have a silicon-oxygen ratio of 2:1, and the space for other elements is given by the substitution of Si4+ by Al3+ to give a base unit of [AlSi3O8]\u2212; without the substitution, the formula would be charge-balanced as SiO2, giving quartz.[56] The significance of this structural property will be explained further by coordination polyhedra. The second substitution occurs between Na+ and Ca2+; however, the difference in charge has to accounted for by making a second substitution of Si4+ by Al3+.[57] Coordination polyhedra are geometric representations of how a cation is surrounded by an anion. In mineralogy, coordination polyhedra are usually considered in terms of oxygen, due its abundance in the crust. The base unit of silicate minerals is the silica tetrahedron\u00a0\u2013 one Si4+ surrounded by four O2\u2212. An alternate way of describing the coordination of the silicate is by a number: in the case of the silica tetrahedron, the silicon is said to have a coordination number of 4. Various cations have a specific range of possible coordination numbers; for silicon, it is almost always 4, except for very high-pressure minerals where the compound is compressed such that silicon is in six-fold (octahedral) coordination with oxygen. Bigger cations have a bigger coordination numbers because of the increase in relative size as compared to oxygen (the last orbital subshell of heavier atoms is different too). Changes in coordination numbers leads to physical and mineralogical differences; for example, at high pressure, such as in the mantle, many minerals, especially silicates such as olivine and garnet, will change to a perovskite structure, where silicon is in octahedral coordination. Other examples are the aluminosilicates kyanite, andalusite, and sillimanite (polymorphs, since they share the formula Al2SiO5), which differ by the coordination number of the Al3+; these minerals transition from one another as a response to changes in pressure and temperature.[48] In the case of silicate materials, the substitution of Si4+ by Al3+ allows for a variety of minerals because of the need to balance charges.[58] Because the eight most common elements make up over 98% of the Earth's crust, the small quantities of the other elements that are typically present are substituted into the common rock-forming minerals. The distinctive minerals of most elements are quite rare, being found only where these elements have been concentrated by geological processes, such as hydrothermal circulation, to the point where they can no longer be accommodated in common minerals.[59] Changes in temperature and pressure and composition alter the mineralogy of a rock sample. Changes in composition can be caused by processes such as weathering or metasomatism (hydrothermal alteration). Changes in temperature and pressure occur when the host rock undergoes tectonic or magmatic movement into differing physical regimes. Changes in thermodynamic conditions make it favourable for mineral assemblages to react with each other to produce new minerals; as such, it is possible for two rocks to have an identical or a very similar bulk rock chemistry without having a similar mineralogy. This process of mineralogical alteration is related to the rock cycle. An example of a series of mineral reactions is illustrated as follows.[60] Orthoclase feldspar (KAlSi3O8) is a mineral commonly found in granite, a plutonic igneous rock. When exposed to weathering, it reacts to form kaolinite (Al2Si2O5(OH)4, a sedimentary mineral, and silicic acid): Under low-grade metamorphic conditions, kaolinite reacts with quartz to form pyrophyllite (Al2Si4O10(OH)2): As metamorphic grade increases, the pyrophyllite reacts to form kyanite and quartz: Alternatively, a mineral may change its crystal structure as a consequence of changes in temperature and pressure without reacting. For example, quartz will change into a variety of its SiO2 polymorphs, such as tridymite and cristobalite at high temperatures, and coesite at high pressures.[61] Classifying minerals ranges from simple to difficult. A mineral can be identified by several physical properties, some of them being sufficient for full identification without equivocation. In other cases, minerals can only be classified by more complex optical, chemical or X-ray diffraction analysis; these methods, however, can be costly and time-consuming.  Physical properties applied for classification include crystal structure and habit, hardness, lustre, diaphaneity, colour, streak, cleavage and fracture, and specific gravity. Other less general tests include fluorescence, phosphorescence, magnetism, radioactivity, tenacity (response to mechanical induced changes of shape or form), piezoelectricity and reactivity to dilute acids.[62] Crystal structure results from the orderly geometric spatial arrangement of atoms in the internal structure of a mineral. This crystal structure is based on regular internal atomic or ionic arrangement that is often expressed in the geometric form that the crystal takes. Even when the mineral grains are too small to see or are irregularly shaped, the underlying crystal structure is always periodic and can be determined by X-ray diffraction.[15] Minerals are typically described by their symmetry content. Crystals are restricted to 32 point groups, which differ by their symmetry. These groups are classified in turn into more broad categories, the most encompassing of these being the six crystal families.[63] These families can be described by the relative lengths of the three crystallographic axes, and the angles between them; these relationships correspond to the symmetry operations that define the narrower point groups. They are summarized below; a, b, and c represent the axes, and \u03b1, \u03b2, \u03b3 represent the angle opposite the respective crystallographic axis (e.g. \u03b1 is the angle opposite the a-axis, viz. the angle between the b and c axes):[63] The hexagonal crystal family is also split into two crystal systems\u00a0\u2013 the trigonal, which has a three-fold axis of symmetry, and the hexagonal, which has a six-fold axis of symmetry. Chemistry and crystal structure together define a mineral. With a restriction to 32 point groups, minerals of different chemistry may have identical crystal structure. For example, halite (NaCl), galena (PbS), and periclase (MgO) all belong to the hexaoctahedral point group (isometric family), as they have a similar stoichiometry between their different constituent elements. In contrast, polymorphs are groupings of minerals that share a chemical formula but have a different structure. For example, pyrite and marcasite, both iron sulfides, have the formula FeS2; however, the former is isometric while the latter is orthorhombic. This polymorphism extends to other sulfides with the generic AX2 formula; these two groups are collectively known as the pyrite and marcasite groups.[64] Polymorphism can extend beyond pure symmetry content. The aluminosilicates are a group of three minerals\u00a0\u2013 kyanite, andalusite, and sillimanite\u00a0\u2013 which share the chemical formula Al2SiO5. Kyanite is triclinic, while andalusite and sillimanite are both orthorhombic and belong to the dipyramidal point group. These differences arise corresponding to how aluminium is coordinated within the crystal structure. In all minerals, one aluminium ion is always in six-fold coordination with oxygen. Silicon, as a general rule, is in four-fold coordination in all minerals; an exception is a case like stishovite (SiO2, an ultra-high pressure quartz polymorph with rutile structure).[65] In kyanite, the second aluminium is in six-fold coordination; its chemical formula can be expressed as Al[6]Al[6]SiO5, to reflect its crystal structure. Andalusite has the second aluminium in five-fold coordination (Al[6]Al[5]SiO5) and sillimanite has it in four-fold coordination (Al[6]Al[4]SiO5).[66] Differences in crystal structure and chemistry greatly influence other physical properties of the mineral. The carbon allotropes diamond and graphite have vastly different properties; diamond is the hardest natural substance, has an adamantine lustre, and belongs to the isometric crystal family, whereas graphite is very soft, has a greasy lustre, and crystallises in the hexagonal family. This difference is accounted for by differences in bonding. In diamond, the carbons are in sp3 hybrid orbitals, which means they form a framework where each carbon is covalently bonded to four neighbours in a tetrahedral fashion; on the other hand, graphite is composed of sheets of carbons in sp2 hybrid orbitals, where each carbon is bonded covalently to only three others. These sheets are held together by much weaker van der Waals forces, and this discrepancy translates to large macroscopic differences.[67] Twinning is the intergrowth of two or more crystals of a single mineral species. The geometry of the twinning is controlled by the mineral's symmetry. As a result, there are several types of twins, including contact twins, reticulated twins, geniculated twins, penetration twins, cyclic twins, and polysynthetic twins. Contact, or simple twins, consist of two crystals joined at a plane; this type of twinning is common in spinel. Reticulated twins, common in rutile, are interlocking crystals resembling netting. Geniculated twins have a bend in the middle that is caused by start of the twin. Penetration twins consist of two single crystals that have grown into each other; examples of this twinning include cross-shaped staurolite twins and Carlsbad twinning in orthoclase. Cyclic twins are caused by repeated twinning around a rotation axis. This type of twinning occurs around three, four, five, six, or eight-fold axes, and the corresponding patterns are called threelings, fourlings, fivelings, sixlings, and eightlings. Sixlings are common in aragonite. Polysynthetic twins are similar to cyclic twins through the presence of repetitive twinning; however, instead of occurring around a rotational axis, polysynthetic twinning occurs along parallel planes, usually on a microscopic scale.[68][69] Crystal habit refers to the overall shape of the aggregate crystal of any mineral. Several terms are used to describe this property. Common habits include acicular, which describes needle-like crystals as in natrolite; dendritic (tree-pattern) is common in native copper or native gold with a groundmass (matrix); equant, which is typical of garnet; prismatic (elongated in one direction) as seen in kunzite or stibnite; botryoidal (like a bunch of grapes) seen in chalcedony; fibrous, which has fibre-like crystals as seen in wollastonite; tabular, which differs from bladed habit in that the former is platy whereas the latter has a defined elongation as seen in muscovite; and massive, which has no definite shape as seen in carnallite.[7] Related to crystal form, the quality of crystal faces is diagnostic of some minerals, especially with a petrographic microscope. Euhedral crystals have a defined external shape, while anhedral crystals do not; those intermediate forms are termed subhedral.[70][71] The hardness of a mineral defines how much it can resist scratching or indentation. This physical property is controlled by the chemical composition and crystalline structure of a mineral. The most commonly used scale of measurement is the ordinal Mohs hardness scale, which measures resistance to scratching. Defined by ten indicators, a mineral with a higher index scratches those below it. The scale ranges from talc, a phyllosilicate, to diamond, a carbon polymorph that is the hardest natural material. The scale is provided below:[72][7] A mineral's hardness is a function of its structure. Hardness is not necessarily constant for all crystallographic directions; crystallographic weakness renders some directions softer than others.[72] An example of this hardness variability exists in kyanite, which has a Mohs hardness of 51\u20442 parallel to [001] but 7 parallel to [100].[73] Other scales include these;[74] Lustre indicates how light reflects from the mineral's surface, with regard to its quality and intensity. There are numerous qualitative terms used to describe this property, which are split into metallic and non-metallic categories. Metallic and sub-metallic minerals have high reflectivity like metal; examples of minerals with this lustre are galena and pyrite. Non-metallic lustres include: adamantine, such as in diamond; vitreous, which is a glassy lustre very common in silicate minerals; pearly, such as in talc and apophyllite; resinous, such as members of the garnet group; silky which is common in fibrous minerals such as asbestiform chrysotile.[76] The diaphaneity of a mineral describes the ability of light to pass through it. Transparent minerals do not diminish the intensity of light passing through them. An example of a transparent mineral is muscovite (potassium mica); some varieties are sufficiently clear to have been used for windows. Translucent minerals allow some light to pass, but less than those that are transparent. Jadeite and nephrite (mineral forms of jade are examples of minerals with this property). Minerals that do not allow light to pass are called opaque.[77][78] The diaphaneity of a mineral depends on the thickness of the sample. When a mineral is sufficiently thin (e.g., in a thin section for petrography), it may become transparent even if that property is not seen in a hand sample. In contrast, some minerals, such as hematite or pyrite, are opaque even in thin-section.[78] Colour is the most obvious property of a mineral, but it is often non-diagnostic.[79] It is caused by electromagnetic radiation interacting with electrons (except in the case of incandescence, which does not apply to minerals).[80] Two broad classes of elements (idiochromatic and allochromatic) are defined with regard to their contribution to a mineral's colour: Idiochromatic elements are essential to a mineral's composition; their contribution to a mineral's colour is diagnostic.[77][81] Examples of such minerals are malachite (green) and azurite (blue). In contrast, allochromatic elements in minerals are present in trace amounts as impurities. An example of such a mineral would be the ruby and sapphire varieties of the mineral corundum.[81]\nThe colours of pseudochromatic minerals are the result of interference of light waves. Examples include labradorite and bornite. In addition to simple body colour, minerals can have various other distinctive optical properties, such as play of colours, asterism, chatoyancy, iridescence, tarnish, and pleochroism. Several of these properties involve variability in colour. Play of colour, such as in opal, results in the sample reflecting different colours as it is turned, while pleochroism describes the change in colour as light passes through a mineral in a different orientation. Iridescence is a variety of the play of colours where light scatters off a coating on the surface of crystal, cleavage planes, or off layers having minor gradations in chemistry.[82] In contrast, the play of colours in opal is caused by light refracting from ordered microscopic silica spheres within its physical structure.[83] Chatoyancy (\"cat's eye\") is the wavy banding of colour that is observed as the sample is rotated; asterism, a variety of chatoyancy, gives the appearance of a star on the mineral grain. The latter property is particularly common in gem-quality corundum.[82][83] The streak of a mineral refers to the colour of a mineral in powdered form, which may or may not be identical to its body colour.[81] The most common way of testing this property is done with a streak plate, which is made out of porcelain and coloured either white or black. The streak of a mineral is independent of trace elements[77] or any weathering surface.[81] A common example of this property is illustrated with hematite, which is coloured black, silver or red in hand sample, but has a cherry-red[77] to reddish-brown streak;[81][7] or with chalcopyrite, which is brassy golden in colour and leaves a black streak.[7] Streak is more often distinctive for metallic minerals, in contrast to non-metallic minerals whose body colour is created by allochromatic elements.[77] Streak testing is constrained by the hardness of the mineral, as those harder than 7 powder the streak plate instead.[81] By definition, minerals have a characteristic atomic arrangement. Weakness in this crystalline structure causes planes of weakness, and the breakage of a mineral along such planes is termed cleavage. The quality of cleavage can be described based on how cleanly and easily the mineral breaks; common descriptors, in order of decreasing quality, are \"perfect\", \"good\", \"distinct\", and \"poor\". In particularly transparent minerals, or in thin-section, cleavage can be seen as a series of parallel lines marking the planar surfaces when viewed from the side. Cleavage is not a universal property among minerals; for example, quartz, consisting of extensively interconnected silica tetrahedra, does not have a crystallographic weakness which would allow it to cleave. In contrast, micas, which have perfect basal cleavage, consist of sheets of silica tetrahedra which are very weakly held together.[84][85] As cleavage is a function of crystallography, there are a variety of cleavage types. Cleavage occurs typically in either one, two, three, four, or six directions. Basal cleavage in one direction is a distinctive property of the micas. Two-directional cleavage is described as prismatic, and occurs in minerals such as the amphiboles and pyroxenes. Minerals such as galena or halite have cubic (or isometric) cleavage in three directions, at 90\u00b0; when three directions of cleavage are present, but not at 90\u00b0, such as in calcite or rhodochrosite, it is termed rhombohedral cleavage. Octahedral cleavage (four directions) is present in fluorite and diamond, and sphalerite has six-directional dodecahedral cleavage.[84][85] Minerals with many cleavages might not break equally well in all of the directions; for example, calcite has good cleavage in three directions, but gypsum has perfect cleavage in one direction, and poor cleavage in two other directions. Angles between cleavage planes vary between minerals. For example, as the amphiboles are double-chain silicates and the pyroxenes are single-chain silicates, the angle between their cleavage planes is different. The pyroxenes cleave in two directions at approximately 90\u00b0, whereas the amphiboles distinctively cleave in two directions separated by approximately 120\u00b0 and 60\u00b0. The cleavage angles can be measured with a contact goniometer, which is similar to a protractor.[84][85] Parting, sometimes called \"false cleavage\", is similar in appearance to cleavage but is instead produced by structural defects in the mineral, as opposed to systematic weakness. Parting varies from crystal to crystal of a mineral, whereas all crystals of a given mineral will cleave if the atomic structure allows for that property. In general, parting is caused by some stress applied to a crystal. The sources of the stresses include deformation (e.g. an increase in pressure), exsolution, or twinning. Minerals that often display parting include the pyroxenes, hematite, magnetite, and corundum.[84][86] When a mineral is broken in a direction that does not correspond to a plane of cleavage, it is termed to have been fractured. There are several types of uneven fracture. The classic example is conchoidal fracture, like that of quartz; rounded surfaces are created, which are marked by smooth curved lines. This type of fracture occurs only in very homogeneous minerals. Other types of fracture are fibrous, splintery, and hackly. The latter describes a break along a rough, jagged surface; an example of this property is found in native copper.[87] Tenacity is related to both cleavage and fracture. Whereas fracture and cleavage describes the surfaces that are created when a mineral is broken, tenacity describes how resistant a mineral is to such breaking. Minerals can be described as brittle, ductile, malleable, sectile, flexible, or elastic.[88] Specific gravity numerically describes the density of a mineral. The dimensions of density are mass divided by volume with units: kg/m3 or g/cm3. Specific gravity is defined as the density of the mineral divided by the density of water at 4\u00a0\u00b0C and thus is a dimensionless quantity, identical in all unit systems.[89] It can be measured as the quotient of the mass of the sample and difference between the weight of the sample in air and its corresponding weight in water. Among most minerals, this property is not diagnostic. Rock forming minerals\u00a0\u2013 typically silicates or occasionally carbonates\u00a0\u2013 have a specific gravity of 2.5\u20133.5.[90] High specific gravity is a diagnostic property of a mineral. A variation in chemistry (and consequently, mineral class) correlates to a change in specific gravity. Among more common minerals, oxides and sulfides tend to have a higher specific gravity as they include elements with higher atomic mass. A generalization is that minerals with metallic or adamantine lustre tend to have higher specific gravities than those having a non-metallic to dull lustre. For example, hematite, Fe2O3, has a specific gravity of 5.26[91] while galena, PbS, has a specific gravity of 7.2\u20137.6,[92] which is a result of their high iron and lead content, respectively. A very high specific gravity is characteristic of native metals; for example, kamacite, an iron-nickel alloy common in iron meteorites has a specific gravity of 7.9,[93] and gold has an observed specific gravity between 15 and 19.3.[90][94] Other properties can be used to diagnose minerals. These are less general, and apply to specific minerals. Dropping dilute acid (often 10% HCl) onto a mineral aids in distinguishing carbonates from other mineral classes. The acid reacts with the carbonate ([CO3]2\u2212) group, which causes the affected area to effervesce, giving off carbon dioxide gas. This test can be further expanded to test the mineral in its original crystal form or powdered form. An example of this test is done when distinguishing calcite from dolomite, especially within the rocks (limestone and dolomite respectively). Calcite immediately effervesces in acid, whereas acid must be applied to powdered dolomite (often to a scratched surface in a rock), for it to effervesce.[95] Zeolite minerals will not effervesce in acid; instead, they become frosted after 5\u201310 minutes, and if left in acid for a day, they dissolve or become a silica gel.[96] Magnetism is a very conspicuous property of a few minerals. Among common minerals, magnetite exhibits this property strongly, and magnetism is also present, albeit not as strongly, in pyrrhotite and ilmenite.[95] Some minerals exhibit electrical properties \u2013 for example, quartz is piezoelectric \u2013 but electrical properties are rarely used as diagnostic criteria for minerals because of incomplete data and natural variation.[97] Minerals can also be tested for taste or smell. Halite, NaCl, is table salt; its potassium-bearing counterpart, sylvite, has a pronounced bitter taste. Sulfides have a characteristic smell, especially as samples are fractured, reacting, or powdered.[95] Radioactivity is a rare property found in minerals containing radioactive elements. The radioactive elements could be a defining constituent, such as uranium in uraninite, autunite, and carnotite, or present as trace impurities, as in zircon. The decay of a radioactive element damages the mineral crystal structure rendering it locally amorphous (metamict state); the optical result, termed a radioactive halo or pleochroic halo, is observable with various techniques, such as thin-section petrography.[95] In 315 BCE, Theophrastus presented his classification of minerals in his treatise On Stones. His classification was influenced by the ideas of his teachers Plato  and Aristotle. Theophrastus classified minerals as stones, earths or metals.[98] Georgius Agricola's classification of minerals in his book De Natura Fossilium, published in 1546, divided minerals into three types of substance: simple (stones, earths, metals, and congealed juices), compound (intimately mixed) and composite (separable).[98] An early classification of minerals was given by Carl Linnaeus in his seminal 1735 book Systema Naturae. He divided the natural world into three kingdoms\u00a0\u2013 plants, animals, and minerals\u00a0\u2013 and classified each with the same hierarchy.[99] In descending order, these were Phylum, Class, Order, Family, Tribe, Genus, and Species. However, while his system was justified by Charles Darwin's theory of species formation and has been largely adopted and expanded by biologists in the following centuries (who still use his Greek- and Latin-based binomial naming scheme), it had little success among mineralogists (although each distinct mineral is still formally referred to as a mineral species). Minerals are classified by variety, species, series and group, in order of increasing generality. The basic level of definition is that of mineral species, each of which is distinguished from the others by unique chemical and physical properties. For example, quartz is defined by its formula, SiO2, and a specific crystalline structure that distinguishes it from other minerals with the same chemical formula (termed polymorphs). When there exists a range of composition between two minerals species, a mineral series is defined. For example, the biotite series is represented by variable amounts of the endmembers phlogopite, siderophyllite, annite, and eastonite. In contrast, a mineral group is a grouping of mineral species with some common chemical properties that share a crystal structure. The pyroxene group has a common  formula of XY(Si,Al)2O6, where X and Y are both cations, with X typically bigger than Y; the pyroxenes are single-chain silicates that crystallize in either the orthorhombic or monoclinic crystal systems. Finally, a mineral variety is a specific type of mineral species that differs by some physical characteristic, such as colour or crystal habit. An example is amethyst, which is a purple variety of quartz.[18] Two common classifications, Dana and Strunz, are used for minerals; both rely on composition, specifically with regard to important chemical groups, and structure. James Dwight Dana, a leading geologist of his time, first published his System of Mineralogy in 1837; as of 1997[update], it is in its eighth edition. The Dana classification assigns a four-part number to a mineral species. Its class number is based on important compositional groups; the type gives the ratio of cations to anions in the mineral, and the last two numbers group minerals by structural similarity within a given type or class. The less commonly used Strunz classification, named for German mineralogist Karl Hugo Strunz, is based on the Dana system, but combines both chemical and structural criteria, the latter with regard to distribution of chemical bonds.[100] As the composition of the Earth's crust is dominated by silicon and oxygen, silicates are by far the most important class of minerals in terms of rock formation and diversity. However, non-silicate minerals are of great economic importance, especially as ores.[101][102] Non-silicate minerals are subdivided into several other classes by their dominant chemistry, which includes native elements, sulfides, halides, oxides and hydroxides, carbonates and nitrates, borates, sulfates, phosphates, and organic compounds. Most non-silicate mineral species are rare (constituting in total 8% of the Earth's crust), although some are relatively common, such as calcite, pyrite, magnetite, and hematite. There are two major structural styles observed in non-silicates: close-packing and silicate-like linked tetrahedra. Close-packed structures are a way to densely pack atoms while minimizing interstitial space. Hexagonal close-packing involves stacking layers where every other layer is the same (\"ababab\"), whereas cubic close-packing involves stacking groups of three layers (\"abcabcabc\"). Analogues to linked silica tetrahedra include SO4\u22124 (sulfate), PO4\u22124 (phosphate), AsO4\u22124 (arsenate), and VO4\u22124 (vanadate) structures. The non-silicates have great economic importance, as they concentrate elements more than the silicate minerals do.[103] The largest grouping of minerals by far are the silicates; most rocks are composed of greater than 95% silicate minerals, and over 90% of the Earth's crust is composed of these minerals.[104] The two main constituents of silicates are silicon and oxygen, which are the two most abundant elements in the Earth's crust. Other common elements in silicate minerals correspond to other common elements in the Earth's crust, such as aluminium, magnesium, iron, calcium, sodium, and potassium.[105] Some important rock-forming silicates include the feldspars, quartz, olivines, pyroxenes, amphiboles, garnets, and micas. The base unit of a silicate mineral is the [SiO4]4\u2212 tetrahedron. In the vast majority of cases, silicon is in four-fold or tetrahedral coordination with oxygen. In very high-pressure situations, silicon will be in six-fold or octahedral coordination, such as in the perovskite structure or the quartz polymorph stishovite (SiO2). In the latter case, the mineral no longer has a silicate structure, but that of rutile (TiO2), and its associated group, which are simple oxides. These silica tetrahedra are then polymerized to some degree to create various structures, such as one-dimensional chains, two-dimensional sheets, and three-dimensional frameworks. The basic silicate mineral where no polymerization of the tetrahedra has occurred requires other elements to balance out the base 4- charge. In other silicate structures, different combinations of elements are required to balance out the resultant negative charge. It is common for the Si4+ to be substituted by  Al3+ because of similarity in ionic radius and charge; in those cases, the [AlO4]5\u2212 tetrahedra form the same structures as do the unsubstituted tetrahedra, but their charge-balancing requirements are different.[106] The degree of polymerization can be described by both the structure formed and how many tetrahedral corners (or coordinating oxygens) are shared (for aluminium and silicon in tetrahedral sites):[107][108] The silicate subclasses are described below in order of decreasing polymerization. Tectosilicates, also known as framework silicates, have the highest degree of polymerization. With all corners of a tetrahedra shared, the silicon:oxygen ratio becomes 1:2. Examples are quartz, the feldspars, feldspathoids, and the zeolites. Framework silicates tend to be particularly chemically stable as a result of strong covalent bonds.[109] Forming 12% of the Earth's crust, quartz (SiO2) is the most abundant mineral species. It is characterized by its high chemical and physical resistivity. Quartz has several polymorphs, including tridymite and cristobalite at high temperatures, high-pressure coesite, and ultra-high pressure stishovite. The latter mineral can only be formed on Earth by meteorite impacts, and its structure has been compressed so much that it has changed from a silicate structure to that of rutile (TiO2). The silica polymorph that is most stable at the Earth's surface is \u03b1-quartz. Its counterpart, \u03b2-quartz, is present only at high temperatures and pressures (changes to \u03b1-quartz below 573\u00a0\u00b0C at 1 bar). These two polymorphs differ by a \"kinking\" of bonds; this change in structure gives \u03b2-quartz greater symmetry than \u03b1-quartz, and they are thus also called high quartz (\u03b2) and low quartz (\u03b1).[104][110] Feldspars are the most abundant group in the Earth's crust, at about 50%. In the feldspars, Al3+ substitutes for Si4+, which creates a charge imbalance that must be accounted for by the addition of cations. The base structure becomes either [AlSi3O8]\u2212 or [Al2Si2O8]2\u2212  There are 22 mineral species of feldspars, subdivided into two major subgroups \u2013 alkali and plagioclase \u2013 and two less common groups \u2013 celsian and banalsite. The alkali feldspars are most commonly in a series between potassium-rich orthoclase and sodium-rich albite; in the case of plagioclase, the most common series ranges from albite to calcium-rich anorthite. Crystal twinning is common in feldspars, especially polysynthetic twins in plagioclase and Carlsbad twins in alkali feldspars. If the latter subgroup cools slowly from a melt, it forms exsolution lamellae because the two components \u2013 orthoclase and albite \u2013 are unstable in solid solution. Exsolution can be on a scale from microscopic to readily observable in hand-sample; perthitic texture forms when Na-rich feldspar exsolve in a K-rich host. The opposite texture (antiperthitic), where K-rich feldspar exsolves in a Na-rich host, is very rare.[111] Feldspathoids are structurally similar to feldspar, but differ in that they form in Si-deficient conditions, which allows for further substitution by Al3+. As a result, feldspathoids are almost never found in association with quartz. A common example of a feldspathoid is nepheline ((Na, K)AlSiO4); compared to alkali feldspar, nepheline has an Al2O3:SiO2 ratio of 1:2, as opposed to 1:6 in alkali feldspar.[112] Zeolites often have distinctive crystal habits, occurring in needles, plates, or blocky masses. They form in the presence of water at low temperatures and pressures, and have channels and voids in their structure. Zeolites have several industrial applications, especially in waste water treatment.[113] Phyllosilicates consist of sheets of polymerized tetrahedra. They are bound at three oxygen sites, which gives a characteristic silicon:oxygen ratio of 2:5. Important examples include the mica, chlorite, and the kaolinite-serpentine groups. In addition to the tetrahedra, phyllosilicates have a sheet of octahedra (elements in six-fold coordination by oxygen) that balance out the basic tetrahedra, which have a negative charge (e.g. [Si4O10]4\u2212) These tetrahedra (T) and octahedra (O) sheets are stacked in a variety of combinations to create phyllosilicate layers. Within an octahedral sheet, there are three octahedral sites in a unit structure; however, not all of the sites may be occupied. In that case, the mineral is termed dioctahedral, whereas in other case it is termed trioctahedral.[114] The layers are weakly bound by van der Waals forces, hydrogen bonds, or sparse ionic bonds, which causes a crystallographic weakness, in turn leading to a prominent basal cleavage among the phyllosilicates.[115] The kaolinite-serpentine group consists of T-O stacks (the 1:1 clay minerals); their hardness ranges from 2 to 4, as the sheets are held by hydrogen bonds. The 2:1 clay minerals (pyrophyllite-talc) consist of T-O-T stacks, but they are softer (hardness from 1 to 2), as they are instead held together by van der Waals forces. These two groups of minerals are subgrouped by octahedral occupation; specifically, kaolinite and pyrophyllite are dioctahedral whereas serpentine and talc trioctahedral.[116] Micas are also T-O-T-stacked phyllosilicates, but differ from the other T-O-T and T-O-stacked subclass members in that they incorporate aluminium into the tetrahedral sheets (clay minerals have Al3+ in octahedral sites). Common examples of micas are muscovite, and the biotite series. Mica T-O-T layers are bonded together by metal ions, giving them a greater hardness than other phyllosilicate minerals, though they retain perfect basal cleavage.[117] The chlorite group is related to mica group, but a brucite-like (Mg(OH)2) layer between the T-O-T stacks.[118] Because of their chemical structure, phyllosilicates typically have flexible, elastic, transparent layers that are electrical insulators and can be split into very thin flakes. Micas can be used in electronics as insulators, in construction, as optical filler, or even cosmetics. Chrysotile, a species of serpentine, is the most common mineral species in industrial asbestos, as it is less dangerous in terms of health than the amphibole asbestos.[119] Inosilicates consist of tetrahedra repeatedly bonded in chains. These chains can be single, where a tetrahedron is bound to two others to form a continuous chain; alternatively, two chains can be merged to create double-chain silicates. Single-chain silicates have a silicon:oxygen ratio of 1:3 (e.g. [Si2O6]4\u2212), whereas the double-chain variety has a ratio of 4:11, e.g. [Si8O22]12\u2212. Inosilicates contain two important rock-forming mineral groups; single-chain silicates are most commonly pyroxenes, while double-chain silicates are often amphiboles.[120] Higher-order chains exist (e.g. three-member, four-member, five-member chains, etc.) but they are rare.[121] The pyroxene group consists of 21 mineral species.[122] Pyroxenes have a general structure formula of XY(Si2O6), where X is an octahedral site, while Y can vary in coordination number from six to eight. Most varieties of pyroxene consist of permutations of Ca2+, Fe2+ and Mg2+ to balance the negative charge on the backbone. Pyroxenes are common in the Earth's crust (about 10%) and are a key constituent of mafic igneous rocks.[123] Amphiboles have great variability in chemistry, described variously as a \"mineralogical garbage can\" or a \"mineralogical shark swimming a sea of elements\". The backbone of the amphiboles is the [Si8O22]12\u2212; it is balanced by cations in three possible positions, although the third position is not always used, and one element can occupy both remaining ones. Finally, the amphiboles are usually hydrated, that is, they have a hydroxyl group ([OH]\u2212), although it can be replaced by a fluoride, a chloride, or an oxide ion.[124] Because of the variable chemistry, there are over 80 species of amphibole, although variations, as in the pyroxenes, most commonly involve mixtures of Ca2+, Fe2+ and Mg2+.[122] Several amphibole mineral species can have an asbestiform crystal habit. These asbestos minerals form long, thin, flexible, and strong fibres, which are electrical insulators, chemically inert and heat-resistant; as such, they have several applications, especially in construction materials. However, asbestos are known carcinogens, and cause various other illnesses, such as asbestosis; amphibole asbestos (anthophyllite, tremolite, actinolite, grunerite, and riebeckite) are considered more dangerous than chrysotile serpentine asbestos.[125] Cyclosilicates, or ring silicates, have a ratio of silicon to oxygen of 1:3. Six-member rings are most common, with a base structure of [Si6O18]12\u2212; examples include the tourmaline group and beryl. Other ring structures exist, with 3, 4, 8, 9, 12 having been described.[126]  Cyclosilicates tend to be strong, with elongated, striated crystals.[127] Tourmalines have a very complex chemistry that can be described by a general formula XY3Z6(BO3)3T6O18V3W. The T6O18 is the basic ring structure, where T is usually Si4+, but substitutable by Al3+ or B3+. Tourmalines can be subgrouped by the occupancy of the X site, and from there further subdivided by the chemistry of the W site. The Y and Z sites can accommodate a variety of cations, especially various transition metals; this variability in structural transition metal content gives the tourmaline group greater variability in colour. Other cyclosilicates include beryl, Al2Be3Si6O18, whose varieties include the gemstones emerald (green) and aquamarine (bluish). Cordierite is structurally similar to beryl, and is a common metamorphic mineral.[128] Sorosilicates, also termed disilicates, have tetrahedron-tetrahedron bonding at one oxygen, which results in a 2:7 ratio of silicon to oxygen. The resultant common structural element is the [Si2O7]6\u2212 group. The most common disilicates by far are members of the epidote group. Epidotes are found in variety of geologic settings, ranging from mid-ocean ridge to granites to metapelites. Epidotes are built around the structure [(SiO4)(Si2O7)]10\u2212 structure; for example, the mineral species epidote has calcium, aluminium, and ferric iron to charge balance: Ca2Al2(Fe3+, Al)(SiO4)(Si2O7)O(OH). The presence of iron as Fe3+ and Fe2+ helps buffer oxygen fugacity, which in turn is a significant factor in petrogenesis.[129] Other examples of sorosilicates include lawsonite, a  metamorphic mineral forming in the blueschist facies (subduction zone setting with low temperature and high pressure), vesuvianite, which takes up a significant amount of calcium in its chemical structure.[129][130] Orthosilicates consist of isolated tetrahedra that are charge-balanced by other cations.[131] Also termed nesosilicates, this type of silicate has a silicon:oxygen ratio of 1:4 (e.g. SiO4). Typical orthosilicates tend to form blocky equant crystals, and are fairly hard.[132] Several rock-forming minerals are part of this subclass, such as the aluminosilicates, the olivine group, and the garnet group. The aluminosilicates \u2013bkyanite, andalusite, and sillimanite, all Al2SiO5 \u2013 are structurally composed of one [SiO4]4\u2212 tetrahedron, and one Al3+ in octahedral coordination. The remaining Al3+ can be in six-fold coordination (kyanite), five-fold (andalusite) or four-fold (sillimanite); which mineral forms in a given environment is depend on pressure and temperature conditions. In the olivine structure, the main olivine series of (Mg, Fe)2SiO4 consist of magnesium-rich forsterite and iron-rich fayalite. Both iron and magnesium are in octahedral by oxygen. Other mineral species having this structure exist, such as tephroite, Mn2SiO4.[133] The garnet group has a general formula of X3Y2(SiO4)3, where X is a large eight-fold coordinated cation, and Y is a smaller six-fold coordinated cation. There are six ideal endmembers of garnet, split into two group. The pyralspite garnets have Al3+ in the Y position: pyrope (Mg3Al2(SiO4)3), almandine (Fe3Al2(SiO4)3), and spessartine (Mn3Al2(SiO4)3). The ugrandite garnets have Ca2+ in the X position: uvarovite (Ca3Cr2(SiO4)3), grossular (Ca3Al2(SiO4)3) and andradite (Ca3Fe2(SiO4)3). While there are two subgroups of garnet, solid solutions exist between all six end-members.[131] Other orthosilicates include zircon, staurolite, and topaz. Zircon (ZrSiO4) is useful in geochronology as U6+ can substitute for Zr4+; furthermore, because of its very resistant structure, it is difficult to reset it as a chronometer. Staurolite is a common metamorphic intermediate-grade index mineral. It has a particularly complicated crystal structure that was only fully described in 1986. Topaz (Al2SiO4(F, OH)2, often found in granitic pegmatites associated with tourmaline, is a common gemstone mineral.[134] Native elements are those that are not chemically bonded to other elements. This mineral group includes native metals, semi-metals, and non-metals, and various alloys and solid solutions. The metals are held together by metallic bonding, which confers distinctive physical properties such as their shiny metallic lustre, ductility and malleability, and electrical conductivity. Native elements are subdivided into groups by their structure or chemical attributes. The gold group, with a cubic close-packed structure, includes metals such as gold, silver, and copper. The platinum group is similar in structure to the gold group. The iron-nickel group is characterized by several iron-nickel alloy species. Two examples are kamacite and taenite, which are found in iron meteorites; these species differ by the amount of Ni in the alloy; kamacite has less than 5\u20137% nickel and is a variety of native iron, whereas the nickel content of taenite ranges from 7\u201337%. Arsenic group minerals consist of semi-metals, which have only some metallic traits; for example, they lack the malleability of metals. Native carbon occurs in two allotropes, graphite and diamond; the latter forms at very high pressure in the mantle, which gives it a much stronger structure than graphite.[135] The sulfide minerals are chemical compounds of one or more metals or semimetals with a chalcogen or pnictogen, of which sulfur is most common. Tellurium, arsenic, or selenium can substitute for the sulfur. Sulfides tend to be soft, brittle minerals with a high specific gravity. Many powdered sulfides, such as pyrite, have a sulfurous smell when powdered. Sulfides are susceptible to weathering, and many readily dissolve in water; these dissolved minerals can be later redeposited, which creates enriched secondary ore deposits.[136] Sulfides are classified by the ratio of the metal or semimetal to the sulfur, such as M:S equal to 2:1, or 1:1.[137] Many sulfide minerals are economically important as metal ores; examples include sphalerite (ZnS), an ore of zinc, galena (PbS), an ore of lead, cinnabar (HgS), an ore of mercury, and molybdenite (MoS2, an ore of molybdenum.[138] Pyrite (FeS2), is the most commonly occurring sulfide, and can be found in most geological environments. It is not, however, an ore of iron, but can be instead oxidized to produce sulfuric acid.[139] Related to the sulfides are the rare sulfosalts, in which a metallic element is bonded to sulfur and a semimetal such as antimony, arsenic, or bismuth. Like the sulfides, sulfosalts are typically soft, heavy, and brittle minerals.[140] Oxide minerals are divided into three categories: simple oxides, hydroxides, and multiple oxides. Simple oxides are characterized by O2\u2212 as the main anion and primarily ionic bonding. They can be further subdivided by the ratio of oxygen to the cations. The periclase group consists of minerals with a 1:1 ratio. Oxides with a 2:1 ratio include cuprite (Cu2O) and water ice. Corundum group minerals have a 2:3 ratio, and includes minerals such as corundum (Al2O3), and hematite (Fe2O3). Rutile group minerals have a ratio of 1:2; the eponymous species, rutile (TiO2) is the chief ore of titanium; other examples include cassiterite (SnO2; ore of tin), and pyrolusite (MnO2; ore of manganese).[141][142]  In hydroxides, the dominant anion is the hydroxyl ion, OH\u2212. Bauxites are the chief aluminium ore, and are a heterogeneous mixture of the hydroxide minerals diaspore, gibbsite, and bohmite; they form in areas with a very high rate of chemical weathering (mainly tropical conditions).[143]  Finally, multiple oxides are compounds of two metals with oxygen. A major group within this class are the spinels, with a general formula of X2+Y3+2O4. Examples of species include spinel (MgAl2O4), chromite (FeCr2O4), and magnetite (Fe3O4). The latter is readily distinguishable by its strong magnetism, which occurs as it has iron in two oxidation states (Fe2+Fe3+2O4), which makes it a multiple oxide instead of a single oxide.[144] The halide minerals are compounds in which a halogen (fluorine, chlorine, iodine, or bromine) is the main anion. These minerals tend to be soft, weak, brittle, and water-soluble. Common examples of halides include halite (NaCl, table salt), sylvite (KCl), and fluorite (CaF2). Halite and sylvite commonly form as evaporites, and can be dominant minerals in chemical sedimentary rocks. Cryolite, Na3AlF6, is a key mineral in the extraction of aluminium from bauxites; however, as the only significant occurrence at Ivittuut, Greenland, in a granitic pegmatite, was depleted, synthetic cryolite can be made from fluorite.[145] The carbonate minerals are those in which the main anionic group is carbonate, [CO3]2\u2212. Carbonates tend to be brittle, many have rhombohedral cleavage, and all react with acid.[146] Due to the last characteristic, field geologists often carry dilute hydrochloric acid to distinguish carbonates from non-carbonates. The reaction of acid with carbonates, most commonly found as the polymorph calcite and aragonite (CaCO3), relates to the dissolution and precipitation of the mineral, which is a key in the formation of limestone caves, features within them such as stalactite and stalagmites, and karst landforms. Carbonates are most often formed as biogenic or chemical sediments in marine environments. The carbonate group is structurally a triangle, where a central C4+ cation is surrounded by three O2\u2212 anions; different groups of minerals form from different arrangements of these triangles.[147] The most common carbonate mineral is calcite, which is the primary constituent of sedimentary limestone and metamorphic marble. Calcite, CaCO3, can have a significant percentage of magnesium substituting for calcium. Under high-Mg conditions, its polymorph aragonite will form instead; the marine geochemistry in this regard can be described as an aragonite or calcite sea, depending on which mineral preferentially forms. Dolomite is a double carbonate, with the formula CaMg(CO3)2. Secondary dolomitization of limestone is common, in which calcite or aragonite are converted to dolomite; this reaction increases pore space (the unit cell volume of dolomite is 88% that of calcite), which can create a reservoir for oil and gas. These two mineral species are members of eponymous mineral groups: the calcite group includes carbonates with the general formula XCO3, and the dolomite group constitutes minerals with the general formula XY(CO3)2.[148] The sulfate minerals all contain the sulfate anion, [SO4]2\u2212. They tend to be transparent to translucent, soft, and many are fragile.[149] Sulfate minerals commonly form as evaporites, where they precipitate out of evaporating saline waters. Sulfates can also be found in hydrothermal vein systems associated with sulfides,[150] or as oxidation products of sulfides.[151] Sulfates can be subdivided into anhydrous and hydrous minerals. The most common hydrous sulfate by far is gypsum, CaSO4\u22c52H2O. It forms as an evaporite, and is associated with other evaporites such as calcite and halite; if it incorporates sand grains as it crystallizes, gypsum can form desert roses. Gypsum has very low thermal conductivity and maintains a low temperature when heated as it loses that heat by dehydrating; as such, gypsum is used as an insulator in materials such as plaster and drywall. The anhydrous equivalent of gypsum is anhydrite; it can form directly from seawater in highly arid conditions. The barite group has the general formula XSO4, where the X is a large 12-coordinated cation. Examples include barite (BaSO4), celestine (SrSO4), and anglesite (PbSO4); anhydrite is not part of the barite group, as the smaller Ca2+ is only in eight-fold coordination.[152] The phosphate minerals are characterized by the tetrahedral [PO4]3\u2212 unit, although the structure can be generalized, and phosphorus is replaced by antimony, arsenic, or vanadium. The most common phosphate is the apatite group; common species within this group are fluorapatite (Ca5(PO4)3F), chlorapatite (Ca5(PO4)3Cl) and hydroxylapatite (Ca5(PO4)3(OH)). Minerals in this group are the main crystalline constituents of teeth and bones in vertebrates. The relatively abundant monazite group has a general structure of ATO4, where T is phosphorus or arsenic, and A is often a rare-earth element (REE). Monazite is important in two ways: first, as a REE \"sink\", it can sufficiently concentrate these elements to become an ore; secondly, monazite group elements can incorporate relatively large amounts of uranium and thorium, which can be used in monazite geochronology to date the rock based on the decay of the U and Th to lead.[153] The Strunz classification includes a class for organic minerals. These rare compounds contain organic carbon, but can be formed by a geologic process. For example, whewellite, CaC2O4\u22c5H2O is an oxalate that can be deposited in hydrothermal ore veins. While hydrated calcium oxalate can be found in coal seams and other sedimentary deposits involving organic matter, the hydrothermal occurrence is not considered to be related to biological activity.[102] Mineral classification schemes and their definitions are evolving to match recent advances in mineral science. Recent changes have included the addition of an organic class, in both the new Dana and the Strunz classification schemes.[154][155] The organic class includes a very rare group of minerals with hydrocarbons. The IMA Commission on New Minerals and Mineral Names adopted in 2009 a hierarchical scheme for the naming and classification of mineral groups and group names and established seven commissions and four working groups to review and classify minerals into an official listing of their published names.[156][157]  According to these new rules, \"mineral species can be grouped in a number of different ways, on the basis of chemistry, crystal structure, occurrence, association, genetic history, or resource, for example, depending on the purpose to be served by the classification.\"[156] It has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on Mars.  Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.[158] In January 2014, NASA reported that studies by the Curiosity and Opportunity rovers on Mars would search for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable.[159][160][161][162] The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars became a primary NASA objective.[159][160]",
      "ground_truth_chunk_ids": [
        "75_fixed_chunk1"
      ],
      "source_ids": [
        "S075"
      ],
      "category": "factual",
      "id": 9
    },
    {
      "question": "What is Funeral?",
      "ground_truth": "A funeral is a ceremony connected with the final disposition of a corpse, such as a burial, entombment or cremation with the attendant observances.[1] Funerary customs comprise the complex of beliefs and practices used by a culture to remember and respect the dead, from interment, to various monuments, prayers, and rituals undertaken in their honour. Customs vary between cultures and religious groups. Funerals have both normative and legal components. Common secular motivations for funerals include mourning the deceased, celebrating their life, and offering support and sympathy to the bereaved; additionally, funerals may have religious aspects that are intended to help the soul of the deceased reach the afterlife, resurrection or reincarnation. The funeral usually includes a ritual through which the corpse receives a final disposition.[2] Depending on culture and religion, these can involve either the destruction of the body (for example, by cremation, sky burial, decomposition, disintegration or dissolution) or its preservation (for example, by mummification). Differing beliefs about cleanliness and the relationship between body and soul are reflected in funerary practices. A memorial service (service of remembrance or celebration of life) is a funerary ceremony that is performed without the remains of the deceased person.[3] In both a closed casket funeral[4] and a memorial service, photos of the deceased representing stages of life would be displayed on an altar. Relatives or friends would give out eulogies in both services as well.[5] The word funeral comes from the Latin funus, which had a variety of meanings, including the corpse and the funerary rites themselves. Funerary art is art produced in connection with burials, including many kinds of tombs, and objects specially made for burial like flowers with a corpse. Funeral rites pre-date modern Homo sapiens and dated to at least 300,000 years ago.[6] For example, in the Shanidar Cave in",
      "expected_answer": "A funeral is a ceremony connected with the final disposition of a corpse, such as a burial, entombment or cremation with the attendant observances.[1] Funerary customs comprise the complex of beliefs and practices used by a culture to remember and respect the dead, from interment, to various monuments, prayers, and rituals undertaken in their honour. Customs vary between cultures and religious groups.  Funerals have both normative and legal components. Common secular motivations for funerals include mourning the deceased, celebrating their life, and offering support and sympathy to the bereaved; additionally, funerals may have religious aspects that are intended to help the soul of the deceased reach the afterlife, resurrection or reincarnation. The funeral usually includes a ritual through which the corpse receives a final disposition.[2] Depending on culture and religion, these can involve either the destruction of the body (for example, by cremation, sky burial, decomposition, disintegration or dissolution) or its preservation (for example, by mummification). Differing beliefs about cleanliness and the relationship between body and soul are reflected in funerary practices. A memorial service (service of remembrance or celebration of life) is a funerary ceremony that is performed without the remains of the deceased person.[3] In both a closed casket funeral[4] and a memorial service, photos of the deceased representing stages of life would be displayed on an altar. Relatives or friends would give out eulogies in both services as well.[5] The word funeral comes from the Latin funus, which had a variety of meanings, including the corpse and the funerary rites themselves. Funerary art is art produced in connection with burials, including many kinds of tombs, and objects specially made for burial like flowers with a corpse. Funeral rites pre-date modern Homo sapiens and dated to at least 300,000 years ago.[6] For example, in the Shanidar Cave in Iraq, in Pontnewydd Cave in Wales and at other sites across Europe and the Near East,[6] Archaeologists have discovered Neanderthal skeletons with a characteristic layer of flower pollen. This deliberate burial and reverence given to the dead has been interpreted as suggesting that Neanderthals had religious beliefs,[6] although the evidence is not unequivocal \u2013 while the dead were apparently buried deliberately, burrowing rodents could have introduced the flowers.[7] Substantial cross-cultural and historical research document funeral customs as a highly predictable, stable force in communities.[8][9] Funeral customs tend to be characterized by five \"anchors\": significant symbols, gathered community, ritual action, cultural heritage, and transition of the dead body (corpse).[2] The most common venues for funeral services would be in a place of worship (synagogue or church) or a funeral home. However, a cemetery's chapel features a reflecting serene intimacy as well as a respectful environment for clergy, mourning families and friends. Graveside services are a less common option for these rituals. A mausoleum's chapel mostly intends to be for entombment after the funeral itself. These two funerary chapels both generously accommodate open or closed-casket services prior to a traditional burial within the cemetery. If a funeral is subsequently followed by cremation, the service would be in a crematorium. In the Bah\u00e1\u02bc\u00ed Faith, burial law prescribes both the location of burial and burial practices and precludes cremation of the dead. It is forbidden to carry the body for more than one hour's journey from the place of death. Before interment the body should be wrapped in a shroud of silk or cotton, and a ring should be placed on its finger bearing the inscription \"I came forth from God, and return unto Him, detached from all save Him, holding fast to His Name, the Merciful, the Compassionate\". The coffin should be of crystal, stone or hard fine wood. Also, before interment, a specific Prayer for the Dead[10] is ordained. The body should be placed with the feet facing the Qiblih. The formal prayer and the ring are meant to be used for those who have reached 15 years of age. Since there are no Bah\u00e1'\u00ed clergy, services are usually conducted under the guidance, or with the assistance of, a Local Spiritual Assembly.[11][12][13] A Buddhist funeral marks the transition from one life to the next for the deceased. It also reminds the living of their own mortality. Cremation is the preferred choice,[14] although burial is also allowed. Buddhists in Tibet perform sky burials where the body is exposed to be eaten by vultures. The body is dissected with a blade on the mountain top before the exposure. Crying and wailing is discouraged and the rogyapas (body breakers who perform the ritual) laugh as if they are doing farm work. Tibetan Buddhists believe that a lighthearted atmosphere during the funeral helps the soul of the dead to get a better afterlife. After the vultures consume all the flesh the rogpyas smash the bones into pieces and mix them with tsampa to feed to the vultures.[15] Congregations of varied denominations perform different funeral ceremonies, but most involve offering prayers, scripture reading from the Bible, a sermon, homily, or eulogy, and music.[2][16] One issue of concern as the 21st century began was with the use of secular music at Christian funerals, a custom generally forbidden by the Catholic Church.[17] Christian burials have traditionally occurred on consecrated ground such as in churchyards. There are many funeral norms in Christianity.[18] Burial, rather than a destructive process such as cremation, was the traditional practice amongst Christians, because of the belief in the resurrection of the body. Cremations later came into widespread use, although some denominations forbid them. The US Conference of Catholic Bishops said \"The Church earnestly recommends that the pious custom of burying the bodies of the deceased be observed; nevertheless, the Church does not prohibit cremation unless it was chosen for reasons contrary to Christian doctrine\" (canon 1176.3).[19][20] Antyesti, literally 'last rites' or 'last sacrifice', refers to the rite-of-passage rituals associated with a funeral in Hinduism.[21] It is sometimes referred to as Antima Samskaram, Antya-kriya, Anvarohanyya, or Vahni Sanskara. A dead adult Hindu is cremated, while a dead child is typically buried.[22][23] The rite of passage is said to be performed in harmony with the sacred premise that the microcosm of all living beings is a reflection of a macrocosm of the universe.[24] The soul (Atman, Brahman) is believed to be the immortal essence that is released at the Antyeshti ritual, but both the body and the universe are vehicles and transitory in various schools of Hinduism. They consist of five elements: air, water, fire, earth and space.[24] The last rite of passage returns the body to the five elements and origins.[22][24] The roots of this belief are found in the Vedas, for example in the hymns of Rigveda in section 10.16, as follows: Burn him not up, nor quite consume him, Agni: let not his body or his skin be scattered,\nO all possessing Fire, when thou hast matured him, then send him on his way unto the Fathers.\nWhen thou hast made him ready, all possessing Fire, then do thou give him over to the Fathers,\nWhen he attains unto the life that waits him, he shall become subject to the will of gods.\nThe Sun receive thine eye, the Wind thy Prana (life-principle, breathe); go, as thy merit is, to earth or heaven.\nGo, if it be thy lot, unto the waters; go, make thine home in plants with all thy members. \u2014\u200aRigveda 10.16[25] The final rites of a burial, in case of untimely death of a child, is rooted in Rigveda's section 10.18, where the hymns mourn the death of the child, praying to deity Mrityu to \"neither harm our girls nor our boys\", and pleads the earth to cover, protect the deceased child as a soft wool.[26][27] Among Hindus, the dead body is usually cremated within a day of death. In Hindu tradition, the body is usually kept at home with the family until its time for cremation. A typical Hindu funeral includes three main stages: a gathering or wake in the home, the cremation itself\u2014referred to as mukhagni\u2014and a follow-up ritual called the shraddha ceremony.[28] The body is washed, wrapped in white cloth for a man or a widow, red for a married woman,[23] the two toes tied together with a string, a Tilak (red mark) placed on the forehead.[22] The dead adult's body is carried to the cremation ground near a river or water, by family and friends, and placed on a pyre with feet facing south.[23] The eldest son, or a male mourner, or a priest then bathes before leading the cremation ceremonial function.[22][29] He circumambulates the dry wood pyre with the body, says a eulogy or recites a hymn in some cases, places sesame seed in the dead person's mouth, sprinkles the body and the pyre with ghee (clarified butter), then draws three lines signifying Yama (deity of the dead), Kala (time, deity of cremation) and the dead.[22] The pyre is then set ablaze, while the mourners mourn. The ash from the cremation is consecrated to the nearest river or sea.[29] After the cremation, a period of mourning is observed for 10 to 12 days after which the immediate male relatives or the sons of the deceased shave their head, trim their nails, recites prayers with the help of priest or Brahmin and invite all relatives, kins, friends and neighbours to eat a simple meal together in remembrance of the deceased.\nDuring the mourning period, sleeping arrangements in the home change too. Mattresses are taken off the beds and placed on the floor, and for twelve days, everyone in the household sleeps on the floor as part of the funeral customs.[30] This day, in some communities, also marks a day when the poor and needy are offered food in memory of the dead.[31] In most Hindu communities the last day of the mourning is called as Terahveen (the thirteenth day), and on this day items of basic needs along with some favourite items of the deceased are donated to the priests. Also on the same day the eldest son of the family is ceremonially crowned (called Pagdi Rasm) for he is now the head of the family. A feast is also organised for Brahmins, family members, and friends.[32] The belief that bodies are infested by Nasu upon death greatly influenced Zoroastrian burial ceremonies and funeral rites. Burial and cremation of corpses was prohibited, as such acts would defile the sacred creations of earth and fire respectively.[33] Burial of corpses was so looked down upon that the exhumation of \"buried corpses was regarded as meritorious.\" For these reasons, \"Towers of Silence\" were developed\u2014open air, amphitheater like structures in which corpses were placed so carrion-eating birds could feed on them. Sagd\u012bd, meaning 'seen by a dog,' is a ritual that must be performed as promptly after death as possible. The dog is able to calculate the degree of evil within the corpse, and entraps the contamination so it may not spread further, expelling Nasu from the body.[34] Nasu remains within the corpse until it has been seen by a dog, or until it has been consumed by a dog or a carrion-eating bird.[35] According to chapter 31 of the Denkard, the reasoning for the required consumption of corpses is that the evil influences of Nasu are contained within the corpse until, upon being digested, the body is changed from the form of nasa into nourishment for animals. The corpse is thereby delivered over to the animals, changing from the state of corrupted nasa to that of hixr, which is \"dry dead matter,\" considered to be less polluting. A path through which a funeral procession has traveled must not be passed again, as Nasu haunts the area thereafter, until the proper rites of banishment are performed.[36] Nasu is expelled from the area only after \"a yellow dog with four eyes, or a white dog with yellow ears\" is walked through the path three times.[37] If the dog goes unwillingly down the path, it must be walked back and forth up to nine times to ensure that Nasu has been driven off.[38] Zoroastrian ritual exposure of the dead is first known of from the writings of the mid-5th century BCE Herodotus, who observed the custom amongst Iranian expatriates in Asia Minor. In Herodotus' account (Histories i.140), the rites are said to have been \"secret\", but were first performed after the body had been dragged around by a bird or dog. The corpse was then embalmed with wax and laid in a trench. While the discovery of ossuaries in both eastern and western Iran dating to the 5th and 4th centuries BCE indicates that bones were isolated, that this separation occurred through ritual exposure cannot be assumed: burial mounds,[39] where the bodies were wrapped in wax, have also been discovered. The tombs of the Achaemenid emperors at Naqsh-e Rustam and Pasargadae likewise suggest non-exposure, at least until the bones could be collected. According to legend (incorporated by Ferdowsi into his Shahnameh), Zoroaster is himself interred in a tomb at Balkh (in present-day Afghanistan). Writing on the culture of the Persians, Herodotus reports on the Persian burial customs performed by the Magi, which are kept secret. However, he writes that he knows they expose the body of male dead to dogs and birds of prey, then they cover the corpse in wax, and then it is buried.[40] The Achaemenid custom is recorded for the dead in the regions of Bactria, Sogdia, and Hyrcania, but not in Western Iran. The Byzantine historian Agathias has described the burial of the Sasanian general Mihr-Mihroe: \"the attendants of Mermeroes took up his body and removed it to a place outside the city and laid it there as it was, alone and uncovered according to their traditional custom, as refuse for dogs and horrible carrion\". Towers are a much later invention and are first documented in the early 9th century CE. The ritual customs surrounding that practice appear to date to the Sassanid era (3rd\u20137th century CE). They are known in detail from the supplement to the Sh\u0101yest n\u0113 Sh\u0101yest, the two Revayats collections, and the two Saddars. Funerals in Islam (called Janazah in Arabic) follow fairly specific rites. In all cases, however, sharia (Islamic religious law) calls for burial of the body, preceded by a simple ritual involving bathing and shrouding the body, followed by salat (prayer). Burial rituals should normally take place as soon as possible and include: The mourning period is 40 days long.[44] In Judaism, funerals follow fairly specific rites, though they are subject to variation in custom. Halakha calls for preparatory rituals involving bathing and shrouding the body accompanied by prayers and readings from the Hebrew Bible, and then a funeral service marked by eulogies and brief prayers, and then the lowering of the body into the grave and the filling of the grave. Traditional law and practice forbid cremation of the body; the Reform Jewish movement generally discourages cremation but does not outright forbid it.[45][46] Burial rites should normally take place as soon as possible and include: In Sikhism death is considered a natural process, an event that has absolute certainty and only happens as a direct result of God's Will or Hukam.[48] In Sikhism, birth and death are closely associated, as they are part of the cycle of human life of \"coming and going\" (Punjabi: \u0a06\u0a35\u0a23\u0a41 \u0a1c\u0a3e\u0a23\u0a3e, romanized:\u00a0Aana Jaana) which is seen as a transient stage towards Liberation (\u0a2e\u0a4b\u0a16\u0a41 \u0a26\u0a41\u0a06\u0a30\u0a41, Mokh Du-aar), understood as completely in unity with God. Sikhs believe in reincarnation. Death is only the progression of the soul on its journey from God, through the created universe and back to God again. In life a Sikh is expected to constantly remember death so that they may be sufficiently prayerful, detached and righteous to break the cycle of birth and death and return to God. The public display of grief by wailing or crying out loud at the funeral (called Antam Sanskar) is discouraged and should be kept to a minimum. Cremation is the preferred method of disposal, burial and burial at sea are also allowed if by necessity or by the will of the person. Markers such as gravestones, monuments, etc. are not allowed, because the body is considered to be just the shell and the person's soul is their real self.[49] On the day of the cremation, the body is washed and dressed and then taken to the Gurdwara or home where hymns (Shabadads) from Sri Guru Granth Sahib Ji, the Sikh Scriptures are recited by the congregation. Kirtan may also be performed by Ragis while the relatives of the deceased recite \"Waheguru\" sitting near the coffin. This service normally takes from 30 to 60 minutes. At the conclusion of the service, an Ardas is said before the coffin is taken to the cremation site. At the point of cremation, a few more Shabadads may be sung and final speeches are made about the deceased person. The eldest son or a close relative generally lights the fire. This service usually lasts about 30 to 60 minutes. The ashes are later collected and disposed of by immersing them in a river, preferably one of the five rivers in the state of Punjab, India. The ceremony in which the Sidharan Paath is begun after the cremation ceremony, may be held when convenient, wherever the Sri Guru Granth Sahib Ji is present. Hymns are sung from Sri Guru Granth Sahib Ji; the first five and final verses of \"Anand Sahib,\" the \"Song of Bliss,\" are recited or sung. The first five verses of Sikhism's morning prayer, \"Japji Sahib\", are read aloud to begin the Sidharan paath. A hukam, or random verse, is then read from Sri Guru Granth Sahib Ji. Ardas, a prayer, is offered, and Prashad, a sacred sweet, is distributed. Langar, a meal, is then served to guests. While the Sidharan paath is being read, the family may also sing hymns daily. Reading may take as long as needed to complete the paath. This ceremony is followed by Sahaj Paath Bhog, Kirtan Sohila, night time prayer is recited for one week, and finally Ardas called the \"Antim Ardas\" (\"Final Prayer\") is offered the last week.[50] It was custom for an officiant to walk in front of the coffin with a horse's skull; this tradition was still observed by Welsh peasants up until the 19th century.[51] The Greek word for funeral \u2013 k\u0113de\u00eda (\u03ba\u03b7\u03b4\u03b5\u03af\u03b1) \u2013 derives from the verb k\u0113domai (\u03ba\u03ae\u03b4\u03bf\u03bc\u03b1\u03b9), that means attend to, take care of someone. Derivative words are also k\u0113dem\u00f3n (\u03ba\u03b7\u03b4\u03b5\u03bc\u03ce\u03bd, \"guardian\") and k\u0113demon\u00eda (\u03ba\u03b7\u03b4\u03b5\u03bc\u03bf\u03bd\u03af\u03b1, \"guardianship\"). From the Cycladic civilization in 3000 BCE until the Hypo-Mycenaean era in 1200\u20131100 BCE the main practice of burial is interment. The cremation of the dead that appears around the 11th century BCE constitutes a new practice of burial and is probably an influence from the East. Until the Christian era, when interment becomes again the only burial practice, both cremation and interment had been practiced depending on the area.[52] The ancient Greek funeral since the Homeric era included the pr\u00f3thesis (\u03c0\u03c1\u03cc\u03b8\u03b5\u03c3\u03b9\u03c2), the ekphor\u00e1 (\u1f10\u03ba\u03c6\u03bf\u03c1\u03ac), the burial and the per\u00eddeipnon (\u03c0\u03b5\u03c1\u03af\u03b4\u03b5\u03b9\u03c0\u03bd\u03bf\u03bd). In most cases, this process is followed faithfully in Greece until today.[53] Pr\u00f3thesis is the deposition of the body of the deceased on the funeral bed and the threnody of his relatives. Today the body is placed in the casket, that is always open in Greek funerals. This part takes place in the house where the deceased had lived. An important part of the Greek tradition is the epicedium, the mournful songs that are sung by the family of the deceased along with professional mourners (who are extinct in the modern era). The deceased was watched over by his beloved the entire night before the burial, an obligatory ritual in popular thought, which is maintained still. Ekphor\u00e1 is the process of transport of the mortal remains of the deceased from his residence to the church, nowadays, and afterward to the place of burial. The procession in the ancient times, according to the law, should have passed silently through the streets of the city. Usually certain favourite objects of the deceased were placed in the coffin in order to \"go along with him\". In certain regions, coins to pay Charon, who ferries the dead to the underworld, are also placed inside the casket. A last kiss is given to the beloved dead by the family before the coffin is closed. The Roman orator Cicero[54] describes the habit of planting flowers around the tomb as an effort to guarantee the repose of the deceased and the purification of the ground, a custom that is maintained until today. After the ceremony, the mourners return to the house of the deceased for the per\u00eddeipnon, the dinner after the burial. According to archaeological findings \u2013 traces of ash, bones of animals, shards of crockery, dishes and basins \u2013 the dinner during the classical era was also organized at the burial spot. Taking into consideration the written sources, however, the dinner could also be served in the houses.[55] The Necrodeipnon (\u039d\u03b5\u03ba\u03c1\u03cc\u03b4\u03b5\u03b9\u03c0\u03bd\u03bf\u03bd) was the funeral banquet which was given at the house of the nearest relative.[56][57] Two days after the burial, a ceremony called \"the thirds\" was held. Eight days after the burial the relatives and the friends of the deceased assembled at the burial spot, where \"the ninths\" would take place, a custom still kept. In addition to this, in the modern era, memorial services take place 40 days, 3 months, 6 months, 9 months, 1 year after the death and from then on every year on the anniversary of the death. The relatives of the deceased, for an unspecified length of time that depends on them, are in mourning, during which women wear black clothes and men a black armband.[clarification needed] Nekysia (\u039d\u03b5\u03ba\u03cd\u03c3\u03b9\u03b1), meaning the day of the dead, and Genesia (\u0393\u03b5\u03bd\u03ad\u03c3\u03b9\u03b1), meaning the day of the forefathers (ancestors), were yearly feasts in honour of the dead.[58][59] Nemesia (\u039d\u03b5\u03bc\u03ad\u03c3\u03b9\u03b1) or Nemeseia (N\u03b5\u03bc\u03ad\u03c3\u03b5\u03b9\u03b1) was also a yearly feast in honour of the dead, most probably intended for averting the anger of the dead.[60][61] In ancient Rome, the eldest surviving male of the household, the pater familias, was summoned to the death-bed, where he attempted to catch and inhale the last breath of the decedent. Funerals of the socially prominent usually were undertaken by professional undertakers called libitinarii. No direct description has been passed down of Roman funeral rites. These rites usually included a public procession to the tomb or pyre where the body was to be cremated. The surviving relations bore masks bearing the images of the family's deceased ancestors. The right to carry the masks in public eventually was restricted to families prominent enough to have held curule magistracies. Mimes, dancers, and musicians hired by the undertakers, and professional female mourners, took part in these processions. Less well-to-do Romans could join benevolent funerary societies (collegia funeraticia) that undertook these rites on their behalf. Nine days after the disposal of the body, by burial or cremation, a feast was given (cena novendialis) and a libation poured over the grave or the ashes. Since most Romans were cremated, the ashes typically were collected in an urn and placed in a niche in a collective tomb called a columbarium (literally, \"dovecote\").[62] During this nine-day period, the house was considered to be tainted, funesta, and was hung with Taxus baccata or Mediterranean Cypress branches to warn passersby. At the end of the period, the house was swept out to symbolically purge it of the taint of death. Several Roman holidays commemorated a family's dead ancestors, including the Parentalia, held February 13 through 21, to honor the family's ancestors; and the Feast of the Lemures, held on May 9, 11, and 13, in which ghosts (larvae) were feared to be active, and the pater familias sought to appease them with offerings of beans. The Romans prohibited cremation or inhumation within the sacred boundary of the city (pomerium), for both religious and civil reasons, so that the priests might not be contaminated by touching a dead body, and that houses would not be endangered by funeral fires. Restrictions on the length, ostentation, expense of, and behaviour during funerals and mourning gradually were enacted by a variety of lawmakers. Often the pomp and length of rites could be politically or socially motivated to advertise or aggrandise a particular kin group in Roman society. This was seen as deleterious to society and conditions for grieving were set. For instance, under some laws, women were prohibited from loud wailing or lacerating their faces and limits were introduced for expenditure on tombs and burial clothes. The Romans commonly built tombs for themselves during their lifetime. Hence these words frequently occur in ancient inscriptions, V.F. Vivus Facit, V.S.P. Vivus Sibi Posuit. The tombs of the rich usually were constructed of marble, the ground enclosed with walls, and planted around with trees. But common sepulchres usually were built below ground, and called hypogea. There were niches cut out of the walls, in which the urns were placed; these, from their resemblance to the niche of a pigeon-house, were called columbaria. Within the United States and Canada, in most cultural groups and regions, the funeral rituals can be divided into three parts: visitation, funeral, and the burial service. A home funeral (services prepared and conducted by the family, with little or no involvement from professionals) is legal in nearly every part of North America, but in the 21st century, they are uncommon in the US.[63] At the visitation (also called a \"viewing\", \"wake\" or \"calling hours\"), in Christian or secular Western custom, the body of the deceased person (or decedent) is placed on display in the casket (also called a coffin, however almost all body containers are caskets). The viewing often takes place on one or two evenings before the funeral. In the past, it was common practice to place the casket in the decedent's home or that of a relative for viewing. This practice continues in many areas of Ireland and Scotland. The body is traditionally dressed in the decedent's best clothes. In recent times there has been more variation in what the decedent is dressed in \u2013 some people choose to be dressed in clothing more reflective of how they dressed in life. The body will often be adorned with common jewelry, such as watches, necklaces, brooches, etc. The jewelry may be taken off and given to the family of the deceased prior to burial or be buried with the deceased. Jewelry has to be removed before cremation in order to prevent damage to the crematory. The body may or may not be embalmed, depending upon such factors as the amount of time since the death has occurred, religious practices, or requirements of the place of burial. The most commonly prescribed aspects of this gathering are that the attendees sign a book kept by the deceased's survivors to record who attended. In addition, a family may choose to display photographs taken of the deceased person during his/her life (often, formal portraits with other family members and candid pictures to show \"happy times\"), prized possessions and other items representing his/her hobbies and/or accomplishments. A more recent trend[when?] is to create a DVD with pictures and video of the deceased, accompanied by music, and play this DVD continuously during the visitation. The viewing is either \"open casket\", in which the embalmed body of the deceased has been clothed and treated with cosmetics for display; or \"closed casket\", in which the coffin is closed. The coffin may be closed if the body was too badly damaged because of an accident or fire or other trauma, deformed from illness, if someone in the group is emotionally unable to cope with viewing the corpse, or if the deceased did not wish to be viewed. In cases such as these, a picture of the deceased, usually a formal photo, is placed atop the casket. However, this step is foreign to Judaism; Jewish funerals are held soon after death (preferably within a day or two, unless more time is needed for relatives to come), and the corpse is never displayed. Torah law forbids embalming.[64] Traditionally flowers (and music) are not sent to a grieving Jewish family as it is a reminder of the life that is now lost. The Jewish shiva tradition discourages family members from cooking, so food is brought by friends and neighbors.[44] (See also Jewish bereavement.) The decedent's closest friends and relatives who are unable to attend frequently send flowers to the viewing, with the exception of a Jewish funeral,[65] where flowers would not be appropriate (donations are often given to a charity instead). Obituaries sometimes contain a request that attendees do not send flowers (e.g. \"In lieu of flowers\"). The use of these phrases has been on the rise for the past century. In the US in 1927, only 6% of the obituaries included the directive, with only 2% of those mentioned charitable contributions instead. By the middle of the century, they had grown to 15%, with over 54% of those noting a charitable contribution as the preferred method of expressing sympathy.[66] The deceased is usually transported from the funeral home to a church in a hearse, a specialized vehicle designed to carry casketed remains. The deceased is often transported in a procession (also called a funeral cort\u00e8ge), with the hearse, funeral service vehicles, and private automobiles traveling in a procession to the church or other location where the services will be held. In a number of jurisdictions, special laws cover funeral processions \u2013 such as requiring most other vehicles to give right-of-way to a funeral procession. Funeral service vehicles may be equipped with light bars and special flashers to increase their visibility on the roads. They may also all have their headlights on, to identify which vehicles are part of the cortege, although the practice also has roots in ancient Roman customs.[67] After the funeral service, if the deceased is to be buried the funeral procession will proceed to a cemetery if not already there. If the deceased is to be cremated, the funeral procession may then proceed to the crematorium. Funeral customs vary from country to country. In the United States, any type of noise other than quiet whispering or mourning is considered disrespectful. A burial tends to cost more than a cremation.[68] At a religious burial service, conducted at the side of the grave, tomb, mausoleum or cremation, the body of the decedent is buried or cremated at the conclusion. Sometimes, the burial service will immediately follow the funeral, in which case a funeral procession travels from the site of the funeral to the burial site. In some other cases, the burial service is the funeral, in which case the procession might travel from the cemetery office to the grave site. Other times, the burial service takes place at a later time, when the final resting place is ready, if the death occurred in the middle of winter. If the decedent served in a branch of the Armed forces, military rites are often accorded at the burial service.[69] In many religious traditions, pallbearers, usually males who are relatives or friends of the decedent, will carry the casket from the chapel (of a funeral home or church) to the hearse, and from the hearse to the site of the burial service.[70] Most religions expect coffins to be kept closed during the burial ceremony. In Eastern Orthodox funerals, the coffins are reopened just before burial to allow mourners to look at the deceased one last time and give their final farewells. Greek funerals are an exception as the coffin is open during the whole procedure unless the state of the body does not allow it. Morticians may ensure that all jewelry, including wristwatch, that were displayed at the wake are in the casket before it is buried or entombed. Custom requires that everything goes into the ground; however this is not true for Jewish services. Jewish tradition stipulates that nothing of value is buried with the deceased. In the case of cremation such items are usually removed before the body goes into the furnace. Pacemakers are removed prior to cremation \u2013 if left in they could explode. Funerals for indigenous people, like many other cultures, are a method to remember, commemorate and respect the dead through their own cultural practices and traditions. In the past, there has been scrutiny when the topic of indigenous funeral sites was approached. Thus the federal government deemed it necessary to include a series of acts that would protect and accurately affiliate some of these burials with their correct native individuals or groups. This was enacted through the Native American Graves Protection and Repatriation Act. Furthermore, in 2001 California created the California Native American Graves Protection and Repatriation Act that would \"require all state agencies and museums that receive state funding and that have possession or control over collections of humans remains or cultural items to provide a process for identification and repatriates of these items to appropriate tribes.\" In 2020, it was amended to include tribes that were beyond State and Federal knowledge. In the Ipai, Tipai, Paipai, and Kiliwa regions funeral practices are similar in their social and power dynamics. The way that these funeral sites were created was based on previous habitation. Meaning, these were sites were their peoples may have died or if they had been a temporary home for some of these groups.[71] Additionally, these individual burials were characterized by grave markers and/or grave offerings. The markers included inverted metates, fractured pieces of metates as well as cairns. As for offerings, food, shell and stone beads were often found in burial mounds along with portions human remains. The state of the human remains found at the site can vary, data suggests[71] that cremations are recent in prehistory compared to just burials. Ranging from the middle Holocene era to the Late Prehistoric Period. Additionally, the position these people were placed in plays a role in how the afterlife was viewed. With recent ethnographic evidence coming from the Yuman people, it is believed that the spirits of the dead could potentially harm the living. so, they would often layer the markers or offerings above the body so that they would be unable to \"leave\" their graves and enact harm. In the Los Angeles Basin, researchers discovered communal mourning features at West Bluffs and Landing Hill. These communal mourning rituals were estimated to have taken place during the Intermediate Period (3,000-1,000 B.P.). Archaeologists have found fragmented pieces of a large schist pestle which was deliberately broken in a methodical way. Other fragmented vessels show signs of uneven burning on the interior surface presumed to have been caused by burning combustible material. In the West Bluffs and Landing Hill assemblages there are many instances of artifacts that were dyed in red ochre pigment after being broken. The tradition of intentionally breaking objects has been a custom in the region for thousands of  years for the purpose of releasing the spirit within the object, reducing harm to the community, or as an expression of grief. Pigmentation of grave goods also has many interpretations, the Chumash associate the color red with both earth and fire. While some researchers consider the usage of the red pigment as an important transitional moment in the adult life cycle.[72] A memorial service[73] or memorial gathering is one given for the deceased, often without the body present. The service takes place after cremation or burial at sea, after an entombment in a mausoleum's crypt, after donation of the body to an academic or research institution, after a traditional burial in a cemetery plot (remains either in a coffin or an urn) or after the ashes have been scattered someplace. It is also significant when the person is missing and presumed dead, or known to be deceased though the body is not recoverable. These services often take place at a funeral home;[74] however, they can be held in a home, cemetery chapel, university, town hall, country club, restaurant, beach, community center, workplace, place of worship, hospital chapel, health club, performing arts center, wedding chapel, national park, townhouse, civic center, hotel, museum, sports field, pub, urban park or other location of some significance.[75][76] A memorial service may include speeches (eulogies), prayers, poems, or songs (most particularly hymns) to commemorate the deceased. Pictures of the deceased and flowers with sometimes an urn are usually placed where the coffin would normally be placed. After the sudden deaths of important public officials, public memorial services have been held by communities, including those without any specific connection to the deceased. For examples, community memorial services were held after the assassinations of US presidents James A. Garfield and William McKinley. In Finland, religious funerals (hautajaiset) are quite ascetic and typically follow Lutheran traditions.[77] The local priest or minister says prayers and blesses the deceased in their house. The mourners (saattov\u00e4ki) traditionally bring food to the mourners' house. Common current practice has the deceased placed into the coffin in the place where they died. The undertaker will pick up the coffin and place it in the hearse and drive it to the funeral home, while the closest relatives or friends of the deceased will follow the hearse in a funeral procession in their own cars. The coffin will be held at the funeral home until the day of the funeral. The funeral services may be divided into two parts. First is the church service (siunaustilaisuus) in a cemetery chapel or local church, then the burial.[78] The majority of Italians are Roman Catholic and follow Catholic funeral traditions. Historically, mourners would walk in a funeral procession to the gravesite; today vehicles are used. Greek funerals are generally held in churches, including a Trisagion service. There is usually a 40-day mourning period, and the end of which, a memorial service is held. Every year following, a similar service takes place, to mark the anniversary of the death.[79][80] In Poland, in urban areas, there are usually two, or just one \"stop\". The body, brought by a hearse from the mortuary, may be taken to a church or to a cemetery chapel. There is then a funeral mass or service at the cemetery chapel. Following the mass or Service the casket is carried in procession (usually on foot) by hearse to the grave. Once at the grave-site, the priest will commence the graveside committal service and the casket is lowered. The mass or service usually takes place at the cemetery. In some traditional rural areas, the wake (czuwanie) takes place in the house of the deceased or their relatives. The body lies in state for three days in the house. The funeral usually takes place on the third day. Family, neighbors and friends gather and pray during the day and night on those three days and nights. There are usually three stages in the funeral ceremony (ceremonia pogrzebowa, pogrzeb): the wake (czuwanie), then the body is carried by procession (usually on foot) or people drive in their own cars to the church or cemetery chapel for mass, and another procession by foot to the gravesite. After the funeral, families gather for a post-funeral get-together (stypa). It can be at the family home, or at a function hall. In Poland cremation is less popular because the Catholic Church in Poland prefers traditional burials (though cremation is allowed). Cremation is more popular among non-religious people and Protestants in Poland. An old funeral rite from the Scottish Highlands involved burying the deceased with a wooden plate resting on his chest. On the plate were placed a small amount of earth and salt, to represent the future of the deceased. The earth hinted that the body would decay and become one with the earth, while the salt represented the soul, which does not decay. This rite was known as \"earth laid upon a corpse\". This practice was also carried out in Ireland, as well as in parts of England, particularly in Leicestershire, although in England the salt was intended to prevent air from distending the corpse.[81] In Spain, a burial or cremation may occur very soon after a death. Most Spaniards are Roman Catholics and follow Catholic funeral traditions. First, family and friends sit with the deceased during the wake until the burial. Wakes are a social event and a time to laugh and honor the dead. Following the wake comes the funeral mass (Tanatorio) at the church or cemetery chapel. Following the mass is the burial. The coffin is then moved from the church to the local cemetery, often with a procession of locals walking behind the hearse. The first Swedish evangelical order of burial was given in Olaus Petri's handbook of 1529. From the medieval order, it had only kept burial and cremation.[82] The funeral where the priest blessed the recently deceased, which after the Reformation came to be called a reading, was forbidden in the church order of 1686, but was taken over by lay people instead. It was then followed by the wake, which was banned by the church law in 1686, when it was often considered degenerate to do dancing and games where beer and brandy were served.[83] It came however, to live on in the custom of \"singing out corpses\". In older times, the grave was often shoveled closed during the hymn singing. During the 17th century, homilies became common, they were later replaced by grift speeches, which, however, never became mandatory. In 1686, it was decided that those who had lived a Christian life should be honestly and properly buried in a grave. It also determined that the burial would be performed by a priest in the Church of Sweden (later some religious communities were given the right to bury their dead themselves). Burial could only take place at a burial site intended for the purpose. Loss of honorable burial became a punishment. A distinction was made between silent burial (for some serious criminals) and quiet burial without singing and bell ringing and with abbreviated ritual (for some criminals, unbaptized children and for those who committed suicide). Church burial was compulsory for members of the Church of Sweden until 1926, when the possibility was opened for civil burial.[82] In the UK, funerals are commonly held at a church, crematorium or cemetery chapel.[84] Historically, it was customary to bury the dead, but since the 1960s, cremation has been more common.[85] While there is no visitation ceremony like in North America, relatives may view the body beforehand at the funeral home. A room for viewing is usually called a chapel of rest.[86] Funerals typically last about half an hour.[87] They are sometimes split into two ceremonies: a main funeral and a shorter committal ceremony. In the latter, the coffin is either handed over to a crematorium[87] or buried in a cemetery.[88] This allows the funeral to be held at a place without cremation or burial facilities. Alternatively, the entire funeral may be held in the chapel of the crematorium or cemetery. It is not customary to view a cremation; instead, the coffin may be removed from the chapel or hidden with curtains towards the end of the funeral.[87] After the funeral, it is common for the mourners to gather for refreshments. This is sometimes called a wake, though this is different from how the term is used in other countries, where a wake is a ceremony before the funeral.[84] Traditionally, a good funeral (as they were called) had one draw the curtains for a period of time; at the wake, when new visitors arrived, they would enter from the front door and leave through the back door. The women stayed at home whilst the men attended the funeral, the village priest would then visit the family at their home to talk about the deceased and to console them.[89] The first child of William Price, a Welsh Neo-Druidic priest, died in 1884. Believing that it was wrong to bury a corpse, and thereby pollute the earth, Price decided to cremate his son's body, a practice which had been common in Celtic societies.\nThe police arrested him for the illegal disposal of a corpse.[90] Price successfully argued in court that while the law did not state that cremation was legal, it also did not state that it was illegal. The case set a precedent that, together with the activities of the newly founded Cremation Society of Great Britain, led to the Cremation Act 1902.[91] The Act imposed procedural requirements before a cremation could occur and restricted the practice to authorised places.[92] A growing number of families choose to hold a life celebration or celebration of life[93][94] event for the deceased. Like memorial services, this ceremony is held after burial, entombment or cremation of the deceased. An urn can be on display with flowers and photos on the altar after cremation like in a memorial service. Unlike funerals, the focus of the ceremony is on the life that was lived.[95] Such ceremonies may be held outside the funeral home or place of worship; country clubs, cemetery chapels, restaurants, beaches, performing arts centers, wedding chapels, urban parks, sports fields, hotels, civic centers, museums, hospital chapels, community centers, town halls, pubs and sporting facilities are popular choices based on the specific interests of the deceased. Celebrations of life focus on including the person's best qualities, interests, achievements and impact, rather than mourning a death.[93] Some events are portrayed as joyous parties, instead of a traditional somber funeral. Taking on happy and hopeful tones, celebrations of life discourage wearing black and focus on the deceased's individuality.[93] An extreme example might have \"a fully stocked open bar, catered food, and even favors.\"[94] Notable recent celebrations of life ceremonies include those for Ren\u00e9 Ang\u00e9lil[96] and Maya Angelou.[97] In Australia, funerary customs continue to evolve in response to cultural diversity and environmental awareness; see Funeral rituals and trends in Australia for details of current practices. Originating in New Orleans, Louisiana, U.S., alongside the emergence of jazz music in late 19th and early 20th centuries, the jazz funeral is a traditionally African-American burial ceremony and celebration of life unique to New Orleans that involves a parading funeral procession accompanied by a brass band playing somber hymns followed by upbeat jazz music. Traditional jazz funerals begin with a processional led by the funeral director, family, friends, and the brass band, i.e., the \"main line\", who march from the funeral service to the burial site while the band plays slow dirges and Christian hymns. After the body is buried, or \"cut loose\", the band begins to play up-tempo, joyful jazz numbers, as the main line parades through the streets and crowds of \"second liners\" join in and begin dancing and marching along, transforming the funeral into a street festival.[98] The terms \"green burial\" and \"natural burial\", used interchangeably, apply to ceremonies that aim to return the body with the earth with little to no use of artificial, non-biodegradable materials. As a concept, the idea of uniting an individual with the natural world after they die appears as old as human death itself, being widespread before the rise of the funeral industry. Holding environmentally-friendly ceremonies as a modern concept first attracted widespread attention in the 1990s. In terms of North America, the opening of the first explicitly \"green\" burial cemetery in the U.S. took place in the state of South Carolina. However, the Green Burial Council, which came into being in 2005, has based its operations out of California. The institution works to officially certify burial practices for funeral homes and cemeteries, making sure that appropriate materials are used.[99] Religiously, some adherents of the Roman Catholic Church often have particular interest in \"green\" funerals given the faith's preference to full burial of the body as well as the theological commitments to care for the environment stated in Catholic social teaching.[99] Those with concerns about the effects on the environment of traditional burial or cremation may be placed into a natural bio-degradable green burial shroud. That, in turn, sometimes gets placed into a simple coffin made of cardboard or other easily biodegradable material. Furthermore, individuals may choose their final resting place to be in a specially designed park or woodland, sometimes known as an \"ecocemetery\", and may have a tree or other item of greenery planted over their grave both as a contribution to the environment and a symbol of remembrance. Humanists UK organises a network of humanist funeral celebrants or officiants across England and Wales, Northern Ireland, and the Channel Islands[100] and a similar network is organised by the Humanist Society Scotland. Humanist officiants are trained and experienced in devising and conducting suitable ceremonies for non-religious individuals.[101] Humanist funerals recognise no \"afterlife\", but celebrate the life of the person who has died.[100] In the twenty-first century, humanist funerals were held for well-known people including Claire Rayner,[102] Keith Floyd,[103][104] Linda Smith,[105] and Ronnie Barker.[106] In areas outside of the United Kingdom, Ireland has featured an increasing number of non-religious funeral arrangements according to publications such as Dublin Live. This has occurred in parallel with a trend of increasing numbers of people carefully scripting their own funerals before they die, writing the details of their own ceremonies. The Irish Association of Funeral Directors has reported that funerals without a religious focus occur mainly in more urbanized areas in contrast to rural territories.[107] Notably, humanist funerals have started to become more prominent in other nations such as the Republic of Malta, in which civil rights activist and humanist Ramon Casha had a large scale event at the Radisson Blu Golden Sands resort devoted to laying him to rest. Although such non-religious ceremonies are \"a rare scene in Maltese society\" due to the large role of the Roman Catholic Church within that country's culture, according to Lovin Malta, \"more and more Maltese people want to know about alternative forms of burial... without any religion being involved\".[108][109] Actual events during non-religious funerals vary, but they frequently reflect upon the interests and personality of the deceased. For example, the humanist ceremony for the aforementioned Keith Floyd, a restaurateur and television personality, included a reading of Rudyard Kipling's poetic work \"If\u2014\" and a performance by musician Bill Padley.[103] Organizations such as the Irish Institute of Celebrants have stated that more and more regular individuals request training for administering funeral ceremonies, instead of leaving things to other individuals.[107] More recently, some commercial organisations offer civil funerals that can integrate traditionally religious content.[110] Funerals specifically for fallen members of fire or police services are common in United States and Canada. These funerals involve honour guards from police forces and/or fire services from across the country and sometimes from overseas.[111] A parade of officers often precedes or follows the hearse carrying the fallen comrade.[111] A traditional fire department funeral consists of two raised aerial ladders.[112] The firefighters travel under the aerials on their ride, on the fire apparatus, to the cemetery. Once there, the grave service includes the playing of bagpipes. The pipes have come to be a distinguishing feature of a fallen hero's funeral. Also a \"Last Alarm Bell\" is rung. A portable fire department bell is tolled at the conclusion of the ceremony. A Masonic funeral is held at the request of a departed Mason or family member. The service may be held in any of the usual places or a Lodge room with committal at graveside, or the complete service can be performed at any of the aforementioned places without a separate committal. Freemasonry does not require a Masonic funeral. There is no single convention for a Masonic funeral service. Some Grand Lodges have a prescribed service (as it is a worldwide organisation). Some of the customs include the presiding officer wearing a hat while doing his part in the service, the Lodge members placing sprigs of evergreen on the casket, and a small white leather apron may being placed in or on the casket. The hat may be worn because it is Masonic custom (in some places in the world) for the presiding officer to have his head covered while officiating. To Masons, the sprig of evergreen is a symbol of immortality. A Mason wears a white leather apron, called a \"lambskin\", on becoming a Mason, and he may continue to wear it even in death.[113][114] In most East Asian, South Asian and many Southeast Asian cultures, the wearing of white is symbolic of death. In these societies, white or off-white robes are traditionally worn to symbolize that someone has died and can be seen worn among relatives of the deceased during a funeral ceremony. In Chinese culture, red is strictly forbidden as it is a traditionally symbolic color of happiness. Exceptions are sometimes made if the deceased has reached an advanced age such as 85, in which case the funeral is considered a celebration, where wearing white with some red is acceptable. Contemporary Western influence however has meant that dark-colored or black attire is now often also acceptable for mourners to wear (particularly for those outside the family). In such cases, mourners wearing dark colors at times may also wear a white or off-white armband or white robe. Contemporary South Korean funerals typically mix western culture with traditional Korean culture, largely depending on socio-economic status, region, and religion. In almost all cases, all related males in the family wear woven armbands representing seniority and lineage in relation to the deceased, and must grieve next to the deceased for a period of three days before burying the body. During this period of time, it is customary for the males in the family to personally greet all who come to show respect. While burials have been preferred historically, recent trends show a dramatic increase in cremations due to shortages of proper burial sites and difficulties in maintaining a traditional grave. The ashes of the cremated corpse are commonly stored in columbaria. Most Japanese funerals are conducted with Buddhist and/or Shinto rites.[115] Many ritually bestow a new name on the deceased; funerary names typically use obsolete or archaic kanji and words, to avoid the likelihood of the name being used in ordinary speech or writing. The new names are typically chosen by a Buddhist priest, after consulting the family of the deceased. Religious thought among the Japanese people is generally a blend of Shint\u014d and Buddhist beliefs. In modern practice, specific rites concerning an individual's passage through life are generally ascribed to one of these two faiths. Funerals and follow-up memorial services fall under the purview of Buddhist ritual, and 90% Japanese funerals are conducted in a Buddhist manner[?]. Aside from the religious aspect, a Japanese funeral usually includes a wake, the cremation of the deceased, and inclusion within the family grave. Follow-up services are then performed by a Buddhist priest on specific anniversaries after death. According to an estimate in 2005, 99% of all deceased Japanese are cremated.[116] In most cases the cremated remains are placed in an urn and then deposited in a family grave. In recent years however, alternative methods of disposal have become more popular, including scattering of the ashes, burial in outer space, and conversion of the cremated remains into a diamond that can be set in jewelry. Funeral practices and burial customs in the Philippines encompass a wide range of personal, cultural, and traditional beliefs and practices which Filipinos observe in relation to death, bereavement, and the proper honoring, interment, and remembrance of the dead. These practices have been vastly shaped by the variety of religions and cultures that entered the Philippines throughout its complex history. Most if not all present-day Filipinos, like their ancestors, believe in some form of an afterlife and give considerable attention to honouring the dead.[117] Except amongst Filipino Muslims (who are obliged to bury a corpse less than 24 hours after death), a wake is generally held from three days to a week.[118] Wakes in rural areas are usually held in the home, while in urban settings the dead is typically displayed in a funeral home. Friends and neighbors bring food to the family, such as pancit noodles and bibingka cake; any leftovers are never taken home by guests, because of a superstition against it.[44] Apart from spreading the news about someone's death verbally,[118] obituaries are also published in newspapers. Although the majority of the Filipino people are Christians,[119] they have retained some traditional indigenous beliefs concerning death.[120][121] In Korea, funerals are typically held for three days and different things are done in each day. The first day: on the day a person dies, the body is moved to a funeral hall. They prepare clothes for the body and put them into a chapel of rest. Then food is prepared for the deceased. It is made up of three bowls of rice and three kinds of Korean side dishes. Also, there has to be three coins and three straw shoes. This can be cancelled if the family of the dead person have a particular religion.[122] On the second day the funeral director washes the body and shrouding is done. Then, a family member of the dead person puts uncooked rice in the mouth of the body. This step does not have to be done if the family has a certain religion. After putting the rice in the mouth, the body is moved into a coffin. Family members, including close relatives, of the dead person will wear mourning clothing. Typically, mourning for a woman includes Korean traditional clothes, Hanbok, and mourning for man includes a suit. The color has to be black. The ritual ceremony begins when they are done with changing clothes and preparing foods for the dead person. The ritual ceremony is different depending on their religion. After the ritual ceremony family members will start to greet guests.[123] On the third day, the family decides whether to bury the body in the ground or cremate the body. In the case of burial, three family members sprinkle dirt on the coffin three times. In the case of cremation, there is no specific ritual; the only requirement is a jar to store burned bones and a place to keep the jar. Other than these facts, in Korea, people who come to the funeral bring condolence money. Also, a food called Yukgaejang is served to guests, oftentimes with the Korean distilled drink called soju.[124] In Mongolia, like many other cultures, funeral practices are considered extremely important.[citation needed], possessing significant elements of both native Mongolian rituals and Buddhist tradition.[125] For Mongolians who are very strict about tradition, families choose from three different ways of burial: open-air burial which is most common, cremation, and embalming. Many factors go into deciding which funeral practice to do. These consisted of the family's social standing, the cause of death, and the place of death. Embalming was mainly chosen by members of the Lamaistic Church; by choosing this practice, they are usually buried in a sitting position. This would show that they would always be in the position of prayer. Also, more important people such as nobles would be buried with weapons, horses and food in their coffins to help them prepare for the next world.[126] The coffin is designed and built by three to four relatives, mainly men. The builders bring planks to the hut where the dead is located and put together the box and the lid. The same people who build the coffin also decorate the funeral. Most of this work is done after dusk. With specific instruction, they work on decorations inside the youngest daughter's house. The reason for this is so the deceased is not disturbed at night.[127] In Vietnam, Buddhism is the most commonly practiced religion, however, most burial methods do not coincide with the Buddhist belief of cremation.[128] The body of the deceased is moved to a loved one's house and placed in an expensive coffin. The body usually stays there for about three days, allowing time for people to visit and place gifts in the mouth.[128] This stems from the Vietnamese belief that the dead should be surrounded by their family. This belief goes so far as to include superstition as well. If somebody is dying in Vietnamese culture, they are rushed home from the hospital so they can die there, because if they die away from home it is believed to be bad luck to take a corpse home.[129] Many services are also held in the Vietnamese burial practices. One is held before moving the coffin from the home and the other is held at the burial site.[130] After the burial of the loved one, incense is burned at the gravesite and respect is paid to all the nearby graves. Following this, the family and friends return to the home and enjoy a feast to celebrate the life of the recently departed.[130] Even after the deceased has been buried, the respect and honor continues. For the first 49 days after the burying, the family holds a memorial service every 7 days, where the family and friends come back together to celebrate the life of their loved one. After this, they meet again on the 100th day after the death, then 265 days after the death, and finally they meet on the anniversary of the death of their loved one, a whole year later, to continue to celebrate the glorious life of their recently departed.[131] The Vietnamese funeral, or \u0111\u00e1m gi\u1ed7, is a less somber occasion than most traditional Western funerals. The \u0111\u00e1m gi\u1ed7 is a celebration of the deceased's life and is centered around the deceased's family.[132] Family members might wear a traditional garment called a mourning headband to signify their relationship with the deceased. Typical mourning headbands are thin strips of fabric that are wrapped around the wearer's head. Traditionally, the deceased's closest family members, such as children, siblings, spouses, and parents will wear white mourning headbands. More distant family members' headband colors may vary. In some cultures, the deceased's nieces, nephews, or grandchildren may be required to wear white headbands with red dots. Other societies may encourage grandchildren to wear white headbands with blue dots. Fourth generation grandchildren often wear yellow mourning headbands. The use of mourning headbands emphasizes the importance of personal and familial roles in Vietnamese society. It also allows funeral attendants to carefully choose their interactions and offer condolences to those closest to the deceased.[133] Traditionally, attendants of a Vietnamese funeral service are encouraged to wear the color white. In many East Asian cultures, white is viewed as a sign of loss and mourning. In Vietnam, members of the Caodaist faith believe that white represents purity and the ability to communicate beyond spiritual worlds.[134] African funerals are usually open to many visitors. The custom of burying the dead in the floor of dwelling-houses has been to some degree prevalent on the Gold Coast of Africa. The ceremony depends on the traditions of the ethnicity the deceased belonged to. The funeral may last for as much as a week. Another custom, a kind of memorial, frequently takes place seven years after the person's death. These funerals and especially the memorials may be extremely expensive for the family in question. Cattle, sheep, goats, and poultry, may be offered and then consumed. The Ashanti and Akan ethnic groups in Ghana typically wear red and black during funerals. For special family members, there is typically a funeral celebration with singing and dancing to honor the life of the deceased. Afterwards, the Akan hold a sombre funeral procession and burial with intense displays of sorrow. Other funerals in Ghana are held with the deceased put in elaborate Fantasy coffins colored and shaped after a certain object, such as a fish, crab, boat, and even airplanes.[131] The Kane Kwei Carpentry Workshop in Teshie, named after Seth Kane Kwei who invented this new style of coffin, has become an international reference for this form of art. Evidence of Africa's earliest funeral was found in Kenya in 2021. A 78,000 year old Middle Stone Age grave of a three-year-old child was discovered in Panga ya Saidi cave complex, Kenya. Researchers said the child's head appeared to have been laid on a pillow. The body had been laid in a fetal position.[135][136] In Kenya funerals are an expensive undertaking. Keeping bodies in morgues to allow for fund raising is a common occurrence more so in urban areas. Some families opt to bury their dead in the countryside homes instead of urban cemeteries, thus spending more money on transporting the dead. The first emperor of the Qin dynasty, Qin Shi Huang's mausoleum is located in the Lintong District of Xi'an, Shaanxi Province. Qin Shi Huang's tomb is one of the World Heritage sites in China. Its remarkable feature and size have been known as one of the most important historical sites in China.[137] Qin Shi Huang is the first emperor who united China for the first time. The mausoleum was built in 247 BCE after he became the emperor of the Qin dynasty. Ancient Chinese mausoleums have unique characteristics compared to other cultures[citation?]. Ancient Chinese thought that the soul remains even after death, (immortal soul) regarded funeral practices as an important tradition.[138] From their long history, the construction of mausoleums has developed over time, creating monumental and massive ancient emperor's tomb. Archeologists have found more than 8,000 life-sized figures resembling an army surrounding the emperor's tomb.[139] The primary purpose of the placement of Terracotta Army is to protect the emperor's tomb. The figures were composed of clay and fragments of pottery. The Terracotta Army represents soldiers, horses, government officials, and even musicians. The arrangement and the weapons they are carrying accurately represent the real formations and weapons of the time. Furthermore, facial features aren't identical, each sculpture bearing a unique look. The Imperial Tombs of the Ming and Qing Dynasties are included as World Heritage Sites. The three Imperial Tombs of the Qin dynasty were added in 2000 and 2003.[140] The three tombs were all built in the 17th century. The tombs have been constructed to memorialize the emperors of the Qing dynasty and their ancestors. In tradition, Chinese have followed Feng Shui to build and decorate the interior. All of the tombs are strictly made following the superstition of Feng Shui. The Imperial Tombs of the Ming and Qing Dynasties clearly show the cultural and architectural tradition that has existed in the area for more than 500 years[citation?]. In Chinese culture, the tombs were considered as a portal between the world of the living and the dead[citation?]. Chinese believed that the portal would divide the soul into two parts. The half of the soul would go to heaven, and the other half would remain within the physical body.[141] From about 1600 to 1914 Europe had two professions that have almost entirely disappeared. The mute appears in art quite frequently, but in literature is probably best known from Dickens's Oliver Twist (1837\u20131839). Oliver is working for Mr Sowerberry when characterised thus: \"There's an expression of melancholy in his face, my dear... which is very interesting. He would make a delightful mute, my love.\" And in Martin Chuzzlewit (1842\u20131844), Moult, the undertaker, states: \"This promises to be one of the most impressive funerals,...no limitation of expense...I have orders to put on my whole establishment of mutes, and mutes come very dear, Mr Pecksniff\". The main function of a funeral mute was to stand around at funerals with a sad, pathetic face. A symbolic protector of the deceased, the mute would usually stand near the door of the home or church. In Victorian times, mutes would wear somber clothing including black cloaks, top hats with trailing hatbands, and gloves.[142] The professional mourner, generally a woman, would shriek and wail (often while clawing her face and tearing at her clothing), to encourage others to weep. Records document forms of professional mourning from Ancient Greece.[143][144] The 2003 award-winning Philippine comedy Crying Ladies revolves around the lives of three women who are part-time professional mourners for the Chinese-Filipino community in Manila's Chinatown. According to the film, the Chinese use professional mourners to help expedite the entry of a deceased loved one's soul into heaven by giving the impression that he or she was a good and loving person, well-loved by many. High-ranking national figures such as heads of state, prominent politicians, military figures, national heroes and eminent cultural figures may be offered state funerals. Common methods of disposal are: Some people choose to make their funeral arrangements in advance so that at the time of their death, their wishes are known to their family. However, the extent to which decisions regarding the disposition of a decedent's remains (including funeral arrangements) can be controlled by the decedent while still alive vary from one jurisdiction to another. In the United States, there are states which allow one to make these decisions for oneself if desired, for example by appointing an agent to carry out one's wishes; in other states, the law allows the decedent's next-of-kin to make the final decisions about the funeral without taking the wishes of the decedent into account.[150] The decedent may, in most U.S. jurisdictions, provide instructions as to the funeral by means of a last will and testament. These instructions can be given some legal effect if bequests are made contingent on the heirs carrying them out, with alternative gifts if they are not followed. This requires the will to become available in time; aspects of the disposition of the remains of US President Franklin Delano Roosevelt ran contrary to a number of his stated wishes, which were found in a safe that was not opened until after the funeral. Some people donate their bodies to a medical school for use in research or education. Medical students frequently study anatomy from donated cadavers; they are also useful in forensic research.[151] Some medical conditions, such as amputations or various surgeries can make the cadaver unsuitable for these purposes; in other cases the bodies of people who had certain medical conditions are useful for research into those conditions. Many medical schools rely on the donation of cadavers for the teaching of anatomy.[152] It is also possible to arrange for donate organs and tissue after death for treating the sick, or even whole cadavers for forensic research at body farms.",
      "ground_truth_chunk_ids": [
        "134_random_chunk1"
      ],
      "source_ids": [
        "S334"
      ],
      "category": "factual",
      "id": 10
    },
    {
      "question": "What is Dartmouth Jack-O-Lantern?",
      "ground_truth": "The Dartmouth Jack-O-Lantern (also known as the Jacko)[1] is a college humor magazine, founded at Dartmouth College in 1908. One of the magazine's oldest traditions is \"Stockman's Dogs\". In the October 1934 issue, F.C. Stockman (class of 1935) drew a single-panel cartoon of two dogs talking to each other. That same cartoon has appeared in virtually every issue published since, always with a different caption.[2] The magazine is alluded to in the opening lines of F. Scott Fitzgerald's short story \"The Lost Decade\", which was first published in Esquire in 1939.[3] Jack-O-Lantern writers Nic Duquette and Chris Plehal invented the unofficial Dartmouth mascot Keggy the Keg in the fall of 2003.[4] From 1972 to 1974 the Editor in chief was playwright Robert DeKanter '74. Among the first Dartmouth women on the staff was Barbara Donnelly, '77, later a writer for the Wall Street Journal. DeKanter was succeeded by the team Brad Brinegar and Maxwell Anderson, both '77. One evening in July, 1975, cartoonists Brian \"Hojo\" Hansen '76 and Mike Mosher '77 slipped in and painted a cubist rendition of bibulous alumni in translucent acrylic washes upon the wall. When this was eradicated the following week, Hansen and Mosher replaced it with a Renaissance-style \"pittura infamante\" (topic of an art history lecture in Carpenter Hall) called Allegory of the Evisceration of Humor, depicting Brinegar and Anderson abusing a Jack-O-Lantern figure. \"This was the perfect crime\" enthused Hansen, \"for to paint it over would prove our point: that they have no sense of humor.\" From 1976 to 1978 the Editor was N. Brooks Clark, who published a Jack-O-Lantern calendar during his tenure. Clark wrote a parody of the controversial college-issued sex guide, which he called Thrilling Contraception Comics and Stories, illustrated by Mosher and featuring a wisecracking spermatozoic guide, Snappy Sammy Sperm.",
      "expected_answer": "The Dartmouth Jack-O-Lantern (also known as the Jacko)[1] is a college humor magazine, founded at Dartmouth College in 1908. One of the magazine's oldest traditions is \"Stockman's Dogs\". In the October 1934 issue, F.C. Stockman (class of 1935) drew a single-panel cartoon of two dogs talking to each other. That same cartoon has appeared in virtually every issue published since, always with a different caption.[2] The magazine is alluded to in the opening lines of F. Scott Fitzgerald's short story \"The Lost Decade\", which was first published in Esquire in 1939.[3] Jack-O-Lantern writers Nic Duquette and Chris Plehal invented the unofficial Dartmouth mascot Keggy the Keg in the fall of 2003.[4] From 1972 to 1974 the Editor in chief was playwright Robert DeKanter '74.  Among the first Dartmouth women on the staff was Barbara Donnelly, '77, later a writer for the Wall Street Journal. DeKanter was succeeded by the team Brad Brinegar and Maxwell Anderson, both '77.  One evening in July, 1975, cartoonists Brian \"Hojo\" Hansen '76 and Mike Mosher '77 slipped in and painted a cubist rendition of bibulous alumni in translucent acrylic washes upon the wall.  When this was eradicated the following week, Hansen and Mosher replaced it with a Renaissance-style \"pittura infamante\" (topic of an art history lecture in Carpenter Hall) called Allegory of the Evisceration of Humor, depicting Brinegar and Anderson abusing a Jack-O-Lantern figure.  \"This was the perfect crime\" enthused Hansen, \"for to paint it over would prove our point: that they have no sense of humor.\" From 1976 to 1978 the Editor was N. Brooks Clark, who published a Jack-O-Lantern calendar during his tenure.  Clark wrote a parody of the controversial college-issued sex guide, which he called Thrilling Contraception Comics and Stories, illustrated by Mosher and featuring a wisecracking spermatozoic guide, Snappy Sammy Sperm.  It was reprinted in the 1982 Holt paperback collection of 1970s college humor,[5] whose lead editor Joey Green was the founding editor of the Cornell Lunatic. A 2006 video prank by the Jack-O-Lantern on a Dartmouth College tour group entitled \"Drinkin' Time\" was featured in an article by the Chronicle of Higher Education,[6] posted by AOL on the Online Video Blog,[7] and was mentioned by The Volokh Conspiracy.[8] As of November\u00a02013[update], the video has garnered over 585,000 views on YouTube.[9] The Jacko publishes print issues approximately four times a year, as well as regularly updated online content and occasional video productions. The magazine devotes one publication cycle each year to a parody of the campus newspaper, The Dartmouth.[1] Some notable writers, artists, comedians and politicians began their careers at the Jacko, including:[10]",
      "ground_truth_chunk_ids": [
        "204_random_chunk1"
      ],
      "source_ids": [
        "S404"
      ],
      "category": "factual",
      "id": 11
    },
    {
      "question": "What is Chelsea F.C.\u2013Liverpool F.C. rivalry?",
      "ground_truth": "The Chelsea F.C.\u2013Liverpool F.C. rivalry is an inter-city rivalry between English professional football clubs Chelsea and Liverpool. Chelsea play their home games at Stamford Bridge, while Liverpool play their home games at Anfield. Though both clubs have frequently competed in the same division for over a century, the modern rivalry between Chelsea and Liverpool began in the early 2000s, when the two clubs clashed repeatedly in cup competitions, particularly in the FA Cup, the League Cup, and the UEFA Champions League. The clubs have competed in seven major cup finals: the 2005 League Cup final, which Chelsea won 3\u20132 after extra time, the 2006 Community Shield, which Liverpool won 2\u20131, the 2012 FA Cup final, which Chelsea won 2\u20131, the 2019 UEFA Super Cup, which Liverpool won 5\u20134 on penalties, the 2022 EFL Cup and FA Cup finals, both of which saw Liverpool win on penalties after two goalless affairs, and the 2024 EFL Cup final, which Liverpool won 1\u20130 after extra time. The two clubs also met in five consecutive Champions League campaigns; in the group stage of the 2005\u201306 season, where both legs finished as goalless draws, in the quarter-finals of the 2008\u201309 season, which Chelsea won 7\u20135 on aggregate, and in the semi-finals of the 2004\u201305, 2006\u201307 and 2007\u201308 seasons, with Liverpool winning the former two and Chelsea winning the latter one.[1][2] Overall, Liverpool have won more of the meetings, defeating Chelsea 87 times to their 67 wins, and a further 46 games ended in draws, as of their latest clash in October 2025. Chelsea's record win over the Reds was a 6\u20131 thrashing at Stamford Bridge in August 1937, whereas Liverpool's biggest win was a 6\u20130 home win in April 1935. In 1904, Gus Mears acquired the Stamford Bridge athletics stadium in Fulham with the aim",
      "expected_answer": "The Chelsea F.C.\u2013Liverpool F.C. rivalry is an inter-city rivalry between English professional football clubs Chelsea and Liverpool. Chelsea play their home games at Stamford Bridge, while Liverpool play their home games at Anfield. Though both clubs have frequently competed in the same division for over a century, the modern rivalry between Chelsea and Liverpool began in the early 2000s, when the two clubs clashed repeatedly in cup competitions, particularly in the FA Cup, the League Cup, and the UEFA Champions League. The clubs have competed in seven major cup finals: the 2005 League Cup final, which Chelsea won 3\u20132 after extra time, the 2006 Community Shield, which Liverpool won 2\u20131, the 2012 FA Cup final, which Chelsea won 2\u20131, the 2019 UEFA Super Cup, which Liverpool won 5\u20134 on penalties, the 2022 EFL Cup and FA Cup finals, both of which saw Liverpool win on penalties after two goalless affairs, and the 2024 EFL Cup final, which Liverpool won 1\u20130 after extra time. The two clubs also met in five consecutive Champions League campaigns; in the group stage of the 2005\u201306 season, where both legs finished as goalless draws, in the quarter-finals of the 2008\u201309 season, which Chelsea won 7\u20135 on aggregate, and in the semi-finals of the 2004\u201305, 2006\u201307 and 2007\u201308 seasons, with Liverpool winning the former two and Chelsea winning the latter one.[1][2] Overall, Liverpool have won more of the meetings, defeating Chelsea 87 times to their 67 wins, and a further 46 games ended in draws, as of their latest clash in October 2025. Chelsea's record win over the Reds was a 6\u20131 thrashing at Stamford Bridge in August 1937, whereas Liverpool's biggest win was a 6\u20130 home win in April 1935. In 1904, Gus Mears acquired the Stamford Bridge athletics stadium in Fulham with the aim of turning it into a football ground. An offer to lease it to nearby Fulham F.C. was turned down, so Mears opted to found his own club to use the stadium. As there was already a team named Fulham in the borough, the name of the adjacent borough of Chelsea was chosen for the new club; names like Kensington FC, Stamford Bridge FC and London FC were also considered.[3] Chelsea Football Club was founded on 10 March 1905 at The Rising Sun pub (now The Butcher's Hook),[4][5] opposite the present-day main entrance to the ground on Fulham Road, and were elected to the Football League shortly afterwards. Chelsea won promotion to the First Division in their second season, and yo-yoed between the First and Second Divisions in its early years. The team reached the 1915 FA Cup final, where they lost to Sheffield United at Old Trafford, and finished third in the First Division in 1920, the club's best league campaign to that point.[6] Chelsea had a reputation for signing star players[7] and attracted large crowds. The club had the highest average attendance in English football in ten separate seasons[8] including 1907\u201308,[9] 1909\u201310,[10] 1911\u201312,[11] 1912\u201313,[12] 1913\u201314[13] and 1919\u201320.[14][15] They were FA Cup semi-finalists in 1920 and 1932, and remained in the First Division throughout the 1930s, but success eluded the club in the inter-war years. Liverpool Football Club was founded following a dispute between the Everton committee and John Houlding, club president and owner of the land at Anfield. After eight years at the stadium, Everton relocated to Goodison Park in 1892 and Houlding founded Liverpool F.C. to play at Anfield.[16] Originally named \"Everton F.C. and Athletic Grounds Ltd\" (Everton Athletic for short), the club became Liverpool F.C. in March 1892 and gained official recognition three months later, after The Football Association refused to recognise the club as Everton.[17] Liverpool played their first match on 1 September 1892, a pre-season friendly match against Rotherham Town, which they won 7\u20131. The team Liverpool fielded against Rotherham was composed entirely of Scottish players\u2014the players who came from Scotland to play in England in those days were known as the Scotch Professors. Manager John McKenna had recruited the players after a scouting trip to Scotland\u2014so they became known as the \"team of Macs\".[18] The team won the Lancashire League in its debut season and joined the Football League Second Division at the start of the 1893\u201394 season. After the club was promoted to the First Division in 1896, Tom Watson was appointed manager. He led Liverpool to its first league title in 1901, before winning it again in 1906.[19] Chelsea and Liverpool were not traditional rivals, meeting first for the first time on 25 December 1907 at Anfield in the Football League First Division, which ended in a 4\u20131 win for Chelsea. However, for the next 96 years, Chelsea would only manage one single league title, which came in 1955, whereas Liverpool (who were already two-time champions) would go on to win the First Division title sixteen more times, cementing the Reds' status as one of the biggest clubs in England and in Europe, along with major rivals Manchester United, whereas Chelsea were considered to be a mid-table club, and their rivalry with Liverpool was non-existent during the years leading up to the 21st century. We were the new kids on the block who had a few quid and signed a load of players. Jos\u00e9 Mourinho puffed his chest out and then we kept playing each other. It was a clash of two ideals. \u2014\u200aFrank Lampard on Chelsea's sudden rise to success[20] The seeds of the Chelsea vs. Liverpool rivalry were beginning to be sowed in May 2003. The first major meeting that would spark this feud was on the final day of the 2002\u201303 Premier League season, where fourth-placed Chelsea were to play fifth-placed Liverpool at Stamford Bridge in a clash for UEFA Champions League football. Both teams were level on 64 points, with the Blues having a +8 superior goal difference. The three teams that were above them, Manchester United in 1st, Arsenal in 2nd and Newcastle United in 3rd had already accumulated enough points to qualify for next season's Champions League, and sixth-placed Blackburn Rovers were unable to qualify, meaning Liverpool had to defeat Chelsea otherwise they would miss out on Champions League football next season. A goal from Sami Hyypi\u00e4 in the 11th minute put the Reds 1\u20130 up, but Chelsea equalised just two minutes later through Marcel Desailly. Fourteen minutes later, Chelsea found themselves ahead via a Jesper Gr\u00f8nkj\u00e6r strike. Steven Gerrard was dismissed two minutes from full-time, as Chelsea won 2\u20131 and ensured their place in the Champions League next season, with Liverpool having to settle for UEFA Cup (now Europa League) football instead. In July 2003, long-time chairman of Chelsea Ken Bates sold the club to Russian billionaire Roman Abramovich for \u00a3140,000,000. Chelsea spent \u00a3103,000,000 on transfers in the summer of 2003, which included the signings of Joe Cole from West Ham United and Hern\u00e1n Crespo from Inter Milan. Unlike the previous years, Chelsea under Abramovich had now become serious title contenders, threatening the likes of Manchester United and Arsenal, who combined had won ten of the first eleven Premier League titles. The first meeting between Chelsea and Liverpool after the Abramovich takeover was on the first matchday of the new campaign, at Anfield. Chelsea won 2\u20131, courtesy of goals from Juan Sebasti\u00e1n Ver\u00f3n and Jimmy Floyd Hasselbaink. Liverpool would get revenge in the reverse fixture at Stamford Bridge in January 2004, which saw Bruno Cheyrou condemn Chelsea to a 1\u20130 home defeat. However, despite the mass spend of Chelsea, they would still be unable to win the league, finishing as runners-up to the undefeated Arsenal. Meanwhile, Liverpool finished in fourth place, nineteen points behind Chelsea, but still qualifying for the Champions League. In the summer of 2004, Chelsea and Liverpool had respectively appointed managers Jos\u00e9 Mourinho and Rafael Ben\u00edtez, which was the beginning of a vicious rivalry between the pair. In their first season as rivals, they clashed five times, including two Premier League victories for Mourinho, both of which finished 1\u20130 to Chelsea and both of those goals being scored by Joe Cole. In the year 2005 alone, Chelsea and Liverpool met seven times. On 27 February 2005, Liverpool faced Chelsea in the final of the Football League Cup. This was Liverpool's tenth appearance in a Football League Cup final, having won seven of them (1981, 1982, 1983, 1984, 1995, 2001, 2003) and losing twice (1978, 1987). For Chelsea, this was their fourth appearance in the final, winning the cup final in 1965 and 1998, and losing in 1972. Liverpool had defeated Millwall, Middlesbrough, Tottenham Hotspur and Watford en route to the final, whereas Chelsea got past West Ham United, Newcastle United, West London rivals Fulham and Manchester United. A crowd of 78,000 at the Millennium Stadium in Cardiff saw John Arne Riise score a volley inside the first minute to put Liverpool ahead. The score remained 1-0 to the Reds for 79 minutes, until Steven Gerrard headed into his own net from a Chelsea free kick to give the Blues a lifeline. Jos\u00e9 Mourinho was also made to watch from the stands after making a gesture to the Liverpool fans. The score was 1\u20131 at full-time, taking the game to extra-time. Goals from Didier Drogba and Mateja Ke\u017eman put Chelsea 3\u20131 up. A goal from Antonio N\u00fa\u00f1ez a minute later reduced the deficit for Liverpool, but the Blues would triumph 3\u20132 and win the League Cup for the third time. Following the match, Mourinho defended the gesture that saw him dismissed, claiming that it had been intended for the media and not Liverpool fans: \"The signal of close your mouth was not for them but for the press, they speak too much and in my opinion they try to do everything to disturb Chelsea. Wait, don't speak too soon. We lost two matches and in my opinion you (the media) try to take confidence from us and put pressure on us.\" Mourinho was happy that Chelsea had won, but said the victory was not special: \"It's just one more. I had a few before this, I'm very happy to win. It's important for the fans, for the club and especially for the players.\"[21] Just two months after the League Cup final, the two clubs met yet again in the semi-finals of the Champions League. The first leg at Stamford Bridge ended in a goalless stalemate, however, in the second leg at Anfield, Liverpool controversially won 1\u20130, thanks to a goal scored by Luis Garc\u00eda, which despite Chelsea's attempts to clear the ball off the line, the goal was given. It was dubbed as a \"ghost goal\" by Jos\u00e9 Mourinho, which popularised the term for other future incidents. This would seal Liverpool's place in the Champions League final, where they would take on AC Milan, famously coming back from 3\u20130 down and winning the Champions League on penalties. Prior to the match, Chelsea were in hot pursuit of Steven Gerrard. Liverpool had rejected a \u00a332,000,000 bid from Chelsea, however, in a shocking turn of events, Gerrard had rejected a \u00a3100,000-a-week contract offer and had submitted a transfer request, just six weeks after inspiring the comeback to help Liverpool win their fifth Champions League. He eventually changed his mind, soon after signing a new four-year deal and later stating that he would rather win one Premier League title at Liverpool than multiple at Chelsea, as it would mean more to him. Chelsea's failed signing of Liverpool's elite poster boy resulted in yet more bad blood developing between the two sets of supporters. As for the Premier League season, Chelsea were runaway winners, winning their first Premier League title (second overall), finishing twelve points clear of second-placed Arsenal. They amassed a then record-setting 95 points, also winning 29 games, a record Chelsea themselves broke in 2016-17 with 30 wins[22][circular reference] (both broken by Manchester City in 2017\u201318) and conceding 15 goals, a record that still stands to this day as the best defensive record in Premier League history. Liverpool, meanwhile, finished fifth (a regression from the previous season), behind their Merseyside derby rivals Everton, who finished fourth, and 37 points behind Chelsea. However, this also meant that despite being the winners of the 2004\u201305 Champions League, they were not guaranteed a place in next season's edition, as they had finished outside of the top four of the Premier League. On 10 June 2005, UEFA decided to grant Liverpool special dispensation to defend their title, however, they would have to enter in the first qualifying round, and were denied country protection, which meant they could face any English team at any stage of the competition.[23][24][25] Liverpool would go on to defeat The New Saints, FBK Kaunas and CSKA Sofia in the Champions League first, second and third qualifying rounds, respectively, to advance to the group stage, where they were drawn in Group G, along with Chelsea, Anderlecht, and Real Betis, although both of their matches against Chelsea were 0\u20130 draws.[26] Liverpool would finish top of the group with 12 points, with Chelsea finishing second, just behind the Reds with 11. Mourinho's Chelsea would manage to get the better of Liverpool in their Premier League clashes, defeating them 4\u20131 away at Anfield in October 2005, which made them the first Premier League opposition team to score four goals at Anfield[a] and also beating them 2\u20130 at Stamford Bridge in February 2006. However, Ben\u00edtez's Liverpool were victorious in their semi-final encounter in the FA Cup, winning 2\u20131, ending Chelsea's hopes for their first ever double and progressing to the FA Cup final, where Steven Gerrard would score an equaliser in added time to help Liverpool defeat West Ham United 3\u20131 on penalties, in what became known as The Gerrard Final. After the match, Mourinho refused to shake Ben\u00edtez's hand and claimed that the best team had lost, pointing to his side's superior league position, stating: \"Did the best team win? I don't think so. In a one-off game, maybe they will surprise me and they can do it. In the Premiership, the distance between the teams is 45 points over two seasons.\" Chelsea would win the Premier League for a second consecutive season, finishing on 91 points, whereas Liverpool, who were also title contenders throughout the season as well, finished third on 82 points, a point behind second-placed Manchester United, and 9 points behind Chelsea. As Chelsea and Liverpool were the respective winners of the 2005\u201306 Premier League and the 2005\u201306 FA Cup, this meant that they would be playing each other in the 2006 FA Community Shield on 13 August 2006, at the Millennium Stadium in Cardiff, the same venue that hosted the 2005 Football League Cup final a year and a half prior, which saw Chelsea beat Liverpool 3\u20132. Chelsea were also the defending champions, having beaten their London rivals Arsenal the previous year. The Blues were making their sixth appearance in the Community Shield, having previously won in 1955 and 2000, and losing in 1970 and 1997. Liverpool, on the other hand, were appearing for the 21st time, emerging outright victorious eight times (1966, 1976, 1979, 1980, 1982, 1988, 1989, 2001), sharing the shield six times (1964, 1965, 1974, 1977, 1986, 1990) and losing it six times (1922, 1971, 1983, 1984, 1992, 2002). In the match, John Arne Riise opened the scoring for Liverpool early in the first half, only for Chelsea's recently signed forward Andriy Shevchenko to equalise shortly before half-time. Both sides had chances to win the match in the second half, but a Peter Crouch goal late in the half ensured Liverpool won the match 2\u20131, and won their 15th Community Shield. The two teams were again drawn against each other in the Champions League, squaring off in the semi-finals of the competition. Chelsea would win the first leg at Stamford Bridge 1\u20130 courtesy of a goal from Joe Cole, but Liverpool won the second leg 1\u20130 as well at Anfield, with Daniel Agger ensuring the tie finished 1\u20131 on aggregate. The team that would progress was decided in a penalty shootout. Liverpool would win the penalty shootout 4\u20131, sending them to their second Champions League final in three years, which would be a rematch of the 2005 edition, which saw AC Milan get their revenge on Liverpool and defeat them 2\u20131. I'll be honest. I couldn't stand Chelsea as a club. It surpassed Everton and Manchester United as our rivalry for a period. The following season saw Jos\u00e9 Mourinho depart Chelsea in September 2007 by mutual consent, and would replaced by Avram Grant, but they would still defeat Liverpool 2\u20130 in the quarter-finals of the League Cup, with goals from Frank Lampard and Andriy Shevchenko sending the Reds crashing out of the competition. Chelsea and Liverpool were drawn against each other yet again in the semi-finals of the Champions League. In the first leg at Anfield, a Dirk Kuyt goal two minutes before half-time put Liverpool ahead, and the scoreline would remain unchanged until the 95th minute, which saw John Arne Riise scored own goal to give Chelsea an advantage, with the match finishing 1\u20131 and the Blues heading into the second leg at Stamford Bridge with a crucial away goal. Chelsea would defeat Liverpool 3\u20132, with a brace from Didier Drogba and an emotional penalty from Frank Lampard seeing Chelsea finally get the better of Liverpool in the Champions League, and sending them to their first ever Champions League final, which they would go on to lose 6\u20135 on penalties to Manchester United. On 26 October 2008, Chelsea hosted Liverpool at Stamford Bridge in the ninth gameweek of the new Premier League campaign. At this point, Chelsea were top of the Premier League, and Liverpool were second, with both teams having 20 points and Chelsea having a +9 superior goal difference. Chelsea hadn't lost a home match in the Premier League in over four and a half years, last losing at Stamford Bridge to Arsenal in February 2004, and were looking to extend their lead at the top of the table and their home unbeaten run to 87 games. In surprising fashion, however, Liverpool would defeat Chelsea 1\u20130, with a 10th-minute strike from Xabi Alonso that deflected off Chelsea defender Jos\u00e9 Bosingwa sending the Reds to the top of the Premier League and ending the Blues' record-setting 86-game home unbeaten run, their first home league defeat in over four years, which is still the record for the most home games unbeaten in the Premier League.[29] In the reverse fixture at Anfield in February 2009, Liverpool defeated Chelsea again, this time winning 2\u20130, with both goals coming from Fernando Torres late in the game. This was the first season in Premier League history that Liverpool had completed a Premier League double over Chelsea. They would also finish second, above Chelsea who finished third, making it the first Premier League season since 2001\u201302 where Liverpool finished above Chelsea in the Premier League table. In the quarter finals of the Champions League, Liverpool and Chelsea were drawn against each other again, marking the fifth consecutive season in which they played together in the Champions League, the most in Champions League history. In the first leg at Anfield, Chelsea emphatically won 3\u20131, with two goals from defender Branislav Ivanovi\u0107 and a goal from Didier Drogba giving Chelsea an advantage in the second leg at Stamford Bridge, which saw both teams play out a thrilling 4\u20134 draw, with Chelsea winning 7\u20135 on aggregate and progressing to the semi-finals of the Champions League. From 2004 to 2009, Chelsea and Liverpool met a staggering 24 times.[30][31][32] After Rafael Ben\u00edtez departed from Liverpool in June 2010, the club struggled greatly under new manager Roy Hodgson, which saw them nine out of their first twenty matches in the Premier League and sitting 12th in the table, and one of the players who struggled was elite Spanish striker Fernando Torres. Despite Torres having a successful three and a half seasons at Liverpool, which saw him score 81 goals in nearly 150 appearances, he failed to win a single trophy at the club. Chelsea had previously expressed interest in signing Torres in 2008, but Torres responded by saying it would be \"many years\" before he left Liverpool.[33][34] On 27 January 2011, Liverpool rejected a \u00a340,000,000 bid from Chelsea for Torres, which was followed by Torres handing in a transfer request the next day, which was also rejected. Chelsea finally completed the signing of Torres on 31 January 2011, for \u00a350,000,000, a then British transfer record and making Torres the sixth most expensive player in football history at the time, with the signing enraging Liverpool fans and boiling the blood in the rivalry even further. Ironically, Torres made his Chelsea debut against Liverpool at Stamford Bridge on 6 February, where was he was greeted with flags and signs held up by Liverpool fans labelling him as a \"traitor\". Liverpool would go on to beat Chelsea 1\u20130, with a 69th-minute goal from Raul Meireles putting Fernando Torres' Chelsea debut in vain. The next season's edition of the Premier League saw both Chelsea and Liverpool underperform, with both teams finishing outside of the top four, which in normal circumstances would have saw them both absent from Europe entirely next season, with Chelsea, who finished sixth, qualifying for the Champions League as the Champions League winners, which also put their fierce London rivals Tottenham Hotspur, who finished fourth, in the nightmare scenario of finishing in the top four and still not qualifying for the Champions League, who had to settle for Europa League football instead. Meanwhile, Liverpool, who finished 8th, qualified for the third qualifying round of the Europa League as the runners-up of the FA Cup. However, Liverpool still managed to do a Premier League double over Chelsea, defeating them both home and away, which included a 4\u20131 humiliation at Anfield in May 2012. On 5 May 2012, Chelsea and Liverpool faced off in the final of the FA Cup for the very first time, at Wembley Stadium. Chelsea were looking to win their first trophy of the season, being managed by interim manager Roberto Di Matteo, who was prosperous about being appointed as Chelsea manager on a permanent basis. Meanwhile, Liverpool, who were being managed by club legend Kenny Dalglish, had already won the League Cup by beating Cardiff City on penalties in the final, also defeating Chelsea 2\u20130 in the fifth round en route to the final, and were aiming for a double. For Chelsea, this was their 11th appearance in a FA Cup final, having won on six occasions (1970, 1997, 2000, 2007, 2009, 2010) and lost on four occasions (1915, 1967, 1994, 2002). As for Liverpool, this was their 14th FA Cup final, winning the trophy seven times (1965, 1974, 1986, 1989, 1992, 2001, 2006) and being beaten six times (1914, 1950, 1971, 1977, 1988, 1996). On their way to the final, Chelsea defeated Portsmouth, West London rivals Queens Park Rangers, Birmingham City, Leicester City, and London rivals Tottenham Hotspur, whereas Liverpool defeated Oldham Athletic, rivals Manchester United, Brighton & Hove Albion, Stoke City, and Merseyside rivals Everton to earn their place in the final. In the match, Ramires put Chelsea in front in the 11th minute after he dispossessed Liverpool midfielder Jay Spearing and beat Pepe Reina in the Liverpool area. Chelsea extended their lead in the 52nd minute when striker Didier Drogba scored. Liverpool substitute Andy Carroll scored in the 64th minute to reduce the deficit to one goal. Carroll thought he had scored a second in the 81st minute, but his header was saved on the line by Chelsea goalkeeper Petr \u010cech. Carroll ran off celebrating, thinking he had equalised and the ball had crossed the line, but referee Phil Dowd did not award a goal (unlike the Luis Garc\u00eda \"ghost goal\" seven years prior), and Chelsea held on to win the match 2\u20131 and the FA Cup for the seventh time. On 21 April 2013, during Liverpool's 2\u20132 draw with Chelsea in a Premier League match at Anfield, Liverpool striker Luis Su\u00e1rez bit Chelsea defender Branislav Ivanovi\u0107. This was not the first time that something like this had happened; it was the second time that Su\u00e1rez had bitten an opponent.[35] It was not noticed by the officials, and Su\u00e1rez scored an equalizer in injury time.[36] The bite prompted UK Prime Minister David Cameron to call on the FA to take a hard line with Su\u00e1rez: the FA charged him with violent conduct and he was fined an undisclosed sum by his club.[37] Contrary to claims from Su\u00e1rez, Ivanovi\u0107 did not accept an apology.[37] Su\u00e1rez accepted the violent conduct charge but denied the FA's claim the standard punishment of three matches was clearly insufficient for his offence.[38] A three-man independent panel appointed by the FA decided on a ten-game ban for Su\u00e1rez, who did not appeal the ban; the panel criticized Su\u00e1rez for not appreciating \"the seriousness\" of the incident when he argued against a long ban. The panel also wanted to send a \"strong message that such deplorable behaviours do not have a place in football\", while noting that \"all players in the higher level of the game are seen as role models, have the duty to act professionally and responsibly, and set the highest example of good conduct to the rest of the game \u2013 especially to young players\".[39] The 2013\u201314 Premier League season saw Chelsea, Liverpool, Arsenal and Manchester City battle it out in a four-way title race, which eventually boiled down to Chelsea, Liverpool, and City. Chelsea did the Premier League double over both Liverpool and Manchester City, but inconsistent form and losses against the low-block teams saw them fail to win the Premier League. On 27 April 2014, Liverpool, now managed by Brendan Rodgers, welcomed Chelsea, now managed by a returning Jos\u00e9 Mourinho, to Anfield. At this point, the high-flying Reds were top of the Premier League on 80 points with just three games to go, five points clear of second-placed Chelsea and six of third-placed Manchester City (who had a game in hand). They had also scored nearly a century of Premier League goals, and were on course to win their first ever Premier League title, which would have happened if they were to win their last three games, which were Chelsea at home, Crystal Palace away, and Newcastle United at home. Additionally, a win against Chelsea would have seen the Blues be unable to catch Liverpool, as they would have been eight points behind them with two games left had they have won. The match saw Steven Gerrard infamously slip while receiving a pass in first half injury-time, which allowed Demba Ba to score for Chelsea and put them 1\u20130 up at Anfield. Liverpool ultimately were unable to equalise, as a goal from Willian in the dying moments of the game saw Chelsea run out 2\u20130 winners, with Liverpool only now being two points clear of their rivals from London and three points of clear of Manchester City, who had a game in hand and had +8 superior goal difference. Liverpool followed this up by throwing away a 3\u20130 lead at Crystal Palace and only managing to come out with a 3\u20133 draw, all but confirming Manchester City's Premier League victory. The next season, Chelsea hosted Liverpool at Stamford Bridge on 10 May 2015, who at this point had been top of the Premier League for every single matchday. Liverpool provided a guard of honour for Chelsea before kick off. The match finished 1\u20131, with the goals coming from John Terry and Steven Gerrard. In the match, Gerrard, who had confirmed a few months prior that he would be departing from Liverpool at the end of the season, received a standing ovation from both the Liverpool and Chelsea fans as he was being substituted off. In a post-match interview, Gerrard had mixed feelings about being clapped off the pitch by Chelsea fans, stating: \"I was more happy with the ovation from the Liverpool fans. I think Chelsea fans have showed respect for a couple of seconds for me, but they've slaughtered me all game, so I'm not going to get drawn into wishing the Chelsea fans very well. It was nice of them to turn up for once today. But yeah, you know when you get a standing ovation at a stadium, it's nice, but what's important to me is the support from the Liverpool fans and they've been with me since day one.\" Since then, the rivalry has cooled down a little bit, though fans of both clubs still hold a dislike for each other. Liverpool, then under J\u00fcrgen Klopp, would beat Chelsea in four successive major finals during this period: the 2019 UEFA Super Cup, the 2022 EFL Cup final and the 2022 FA Cup final (all on penalties), as well as finally winning their first Premier League title in the 2019\u201320 season. The two clubs met again in the final of the 2024 EFL Cup, with Liverpool winning 1\u20130 in extra-time thanks to a header from Reds captain Virgil van Dijk.[41] Chelsea also gave Liverpool a guard of honour when the latter won the 2019\u201320 and 2024\u201325 titles. Below are the players who have played for both Chelsea and Liverpool.[42][43] In 2024, former City academy player Rio Ngumoha joined Liverpool, having never played for Chelsea's first team.",
      "ground_truth_chunk_ids": [
        "216_random_chunk1"
      ],
      "source_ids": [
        "S416"
      ],
      "category": "factual",
      "id": 12
    },
    {
      "question": "What is Photography?",
      "ground_truth": "This is an accepted version of this page Photography is the art, application, and practice of creating images by recording light, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. It is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.[1] A person who operates a camera to capture or take photographs is called a photographer, while the captured image, also known as a photograph, is the result produced by the camera. Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive, depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing. Before the emergence of digital photography, photographs that utilized film had to be developed to produce negatives or projectable slides, and negatives had to be printed as positive images, usually in enlarged form. This was typically done by photographic laboratories, but many amateur photographers, students, and photographic artists did their own processing. The word \"photography\" was created from the Greek roots \u03c6\u03c9\u03c4\u03cc\u03c2 (ph\u014dt\u00f3s), genitive of \u03c6\u1ff6\u03c2 (ph\u014ds), \"light\"[2] and \u03b3\u03c1\u03b1\u03c6\u03ae (graph\u00e9)",
      "expected_answer": "Photography is the art, application, and practice of creating images by recording light, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. It is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.[1] A person who operates a camera to capture or take photographs is called a photographer, while the captured image, also known as a photograph, is the result produced by the camera. Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive, depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing. Before the emergence of digital photography, photographs that utilized film had to be developed to produce negatives or projectable slides, and negatives had to be printed as positive images, usually in enlarged form. This was typically done by photographic laboratories, but many amateur photographers, students, and photographic artists did their own processing. The word \"photography\" was created from the Greek roots \u03c6\u03c9\u03c4\u03cc\u03c2 (ph\u014dt\u00f3s), genitive of \u03c6\u1ff6\u03c2 (ph\u014ds), \"light\"[2] and \u03b3\u03c1\u03b1\u03c6\u03ae (graph\u00e9) \"representation by means of lines\" or \"drawing\",[3] together meaning \"drawing with light\".[4] Several people may have coined the same new term from these roots independently. H\u00e9rcules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, photographie, in private notes which a Brazilian historian believes were written in 1834.[5] This claim is widely reported but is not yet largely recognized internationally. The first use of the word by Florence became widely known after the research of Boris Kossoy in 1980.[6] On 25 February 1839, the German newspaper Vossische Zeitung published an article titled Photographie, discussing several priority claims, especially that of Henry Fox Talbot's, in relation to Daguerre's claim of invention.[7] The article is the earliest known occurrence of the word in public print.[8] It was signed \"J.M.\", believed to have been Berlin astronomer Johann von Maedler.[9] The astronomer John Herschel is also credited with coining the word, independent of Talbot, in 1839.[10] The inventors Nic\u00e9phore Ni\u00e9pce, Talbot, and Louis Daguerre seem not to have known or used the word \"photography\", but referred to their processes as \"Heliography\" (Ni\u00e9pce), \"Photogenic Drawing\"/\"Talbotype\"/\"Calotype\" (Talbot), and \"Daguerreotype\" (Daguerre).[9] Photography is the result of combining several technical discoveries relating to seeing an image and capturing the image. The discovery of the camera obscura (\"dark chamber\" in Latin) that provides an image of a scene dates back to ancient China. Greek mathematicians Aristotle and Euclid independently described a camera obscura in the 5th and 4th centuries BCE.[11][12] In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments.[13] The Arab physicist Ibn al-Haytham (Alhazen) (965\u20131040) also invented a camera obscura as well as the first true pinhole camera.[12][14][15] The invention of the camera has been traced back to the work of Ibn al-Haytham.[16] While the effects of a single light passing through a pinhole had been described earlier,[16] Ibn al-Haytham gave the first correct analysis of the camera obscura,[17] including the first geometrical and quantitative descriptions of the phenomenon,[18] and was the first to use a screen in a dark room so that an image from one side of a hole in the surface could be projected onto a screen on the other side.[19] He also first understood the relationship between the focal point and the pinhole,[20] and performed early experiments with afterimages, laying the foundations for the invention of photography in the 19th century.[15] Leonardo da Vinci mentions natural camerae obscurae that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. Renaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western art. It is a box with a small hole in one side, which allows specific light rays to enter, projecting an inverted image onto a viewing screen or paper. The birth of photography was then concerned with inventing means to capture and keep the image produced by the camera obscura. Albertus Magnus (1193\u20131280) discovered silver nitrate,[21] and Georg Fabricius (1516\u20131571) discovered silver chloride.[22] Daniele Barbaro described a diaphragm in 1566.[23] Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694.[24] Around 1717, Johann Heinrich Schulze used a light-sensitive slurry to capture images of cut-out letters on a bottle and on that basis many German sources and some international ones credit Schulze as the inventor of photography.[25][26]\nThe fiction book Giphantie, published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.[23] In June 1802, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance.[27] He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that \"the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver.\" The shadow images eventually darkened all over.[28] The first permanent photoetching was an image produced in 1822 by the French inventor Nic\u00e9phore Ni\u00e9pce, but it was destroyed in a later attempt to make prints from it.[29] Ni\u00e9pce was successful again in 1825. In 1826 he made the View from the Window at Le Gras, the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).[30] Because Ni\u00e9pce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy. Ni\u00e9pce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Ni\u00e9pce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements\u2014a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and \"fixed\" with hot saturated salt water\u2014were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait. In Brazil, Hercules Florence had started working out a silver-salt-based paper process in 1832, later naming it photographia, at least four years before John Herschel coined the English word photography. In 1834, having settled on silver nitrate on paper, a combination which had been the subject of experiments by Thomas Wedgwood around the year 1800, Florence's notebooks indicate that he eventually succeeded in creating light-fast, durable images.[31] Partly because he never published his invention adequately, partly because he was an obscure inventor living in a remote and undeveloped province, H\u00e9rcules Florence died, in Brazil, unrecognized internationally as one of the inventors of photography during his lifetime.[32][33][34] Meanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834[35] but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method in a paper to the Royal Society[35] and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as daguerreotypes could only be replicated by rephotographing them with a camera.[36] Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.[37][38] In March 1837,[39] Franz von Kobell, used silver chloride and a cardboard camera to make pictures in negative of the Frauenkirche and other buildings in Munich, then taking another picture of the negative to get a positive, the actual black and white reproduction of a view on the object. In 1839, Kobell, together with Carl August von Steinheil, reported on their experiments to the Bavarian Academy of Sciences. The pictures produced were round with a diameter of 4\u00a0cm, the method was later named the \"Steinheil method\". In France, Hippolyte Bayard invented his own process for producing direct positive paper prints and claimed to have invented photography earlier than Daguerre or Talbot.[40] British chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the \"blueprint\". He was the first to use the terms \"photography\", \"negative\" and \"positive\". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to \"fix\" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839. In the March 1851 issue of The Chemist, Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper. Many advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908. Glass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 21st century. Hurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised. The first flexible photographic roll film was marketed by George Eastman, founder of Kodak in 1885, but this original \"film\" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose known as nitrate film. Although cellulose acetate or \"safety film\" had been introduced by Kodak in 1908,[42] at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16\u00a0mm and 8\u00a0mm home movies, nitrate film remained standard for theatrical 35\u00a0mm motion pictures until it was finally discontinued in 1951. Films remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats.[43] Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive \"look\" of film based photographs compared to digital images is likely due to a combination of factors, including (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors),[44] (2) resolution, and (3) continuity of tone.[45] Originally, all photography was monochrome, or black-and-white. Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost, chemical stability, and its \"classic\" photographic look. The tones and contrast between light and dark areas define black-and-white photography.[46] Monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process, publicly revealed in 1847, produces brownish tones. Many photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color. Color photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not \"fix\" the photograph to prevent the color from quickly fading when exposed to white light. The first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855.[47][48] The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters.[47][48] This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s. Russian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color \"fringes\" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images. Implementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability. Autochrome, the first commercially successful color process, was introduced by the Lumi\u00e8re brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s. Kodachrome, the first modern \"integral tripack\" (or \"monopack\") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure. Agfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product. Instant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963. Color photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995\u20132005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive \"look\". In 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. The first digital camera to both record and save images in a digital format was the Fujix DS-1P created by Fujifilm in 1988.[49] In 1991, Kodak unveiled the DCS 100, the first commercially available digital single-lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born. Digital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film.[50] An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications. Digital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones. A large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; dual photography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques. The camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.[51] Photographers control the camera and lens to \"expose\" the light recording material to the required amount of light to form a \"latent image\" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal\u2013oxide\u2013semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on paper. The camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera). As soon as photographic materials became \"fast\" (sensitive) enough for taking candid or surreptitious pictures, small \"detective\" cameras were made, some actually disguised as a book or handbag or pocket watch (the Ticka camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens. The movie camera is a type of photographic camera that takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a \"frame\". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the \"frame rate\" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures to create the illusion of motion.[52] Photographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion.[53] While known colloquially as \"3-D\" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras). Dualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.[54] Ultraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions. Modified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350\u00a0nm to 1000\u00a0nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400\u00a0nm to 700\u00a0nm.[55] Replacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters). Uses of full spectrum photography are for fine art photography, geology, forensics and law enforcement. Layering is a photographic composition technique that manipulates the foreground, subject or middle-ground, and background layers in a way that they all work together to tell a story through the image.[56] Layers may be incorporated by altering the focal length, distorting the perspective by positioning the camera in a certain spot.[57] People, movement, light and a variety of objects can be used in layering.[58] Digital methods of image capture and display processing have enabled the new technology of \"light field photography\" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected after the photograph has been captured.[59] As explained by Michael Faraday in 1846, the \"light field\" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point. These additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm. Besides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures. Amateur photographers take photos for personal use, as a hobby or out of casual interest, rather than as a business or job. The quality of amateur work can be comparable to that of many professionals. Amateurs can fill a gap in subjects or topics that might not otherwise be photographed if they are not commercially useful or salable. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera.[60] Twenty-first century social media and near-ubiquitous camera phones have made photographic and video recording pervasive in everyday life. In the mid-2010s smartphone cameras added numerous automatic assistance features like color management, autofocus face detection and image stabilization that significantly decreased skill and effort needed to take high quality images.[61] Commercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. The commercial photographic world could include: During the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.\nAt first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else. The aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images \"written with light\"; Nic\u00e9phore Ni\u00e9pce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art. Clive Bell in his classic essay Art states that only \"significant form\" can distinguish art from what is not art. There must be some one quality without which a work of art cannot exist; possessing which, in the least degree, no work is altogether worthless. What is this quality? What quality is shared by all objects that provoke our aesthetic emotions? What quality is common to Sta. Sophia and the windows at Chartres, Mexican sculpture, a Persian bowl, Chinese carpets, Giotto's frescoes at Padua, and the masterpieces of Poussin, Piero della Francesca, and Cezanne? Only one answer seems possible\u00a0\u2013 significant form. In each, lines and colors combined in a particular way, certain forms and relations of forms, stir our aesthetic emotions.[62] On 7 February 2007, Sotheby's London sold the 2001 photograph 99 Cent II Diptychon for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.[63] Conceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract. In parallel to this development, the then largely separate interface between painting and photography was closed in the second half of the 20th century with the chemigram of Pierre Cordier and the chemogram of Josef H. Neumann.[64] In 1974 the chemograms by Josef H. Neumann concluded the separation of the painterly background and the photographic layer by showing the picture elements in a symbiosis that had never existed before, as an unmistakable unique specimen, in a simultaneous painterly and at the same time real photographic perspective, using lenses, within a photographic layer, united in colors and shapes. This Neumann chemogram from the 1970s thus differs from the beginning of the previously created cameraless chemigrams of a Pierre Cordier and the photogram Man Ray or L\u00e1szl\u00f3 Moholy-Nagy of the previous decades. These works of art were almost simultaneous with the invention of photography by various important artists who characterized Hippolyte Bayard, Thomas Wedgwood, William Henry Fox Talbot in their early stages, and later Man Ray and L\u00e1szl\u00f3 Moholy-Nagy in the twenties and by the painter in the thirties Edmund Kesting and Christian Schad by draping objects directly onto appropriately sensitized photo paper and using a light source without a camera.\n[65] Photojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining, including sports photography. The camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are usually taken from three vantage points: overview, mid-range, and close-up.[66] In 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24-hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century.[67][68] Charles Brooke a little later developed similar instruments for the Greenwich Observatory.[69] Science regularly uses image technology that has derived from the design of the pinhole camera to avoid distortions that can be caused by lenses. X-ray machines are similar in design to pinhole cameras, with high-grade filters and laser radiation.[70]\nPhotography has become universal in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.[71] The first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an \"Ion\" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.[72] Wildlife photography involves capturing images of various forms of wildlife. Unlike other forms of photography such as product or food photography, successful wildlife photography requires a photographer to choose the right place and right time when specific wildlife are present and active. It often requires great patience and considerable skill and command of the right photographic equipment.[73] There are many ongoing questions about different aspects of photography. In her On Photography (1977), Susan Sontag dismisses the objectivity of photography. This is a highly debated subject within the photographic community.[74] Sontag argues, \"To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power.\"[75] Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation. Modern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's Rear Window (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.[75] The camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate \u2013 all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.[75] Digital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make \"photomontages\", passing them as \"real\" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography. Photography is one of the new media forms that changes perception and changes the structure of society.[76] Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that \"to photograph is to turn people into objects that can be symbolically possessed\". Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.[75] One of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a \"tourist gaze\"[77] in which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a \"reverse gaze\"[78] through which indigenous photographees can position the tourist photographer as a shallow consumer of images. Photography is both restricted and protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view.[79] In the UK, the Counter-Terrorism Act (2008) has increased the power of the police to prevent people, even press photographers, from taking pictures in public places.[80] In South Africa, any person may photograph any other person, without their permission, in public spaces and the only specific restriction placed on what may not be photographed by government is related to anything classed as national security. Each country has different laws.",
      "ground_truth_chunk_ids": [
        "93_fixed_chunk1"
      ],
      "source_ids": [
        "S093"
      ],
      "category": "factual",
      "id": 13
    },
    {
      "question": "What is Ferrari 166 S?",
      "ground_truth": "The Ferrari 166 S is a sports car built by Ferrari between 1948 and 1953, as a evolution of its Colombo V12-powered 125 S racer. It was adapted into a sports car for the street in the form of the 166 Inter. Only 12 Ferrari 166 S were produced, nine of them with cycle-fenders as the Spyder Corsa. It was soon followed by the updated and highly successful Ferrari 166 MM (Mille Miglia), of which 47 were made from 1948 to 1953. Its early victories in the Targa Florio and Mille Miglia and others in international competition made the manufacturer a serious competitor in the racing industry.[4] Both were later replaced by the 2.3 L 195 S. The 166 shared its Aurelio Lampredi-designed tube frame[5] and double wishbone/live axle suspension with the 125. Like the 125, the wheelbase was 2420 mm long. Nine 166 Spyder Corsas and three 166 Sports were built. The first two 166 S models were coachbuilt by Carrozzeria Allemano and the last one by Carlo Anderloni at Carrozzeria Touring. Majority of the 166 MM cars were bodied at Touring in a barchetta form. The 1.5 L Gioacchino Colombo-designed V12 engine of the 125 was changed, however, with single overhead camshafts specified and a larger 2.0 L (1995 cc/121 in\u00b3) displacement. This was achieved with both a bore and stroke increase, to 60 by 58.8 mm respectively. Output was 110 PS (81 kW) at 5,600 rpm to 130 PS (96 kW) at 6,500 rpm with three carburetors, giving top speed of 170\u2013215 km/h (106\u2013134 mph).[6][7] For the 166 MM power output rose to 140 PS (103 kW) at 6,600 rpm and top speed to 220 km/h (137 mph).[8] Motor Trend Classic named the 166 MM Barchetta as number six in their list of the ten \"Greatest Ferraris",
      "expected_answer": "The Ferrari 166 S is a sports car built by Ferrari between 1948 and 1953, as a evolution of its Colombo V12-powered 125 S racer. It was adapted into a sports car for the street in the form of the 166 Inter. Only 12 Ferrari 166 S were produced, nine of them with cycle-fenders as the Spyder Corsa. It was soon followed by the updated and highly successful Ferrari 166 MM (Mille Miglia), of which 47 were made from 1948 to 1953. Its early victories in the Targa Florio and Mille Miglia and others in international competition made the manufacturer a serious competitor in the racing industry.[4] Both were later replaced by the 2.3\u00a0L 195 S. The 166 shared its Aurelio Lampredi-designed tube frame[5] and double wishbone/live axle suspension with the 125. Like the 125, the wheelbase was 2420\u00a0mm long. Nine 166 Spyder Corsas and three 166 Sports were built. The first two 166 S models were coachbuilt by Carrozzeria Allemano and the last one by Carlo Anderloni at Carrozzeria Touring. Majority of the 166 MM cars were bodied at Touring in a barchetta form. The 1.5\u00a0L Gioacchino Colombo-designed V12 engine of the 125 was changed, however, with single overhead camshafts specified and a larger 2.0\u00a0L (1995\u00a0cc/121\u00a0in\u00b3) displacement. This was achieved with both a bore and stroke increase, to 60 by 58.8\u00a0mm respectively. Output was 110\u00a0PS (81\u00a0kW) at 5,600\u00a0rpm to 130\u00a0PS (96\u00a0kW) at 6,500\u00a0rpm with three carburetors, giving top speed of 170\u2013215\u00a0km/h (106\u2013134\u00a0mph).[6][7] For the 166 MM power output rose to 140\u00a0PS (103\u00a0kW) at 6,600\u00a0rpm and top speed to 220\u00a0km/h (137\u00a0mph).[8] Motor Trend Classic named the 166 MM Barchetta as number six in their list of the ten \"Greatest Ferraris of all time\".[9] The Ferrari 166 S won Targa Florio with Clemente Biondetti and Igor Troubetzkoy in 1948. In 1949, Biondetti also won in the 166 SC with Benedetti as co-driver. The 166 S won 1948 Mille Miglia, also driven by Biondetti, this time with Giuseppe Navone.[10] In 1949 Mille Miglia, the Ferrari 166 MM Barchettas scored 1-2 victory with Biondetti/Salani and Bonetto/Carpani respectively.[11] In 1949, the 166 MM also won the 24 Hours of Le Mans in the hands of Luigi Chinetti and Lord Selsdon, and so the \n166 was the only car ever to win all three races.[12] Another 166 won the 1949 Spa 24 Hours. A 166 chassis, this time with the bigger 195 S engine, won the Mille Miglia again in 1950 with drivers Giannino Marzotto and Marco Crosara. The oldest Ferrari car with an undisputed pedigree[citation needed] is s/n 002C, a 166 Spider Corsa which was originally a 159 and is currently owned and driven by James Glickenhaus. S/n 0052M, a 1950 166 MM Touring Barchetta was uncovered in a barn and was shown in public for the first time since 1959 in the August 2006 issue of Cavallino magazine. One 166 MM, 1949 s/n 0018M, was bodied by Zagato in 'Panoramica' style, very similar to their one-off Maserati A6 1500, also designed by Vieri Rapi. It is considered as first Ferrari coachbuilt by Zagato. A year later it was rebodied as Zagato Spyder.[13] The original car was recreated in 2007 as part of Zagato's Sanction Lost programme.[14]",
      "ground_truth_chunk_ids": [
        "16_random_chunk1"
      ],
      "source_ids": [
        "S216"
      ],
      "category": "factual",
      "id": 14
    },
    {
      "question": "What is Jack Keeney?",
      "ground_truth": "John Christopher \"Jack\" Keeney (February 19, 1922 \u2013 November 19, 2011) was an American prosecutor who retired in 2010 as U.S. deputy United States Assistant Attorney General. At age 88, he was at the time the DOJ's oldest employee, and one of the longest-serving career employees in the history of the United States government. Upon his retirement, Keeney was the longest-serving federal prosecutor in American history.[1] Keeney spent decades in the United States Department of Justice Criminal Division, starting in 1951. On numerous occasions, Keeney served as Acting Assistant Attorney General. Keeney was born in Ashley, Pennsylvania, on February 19, 1922.[1] Keeney was a pilot in the Army Air Corps during World War II, and was held by German forces as a prisoner of war. Keeney graduated from the University of Scranton in 1947.[1] He received law degrees from Dickinson School of Law in 1949 and from George Washington University Law School in 1953.[1] In 2000, the Justice Department named one of its buildings (1301 New York Avenue, N.W., Washington, D.C.) after Keeney, an honor rarely bestowed on a living person.[1] In the month following his death, the Justice Department created the John C. Keeney Award for Exceptional Integrity and Professionalism. The John C. Keeney Award recognizes a Justice Department employee who has demonstrated outstanding professionalism and integrity over a sustained period of time or an employee who has displayed extraordinary strength of character in a unique situation, as Mr. Keeney displayed during his years of service to the federal government.[2] Keeney died on November 19, 2011, at his home in Kensington, Maryland, aged 89.[3] This American law\u2013related biographical article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "John Christopher \"Jack\" Keeney (February 19, 1922 \u2013 November 19, 2011) was an American prosecutor who retired in 2010 as U.S. deputy United States Assistant Attorney General. At age 88, he was at the time the DOJ's oldest employee, and one of the longest-serving career employees in the history of the United States government. Upon his retirement, Keeney was the longest-serving federal prosecutor in American history.[1] Keeney spent decades in the United States Department of Justice Criminal Division, starting in 1951. On numerous occasions, Keeney served as Acting Assistant Attorney General. Keeney was born in Ashley, Pennsylvania, on February 19, 1922.[1] Keeney was a pilot in the Army Air Corps during World War II, and was held by German forces as a prisoner of war. Keeney graduated from the University of Scranton in 1947.[1] He received law degrees from Dickinson School of Law in 1949 and from George Washington University Law School in 1953.[1] In 2000, the Justice Department named one of its buildings (1301 New York Avenue, N.W., Washington, D.C.) after Keeney, an honor rarely bestowed on a living person.[1] In the month following his death, the Justice Department created the John C. Keeney Award for Exceptional Integrity and Professionalism. The John C. Keeney Award recognizes a Justice Department employee who has demonstrated outstanding professionalism and integrity over a sustained period of time or an employee who has displayed extraordinary strength of character in a unique situation, as Mr. Keeney displayed during his years of service to the federal government.[2] Keeney died on November 19, 2011, at his home in Kensington, Maryland, aged 89.[3] This American law\u2013related biographical article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "280_random_chunk1"
      ],
      "source_ids": [
        "S480"
      ],
      "category": "factual",
      "id": 15
    },
    {
      "question": "What is Ocean?",
      "ground_truth": "The ocean is the body of salt water that covers approximately 70.8% of Earth.[8] The ocean is conventionally divided into large bodies of water, which are also referred to as oceans (in descending order by area: the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Antarctic/Southern Ocean, and the Arctic Ocean),[9][10][11] and are themselves mostly divided into seas, gulfs and subsequent bodies of water. The ocean contains 97% of Earth's water[8] and is the primary component of Earth's hydrosphere, acting as a huge reservoir of heat for Earth's energy budget, as well as for its carbon cycle and water cycle, forming the basis for climate and weather patterns worldwide. The ocean is essential to life on Earth, harbouring most of Earth's animals and protist life,[12] originating photosynthesis and therefore Earth's atmospheric oxygen, still supplying half of it.[13] Ocean scientists split the ocean into vertical and horizontal zones based on physical and biological conditions. Horizontally the ocean covers the oceanic crust, which it shapes. Where the ocean meets dry land it covers relatively shallow continental shelfs, which are part of Earth's continental crust. Human activity is mostly coastal with high negative impacts on marine life. Vertically the pelagic zone is the open ocean's water column from the surface to the ocean floor. The water column is further divided into zones based on depth and the amount of light present. The photic zone starts at the surface and is defined to be \"the depth at which light intensity is only 1% of the surface value\"[14]: 36 (approximately 200 m in the open ocean). This is the zone where photosynthesis can occur. In this process plants and microscopic algae (free-floating phytoplankton) use light, water, carbon dioxide, and nutrients to produce organic matter. As a result, the photic zone is the most biodiverse",
      "expected_answer": "The ocean is the body of salt water that covers approximately 70.8% of Earth.[8] The ocean is conventionally divided into large bodies of water, which are also referred to as oceans (in descending order by area: the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Antarctic/Southern Ocean, and the Arctic Ocean),[9][10][11]  and are themselves mostly divided into seas, gulfs and subsequent bodies of water. The ocean contains 97% of Earth's water[8] and is the primary component of Earth's hydrosphere, acting as a huge reservoir of heat for Earth's energy budget, as well as for its carbon cycle and water cycle, forming the basis for climate and weather patterns worldwide. The ocean is essential to life on Earth, harbouring most of Earth's animals and protist life,[12] originating photosynthesis and therefore Earth's atmospheric oxygen, still supplying half of it.[13] Ocean scientists split the ocean into vertical and horizontal zones based on physical and biological conditions. Horizontally the ocean covers the oceanic crust, which it shapes. Where the ocean meets dry land it covers relatively shallow continental shelfs, which are part of Earth's continental crust. Human activity is mostly coastal with high negative impacts on marine life. Vertically the pelagic zone is the open ocean's water column from the surface to the ocean floor. The water column is further divided into zones based on depth and the amount of light present. The photic zone starts at the surface and is defined to be \"the depth at which light intensity is only 1% of the surface value\"[14]:\u200a36\u200a (approximately 200\u00a0m in the open ocean). This is the zone where photosynthesis can occur. In this process plants and microscopic algae (free-floating phytoplankton) use light, water, carbon dioxide, and nutrients to produce organic matter. As a result, the photic zone is the most biodiverse and the source of the food supply which sustains most of the ocean ecosystem. Light can only penetrate a few hundred more meters; the rest of the deeper ocean is cold and dark (these zones are called mesopelagic and aphotic zones). Ocean temperatures depend on the amount of solar radiation reaching the ocean surface. In the tropics, surface temperatures can rise to over 30\u00a0\u00b0C (86\u00a0\u00b0F). Near the poles where sea ice forms, the temperature in equilibrium is about \u22122\u00a0\u00b0C (28\u00a0\u00b0F). In all parts of the ocean, deep ocean temperatures range between \u22122\u00a0\u00b0C (28\u00a0\u00b0F) and 5\u00a0\u00b0C (41\u00a0\u00b0F).[15] Constant circulation of water in the ocean creates ocean currents. Those currents are caused by forces operating on the water, such as temperature and salinity differences, atmospheric circulation (wind), and the Coriolis effect.[16] Tides create tidal currents, while wind and waves cause surface currents. The Gulf Stream, Kuroshio Current, Agulhas Current and Antarctic Circumpolar Current are all major ocean currents. Such currents transport massive amounts of water, gases, pollutants and heat to different parts of the world, and from the surface into the deep ocean.  All this has impacts on the global climate system. Ocean water contains dissolved gases, including oxygen, carbon dioxide and nitrogen. An exchange of these gases occurs at the ocean's surface. The solubility of these gases depends on the temperature and salinity of the water.[17] The carbon dioxide concentration in the atmosphere is rising due to CO2 emissions, mainly from fossil fuel combustion. As the oceans absorb CO2 from the atmosphere, a higher concentration leads to ocean acidification (a drop in pH value).[18] The ocean provides many benefits to humans such as ecosystem services, access to seafood and other marine resources, and a means of transport. The ocean is known to be the habitat of over 230,000 species, but may hold considerably more \u2013 perhaps over two million species.[19] Yet, the ocean faces many environmental threats, such as marine pollution, overfishing, and the effects of climate change. Those effects include ocean warming, ocean acidification and sea level rise. The continental shelf and coastal waters are most affected by human activity. The terms \"the ocean\" or \"the sea\" used without specification refer to the interconnected body of salt water covering the majority of Earth's surface, i.e., the world ocean.[9][10] It includes the Pacific, Atlantic, Indian, Antarctic/Southern, and Arctic oceans.[20] As a general term, \"the ocean\" and \"the sea\" are often interchangeable.[21] Strictly speaking, a \"sea\" is a body of water (generally a division of the world ocean) partly or fully enclosed by land.[22] The word \"sea\" can also be used for many specific, much smaller bodies of seawater, such as the North Sea or the Red Sea. There is no sharp distinction between seas and oceans, though generally seas are smaller, and are often partly (as marginal seas) or wholly (as inland seas) bordered by land.[23] In medieval Europe, the World Sea was the body of water that encircled the Continent (the mainland of Europe, Asia and Africa), and thus excluded the Mediterranean, Black and Caspian Seas.[citation needed] The contemporary concept of the World Ocean was coined in the early 20th century by the Russian oceanographer Yuly Shokalsky to refer to the continuous ocean that covers and encircles most of Earth.[24][25] The global, interconnected body of salt water is sometimes referred to as the World Ocean, the global ocean or the great ocean.[26][27][28] The concept of a continuous body of water with relatively unrestricted exchange between its components is critical in oceanography.[29] The word ocean comes from the figure in classical antiquity, Oceanus (/o\u028a\u02c8si\u02d0\u0259n\u0259s/; Ancient Greek: \u1f68\u03ba\u03b5\u03b1\u03bd\u03cc\u03c2 \u014ckean\u00f3s,[30] pronounced [\u0254\u02d0kean\u00f3s]), the elder of the Titans in classical Greek mythology. Oceanus was believed by the ancient Greeks and Romans to be the divine personification of an enormous river encircling the world. The concept of \u014ckean\u00f3s could have an Indo-European connection. Greek \u014ckean\u00f3s has been compared to the Vedic epithet \u0101-\u015b\u00e1y\u0101na-, predicated of the dragon V\u1e5btra-, who captured the cows/rivers. Related to this notion, the Okeanos is represented with a dragon-tail on some early Greek vases.[31] Scientists believe that a sizable quantity of water would have been in the material that formed Earth.[32] Water molecules would have escaped Earth's gravity more easily when it was less massive during its formation. This is called atmospheric escape. During planetary formation, Earth possibly had magma oceans. Subsequently, outgassing, volcanic activity and meteorite impacts, produced an early atmosphere of carbon dioxide, nitrogen and water vapor, according to current theories.\nThe gases and the atmosphere are thought to have accumulated over millions of years. After Earth's surface had significantly cooled, the water vapor over time would have condensed, forming Earth's first oceans.[33] The early oceans might have been significantly hotter than today and appeared green due to high iron content.[34] Geological evidence helps constrain the time frame for liquid water existing on Earth. A sample of pillow basalt (a type of rock formed during an underwater eruption) was recovered from the Isua Greenstone Belt and provides evidence that water existed on Earth 3.8\u00a0billion years ago.[35] In the Nuvvuagittuq Greenstone Belt, Quebec, Canada, rocks dated at 3.8\u00a0billion years old by one study[36] and 4.28\u00a0billion years old by another[37] show evidence of the presence of water at these ages.[35] If oceans existed earlier than this, any geological evidence either has yet to be discovered, or has since been destroyed by geological processes like crustal recycling.\nHowever, in August 2020, researchers reported that sufficient water to fill the oceans may have always been on the Earth since the beginning of the planet's formation.[38][39][40] In this model, atmospheric greenhouse gases kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity.[41] The origin of Earth's oceans is unknown. Oceans are thought to have formed in the Hadean eon and may have been the cause for the emergence of life. Plate tectonics, post-glacial rebound, and sea level rise continually change the coastline and structure of the world ocean. A global ocean has existed in one form or another on Earth for eons. Since its formation the ocean has taken many conditions and shapes with many past ocean divisions and potentially at times covering the whole globe.[42] During colder climatic periods, more ice caps and glaciers form, and enough of the global water supply accumulates as ice to lessen the amounts in other parts of the water cycle. The reverse is true during warm periods. During the last ice age, glaciers covered almost one-third of Earth's land mass with the result being that the oceans were about 122\u00a0m (400\u00a0ft) lower than today. During the last global \"warm spell,\" about 125,000 years ago, the seas were about 5.5\u00a0m (18\u00a0ft) higher than they are now. About three million years ago the oceans could have been up to 50\u00a0m (165\u00a0ft) higher.[43] The entire ocean, containing 97% of Earth's water, spans 70.8% of Earth's surface,[8] making it Earth's global ocean or world ocean.[24][26] This makes Earth, along with its vibrant hydrosphere a \"water world\"[44][45] or \"ocean world\",[46][47] particularly in Earth's early history when the ocean is thought to have possibly covered Earth completely.[42] The ocean's shape is irregular, unevenly dominating Earth's surface. This leads to the distinction of Earth's surface into land and water hemispheres, as well as the division of the ocean into different oceans. Seawater covers about 361,000,000\u00a0km2 (139,000,000\u00a0sq\u00a0mi) and the ocean's furthest pole of inaccessibility, known as \"Point Nemo\", in a region known as spacecraft cemetery of the South Pacific Ocean, at 48\u00b052.6\u2032S 123\u00b023.6\u2032W\ufeff / \ufeff48.8767\u00b0S 123.3933\u00b0W\ufeff / -48.8767; -123.3933\ufeff (Point Nemo). This point is roughly 2,688\u00a0km (1,670\u00a0mi) from the nearest land.[48] There are different customs to subdivide the ocean and are adjourned by smaller bodies of water such as bays, bights, gulfs, seas, and straits. For practical and historical reasons, it is customary to divide the World Ocean into a set of five major oceans. By convention these are the Pacific, Atlantic, Indian, Arctic, and Southern (Antarctic) oceans. This five-ocean model only fully crystallized in the early 21st century, when the Southern Ocean, delineated by the Antarctic Circumpolar Current, was recognized by various government and international bodies: by the U.S. Board on Geographic Names since 1999,[49] and by the International Hydrographic Organization since 2000.[50] The five principal oceans are listed below in descending order of area and volume: The ocean fills Earth's oceanic basins. Earth's oceanic basins cover different geologic provinces of Earth's oceanic crust as well as continental crust. As such it covers mainly Earth's structural basins, but also continental shelves. Beside the world ocean, the Black and Caspian seas also occupy oceanic basins; the Mediterranean has in the past been cut off into its own basin. In mid-ocean, magma is constantly being thrust through the seabed between adjoining plates to form mid-oceanic ridges and here convection currents within the mantle tend to drive the two plates apart. Parallel to these ridges and nearer the coasts, one oceanic plate may slide beneath another oceanic plate in a process known as subduction. Deep trenches are formed here and the process is accompanied by friction as the plates grind together. The movement proceeds in jerks which cause earthquakes, heat is produced and magma is forced up creating underwater mountains, some of which may form chains of volcanic islands near to deep trenches. Near some of the boundaries between the land and sea, the slightly denser oceanic plates slide beneath the continental plates and more subduction trenches are formed. As they grate together, the continental plates are deformed and buckle causing mountain building and seismic activity.[60][61] Every ocean basin has a mid-ocean ridge, which creates a long mountain range beneath the ocean. Together they form the global mid-oceanic ridge system that features the longest mountain range in the world. The longest continuous mountain range is 65,000\u00a0km (40,000\u00a0mi). This underwater mountain range is several times longer than the longest continental mountain range\u00a0\u2013 the Andes.[62] Oceanographers of the Nippon Foundation-GEBCO Seabed 2030 Project (Seabed 2030) state that as of 2024 just over 26% of the ocean floor has been mapped at a higher resolution than provided by satellites, while the ocean as a whole will never be fully explored,[63] with some estimating 5% of it having been explored.[64] The zone where land meets sea is known as the coast, and the part between the lowest spring tides and the upper limit reached by splashing waves is the shore. A beach is the accumulation of sand or shingle on the shore.[65] A headland is a point of land jutting out into the sea and a larger promontory is known as a cape. The indentation of a coastline, especially between two headlands, is a bay. A small bay with a narrow inlet is a cove and a large bay may be referred to as a gulf.[66] Coastlines are influenced by several factors, including the strength of the waves arriving on the shore, the gradient of the land margin, the composition and hardness of the coastal rock, the inclination of the off-shore slope and the changes of the level of the land due to local uplift or submergence.[65] Normally, waves roll towards the shore at the rate of six to eight per minute and these are known as constructive waves as they tend to move material up the beach and have little erosive effect. Storm waves arrive on shore in rapid succession and are known as destructive waves as the swash moves beach material seawards. Under their influence, the sand and shingle on the beach is ground together and abraded. Around high tide, the power of a storm wave impacting on the foot of a cliff has a shattering effect as air in cracks and crevices is compressed and then expands rapidly with release of pressure. At the same time, sand and pebbles have an erosive effect as they are thrown against the rocks. This tends to undercut the cliff, and normal weathering processes such as the action of frost follows, causing further destruction. Gradually, a wave-cut platform develops at the foot of the cliff and this has a protective effect, reducing further wave-erosion.[65] Material worn from the margins of the land eventually ends up in the sea. Here it is subject to attrition as currents flowing parallel to the coast scour out channels and transport sand and pebbles away from their place of origin. Sediment carried to the sea by rivers settles on the seabed causing deltas to form in estuaries. All these materials move back and forth under the influence of waves, tides and currents.[65] Dredging removes material and deepens channels but may have unexpected effects elsewhere on the coastline. Governments make efforts to prevent flooding of the land by the building of breakwaters, seawalls, dykes and levees and other sea defences. For instance, the Thames Barrier is designed to protect London from a storm surge,[67] while the failure of the dykes and levees around New Orleans during Hurricane Katrina created a humanitarian crisis in the United States. Most of the ocean is blue in color, but in some places the ocean is blue-green, green, or even yellow to brown.[68] Blue ocean color is a result of several factors. First, water preferentially absorbs red light, which means that blue light remains and is reflected back out of the water. Red light is most easily absorbed and thus does not reach great depths, usually to less than 50 meters (164\u00a0ft). Blue light, in comparison, can penetrate up to 200 meters (656\u00a0ft).[69] Second, water molecules and very tiny particles in ocean water preferentially scatter blue light more than light of other colors. Blue light scattering by water and tiny particles happens even in the very clearest ocean water,[70] and is similar to blue light scattering in the sky. The main substances that affect the color of the ocean include dissolved organic matter, living phytoplankton with chlorophyll pigments, and non-living particles like marine snow and mineral sediments.[71] Chlorophyll can be measured by satellite observations and serves as a proxy for ocean productivity (marine primary productivity) in surface waters. In long term composite satellite images, regions with high ocean productivity show up in yellow and green colors because they contain more (green) phytoplankton, whereas areas of low productivity show up in blue. Ocean water represents the largest body of water within the global water cycle (oceans contain 97% of Earth's water). Evaporation from the ocean moves water into the atmosphere to later rain back down onto land and the ocean.[72] Oceans have a significant effect on the biosphere. The ocean as a whole is thought to cover approximately 90% of the Earth's biosphere.[73] Oceanic evaporation, as a phase of the water cycle, is the source of most rainfall (about 90%),[72] causing a global cloud cover of 67% and a consistent oceanic cloud cover of 72%.[74] Ocean temperatures affect climate and wind patterns that affect life on land. One of the most dramatic forms of weather occurs over the oceans: tropical cyclones (also called \"typhoons\" and \"hurricanes\" depending upon where the system forms). As the world's ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth, forms part of the carbon cycle and water cycle, and \u2013 as a huge heat reservoir \u2013 influences climate and weather patterns. The motions of the ocean surface, known as undulations or wind waves, are the partial and alternate rising and falling of the ocean surface. The series of mechanical waves that propagate along the interface between water and air is called swell \u2013 a term used in sailing, surfing and navigation.[75] These motions profoundly affect ships on the surface of the ocean and the well-being of people on those ships who might suffer from sea sickness. Wind blowing over the surface of a body of water forms waves that are perpendicular to the direction of the wind. The friction between air and water caused by a gentle breeze on a pond causes ripples to form. A stronger gust blowing over the ocean causes larger waves as the moving air pushes against the raised ridges of water. The waves reach their maximum height when the rate at which they are travelling nearly matches the speed of the wind. In open water, when the wind blows continuously as happens in the Southern Hemisphere in the Roaring Forties, long, organized masses of water called swell roll across the ocean.[76]:\u200a83\u201384\u200a[77][78] If the wind dies down, the wave formation is reduced, but already-formed waves continue to travel in their original direction until they meet land. The size of the waves depends on the fetch, the distance that the wind has blown over the water and the strength and duration of that wind. When waves meet others coming from different directions, interference between the two can produce broken, irregular seas.[77] Constructive interference can lead to the formation of unusually high rogue waves.[79] Most waves are less than 3\u00a0m (10\u00a0ft) high[79] and it is not unusual for strong storms to double or triple that height.[80] Rogue waves, however, have been documented at heights above 25 meters (82\u00a0ft).[81][82] The top of a wave is known as the crest, the lowest point between waves is the trough and the distance between the crests is the wavelength. The wave is pushed across the surface of the ocean by the wind, but this represents a transfer of energy and not horizontal movement of water. As waves approach land and move into shallow water, they change their behavior. If approaching at an angle, waves may bend (refraction) or wrap around rocks and headlands (diffraction). When the wave reaches a point where its deepest oscillations of the water contact the ocean floor, they begin to slow down. This pulls the crests closer together and increases the waves' height, which is called wave shoaling. When the ratio of the wave's height to the water depth increases above a certain limit, it \"breaks\", toppling over in a mass of foaming water.[79] This rushes in a sheet up the beach before retreating into the ocean under the influence of gravity.[83] Earthquakes, volcanic eruptions or other major geological disturbances can set off waves that can lead to tsunamis in coastal areas which can be very dangerous.[84][85] The ocean's surface is an important reference point for oceanography and geography, particularly as mean sea level. The ocean surface has globally little, but measurable topography, depending on the ocean's volumes. The ocean surface is a crucial interface for oceanic and atmospheric processes. Allowing interchange of particles, enriching the air and water, as well as grounds by some particles becoming sediments. This interchange has fertilized life in the ocean, on land and air. All these processes and components together make up ocean surface ecosystems. Tides are the regular rise and fall in water level experienced by oceans, primarily driven by the Moon's gravitational tidal forces upon the Earth. Tidal forces affect all matter on Earth, but only fluids like the ocean demonstrate the effects on human timescales. (For example, tidal forces acting on rock may produce tidal locking between two planetary bodies.) Though primarily driven by the Moon's gravity, oceanic tides are also substantially modulated by the Sun's tidal forces, by the rotation of the Earth, and by the shape of the rocky continents blocking oceanic water flow. (Tidal forces vary more with distance than the \"base\" force of gravity: the Moon's tidal forces on Earth are more than double the Sun's,[86] despite the latter's much stronger gravitational force on Earth. Earth's tidal forces upon the Moon are 20x stronger than the Moon's tidal forces on the Earth.) The primary effect of lunar tidal forces is to bulge Earth matter towards the near and far sides of the Earth, relative to the moon. The \"perpendicular\" sides, from which the Moon appears in line with the local horizon, experience \"tidal troughs\". Since it takes nearly 25 hours for the Earth to rotate under the Moon (accounting for the Moon's 28-day orbit around Earth), tides thus cycle over a course of 12.5 hours. However, the rocky continents pose obstacles for the tidal bulges, so the timing of tidal maxima may not actually align with the Moon in most localities on Earth, as the oceans are forced to \"dodge\" the continents. Timing and magnitude of tides vary widely across the Earth as a result of the continents. Thus, knowing the Moon's position does not allow a local to predict tide timings, instead requiring precomputed tide tables which account for the continents and the Sun, among others. During each tidal cycle, at any given place the tidal waters rise to maximum height, high tide, before ebbing away again to the minimum level, low tide. As the water recedes, it gradually reveals the foreshore, also known as the intertidal zone. The difference in height between the high tide and low tide is known as the tidal range or tidal amplitude.[87][88]  When the sun and moon are aligned (full moon or new moon), the combined effect results in the higher \"spring tides\", while the sun and moon misaligning (half moons) result in lesser tidal ranges.[87] In the open ocean tidal ranges are less than 1 meter, but in coastal areas these tidal ranges increase to more than 10 meters in some areas.[89] Some of the largest tidal ranges in the world occur in the Bay of Fundy and Ungava Bay in Canada, reaching up to 16 meters.[90] Other locations with record high tidal ranges include the Bristol Channel between England and Wales, Cook Inlet in Alaska, and the R\u00edo Gallegos in Argentina.[91] Tides are not to be confused with storm surges, which can occur when high winds pile water up against the coast in a shallow area and this, coupled with a low pressure system, can raise the surface of the ocean dramatically above a typical high tide. The average depth of the oceans is about 4\u00a0km. More precisely the average depth is 3,688 meters (12,100\u00a0ft).[77] Nearly half of the world's marine waters are over 3,000 meters (9,800\u00a0ft) deep.[28] \"Deep ocean,\" which is anything below 200 meters (660\u00a0ft), covers about 66% of Earth's surface.[92] This figure does not include seas not connected to the World Ocean, such as the Caspian Sea. The deepest region of the ocean is at the Mariana Trench, located in the Pacific Ocean near the Northern Mariana Islands.[93] The maximum depth has been estimated to be 10,971 meters (35,994\u00a0ft). The British naval vessel Challenger II surveyed the trench in 1951 and named the deepest part of the trench the \"Challenger Deep\". In 1960, the Trieste successfully reached the bottom of the trench, manned by a crew of two men. Oceanographers classify the ocean into vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The ocean zones can be grouped by light penetration into (from top to bottom): the photic zone, the mesopelagic zone and the aphotic deep ocean zone: The pelagic part of the aphotic zone can be further divided into vertical regions according to depth and temperature:[95] Distinct boundaries between ocean surface waters and deep waters can be drawn based on the properties of the water. These boundaries are called thermoclines (temperature), haloclines (salinity), chemoclines (chemistry), and pycnoclines (density). If a zone undergoes dramatic changes in temperature with depth, it contains a thermocline, a distinct boundary between warmer surface water and colder deep water. In tropical regions, the thermocline is typically deeper compared to higher latitudes. Unlike polar waters, where solar energy input is limited, temperature stratification is less pronounced, and a distinct thermocline is often absent. This is due to the fact that surface waters in polar latitudes are nearly as cold as deeper waters. Below the thermocline, water everywhere in the ocean is very cold, ranging from \u22121\u00a0\u00b0C to 3\u00a0\u00b0C. Because this deep and cold layer contains the bulk of ocean water, the average temperature of the world ocean is 3.9\u00a0\u00b0C.[96] If a zone undergoes dramatic changes in salinity with depth, it contains a halocline. If a zone undergoes a strong, vertical chemistry gradient with depth, it contains a chemocline. Temperature and salinity control ocean water density. Colder and saltier water is denser, and this density plays a crucial role in regulating the global water circulation within the ocean.[95] The halocline often coincides with the thermocline, and the combination produces a pronounced pycnocline, a boundary between less dense surface water and dense deep water. The pelagic zone can be further subdivided into two sub regions based on distance from land: the neritic zone and the oceanic zone. The neritic zone covers the water directly above the continental shelves, including coastal waters. On the other hand, the oceanic zone includes all the completely open water. The littoral zone covers the region between low and high tide and represents the transitional area between marine and terrestrial conditions. It is also known as the intertidal zone because it is the area where tide level affects the conditions of the region.[95] The combined volume of water in all the oceans is roughly 1.335\u00a0billion cubic kilometers (1.335 sextillion liters, 320.3\u00a0million cubic miles).[77][97][98] It has been estimated that there are 1.386 billion cubic kilometres (333 million cubic miles) of water on Earth.[99][100][101] This includes water in gaseous, liquid and frozen forms as soil moisture, groundwater and permafrost in the Earth's crust (to a depth of 2\u00a0km); oceans and seas, lakes, rivers and streams, wetlands, glaciers, ice and snow cover on Earth's surface; vapour, droplets and crystals in the air; and part of living plants, animals and unicellular organisms of the biosphere. Saltwater accounts for 97.5% of this amount, whereas fresh water accounts for only 2.5%. Of this fresh water, 68.9% is in the form of ice and permanent snow cover in the Arctic, the Antarctic and mountain glaciers; 30.8% is in the form of fresh groundwater; and only 0.3% of the fresh water on Earth is in easily accessible lakes, reservoirs and river systems.[102] The total mass of Earth's hydrosphere is about 1.4 \u00d7 1018 tonnes, which is about 0.023% of Earth's total mass. At any given time, about 2 \u00d7 1013 tonnes of this is in the form of water vapor in the Earth's atmosphere (for practical purposes, 1 cubic metre of water weighs 1 tonne). Approximately 71% of Earth's surface, an area of some 361 million square kilometres (139.5 million square miles), is covered by ocean. The average salinity of Earth's oceans is about 35\u00a0grams of salt per kilogram of sea water (3.5%).[103] Ocean temperatures depends on the amount of solar radiation falling on its surface. In the tropics, with the Sun nearly overhead, the temperature of the surface layers can rise to over 30\u00a0\u00b0C (86\u00a0\u00b0F) while near the poles the temperature in equilibrium with the sea ice is about \u22122\u00a0\u00b0C (28\u00a0\u00b0F). There is a continuous circulation of water in the oceans. Warm surface currents cool as they move away from the tropics, and the water becomes denser and sinks. The cold water moves back towards the equator as a deep sea current, driven by changes in the temperature and density of the water, before eventually welling up again towards the surface. Deep ocean water has a temperature between \u22122\u00a0\u00b0C (28\u00a0\u00b0F) and 5\u00a0\u00b0C (41\u00a0\u00b0F) in all parts of the globe.[15] The temperature gradient over the water depth is related to the way the surface water mixes with deeper water or does not mix (a lack of mixing is called ocean stratification). This depends on the temperature: in the tropics the warm surface layer of about 100 m is quite stable and does not mix much with deeper water, while near the poles winter cooling and storms makes the surface layer denser and it mixes to great depth and then stratifies again in summer. The photic depth is typically about 100 m (but varies) and is related to this heated surface layer.[104] It is clear that the ocean is warming as a result of climate change, and this rate of warming is increasing.[105]:\u200a9\u200a The global ocean was the warmest it had ever been recorded by humans in 2022.[106] This is determined by the ocean heat content, which exceeded the previous 2021 maximum in 2022.[106] The steady rise in ocean temperatures is an unavoidable result of the Earth's energy imbalance, which is primarily caused by rising levels of greenhouse gases.[106] Between pre-industrial times and the 2011\u20132020 decade, the ocean's surface has heated between 0.68 and 1.01\u00a0\u00b0C.[107]:\u200a1214 The temperature and salinity of ocean waters vary significantly across different regions. This is due to differences in the local water balance (precipitation vs. evaporation) and the \"sea to air\" temperature gradients. These characteristics can vary widely from one ocean region to another. The table below provides an illustration of the sort of values usually encountered. Seawater with a typical salinity of 35\u2030 has a freezing point of about \u22121.8\u00a0\u00b0C (28.8\u00a0\u00b0F).[95][113] Because sea ice is less dense than water, it floats on the ocean's surface (as does fresh water ice, which has an even lower density). Sea ice covers about 7% of the Earth's surface and about 12% of the world's oceans.[114][115][116] Sea ice usually starts to freeze at the very surface, initially as a very thin ice film. As further freezing takes place, this ice film thickens and can form ice sheets. The ice formed incorporates some sea salt, but much less than the seawater it forms from. As the ice forms with low salinity this results in saltier residual seawater. This in turn increases density and promotes vertical sinking of the water.[117] An ocean current is a continuous, directed flow of seawater caused by several forces acting upon the water. These include wind, the Coriolis effect, temperature and salinity differences.[16] Ocean currents are primarily horizontal water movements that have different origins such as tides for tidal currents, or wind and waves for surface currents. Tidal currents are in phase with the tide, hence are quasiperiodic; associated with the influence of the moon and sun pull on the ocean water. Tidal currents may form various complex patterns in certain places, most notably around headlands.[118] Non-periodic or non-tidal currents are created by the action of winds and changes in density of water. In littoral zones, breaking waves are so intense and the depth measurement so low, that maritime currents reach often 1 to 2 knots.[119] The wind and waves create surface currents (designated as \"drift currents\"). These currents can decompose in one quasi-permanent current (which varies within the hourly scale) and one movement of Stokes drift under the effect of rapid waves movement (which vary on timescales of a couple of seconds). The quasi-permanent current is accelerated by the breaking of waves, and in a lesser governing effect, by the friction of the wind on the surface.[119] This acceleration of the current takes place in the direction of waves and dominant wind. Accordingly, when the ocean depth increases, the rotation of the earth changes the direction of currents in proportion with the increase of depth, while friction lowers their speed. At a certain ocean depth, the current changes direction and is seen inverted in the opposite direction with current speed becoming null: known as the Ekman spiral. The influence of these currents is mainly experienced at the mixed layer of the ocean surface, often from 400 to 800 meters of maximum depth. These currents can considerably change and are dependent on the yearly seasons. If the mixed layer is less thick (10 to 20 meters), the quasi-permanent current at the surface can adopt quite a different direction in relation to the direction of the wind. In this case, the water column becomes virtually homogeneous above the thermocline.[119] The wind blowing on the ocean surface will set the water in motion. The global pattern of winds (also called atmospheric circulation) creates a global pattern of ocean currents. These are driven not only by the wind but also by the effect of the circulation of the earth (coriolis force). These major ocean currents include the Gulf Stream, Kuroshio Current, Agulhas Current and Antarctic Circumpolar Current. The Antarctic Circumpolar Current encircles Antarctica and influences the area's climate, connecting currents in several oceans.[119] Collectively, currents move enormous amounts of water and heat around the globe influencing climate. These wind driven currents are largely confined to the top hundreds of meters of the ocean. At greater depth, the thermohaline circulation drives water motion. For example, the Atlantic meridional overturning circulation (AMOC) is driven by the cooling of surface waters in the polar latitudes in the north and south, creating dense water which sinks to the bottom of the ocean. This cold and dense water moves slowly away from the poles which is why the waters in the deepest layers of the world ocean are so cold. This deep ocean water circulation is relatively slow and water at the bottom of the ocean can be isolated from the ocean surface and atmosphere for hundreds or even a few thousand years.[119] This circulation has important impacts on the global climate system and on the uptake and redistribution of pollutants and gases such as carbon dioxide, for example by moving contaminants from the surface into the deep ocean. Ocean currents greatly affect Earth's climate by transferring heat from the tropics to the polar regions. This affects air temperature and precipitation in coastal regions and further inland. Surface heat and freshwater fluxes create global density gradients, which drive the thermohaline circulation that is a part of large-scale ocean circulation. It plays an important role in supplying heat to the polar regions, and thus in sea ice regulation.[citation needed] Oceans moderate the climate of locations where prevailing winds blow in from the ocean. At similar latitudes, a place on Earth with more influence from the ocean will have a more moderate climate than a place with more influence from land. For example, the cities San Francisco (37.8 N) and New York (40.7 N) have different climates because San Francisco has more influence from the ocean. San Francisco, on the west coast of North America, gets winds from the west over the Pacific Ocean. New York, on the east coast of North America gets winds from the west over land, so New York has colder winters and hotter, earlier summers than San Francisco. Warmer ocean currents yield warmer climates in the long term, even at high latitudes. At similar latitudes, a place influenced by warm ocean currents will have a warmer climate overall than a place influenced by cold ocean currents.[citation needed] Changes in the thermohaline circulation are thought to have significant impacts on Earth's energy budget. Because the thermohaline circulation determines the rate at which deep waters reach the surface, it may also significantly influence atmospheric carbon dioxide concentrations. Modern observations, climate simulations and paleoclimate reconstructions suggest that the Atlantic meridional overturning circulation (AMOC) has weakened since the preindustrial era. The latest climate change projections in 2021 suggest that the AMOC is likely to weaken further over the 21st century.[120]:\u200a19\u200a Such a weakening could cause large changes to global climate, with the North Atlantic particularly vulnerable.[120]:\u200a19 Salinity is a measure of the total amounts of dissolved salts in seawater. It was originally measured via measurement of the amount of chloride in seawater and hence termed chlorinity. It is now standard practice to gauge it by measuring electrical conductivity of the water sample. Salinity can be calculated using the chlorinity, which is a measure of the total mass of halogen ions (includes fluorine, chlorine, bromine, and iodine) in seawater. According to an international agreement, the following formula is used to determine salinity:[122] The average ocean water chlorinity is about 19.2\u2030 (equal to 1.92%), and thus the average salinity is around 34.7\u2030 (3.47%).[122] Salinity has a major influence on the density of seawater. A zone of rapid salinity increase with depth is called a halocline. As seawater's salt content increases, so does the temperature at which its maximum density occurs. Salinity affects both the freezing and boiling points of water, with the boiling point increasing with salinity. At atmospheric pressure,[123] normal seawater freezes at a temperature of about \u22122\u00a0\u00b0C. Salinity is higher in Earth's oceans where there is more evaporation and lower where there is more precipitation. If precipitation exceeds evaporation, as is the case in polar and some temperate regions, salinity will be lower. Salinity will be higher if evaporation exceeds precipitation, as is sometimes the case in tropical regions. For example, evaporation is greater than precipitation in the Mediterranean Sea, which has an average salinity of 38\u2030, more saline than the global average of 34.7\u2030.[124] Thus, oceanic waters in polar regions have lower salinity content than oceanic waters in tropical regions.[122] However, when sea ice forms at high latitudes, salt is excluded from the ice as it forms, which can increase the salinity in the residual seawater in polar regions such as the Arctic Ocean.[95][125] Due to the effects of climate change on oceans, observations of sea surface salinity between 1950 and 2019 indicate that regions of high salinity and evaporation have become more saline while regions of low salinity and more precipitation have become fresher.[126] It is very likely that the Pacific and Antarctic/Southern Oceans have freshened while the Atlantic has become more saline.[126] Ocean water contains large quantities of dissolved gases, including oxygen, carbon dioxide and nitrogen. These dissolve into ocean water via gas exchange at the ocean surface, with the solubility of these gases depending on the temperature and salinity of the water.[17] The four most abundant gases in earth's atmosphere and oceans are nitrogen, oxygen, argon, and carbon dioxide. In the ocean by volume, the most abundant gases dissolved in seawater are carbon dioxide (including bicarbonate and carbonate ions, 14 mL/L on average), nitrogen (9 mL/L), and oxygen (5 mL/L) at equilibrium at 24\u00a0\u00b0C (75\u00a0\u00b0F)[128][129][130] All gases are more soluble \u2013 more easily dissolved \u2013 in colder water than in warmer water. For example, when salinity and pressure are held constant, oxygen concentration in water almost doubles when the temperature drops from that of a warm summer day 30\u00a0\u00b0C (86\u00a0\u00b0F) to freezing 0\u00a0\u00b0C (32\u00a0\u00b0F). Similarly, carbon dioxide and nitrogen gases are more soluble at colder temperatures, and their solubility changes with temperature at different rates.[128][131] Photosynthesis in the surface ocean releases oxygen and consumes carbon dioxide. Phytoplankton, a type of microscopic free-floating algae, controls this process. After the plants have grown, oxygen is consumed and carbon dioxide released, as a result of bacterial decomposition of the organic matter created by photosynthesis in the ocean. The sinking and bacterial decomposition of some organic matter in deep ocean water, at depths where the waters are out of contact with the atmosphere, leads to a reduction in oxygen concentrations and increase in carbon dioxide, carbonate and bicarbonate.[104] This cycling of carbon dioxide in oceans is an important part of the global carbon cycle. The oceans represent a major carbon sink for carbon dioxide taken up from the atmosphere by photosynthesis and by dissolution (see also carbon sequestration). There is also increased attention on carbon dioxide uptake in coastal marine habitats such as mangroves and saltmarshes. This process is often referred to as \"Blue carbon\". The focus is on these ecosystems because they are strong carbon sinks as well as ecologically important habitats under threat from human activities and environmental degradation. As deep ocean water circulates throughout the globe, it contains gradually less oxygen and gradually more carbon dioxide with more time away from the air at the surface. This gradual decrease in oxygen concentration happens as sinking organic matter continuously gets decomposed during the time the water is out of contact with the atmosphere.[104] Most of the deep waters of the ocean still contain relatively high concentrations of oxygen sufficient for most animals to survive. However, some ocean areas have very low oxygen due to long periods of isolation of the water from the atmosphere. These oxygen deficient areas, called oxygen minimum zones or hypoxic waters, will generally be made worse by the effects of climate change on oceans.[133][134] The pH value at the surface of oceans (global mean surface pH) is currently approximately in the range of 8.05[135] to 8.08.[136] This makes it slightly alkaline. The pH value at the surface used to be about 8.2 during the past 300 million years.[137] However, between 1950 and 2020, the average pH of the ocean surface fell from approximately 8.15 to 8.05.[138] Carbon dioxide emissions from human activities are the primary cause of this process called ocean acidification, with atmospheric carbon dioxide (CO2) levels exceeding 410 ppm (in 2020).[139] CO2 from the atmosphere is absorbed by the oceans. This produces carbonic acid (H2CO3) which dissociates into a bicarbonate ion (HCO\u22123) and a hydrogen ion (H+). The presence of free hydrogen ions (H+) lowers the pH of the ocean. There is a natural gradient of pH in the ocean which is related to the breakdown of organic matter in deep water which slowly lowers the pH with depth: The pH value of seawater is naturally as low as 7.8 in deep ocean waters as a result of degradation of organic matter there.[140] It can be as high as 8.4 in surface waters in areas of high biological productivity.[104] The definition of global mean surface pH refers to the top layer of the water in the ocean, up to around 20 or 100 m depth. In comparison, the average depth of the ocean is about 4\u00a0km. The pH value at greater depths (more than 100 m) has not yet been affected by ocean acidification in the same way. There is a large body of deeper water where the natural gradient of pH from 8.2 to about 7.8 still exists and it will take a very long time to acidify these waters, and equally as long to recover from that acidification. But as the top layer of the ocean (the photic zone) is crucial for its marine productivity, any changes to the pH value and temperature of the top layer can have many knock-on effects, for example on marine life and ocean currents (such as effects of climate change on oceans).[104] The key issue in terms of the penetration of ocean acidification is the way the surface water mixes with deeper water or does not mix (a lack of mixing is called ocean stratification). This in turn depends on the water temperature and hence is different between the tropics and the polar regions (see ocean#Temperature).[104] The chemical properties of seawater complicate pH measurement, and several distinct pH scales exist in chemical oceanography.[141] There is no universally accepted reference pH-scale for seawater and the difference between measurements based on multiple reference scales may be up to 0.14 units.[142] Alkalinity is the balance of base (proton acceptors) and acids (proton donors) in seawater, or indeed any natural waters. The alkalinity acts as a chemical buffer, regulating the pH of seawater. While there are many ions in seawater that can contribute to the alkalinity, many of these are at very low concentrations. This means that the carbonate, bicarbonate and borate ions are the only significant contributors to seawater alkalinity in the open ocean with well oxygenated waters. The first two of these ions contribute more than 95% of this alkalinity.[104] The chemical equation for alkalinity in seawater is: The growth of phytoplankton in surface ocean waters leads to the conversion of some bicarbonate and carbonate ions into organic matter. Some of this organic matter sinks into the deep ocean where it is broken down back into carbonate and bicarbonate. This process is related to ocean productivity or marine primary production. Thus alkalinity tends to increase with depth and also along the global thermohaline circulation from the Atlantic to the Pacific and Indian Ocean, although these increases are small. The concentrations vary overall by only a few percent.[104][140] The absorption of CO2 from the atmosphere does not affect the ocean's alkalinity.[143]:\u200a2252\u200a It does lead to a reduction in pH value though (termed ocean acidification).[139] The ocean waters contain many chemical elements as dissolved ions. Elements dissolved in ocean waters have a wide range of concentrations. Some elements have very high concentrations of several grams per liter, such as sodium and chloride, together making up the majority of ocean salts. Other elements, such as iron, are present at tiny concentrations of just a few nanograms (10\u22129 grams) per liter.[122] The concentration of any element depends on its rate of supply to the ocean and its rate of removal. Elements enter the ocean from rivers, the atmosphere and hydrothermal vents. Elements are removed from ocean water by sinking and becoming buried in sediments or evaporating to the atmosphere in the case of water and some gases.  By estimating the residence time of an element, oceanographers examine the balance of input and removal. Residence time is the average time the element would spend dissolved in the ocean before it is removed. Heavily abundant elements in ocean water such as sodium, have high input rates. This reflects high abundance in rocks and rapid rock weathering, paired with very slow removal from the ocean due to sodium ions being comparatively unreactive and highly soluble. In contrast, other elements such as iron and aluminium are abundant in rocks but very insoluble, meaning that inputs to the ocean are low and removal is rapid. These cycles represent part of the major global cycle of elements that has gone on since the Earth first formed. The residence times of the very abundant elements in the ocean are estimated to be millions of years, while for highly reactive and insoluble elements, residence times are only hundreds of years.[122] A few elements such as nitrogen, phosphorus, iron, and potassium essential for life, are major components of biological material, and are commonly known as \"nutrients\". Nitrate and phosphate have ocean residence times of 10,000[146] and 69,000[147] years, respectively, while potassium is a much more abundant ion in the ocean with a residence time of 12 million[148] years. The biological cycling of these elements means that this represents a continuous removal process from the ocean's water column as degrading organic material sinks to the ocean floor as sediment. Phosphate from intensive agriculture and untreated sewage is transported via runoff to rivers and coastal zones to the ocean where it is metabolized. Eventually, it sinks to the ocean floor and is no longer available to humans as a commercial resource.[149] Production of rock phosphate, an essential ingredient in inorganic fertilizer,[150] is a slow geological process that occurs in some of the world's ocean sediments, rendering mineable sedimentary apatite (phosphate) a non-renewable resource (see peak phosphorus). This continual net deposition loss of non-renewable phosphate from human activities, may become a resource issue for fertilizer production and food security in future.[151][152] Life within the ocean evolved 3\u00a0billion years prior to life on land. Both the depth and the distance from shore strongly influence the biodiversity of the plants and animals present in each region.[154] The diversity of life in the ocean is immense, including: Marine life, which is also known as sea life or ocean life, refers to all the marine organisms that live in salt water habitats, or ecological communities that encompass all aquatic animals, plants, algae, fungi, protists, single-celled microorganisms and associated viruses living in the saline water of marine habitats, either the sea water of marginal seas and oceans, or the brackish water of coastal wetlands, lagoons, estuaries and inland seas. As of 2023[update], more than 242,000 marine species have been documented, and perhaps two million marine species are yet to be documented. On average, researches describe about 2,300 new marine species each year.[156][157] The study of marine life spans into multiple fields, which is primarily marine biology, as well as biological oceanography. A marine habitat is a habitat that supports marine life. Marine life depends in some way on the saltwater that is in the sea (the term marine comes from the Latin mare, meaning sea or ocean). A habitat is an ecological or environmental area inhabited by one or more living  species.[158] The marine environment supports many kinds of these habitats. Marine ecosystems are the largest of Earth's aquatic ecosystems and exist in waters that have a high salt content. These systems contrast with freshwater ecosystems, which have a lower salt content. Marine waters cover more than 70% of the surface of the Earth and account for more than 97% of Earth's water supply[159][160] and 90% of habitable space on Earth.[161] Seawater has an average salinity of 35 parts per thousand of water. Actual salinity varies among different marine ecosystems.[162] Marine ecosystems can be divided into many zones depending upon water depth and shoreline features. The oceanic zone is the vast open part of the ocean where animals such as      whales, sharks, and tuna live. The benthic zone consists of substrates below water where many invertebrates live. The intertidal zone is the area between high and low tides. Other near-shore (neritic) zones can include mudflats, seagrass meadows, mangroves, rocky intertidal systems, salt marshes, coral reefs, kelp forests and lagoons. In the deep water, hydrothermal vents may occur where chemosynthetic sulfur bacteria form the base of the food web. The ocean has been linked to human activity throughout history. These activities serve a wide variety of purposes, including navigation and exploration, naval warfare, travel, shipping and trade, food production (e.g. fishing, whaling, seaweed farming, aquaculture), leisure (cruising, sailing, recreational boat fishing, scuba diving), power generation (see marine energy and offshore wind power), extractive industries (offshore drilling and deep sea mining), freshwater production via desalination. Many of the world's goods are moved by ship between the world's seaports.[163] Large quantities of goods are transported across the ocean, especially across the Atlantic and around the Pacific Rim.[164] Many types of cargo including manufactured goods, are typically transported in standard sized, lockable containers that are loaded on purpose-built container ships at dedicated terminals.[165] Containerization greatly boosted the efficiency and reduced the cost of shipping products by sea. This was a major factor in the rise of globalization and exponential increases in international trade in the mid-to-late 20th century.[166] Oceans are also the major supply source for the fishing industry. Some of the major harvests are shrimp, fish, crabs, and lobster.[73] The biggest global commercial fishery is for anchovies, Alaska pollock and tuna.[167]:\u200a6\u200a A report by FAO in 2020 stated that \"in 2017, 34 percent of the fish stocks of the world's marine fisheries were classified as overfished\".[167]:\u200a54\u200a Fish and other fishery products from both wild fisheries and aquaculture are among the most widely consumed sources of protein and other essential nutrients. Data in 2017 showed that \"fish consumption accounted for 17 percent of the global population's intake of animal proteins\".[167] To fulfill this need, coastal countries have exploited marine resources in their exclusive economic zone. Fishing vessels are increasingly venturing out to exploit stocks in international waters.[168] The ocean has a vast amount of energy carried by ocean waves, tides, salinity differences, and ocean temperature differences which can be harnessed to generate electricity.[169] Forms of sustainable marine energy include tidal power, ocean thermal energy and wave power.[169][170] Offshore wind power is captured by wind turbines placed out on the ocean; it has the advantage that wind speeds are higher than on land, though wind farms are more costly to construct offshore.[171] There are large deposits of petroleum, as oil and natural gas, in rocks beneath the ocean floor. Offshore platforms and drilling rigs extract the oil or gas and store it for transport to land.[172] \"Freedom of the seas\" is a principle in international law dating from the seventeenth century. It stresses freedom to navigate the oceans and disapproves of war fought in international waters. Today, this concept is enshrined in the United Nations Convention on the Law of the Sea (UNCLOS).[173] The International Maritime Organization (IMO), which was ratified in 1958, is mainly responsible for maritime safety, liability and compensation, and has held some conventions on marine pollution related to shipping incidents. Ocean governance is the conduct of the policy, actions and affairs regarding the world's oceans.[174] Human activities affect marine life and marine habitats through many negative influences, such as marine pollution (including marine debris and microplastics), overfishing, ocean acidification, and other effects of climate change on oceans. There are many effects of climate change on oceans. One of the most important is an increase in ocean temperatures. More frequent marine heatwaves are linked to this. The rising temperature contributes to a rise in sea levels due to the expansion of water as it warms and the melting of ice sheets on land. Other effects on oceans include sea ice decline, reducing pH values and oxygen levels, as well as increased ocean stratification. All this can lead to changes of ocean currents, for example a weakening of the Atlantic meridional overturning circulation (AMOC).[105] The main cause of these changes are the emissions of greenhouse gases from human activities, mainly burning of fossil fuels and deforestation. Carbon dioxide and methane are examples of greenhouse gases. The additional greenhouse effect leads to ocean warming because the ocean takes up most of the additional heat in the climate system.[176] The ocean also absorbs some of the extra carbon dioxide that is in the atmosphere. This causes the pH value of the seawater to drop.[177] Scientists estimate that the ocean absorbs about 25% of all human-caused CO2 emissions.[177] The various layers of the oceans have different temperatures. For example, the water is colder towards the bottom of the ocean. This temperature stratification will increase as the ocean surface warms due to rising air temperatures.[178]:\u200a471\u200a Connected to this is a decline in mixing of the ocean layers, so that warm water stabilises near the surface. A reduction of cold, deep water circulation follows. The reduced vertical mixing makes it harder for the ocean to absorb heat. So a larger share of future warming goes into the atmosphere and land. One result is an increase in the amount of energy available for tropical cyclones and other storms. Another result is a decrease in nutrients for fish in the upper ocean layers. These changes also reduce the ocean's capacity to store carbon.[179] At the same time, contrasts in salinity are increasing. Salty areas are becoming saltier and fresher areas less salty.[180] Warmer water cannot contain the same amount of oxygen as cold water. As a result, oxygen from the oceans moves to the atmosphere. Increased thermal stratification may reduce the supply of oxygen from surface waters to deeper waters. This lowers the water's oxygen content even more.[181] The ocean has already lost oxygen throughout its water column. Oxygen minimum zones are increasing in size worldwide.[178]:\u200a471 These changes harm marine ecosystems, and this can lead to biodiversity loss or changes in species distribution.[105] This in turn can affect fishing and coastal tourism. For example, rising water temperatures are harming tropical coral reefs. The direct effect is coral bleaching on these reefs, because they are sensitive to even minor temperature changes. So a small increase in water temperature could have a significant impact in these environments. Another example is loss of sea ice habitats due to warming. This will have severe impacts on polar bears and other animals that rely on it. The effects of climate change on oceans put additional pressures on ocean ecosystems which are already under pressure by other impacts from human activities.[105] Marine pollution occurs when substances used or spread by humans, such as industrial, agricultural, and residential waste; particles; noise; excess carbon dioxide; or invasive organisms enter the ocean and cause harmful effects there. The majority of this waste (80%) comes from land-based activity, although marine transportation significantly contributes as well.[182] It is a combination of chemicals and trash, most of which comes from land sources and is washed or blown into the ocean. This pollution results in damage to the environment, to the health of all organisms, and to economic structures worldwide.[183] Since most inputs come from land, via rivers, sewage, or the atmosphere, it means that continental shelves are more vulnerable to pollution. Air pollution is also a contributing factor, as it carries iron, carbonic acid, nitrogen, silicon, sulfur, pesticides, and dust particles into the ocean.[184] The pollution often comes from nonpoint sources such as agricultural runoff, wind-blown debris, and dust. These nonpoint sources are largely due to runoff that enters the ocean through rivers, but wind-blown debris and dust can also play a role, as these pollutants can settle into waterways and oceans.[185] Pathways of pollution include direct discharge, land runoff, ship pollution, bilge pollution, dredging (which can create dredge plumes), atmospheric pollution and, potentially, deep sea mining. Different types of marine pollution can be grouped as pollution from marine debris, plastic pollution, including microplastics, ocean acidification, nutrient pollution, toxins, and underwater noise. Plastic pollution in the ocean is a type of marine pollution by plastics, ranging in size from large original material such as bottles and bags, down to microplastics formed from the fragmentation of plastic materials. Marine debris is mainly discarded human rubbish which floats on, or is suspended in the ocean. Plastic pollution is harmful to marine life. Another concern is the runoff of nutrients (nitrogen and phosphorus) from intensive agriculture, and the disposal of untreated or partially treated sewage to rivers and subsequently oceans. These nitrogen and phosphorus nutrients (which are also contained in fertilizers) stimulate phytoplankton and macroalgal growth, which can lead to harmful algal blooms (eutrophication) which can be harmful to humans as well as marine creatures. Excessive algal growth can also smother sensitive coral reefs and lead to loss of biodiversity and coral health. A second major concern is that the degradation of algal blooms can lead to consumption of oxygen in coastal waters, a situation that may worsen with climate change as warming reduces vertical mixing of the water column.[186] Many potentially toxic chemicals adhere to tiny particles which are then taken up by plankton and benthic animals, most of which are either deposit feeders or filter feeders. In this way, the toxins are concentrated upward within ocean food chains. When pesticides are incorporated into the marine ecosystem, they quickly become absorbed into marine food webs. Once in the food webs, these pesticides can cause mutations, as well as diseases, which can be harmful to humans as well as the entire food web. Toxic metals can also be introduced into marine food webs. These can cause a change to tissue matter, biochemistry, behavior, reproduction, and suppress growth in marine life. Also, many animal feeds have a high fish meal or fish hydrolysate content. In this way, marine toxins can be transferred to land animals, and appear later in meat and dairy products. Overfishing is the removal of aquatic animals\u2014primarily fish\u2014from a body of water at a rate greater than that the species can replenish its population naturally (i.e. the overexploitation of the fishery's existing fish stocks), resulting in the species becoming increasingly underpopulated in that area. Excessive fishing practices can occur in water bodies of any sizes, from ponds, wetlands, rivers, lakes to seas and oceans, and can result in resource depletion, reduced biological growth rates and low biomass levels. Sustained overfishing, especially industrial-scale commercial fishing, can lead to critical depensation, where the fish population is no longer able to sustain itself, resulting in extirpation or even extinction of species. Some forms of overfishing, such as the overfishing of sharks, has led to the upset of entire marine ecosystems.[187] Types of overfishing include growth overfishing, recruitment overfishing, and ecosystem overfishing. Overfishing not only causes negative impacts on biodiversity and ecosystem functioning, but also reduces fish production, which subsequently leads to negative social and economic consequences.[188][page\u00a0needed] Ocean protection serves to safeguard the ecosystems in the oceans upon which humans depend.[189][190] Protecting these ecosystems from threats is a major component of environmental protection. One of protective measures is the creation and enforcement of marine protected areas (MPAs). Marine protection may need to be considered within a national, regional and international context.[191] Other measures include supply chain transparency requirement policies, policies to prevent marine pollution, ecosystem-assistance (e.g. for coral reefs) and support for sustainable seafood (e.g. sustainable fishing practices and types of aquaculture). There is also the protection of marine resources and components whose extraction or disturbance would cause substantial harm, engagement of broader publics and impacted communities,[192] and the development of ocean clean-up projects (removal of marine plastic pollution). Examples of the latter include Clean Oceans International and The Ocean Cleanup. In 2021, 43 expert scientists published the first scientific framework version that \u2013 via integration, review, clarifications and standardization \u2013 enables the evaluation of levels of protection of marine protected areas and can serve as a guide for any subsequent efforts to improve, plan and monitor marine protection quality and extents. Examples are the efforts towards the 30%-protection-goal of the \"Global Deal For Nature\"[193] and the UN's Sustainable Development Goal 14 (\"life below water\").[194][195] In March 2023 a High Seas Treaty was signed. It is legally binding. The main achievement is the new possibility to create marine protected areas in international waters. By doing so the agreement now makes it possible to protect 30% of the oceans by 2030 (part of the 30 by 30 target).[196][197] The treaty has articles regarding the principle \"polluter-pays\", and different impacts of human activities including areas beyond the national jurisdiction of the countries making those activities. The agreement was adopted by the 193 United Nations Member States.[198]",
      "ground_truth_chunk_ids": [
        "60_fixed_chunk1"
      ],
      "source_ids": [
        "S060"
      ],
      "category": "factual",
      "id": 16
    },
    {
      "question": "What is Grete Jenny?",
      "ground_truth": "Grete Jenny (27 February 1930 \u2013 15 November 2015)[1] was an Austrian sprinter. She competed in the women's 4 \u00d7 100 metres relay at the 1948 Summer Olympics.[2] This biographical article relating to Austrian athletics is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Grete Jenny (27 February 1930 \u2013 15 November 2015)[1] was an Austrian sprinter. She competed in the women's 4 \u00d7 100 metres relay at the 1948 Summer Olympics.[2] This biographical article relating to Austrian athletics is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "169_random_chunk1"
      ],
      "source_ids": [
        "S369"
      ],
      "category": "factual",
      "id": 17
    },
    {
      "question": "What is Neuroplasticity?",
      "ground_truth": "Neuroplasticity, also known as neural plasticity or just plasticity, is the medium of neural networks in the brain to change through growth and reorganization. Neuroplasticity refers to the brain's ability to reorganize and rewire its neural connections, enabling it to adapt and function in ways that differ from its prior state. This process can occur in response to learning new skills, experiencing environmental changes, recovering from injuries, or adapting to sensory or cognitive deficits. Such adaptability highlights the dynamic and ever-evolving nature of the brain, even into adulthood.[1] These changes range from individual neuron pathways making new connections, to systematic adjustments like cortical remapping or neural oscillation. Other forms of neuroplasticity include homologous area adaptation, cross modal reassignment, map expansion, and compensatory masquerade.[2] Examples of neuroplasticity include circuit and network changes that result from learning a new ability, information acquisition,[3] environmental influences,[4] pregnancy,[5] caloric intake,[6] practice/training,[7] and psychological stress.[8] Neuroplasticity was once thought by neuroscientists to manifest only during childhood,[9][10] but research in the later half of the 20th century showed that many aspects of the brain exhibit plasticity through adulthood.[11] The developing brain exhibits a higher degree of plasticity than the adult brain.[12] Activity-dependent plasticity can have significant implications for healthy development, learning, memory, and recovery from brain damage.[13][14][15] The term plasticity was first applied to behavior in 1890 by William James in The Principles of Psychology where the term was used to describe \"a structure weak enough to yield to an influence, but strong enough not to yield all at once\".[16][17] The first person to use the term neural plasticity appears to have been the Polish neuroscientist Jerzy Konorski.[11][18] One of the first experiments providing evidence for neuroplasticity was conducted in 1793, by Italian anatomist Michele Vincenzo Malacarne, who described experiments in which he paired animals, trained one",
      "expected_answer": "Neuroplasticity, also known as neural plasticity or just plasticity, is the medium of neural networks in the brain to change through growth and reorganization. Neuroplasticity refers to the brain's ability to reorganize and rewire its neural connections, enabling it to adapt and function in ways that differ from its prior state. This process can occur in response to learning new skills, experiencing environmental changes, recovering from injuries, or adapting to sensory or cognitive deficits. Such adaptability highlights the dynamic and ever-evolving nature of the brain, even into adulthood.[1] These changes range from individual neuron pathways making new connections, to systematic adjustments like cortical remapping or neural oscillation. Other forms of neuroplasticity include homologous area adaptation, cross modal reassignment, map expansion, and compensatory masquerade.[2] Examples of neuroplasticity include circuit and network changes that result from learning a new ability, information acquisition,[3] environmental influences,[4] pregnancy,[5] caloric intake,[6] practice/training,[7] and psychological stress.[8] Neuroplasticity was once thought by neuroscientists to manifest only during childhood,[9][10] but research in the later half of the 20th century showed that many aspects of the brain exhibit plasticity through adulthood.[11] The developing brain exhibits a higher degree of plasticity than the adult brain.[12] Activity-dependent plasticity can have significant implications for healthy development, learning, memory, and recovery from brain damage.[13][14][15] The term plasticity was first applied to behavior in 1890 by William James in The Principles of Psychology where the term was used to describe \"a structure weak enough to yield to an influence, but strong enough not to yield all at once\".[16][17] The first person to use the term neural plasticity appears to have been the Polish neuroscientist Jerzy Konorski.[11][18] One of the first experiments providing evidence for neuroplasticity was conducted in 1793, by Italian anatomist Michele Vincenzo Malacarne, who described experiments in which he paired animals, trained one of the pair extensively for years, and then dissected both. Malacarne discovered that the cerebellums of the trained animals were substantially larger than the cerebellum of the untrained animals. However, while these findings were significant, they were eventually forgotten.[19] In 1890, the idea that the brain and its function are not fixed throughout adulthood was proposed by William James in The Principles of Psychology, though the idea was largely neglected.[17] Up until the 1970s, neuroscientists believed that the brain's structure and function was essentially fixed throughout adulthood.[20] While the brain was commonly understood as a nonrenewable organ in the early 1900s, the pioneering neuroscientist Santiago Ram\u00f3n y Cajal used the term neuronal plasticity to describe nonpathological changes in the structure of adult brains. Based on his renowned neuron doctrine, Cajal first described the neuron as the fundamental unit of the nervous system that later served as an essential foundation to develop the concept of neural plasticity.[21] Many neuroscientists used the term plasticity to explain the regenerative capacity of the peripheral nervous system only. Cajal, however, used the term plasticity to reference his findings of degeneration and regeneration in the adult brain (a part of the central nervous system). This was controversial, with some like Walther Spielmeyer and Max Bielschowsky arguing that the CNS cannot produce new cells.[22][23] The term has since been broadly applied: Given the central importance of neuroplasticity, an outsider would be forgiven for assuming that it was well defined and that a basic and universal framework served to direct current and future hypotheses and experimentation. Sadly, however, this is not the case. While many neuroscientists use the word neuroplasticity as an umbrella term it means different things to different researchers in different subfields ... In brief, a mutually agreed-upon framework does not appear to exist.[24] In 1923, Karl Lashley conducted experiments on rhesus monkeys that demonstrated changes in neuronal pathways, which he concluded were evidence of plasticity. Despite this, and other research that suggested plasticity, neuroscientists did not widely accept the idea of neuroplasticity. Inspired by work from Nicolas Rashevsky,[25] in 1943, McCulloch and Pitts proposed the artificial neuron, with a learning rule, whereby new synapses are produced when neurons fire simultaneously.[26] This is then extensively discussed in The organization of behavior (Hebb, 1949) and is now known as Hebbian learning. In 1945, Justo Gonzalo concluded from his research on brain dynamics, that, contrary to the activity of the projection areas, the \"central\" cortical mass (more or less equidistant from the visual, tactile and auditive projection areas), would be a \"maneuvering mass\", rather unspecific or multisensory, with capacity to increase neural excitability and re-organize the activity by means of plasticity properties.[27] He gives as a first example of adaptation, to see upright with reversing glasses in the Stratton experiment,[28] and specially, several first-hand brain injuries cases in which he observed dynamic and adaptive properties in their disorders, in particular in the inverted perception disorder [e.g., see pp 260\u201362 Vol. I (1945), p 696 Vol. II (1950)].[27] He stated that a sensory signal in a projection area would be only an inverted and constricted outline that would be magnified due to the increase in recruited cerebral mass, and re-inverted due to some effect of brain plasticity, in more central areas, following a spiral growth.[29] Marian Diamond of the University of California, Berkeley, produced the first scientific evidence of anatomical brain plasticity, publishing her research in 1964.[30][31] Other significant evidence was produced in the 1960s and after, notably from scientists including Paul Bach-y-Rita, Michael Merzenich along with Jon Kaas, as well as several others.[20][32] An attempt to describe the mechanisms of neuroplasticity, an early version of the computational theory of mind derived from Hebb's work, was put forward by Peter Putnam and Robert W. Fuller in that time.[33][34] In the 1960s, Paul Bach-y-Rita invented a device that was tested on a small number of people, and involved a person sitting in a chair, embedded in which were nubs that were made to vibrate in ways that translated images received in a camera, allowing a form of vision via sensory substitution.[35][36] Studies in people recovering from stroke also provided support for neuroplasticity, as regions of the brain that remained healthy could sometimes take over, at least in part, functions that had been destroyed; Shepherd Ivory Franz did work in this area.[37][38] Eleanor Maguire documented changes in hippocampal structure associated with acquiring the knowledge of London's layout in local taxi drivers.[39][40][41] A redistribution of grey matter was indicated in London Taxi Drivers compared to controls. This work on hippocampal plasticity not only interested scientists, but also engaged the public and media worldwide. Michael Merzenich is a neuroscientist who has been one of the pioneers of neuroplasticity for over three decades. He has made some of \"the most ambitious claims for the field \u2013 that brain exercises may be as useful as drugs to treat diseases as severe as schizophrenia \u2013 that plasticity exists from cradle to the grave, and that radical improvements in cognitive functioning \u2013 how we learn, think, perceive, and remember are possible even in the elderly.\"[35] Merzenich's work was affected by a crucial discovery made by David Hubel and Torsten Wiesel in their work with kittens. The experiment involved sewing one eye shut and recording the cortical brain maps. Hubel and Wiesel saw that the portion of the kitten's brain associated with the shut eye was not idle, as expected. Instead, it processed visual information from the open eye. It was \"\u2026as though the brain didn't want to waste any 'cortical real estate' and had found a way to rewire itself.\"[35] This implied neuroplasticity during the critical period. However, Merzenich argued that neuroplasticity could occur beyond the critical period. His first encounter with adult plasticity came when he was engaged in a postdoctoral study with Clinton Woosley. The experiment was based on observation of what occurred in the brain when one peripheral nerve was cut and subsequently regenerated. The two scientists micromapped the hand maps of monkey brains before and after cutting a peripheral nerve and sewing the ends together. Afterwards, the hand map in the brain that they expected to be jumbled was nearly normal. This was a substantial breakthrough. Merzenich asserted that, \"If the brain map could normalize its structure in response to abnormal input, the prevailing view that we are born with a hardwired system had to be wrong. The brain had to be plastic.\"[35] Merzenich received the 2016 Kavli Prize in Neuroscience \"for the discovery of mechanisms that allow experience and neural activity to remodel brain function.\"[42] There are different ideas and theories on what biological processes allow for neuroplasticity to occur. The core of this phenomenon is based upon synapses and how connections between them change based on neuron functioning. It is widely agreed upon that neuroplasticity takes on many forms, as it is a result of a variety of pathways. These pathways, mainly signaling cascades, allow for gene expression alterations that lead to neuronal changes, and thus neuroplasticity. There are a number of other factors that are thought to play a role in the biological processes underlying the changing of neural networks in the brain. Some of these factors include synapse regulation via phosphorylation, the role of inflammation and inflammatory cytokines, proteins such as Bcl-2 proteins and neutrophorins, energy production via mitochondria,[43] and acetylcholine.[44] JT Wall and J Xu have traced the mechanisms underlying neuroplasticity. Re-organization is not cortically emergent, but occurs at every level in the processing hierarchy; this produces the map changes observed in the cerebral cortex.[45] Christopher Shaw and Jill McEachern (eds) in \"Toward a theory of Neuroplasticity\", state that there is no all-inclusive theory that overarches different frameworks and systems in the study of neuroplasticity. However, researchers often describe neuroplasticity as \"the ability to make adaptive changes related to the structure and function of the nervous system.\"[46] Correspondingly, two types of neuroplasticity are often discussed: structural neuroplasticity and functional neuroplasticity. Structural plasticity is often understood as the brain's ability to change its neuronal connections. The changes of grey matter proportion or the synaptic strength in the brain are considered as examples of structural neuroplasticity. This type of neuroplasticity often studies the effect of various internal or external stimuli on the brain's anatomical reorganization. New neurons are constantly produced and integrated into the central nervous system based on this type of neuroplasticity.[47] Researchers nowadays use multiple cross-sectional imaging methods (i.e. magnetic resonance imaging (MRI), computerized tomography (CT)) to study the structural alterations of the human brains.[48] Structural neuroplasticity is currently investigated more within the field of neuroscience in current academia.[21] Adult neurogenesis \"has not been convincingly demonstrated in humans\".[47] Functional plasticity refers to the brain's ability to alter and adapt the functional properties of network of neurons. It can occur in four known ways namely: Homologous area adaptation is the assumption of a particular cognitive process by a homologous region in the opposite hemisphere.[49] For instance, through homologous area adaptation a cognitive task is shifted from a damaged part of the brain to its homologous area in opposite side of the brain. Homologous area adaptation is a type of functional neuroplasticity that occur usually in children rather than adults. In map expansion, cortical maps related to particular cognitive tasks expand due to frequent exposure to stimuli. Map expansion has been proven through experiments performed in relation to the study: experiment on effect of frequent stimulus on functional connectivity of the brain was observed in individuals learning spatial routes.[50] Cross-model reassignment involves reception of novel input signals to a brain region which has been stripped of its default input. Functional plasticity through compensatory masquerade occurs using different cognitive processes for an already established cognitive task when the initial process cannot be followed due to impairment. Changes in the brain associated with functional neuroplasticity can occur in response to two different types of events: In the latter case the functions from one part of the brain transfer to another part of the brain based on the demand to produce recovery of behavioral or physiological processes.[51] Regarding physiological forms of activity-dependent plasticity, those involving synapses are referred to as synaptic plasticity. The strengthening or weakening of synapses that results in an increase or decrease of firing rate of the neurons are called long-term potentiation (LTP) and long-term depression (LTD), respectively, and they are considered as examples of synaptic plasticity that are associated with memory.[52] The cerebellum is a typical structure with combinations of LTP/LTD and redundancy within the circuitry, allowing plasticity at several sites.[53] More recently it has become clearer that synaptic plasticity can be complemented by another form of activity-dependent plasticity involving the intrinsic excitability of neurons, which is referred to as intrinsic plasticity.[54][55][56] This, as opposed to homeostatic plasticity does not necessarily maintain the overall activity of a neuron within a network but contributes to encoding memories.[57] Also, many studies have indicated functional neuroplasticity in the level of brain networks, where training alters the strength of functional connections.[58][59] Although a recent study discusses that these observed changes should not directly relate to neuroplasticity, since they may root in the systematic requirement of the brain network for reorganization.[60] The adult brain is not entirely \"hard-wired\" with fixed neuronal circuits. There are many instances of cortical and subcortical rewiring of neuronal circuits in response to training as well as in response to injury. There is ample evidence[61] for the active, experience-dependent re-organization of the synaptic networks of the brain involving multiple inter-related structures including the cerebral cortex.[62] The specific details of how this process occurs at the molecular and ultrastructural levels are topics of active neuroscience research. The way experience can influence the synaptic organization of the brain is also the basis for a number of theories of brain function including the general theory of mind and neural Darwinism. The concept of neuroplasticity is also central to theories of memory and learning that are associated with experience-driven alteration of synaptic structure and function in studies of classical conditioning in invertebrate animal models such as Aplysia.[citation needed] There is evidence that neurogenesis (birth of brain cells) occurs in the adult, rodent brain\u2014and such changes can persist well into old age.[63] The evidence for neurogenesis is mainly restricted to the hippocampus and olfactory bulb, but research has revealed that other parts of the brain, including the cerebellum, may be involved as well.[64] However, the degree of rewiring induced by the integration of new neurons in the established circuits is not known, and such rewiring may well be functionally redundant.[65] Addiction is a state characterized by compulsive engagement in rewarding stimuli, despite adverse consequences. The process of developing an addiction occurs through instrumental learning, which is otherwise known as operant conditioning. Neuroscientists believe that drug addicts\u2019 behavior is a direct correlation to some physiological change in their brain, caused by using drugs. This view believes there is a bodily function in the brain causing the addiction. This is brought on by a change in the brain caused by brain damage or adaptation from chronic drug use.[66][67] In humans, addiction is diagnosed according to diagnostic models such as the Diagnostic and Statistical Manual of Mental Disorders, through observed behaviors. There has been significant advancement in understanding the structural changes that occur in parts of the brain involved in the reward pathway (mesolimbic system) that underlies addiction.[68] Most research has focused on two portions of the brain: the ventral tegmental area, (VTA) and the nucleus accumbens (NAc).[69] The VTA is the portion of the mesolimbic system responsible for spreading dopamine to the whole system. The VTA is stimulated by \u2033rewarding experiences\u2033. The release of dopamine by the VTA induces pleasure, thus reinforcing behaviors that lead to the reward.[70] Drugs of abuse increase the VTA's ability to project dopamine to the rest of the reward circuit.[71] These structural changes only last 7\u201310 days,[72] however, indicating that the VTA cannot be the only part of the brain that is affected by drug use, and changed during the development of addiction. The nucleus accumbens (NAc) plays an essential part in the formation of addiction. Almost every drug with addictive potential induces the release of dopamine into the NAc.[73] In contrast to the VTA, the NAc shows long-term structural changes. Drugs of abuse weaken the connections within the NAc after habitual use,[74] as well as after use then withdrawal.[75] A surprising consequence of neuroplasticity is that the brain activity associated with a given function can be transferred to a different location; this can result from normal experience and also occurs in the process of recovery from brain injury. Neuroplasticity is the fundamental issue that supports the scientific basis for treatment of acquired brain injury with goal-directed experiential therapeutic programs in the context of rehabilitation approaches to the functional consequences of the injury. Neuroplasticity is gaining popularity as a theory that, at least in part, explains improvements in functional outcomes with physical therapy post-stroke. Rehabilitation techniques that are supported by evidence which suggest cortical reorganization as the mechanism of change include constraint-induced movement therapy, functional electrical stimulation, treadmill training with body-weight support, and virtual reality therapy. Robot assisted therapy is an emerging technique, which is also hypothesized to work by way of neuroplasticity, though there is currently insufficient evidence to determine the exact mechanisms of change when using this method.[76] One group has developed a treatment that includes increased levels of progesterone injections in brain-injured patients. \"Administration of progesterone after traumatic brain injury[77] (TBI) and stroke reduces edema, inflammation, and neuronal cell death, and enhances spatial reference memory and sensory-motor recovery.\"[78] In a clinical trial, a group of severely injured patients had a 60% reduction in mortality after three days of progesterone injections.[79] However, a study published in the New England Journal of Medicine in 2014 detailing the results of a multi-center NIH-funded phase III clinical trial of 882 patients found that treatment of acute traumatic brain injury with the hormone progesterone provides no significant benefit to patients when compared with placebo.[80] For decades, researchers assumed that humans had to acquire binocular vision, in particular stereopsis, in early childhood or they would never gain it. In recent years, however, successful improvements in persons with amblyopia, convergence insufficiency or other stereo vision anomalies have become prime examples of neuroplasticity; binocular vision improvements and stereopsis recovery are now active areas of scientific and clinical research.[81][82][83] In the phenomenon of phantom limb sensation, a person continues to feel pain or sensation within a part of their body that has been amputated. This is strangely common, occurring in 60\u201380% of amputees.[84] An explanation for this is based on the concept of neuroplasticity, as the cortical maps of the removed limbs are believed to have become engaged with the area around them in the postcentral gyrus. This results in activity within the surrounding area of the cortex being misinterpreted by the area of the cortex formerly responsible for the amputated limb. The relationship between phantom limb sensation and neuroplasticity is a complex one. In the early 1990s V.S. Ramachandran theorized that phantom limbs were the result of cortical remapping. However, in 1995 Herta Flor and her colleagues demonstrated that cortical remapping occurs only in patients who have phantom pain.[85] Her research showed that phantom limb pain (rather than referred sensations) was the perceptual correlate of cortical reorganization.[86] This phenomenon is sometimes referred to as maladaptive plasticity. In 2009, Lorimer Moseley and Peter Brugger carried out an experiment in which they encouraged arm amputee subjects to use visual imagery to contort their phantom limbs into impossible[clarification needed] configurations. Four of the seven subjects succeeded in performing impossible movements of the phantom limb. This experiment suggests that the subjects had modified the neural representation of their phantom limbs and generated the motor commands needed to execute impossible movements in the absence of feedback from the body.[87] Individuals who have chronic pain experience prolonged pain at sites that may have been previously injured, yet are otherwise currently healthy. This phenomenon is related to neuroplasticity due to a maladaptive reorganization of the nervous system, both peripherally and centrally. During the period of tissue damage, noxious stimuli and inflammation cause an elevation of nociceptive input from the periphery to the central nervous system. Prolonged nociception from the periphery then elicits a neuroplastic response at the cortical level to change its somatotopic organization for the painful site, inducing central sensitization.[88] For instance, individuals experiencing complex regional pain syndrome demonstrate a diminished cortical somatotopic representation of the hand contralaterally as well as a decreased spacing between the hand and the mouth.[89] Additionally, chronic pain has been reported to significantly reduce the volume of grey matter in the brain globally, and more specifically at the prefrontal cortex and right thalamus.[90] However, following treatment, these abnormalities in cortical reorganization and grey matter volume are resolved, as well as their symptoms. Similar results have been reported for phantom limb pain,[91] chronic low back pain[92] and carpal tunnel syndrome.[93] A number of studies have linked meditation practice to differences in cortical thickness or density of gray matter.[94][95][96][97] One of the most well-known studies to demonstrate this was led by Sara Lazar, from Harvard University, in 2000.[98] Richard Davidson, a neuroscientist at the University of Wisconsin, has led experiments in collaboration with the Dalai Lama on effects of meditation on the brain. His results suggest that meditation may lead to change in the physical structure of brain regions associated with attention, anxiety, depression, fear, anger, and compassion as well as the ability of the body to heal itself.[99][100] There is substantial evidence that artistic engagement in a therapeutic environment can create changes in neural network connections as well as increase cognitive flexibility.[101][102] In one 2013 study, researchers found evidence that long-term, habitual artistic training (e.g. musical instrument practice, purposeful painting, etc.) can \"macroscopically imprint a neural network system of spontaneous activity in which the related brain regions become functionally and topologically modularized in both domain-general and domain-specific manners\".[103] In simple terms, brains repeatedly exposed to artistic training over long periods develop adaptations to make such activity both easier and more likely to spontaneously occur. Some researchers and academics have suggested that artistic engagement has substantially altered the human brain throughout our evolutionary history. D.W Zaidel, adjunct professor of behavioral neuroscience and contributor at VAGA, has written that \"evolutionary theory links the symbolic nature of art to critical pivotal brain changes in Homo sapiens supporting increased development of language and hierarchical social grouping\".[104] There is evidence that engaging in music-supported therapy can improve neuroplasticity in patients who are recovering from brain injuries. Music-supported therapy can be used for patients that are undergoing stroke rehabilitation where a one month study of stroke patients participating in music-supported therapy showed a significant improvement in motor control in their affected hand.[105] Another finding was the examination of grey matter volume of adults developing brain atrophy and cognitive decline where playing a musical instrument, such as the piano, or listening to music can increase grey matter volume in areas such as the caudate nucleus, Rolandic operculum, and cerebellum.[106] Evidence also suggests that music-supported therapy can improve cognitive performance, well-being, and social behavior in patients who are recovering from damage to the orbitofrontal cortex (OFC) and recovering from mild traumatic brain injury. Neuroimaging post music-supported therapy revealed functional changes in OFC networks, with improvements observed in both task-based and resting-state fMRI analyses.[107] Beyond clinical rehabilitation, music has been shown to induce neuroplastic changes in healthy individuals through long-term training and repeated exposure.[108] Studies comparing musicians and non-musicians have demonstrated structural and functional brain differences associated with musical practice, particularly when training begins early in life.[98] Musicians often exhibit increased gray and white matter volume in motor, auditory, and cerebellar regions, reflecting adaptations related to fine motor control, auditory processing, and timing.[108] Evidence of cortical remapping has also been observed, such as enlarged cortical representations of the fingers most frequently used during instrument performance.[108] Music training strongly affects the auditory system, with musicians showing enhanced activation and structural differences in primary and secondary auditory cortices involved in processing pitch, rhythm, and melody.[108] Functional changes have been observed not only at the cortical level but also in subcortical structures, including the brainstem, where musicians demonstrate faster and stronger neural responses to sound.[108] Across the lifespan, sustained musical engagement has been associated with reduced age-related decline in certain brain regions and a lower risk of cognitive impairment, suggesting that music-related neuroplasticity may contribute to long-term brain health.[108] Aerobic exercise increases the production of neurotrophic factors (compounds that promote growth or survival of neurons), such as brain-derived neurotrophic factor (BDNF), insulin-like growth factor 1 (IGF-1), and vascular endothelial growth factor (VEGF).[109][110][111] Exercise-induced effects on the hippocampus are associated with measurable improvements in spatial memory.[112][113][114][115] Consistent aerobic exercise over a period of several months induces marked clinically significant improvements in executive function (i.e., the \"cognitive control\" of behavior) and increased gray matter volume in multiple brain regions, particularly those that give rise to cognitive control.[111][112][116][117] The brain structures that show the greatest improvements in gray matter volume in response to aerobic exercise are the prefrontal cortex and hippocampus;[111][112][113] moderate improvements are seen in the anterior cingulate cortex, parietal cortex, cerebellum, caudate nucleus, and nucleus accumbens.[111][112][113] Higher physical fitness scores (measured by VO2 max) are associated with better executive function, faster processing speed, and greater volume of the hippocampus, caudate nucleus, and nucleus accumbens.[112] Due to hearing loss, the auditory cortex and other association areas of the brain in deaf and/or hard of hearing people undergo compensatory plasticity.[118][119][120] The auditory cortex usually reserved for processing auditory information in hearing people now is redirected to serve other functions, especially for vision and somatosensation. Deaf individuals have enhanced peripheral visual attention,[121] better motion change but not color change detection ability in visual tasks,[119][120][122] more effective visual search,[123] and faster response time for visual targets[124][125] compared to hearing individuals. Altered visual processing in deaf people is often found to be associated with the repurposing of other brain areas including primary auditory cortex, posterior parietal association cortex (PPAC), and anterior cingulate cortex (ACC).[126] A review by Bavelier et al. (2006) summarizes many aspects on the topic of visual ability comparison between deaf and hearing individuals.[127] Brain areas that serve a function in auditory processing repurpose to process somatosensory information in congenitally deaf people. They have higher sensitivity in detecting frequency change in vibration above threshold[128] and higher and more widespread activation in auditory cortex under somatosensory stimulation.[129][118] However, speeded response for somatosensory stimuli is not found in deaf adults.[124] Neuroplasticity is involved in the development of sensory function. The brain is born immature and then adapts to sensory inputs after birth. In the auditory system, congenital hearing loss, a rather frequent inborn condition affecting 1 of 1000 newborns, has been shown to affect auditory development, and implantation of a sensory prostheses activating the auditory system has prevented the deficits and induced functional maturation of the auditory system.[130] Due to a sensitive period for plasticity, there is also a sensitive period for such intervention within the first 2\u20134 years of life. Consequently, in prelingually deaf children, early cochlear implantation, as a rule, allows the children to learn the mother language and acquire acoustic communication.[131] Due to vision loss, the visual cortex in blind people may undergo cross-modal plasticity, and therefore other senses may have enhanced abilities. Or the opposite could occur, with the lack of visual input weakening the development of other sensory systems. One study suggests that the right posterior middle temporal gyrus and superior occipital gyrus reveal more activation in the blind than in the sighted people during a sound-moving detection task.[132] Several studies support the latter idea and found weakened ability in audio distance evaluation, proprioceptive reproduction, threshold for visual bisection, and judging minimum audible angle.[133][134] Human echolocation is a learned ability for humans to sense their environment from echoes. This ability is used by some blind people to navigate their environment and sense their surroundings in detail. Studies in 2010[135] and 2011[136] using functional magnetic resonance imaging techniques have shown that parts of the brain associated with visual processing are adapted for the new skill of echolocation. Studies with blind patients, for example, suggest that the click-echoes heard by these patients were processed by brain regions devoted to vision rather than audition.[136] Reviews of MRI and electroencephalography (EEG) studies on individuals with ADHD suggest that the long-term treatment of ADHD with stimulants, such as amphetamine or methylphenidate, decreases abnormalities in brain structure and function found in subjects with ADHD, and improves function in several parts of the brain, such as the right caudate nucleus of the basal ganglia,[137][138][139] left ventrolateral prefrontal cortex (VLPFC), and superior temporal gyrus.[140] In addition to pharmacological treatment, non-pharmacological interventions that leverage neuroplasticity have been proposed as potential approaches for managing ADHD symptoms. Cognitive training and other behavioral therapies aim to improve attention, self-regulation, and impulse control by promoting functional and structural changes in neural circuits associated with executive function.[141] Computerized cognitive training programs have been shown to target underdeveloped neural networks in individuals with ADHD, leading to improvements in attention and working memory through repeated stimulation of specific brain regions.[141] These interventions may produce longer-term neuroplastic changes that overlap with brain areas affected by stimulant medications, suggesting that neuroplasticity-based therapies could complement or, in some cases, reduce reliance on pharmacological treatment.[141] Neuroplasticity is most active in childhood as a part of normal human development, and can also be seen as an especially important mechanism for children in terms of risk and resiliency.[142] Trauma is considered a great risk as it negatively affects many areas of the brain and puts a strain on the sympathetic nervous system from constant activation. Trauma thus alters the brain's connections such that children who have experienced trauma may be hyper vigilant or overly aroused.[143] However, a child's brain can cope with these adverse effects through the actions of neuroplasticity.[144] Neuroplasticity is shown in four different categories in children and covering a wide variety of neuronal functioning. These four types include impaired, excessive, adaptive, and plasticity.[145] There are many examples of neuroplasticity in human development. For example, Justine Ker and Stephen Nelson looked at the effects of musical training on neuroplasticity, and found that musical training can contribute to experience dependent structural plasticity. This is when changes in the brain occur based on experiences that are unique to an individual. Examples of this are learning multiple languages, playing a sport, doing theatre, etc. A study done by Hyde in 2009, showed that changes in the brain of children could be seen in as little as 15 months of musical training.[146] Ker and Nelson suggest this degree of plasticity in the brains of children can \"help provide a form of intervention for children... with developmental disorders and neurological diseases.\"[147] In a single lifespan, individuals of an animal species may encounter various changes in brain morphology. Many of these differences are caused by the release of hormones in the brain; others are the product of evolutionary factors or developmental stages.[148][149][150][151] Some changes occur seasonally in species to enhance or generate response behaviors. Changing brain behavior and morphology to suit other seasonal behaviors is relatively common in animals.[152] These changes can improve the chances of mating during breeding season.[148][149][150][152][153][154] Examples of seasonal brain morphology change can be found within many classes and species. Within the class Aves, black-capped chickadees experience an increase in the volume of their hippocampus and strength of neural connections to the hippocampus during fall months.[155][156] These morphological changes within the hippocampus which are related to spatial memory are not limited to birds, as they can also be observed in rodents and amphibians.[152] In songbirds, many song control nuclei in the brain increase in size during mating season.[152] Among birds, changes in brain morphology to influence song patterns, frequency, and volume are common.[157] Gonadotropin-releasing hormone (GnRH) immunoreactivity, or the reception of the hormone, is lowered in European starlings exposed to longer periods of light during the day.[148][149] The California sea hare, a gastropod, has more successful inhibition of egg-laying hormones outside of mating season due to increased effectiveness of inhibitors in the brain.[150] Changes to the inhibitory nature of regions of the brain can also be found in humans and other mammals.[151] In the amphibian Bufo japonicus, part of the amygdala is larger before breeding and during hibernation than it is after breeding.[153] Seasonal brain variation occurs within many mammals. Part of the hypothalamus of the common ewe is more receptive to GnRH during breeding season than at other times of the year.[154] Humans experience a change in the \"size of the hypothalamic suprachiasmatic nucleus and vasopressin-immunoreactive neurons within it\"[151] during the fall, when these parts are larger. In the spring, both reduce in size.[158] A group of scientists found that if a small stroke (an infarction) is induced by obstruction of blood flow to a portion of a monkey's motor cortex, the part of the body that responds by movement moves when areas adjacent to the damaged brain area are stimulated. In one study, intracortical microstimulation (ICMS) mapping techniques were used in nine normal monkeys. Some underwent ischemic-infarction procedures and the others, ICMS procedures. The monkeys with ischemic infarctions retained more finger flexion during food retrieval and after several months this deficit returned to preoperative levels.[159] With respect to the distal forelimb representation, \"postinfarction mapping procedures revealed that movement representations underwent reorganization throughout the adjacent, undamaged cortex.\"[159] Understanding of interaction between the damaged and undamaged areas provides a basis for better treatment plans in stroke patients. Current research includes the tracking of changes that occur in the motor areas of the cerebral cortex as a result of a stroke. Thus, events that occur in the reorganization process of the brain can be ascertained. The treatment plans that may enhance recovery from strokes, such as physiotherapy, pharmacotherapy, and electrical-stimulation therapy, are also being studied. Jon Kaas, a professor at Vanderbilt University, has been able to show \"how somatosensory area 3b and ventroposterior (VP) nucleus of the thalamus are affected by longstanding unilateral dorsal-column lesions at cervical levels in macaque monkeys.\"[160] Adult brains have the ability to change as a result of injury but the extent of the reorganization depends on the extent of the injury. His recent research focuses on the somatosensory system, which involves a sense of the body and its movements using many senses. Usually, damage of the somatosensory cortex results in impairment of the body perception. Kaas' research project is focused on how these systems (somatosensory, cognitive, motor systems) respond with plastic changes resulting from injury.[160] One recent study of neuroplasticity involves work done by a team of doctors and researchers at Emory University, specifically Donald Stein[161] and David Wright. This is the first treatment in 40 years that has significant results in treating traumatic brain injuries while also incurring no known side effects and being cheap to administer.[79] Stein noticed that female mice seemed to recover from brain injuries better than male mice, and that at certain points in the estrus cycle, females recovered even better. This difference may be attributed to different levels of progesterone, with higher levels of progesterone leading to the faster recovery from brain injury in mice. However, clinical trials showed progesterone offers no significant benefit for traumatic brain injury in human patients.[162] Transcriptional profiling of the frontal cortex of persons ranging from 26 to 106 years of age defined a set of genes with reduced expression after age 40, and especially after age 70.[163] Genes that play central roles in synaptic plasticity were the most significantly affected by age, generally showing reduced expression over time. There was also a marked increase in cortical DNA damage, likely oxidative DNA damage, in gene promoters with aging.[163] Reactive oxygen species appear to have a significant role in the regulation of synaptic plasticity and cognitive function.[164] However age-related increases in reactive oxygen species may also lead to impairments in these functions. There is a beneficial effect of multilingualism on people's behavior and cognition. Numerous studies have shown that people who study more than one language have better cognitive functions and flexibilities than people who only speak one language. Bilinguals are found to have longer attention spans, stronger organization and analyzation skills, and a better theory of mind than monolinguals. Researchers have found that the effect of multilingualism on better cognition is due to neuroplasticity.[citation needed] In one prominent study, neurolinguists used a voxel-based morphometry (VBM) method to visualize the structural plasticity of brains in healthy monolinguals and bilinguals. They first investigated the differences in density of grey and white matter between two groups and found the relationship between brain structure and age of language acquisition. The results showed that grey-matter density in the inferior parietal cortex for multilinguals were significantly greater than monolinguals. The researchers also found that early bilinguals had a greater density of grey matter relative to late bilinguals in the same region. The inferior parietal cortex is a brain region highly associated with the language learning, which corresponds to the VBM result of the study.[165] Recent studies have also found that learning multiple languages not only re-structures the brain but also boosts brain's capacity for plasticity. A recent study found that multilingualism not only affects the grey matter but also white matter of the brain. White matter is made up of myelinated axons that is greatly associated with learning and communication. Neurolinguists used a diffusion tensor imaging (DTI) scanning method to determine the white matter intensity between monolinguals and bilinguals. Increased myelinations in white matter tracts were found in bilingual individuals who actively used both languages in everyday life. The demand of handling more than one language requires more efficient connectivity within the brain, which resulted in greater white matter density for multilinguals.[166] While it is still debated whether these changes in brain are result of genetic disposition or environmental demands, many evidences suggest that environmental, social experience in early multilinguals affect the structural and functional reorganization in the brain.[167][168] Historically, the monoamine imbalance hypothesis of depression played a dominant role in psychiatry and drug development.[169] However, while traditional antidepressants cause a quick increase in noradrenaline, serotonin, or dopamine, there is a significant delay in their clinical effect and often an inadequate treatment response.[170] As neuroscientists pursued this avenue of research, clinical and preclinical data across multiple modalities began to converge on pathways involved in neuroplasticity.[171] They found a strong inverse relationship between the number of synapses and severity of depression symptoms[172] and discovered that in addition to their neurotransmitter effect, traditional antidepressants improved neuroplasticity but over a significantly protracted time course of weeks or months.[173] The search for faster acting antidepressants found success in the pursuit of ketamine, a well-known anesthetic agent, that was found to have potent anti-depressant effects after a single infusion due to its capacity to rapidly increase the number of dendritic spines and to restore aspects of functional connectivity.[174] Additional neuroplasticity promoting compounds with therapeutic effects that were both rapid and enduring have been identified through classes of compounds including serotonergic psychedelics, cholinergic scopolamine, and other novel compounds. To differentiate between traditional antidepressants focused on monoamine modulation and this new category of fast acting antidepressants that achieve therapeutic effects through neuroplasticity, the term psychoplastogen was introduced.[175] Nicotine affects the brain by binding to nicotinic acetylcholine receptors, the same receptors acetylcholine binds to, which has been linked with Neuroplasticity.[176] Nicotine use may lower the rate of neuroplasticity in the brain by damaging the nicotinic-acetylcholine receptors needed to reuptake the acetylcholine necessary for neuroplasticity.[177]",
      "ground_truth_chunk_ids": [
        "164_fixed_chunk1"
      ],
      "source_ids": [
        "S164"
      ],
      "category": "factual",
      "id": 18
    },
    {
      "question": "What is Robots (2005 film)?",
      "ground_truth": "Robots is a 2005 American animated science fiction comedy film directed by Chris Wedge and written by David Lindsay-Abaire, Lowell Ganz and Babaloo Mandel. Produced by 20th Century Fox Animation and Blue Sky Studios, it stars the voices of Ewan McGregor, Halle Berry, Greg Kinnear, Mel Brooks, Amanda Bynes, Drew Carey and Robin Williams. The story follows an ambitious inventor robot named Rodney Copperbottom, who seeks to work for his idol Bigweld's company in Robot City, but discovers a plot by its new leader Phineas T. Ratchet and his mother to forcibly upgrade the city's populace and eradicate struggling robots, known as \"outmodes\". Development of the film began in 2000, following a failed attempt by Wedge and children's author William Joyce to adapt Joyce's 1993 children's book Santa Calls. They instead decided to create an original story based on robots. The project was approved by executive producer Chris Meledandri in 2001 and production began the next year. Robots premiered at the Mann Village Theatre in Westwood, Los Angeles, on March 6, 2005, and was released in the United States on March 11 by 20th Century Fox. The film was praised by critics for its humor, animation, and performances, while its story and characters were deemed somewhat formulaic.[4] The film was commercially successful, grossing $262.5 million worldwide against a $75\u201380 million budget. A sequel was discussed but never produced due to the studio shifting focus to its flagship franchise, Ice Age.[5] In a world of robots, Rodney Copperbottom, son of Herb and Lydia Copperbottom, is an aspiring young inventor from the city of Rivet Town. He idolizes Bigweld, a famous inventor and philanthropist whose company, Bigweld Industries, employs other inventors and provides poor robots with spare parts. Rodney develops a small, flying robot named Wonderbot to assist his father, who works",
      "expected_answer": "Robots is a 2005 American animated science fiction comedy film directed by Chris Wedge and written by David Lindsay-Abaire, Lowell Ganz and Babaloo Mandel. Produced by 20th Century Fox Animation and Blue Sky Studios, it stars the voices of Ewan McGregor, Halle Berry, Greg Kinnear, Mel Brooks, Amanda Bynes, Drew Carey and Robin Williams. The story follows an ambitious inventor robot named Rodney Copperbottom, who seeks to work for his idol Bigweld's company in Robot City, but discovers a plot by its new leader Phineas T. Ratchet and his mother to forcibly upgrade the city's populace and eradicate struggling robots, known as \"outmodes\". Development of the film began in 2000, following a failed attempt by Wedge and children's author William Joyce to adapt Joyce's 1993 children's book Santa Calls. They instead decided to create an original story based on robots. The project was approved by executive producer Chris Meledandri in 2001 and production began the next year. Robots premiered at the Mann Village Theatre in Westwood, Los Angeles, on March 6, 2005, and was released in the United States on March 11 by 20th Century Fox. The film was praised by critics for its humor, animation, and performances, while its story and characters were deemed somewhat formulaic.[4] The film was commercially successful, grossing $262.5 million worldwide against a $75\u201380 million budget. A sequel was discussed but never produced due to the studio shifting focus to its flagship franchise, Ice Age.[5] In a world of robots, Rodney Copperbottom, son of Herb and Lydia Copperbottom, is an aspiring young inventor from the city of Rivet Town. He idolizes Bigweld, a famous inventor and philanthropist whose company, Bigweld Industries, employs other inventors and provides poor robots with spare parts. Rodney develops a small, flying robot named Wonderbot to assist his father, who works as a dishwasher in a restaurant. When Herb's boss, Mr. Gunk, confronts them, however, Wonderbot malfunctions and wreaks havoc in the kitchen. To help Herb pay for the damages, Rodney travels to Robot City, hoping to present Wonderbot to Bigweld Industries. However, upon his arrival, Rodney is ejected from Bigweld Industries by its new CEO, Phineas T. Ratchet. In Bigweld's absence, Ratchet has discontinued production of spare parts and inventions for the poor outmoded robots, prioritizing expensive \"upgrades\". Meanwhile, Ratchet's mother Madame Gasket runs the Chop Shop, a facility that recycles scrap metal, including that of deceased or outmoded robots, into ingots for upgrades. Rodney befriends Fender Pinwheeler, a ne'er-do-well who introduces him to a group of outmoded robots known as the \"Rusties\". Rodney and his new friends help to fix outmodes throughout the neighborhood, but they are eventually unable to cope with the demand due to the spare part shortage. Hoping to enlist Bigweld's help, Rodney and Fender infiltrate the Bigweld Ball, but Ratchet announces that Bigweld will not attend. An enraged Rodney confronts Ratchet, who orders his security team to eliminate him. Cappy, an executive opposed to Ratchet, rescues Rodney and Fender. While Fender is captured by the Chop Shop, he discovers their plan to scrap all outmoded robots with new machines designed to destroy them. Rodney and Cappy fly to Bigweld's mansion, where he lives as a recluse and reveals that Ratchet's greed led to his resignation and refuses to help them. A distraught Rodney calls his parents, but Herb inspires him to fight for his dreams. Fender escapes the Chop Shop and exposes Ratchet's plot. Rodney rallies the Rusties, and Bigweld, reinvigorated by Rodney's spirit, joins them to stop Ratchet. Rodney and his friends return to Bigweld Industries where Ratchet attempts to dispose of Bigweld, who ends up being rolled into the Chop Shop. Rodney upgrades the Rusties and leads them in a battle against Ratchet, Gasket, and their army. Gasket is eventually flung into the incinerator and destroyed, and Ratchet is stripped of his upgrades and left chained with his father. Retaking control of Bigweld Industries, Bigweld holds a public ceremony in Rivet Town, where he nominates Rodney as his new second-in-command and eventual successor. Rodney provides Herb with new replacement parts and a flugelhorn-like instrument to fulfill his life-long dream of being a musician. Herb leads the townspeople in a rendition of \"Get Up Offa That Thing\". Initially, Chris Wedge and William Joyce wanted to make a film adaptation of Joyce's 1993 book Santa Calls. After a failed animation test in 2000 (which resulted in 20th Century Fox Animation declining to make the film), Wedge and Joyce decided to instead develop an original story about a world of robots. In 2001, the duo pitched the concept to 20th Century Fox Animation president Chris Meledandri, as a visual idea. Although not initially impressed, Meledandri agreed to greenlight the film and served as its executive producer.[11] The film began production in 2002, shortly after Ice Age was released. Wedge reunited with the crew from his first film, including Carlos Saldanha as the co-director. In June 2003, the film was announced by Fox at the American Museum of Natural History's IMAX theater. This announcement confirmed the entire cast and slated the film for its 2005 release.[12] Robots was originally scheduled for a 2004 release,[13] but the release date was changed to 2005. The film had its world premiere on March 6, 2005, in Westwood, Los Angeles,[6][7] and it was released theatrically on March 11, 2005. The film was the first to feature the new trailer for Star Wars: Episode III \u2013 Revenge of the Sith;[14] it was reported that Star Wars fans went to see the movie just to see the trailer and hear the voice of Ewan McGregor, who also played Obi-Wan Kenobi in the Star Wars prequel trilogy, as Rodney Copperbottom. The film also featured the exclusive trailer for Blue Sky's next film Ice Age: The Meltdown, then called Ice Age 2.[15] Robots was digitally re-mastered into IMAX format (IMAX DMR) and released in select IMAX theaters around the world. It was the first 20th Century Fox film that was released on the same day on IMAX and conventional 35mm screens. It was also the first IMAX DMR film released in the spring season, and the second IMAX DMR film distributed by Fox.[16] The film was released on DVD and VHS in both fullscreen and widescreen on September 27, 2005.[17] The DVD release was accompanied by an original short animated film based on Robots, titled Aunt Fanny's Tour of Booty.[18][19] The short is a prequel to the film, as it takes place during Fender's arrival in Robot City. In the short, Aunt Fanny gives a tour of the Robot City Train Station to a motley collection of robots, including Fender Pinwheeler, Zinc, Tammy, Hacky and an Old Lady-Bot.[18][19] The film was released in high definition on Blu-ray on March 22, 2011.[20] The Asian Blu-ray release of Robots includes Aunt Fanny's Tour of Booty, but it is not included on either the US nor European Blu-ray releases (possibly due to a request from the Office of Film and Literature Classification (OFLC) to remove the short from the Australian DVD release, for they gave the short a PG rating).[citation needed] The film was released on March 11, 2005, in the United States and Canada and grossed $36 million in 3,776 theaters in its opening weekend, ranking #1 at the box office.[21] It grossed a total of $260.7 million worldwide: $128.2 million in the United States and Canada, and $132.5 million in other territories.[2] On Rotten Tomatoes, the film has an approval rating of 64% based on 184 reviews, with an average rating of 6.6/10. The site's consensus reads: \"Robots delights on a visual level, but the story feels like it came off an assembly line.\"[22] Metacritic gives the film a weighted average score of 64 out of 100 based on 33 reviews, indicating \"generally favorable reviews\".[23] Audiences surveyed by CinemaScore gave the film an \"A\" on a scale of A+ to F.[24] Roger Ebert of the Chicago Sun-Times gave the film three and a half stars out of four, stating that \"this is a movie that is a joy to behold entirely apart from what it is about. It looks happy, and, more to the point, it looks harmonious.\"[25] Caroline Westbrook of Empire magazine gave the film a three stars out of five, and said, \"Kids will love it and their adult companions will be warmly entertained\u2014but it's far from a computer-animated classic.\"[26] Rob Mackie of The Guardian gave the film three stars out of five, saying that it \"skillfully combines adult and kids' comedy. But for all the imaginative splendours and a sharp script, Robots is never quite as distinctive as its predecessor, Ice Age.\"[27] Common Sense Media gave the film four stars out of five, calling it an \"endearing 'follow your dreams' story with plenty of laughs\".[28] Robots won an ASCAP award in the category of top box-office films. The movie received two Annie Award nominations (Outstanding Character Design in a Feature Production and Outstanding Production Design in an Animated Feature Production; both for William Joyce and Steve Martino for the latter) and two Kids' Choice Award nominations (Favorite Animated Movie and Favorite Voice From an Animated Movie for Robin Williams's performance as Fender). Robots was also nominated for a Teen Choice Award (Choice Movie: Animated/Computer Generated) and a Visual Effects Society Award.[citation needed] The film is recognized by American Film Institute in these lists: Robots: Original Motion Picture Score was composed by John Powell, conducted by Pete Anthony, performed by the Hollywood Studio Symphony and released on March 15, 2005, by Var\u00e8se Sarabande and Fox Music.[30][31] A video game based on the film was released on February 24, 2005, for the Game Boy Advance, GameCube, Nintendo DS, PlayStation 2, Xbox and Windows. It was developed by Eurocom for home consoles and Windows, and by Griptonite Games for the Game Boy Advance and Nintendo DS. It was published by Vivendi Universal Games. The game received mixed to average reviews from critics.[32][33][34] Following the release of Robots, both Wedge and Joyce have expressed interest in making a sequel.[35][36] In light of the Release the Snyder Cut movement and the closure of Blue Sky Studios, a movement to release a director's cut of Robots gained traction in 2022.[37] A proposed director's cut was first mentioned on the film's original DVD audio commentary with Wedge and Joyce, in which Wedge said that there would be alternate takes in certain scenes, and that Cappy would have been more fleshed out.[38]",
      "ground_truth_chunk_ids": [
        "11_random_chunk1"
      ],
      "source_ids": [
        "S211"
      ],
      "category": "factual",
      "id": 19
    },
    {
      "question": "What is Richelieu, Kentucky?",
      "ground_truth": "Richelieu is an unincorporated community in Logan County and Butler County, Kentucky, United States.[1] Richelieu is located near Logan County's northeastern boundary with Butler County along Kentucky Route 1038. It is also located near the tripoint where Logan and Butler County boundaries meet with those of Warren County.[2] This Logan County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information. This Butler County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Richelieu is an unincorporated community in Logan County and Butler County, Kentucky, United States.[1] Richelieu is located near Logan County's northeastern boundary with Butler County along Kentucky Route 1038. It is also located near the tripoint where Logan and Butler County boundaries meet with those of Warren County.[2] This Logan County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information. This Butler County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "52_random_chunk1"
      ],
      "source_ids": [
        "S252"
      ],
      "category": "factual",
      "id": 20
    },
    {
      "question": "What is Barbara Walters?",
      "ground_truth": "Barbara Jill Walters (September 25, 1929 \u2013 December 30, 2022) was an American broadcast journalist and television personality.[1][2] Known for her interviewing ability and popularity with viewers, she appeared as a host of numerous television programs, including Today, the ABC Evening News, 20/20, and The View. Walters was a working journalist from 1951 until her retirement in 2014.[3][4][5] Walters was inducted into the Television Hall of Fame in 1989, received a Lifetime Achievement Award from the NATAS in 2000 and a star on the Hollywood Walk of Fame in 2007. Walters began her career at WNBT-TV (NBC's flagship station in New York) in 1953 as writer-producer of a news-and-information program aimed at the juvenile audience, Ask the Camera, hosted by Sandy Becker. She joined the staff of the network's Today show in the early 1960s as a writer and segment producer of women's-interest stories. Her popularity with viewers led to her receiving more airtime, and in 1974 she became co-host of the program, the first woman to hold such a position on an American news program.[6][7][8] During 1976, she continued to be a pioneer for women in broadcasting while becoming the first American female co-anchor of a network evening news program, alongside Harry Reasoner on the ABC Evening News. Walters was a correspondent, producer and co-host on the ABC news magazine 20/20 from 1979 to 2004. She became known for an annual special aired on ABC, Barbara Walters' 10 Most Fascinating People. During her career, Walters interviewed every sitting U.S. president and first lady from Richard and Pat Nixon to Barack and Michelle Obama.[9][10] She also interviewed both Donald Trump and Joe Biden, although not when either was president. She also gained acclaim and notoriety for interviewing subjects such as Fidel Castro, Anwar Sadat, Menachem Begin, Katharine Hepburn, Sean Connery,",
      "expected_answer": "Barbara Jill Walters (September 25, 1929\u00a0\u2013 December 30, 2022) was an American broadcast journalist and television personality.[1][2] Known for her interviewing ability and popularity with viewers, she appeared as a host of numerous television programs, including Today, the ABC Evening News, 20/20, and The View. Walters was a working journalist from 1951 until her retirement in 2014.[3][4][5] Walters was inducted into the Television Hall of Fame in 1989, received a Lifetime Achievement Award from the NATAS in 2000 and a star on the Hollywood Walk of Fame in 2007. Walters began her career at WNBT-TV (NBC's flagship station in New York) in 1953 as writer-producer of a news-and-information program aimed at the juvenile audience, Ask the Camera, hosted by Sandy Becker. She joined the staff of the network's Today show in the early 1960s as a writer and segment producer of women's-interest stories. Her popularity with viewers led to her receiving more airtime, and in 1974 she became co-host of the program, the first woman to hold such a position on an American news program.[6][7][8] During 1976, she continued to be a pioneer for women in broadcasting while becoming the first American female co-anchor of a network evening news program, alongside Harry Reasoner on the ABC Evening News. Walters was a correspondent, producer and co-host on the ABC news magazine 20/20 from 1979 to 2004. She became known for an annual special aired on ABC, Barbara Walters' 10 Most Fascinating People. During her career, Walters interviewed every sitting U.S. president and first lady from Richard and Pat Nixon to Barack and Michelle Obama.[9][10] She also interviewed both Donald Trump and Joe Biden, although not when either was president. She also gained acclaim and notoriety for interviewing subjects such as Fidel Castro, Anwar Sadat, Menachem Begin, Katharine Hepburn, Sean Connery, Monica Lewinsky, Hugo Ch\u00e1vez, Vladimir Putin,[11] Shah Mohammad Reza Pahlavi, Jiang Zemin, Saddam Hussein, and Bashar al-Assad.[12] Walters created, produced, and co-hosted the ABC daytime talk show The View; she appeared on the program from 1997 until 2014.[13] Later she continued to host several special reports for 20/20 as well as documentary series for Investigation Discovery. Her final on-air appearance for ABC News was in 2015.[14][15][16][17][18] Walters last publicly appeared in 2016. Barbara Jill Walters was born on September 25, 1929,[19][a] in Boston, a daughter of Dena (n\u00e9e Seletsky) and Lou Walters (born Louis Abraham Warmwater);[21][22] her parents were children of Russian Jewish immigrants.[23][24] Her paternal grandfather, Abraham Isaac Waremwasser, was born in the Polish city of \u0141\u00f3d\u017a and emigrated to England where he changed his surname to Warmwater.[25] Walters' father was born in London in 1898 and moved to New York City with his father and two brothers on August 28, 1909. His mother and four sisters arrived there the following year.[26] During Walters' childhood, her father managed the Latin Quarter nightclub in Boston, which was owned in partnership with E.\u00a0M. Loew. In 1942, her father opened the club's famous New York location. He also worked as a Broadway producer and produced the Ziegfeld Follies of 1943;[27][28] he was also the entertainment director for the Tropicana Resort and Casino in Las Vegas. He imported the Folies Berg\u00e8re stage show from Paris to the resort's main showroom.[29] Walters' older brother, Burton, was 14 months old when he died of pneumonia.[30][31] Her elder sister, Jacqueline, was born with mental disabilities and died of ovarian cancer in 1985.[32] According to Walters, her father made and lost several fortunes throughout his life in show business. He was a booking agent, and (unlike her uncles in the shoe and dress businesses) his job was not very stable. During the good times, she recalled her father taking her to the rehearsals of the nightclub shows he directed and produced. The actresses and dancers would make a huge fuss over her and twirl her around until she was dizzy, after which she said her father would take her out to get hot dogs.[33] Walters said that being surrounded by celebrities when she was young kept her from being \"in awe\" of them.[34] When she was a young woman, her father lost his night clubs and the family's penthouse on Central Park West. As Walters recalled, \"He had a breakdown. He went down to live in our house in Florida, and then the government took the house, and they took the car, and they took the furniture. [...] My mother should have married the way her friends did, to a man who was a doctor or who was in the dress business.\"[35] During her childhood in Miami Beach, she briefly lived with the mobster Bill Dwyer.[36] Walters attended Lawrence School, a public school in Brookline, Massachusetts; she left halfway through fifth grade when her father moved the family to Miami Beach in 1939.[37] She continued attending public school in Miami Beach.[38] After her father moved the family to New York City, she spent eighth grade at the private Ethical Culture Fieldston School,[39] after which the family moved back to Miami Beach.[40] She returned to New York City after tenth grade and attended Birch Wathen School, another private school.[41][42][43] In 1951, she earned a Bachelor of Arts in English from Sarah Lawrence College in Yonkers, New York.[44] Walters was employed for about a year at a small advertising agency in New York City and began working at the NBC network's flagship station WNBT-TV (now WNBC), doing publicity and writing press releases. In 1953 she produced a 15-minute children's program, Ask the Camera, which was directed by Roone Arledge. She also started producing for TV host Igor Cassini (Cholly Knickerbocker), but left the network after Cassini pressured her to marry him and started a fistfight with the man she was interested in. She went to WPIX to produce the Eloise McElhone Show, which was canceled in 1954.[45] She became a writer on The Morning Show at CBS in 1955.[46] After a few years working at Tex McCrary Inc. as a publicist and as a writer at Redbook magazine, Walters joined NBC's The Today Show as a writer and researcher in 1961.[47] She moved up, becoming the show's regular \"Today Girl,\" handling lighter assignments and the weather. In her autobiography, she described this era before the Women's Movement as a time when it was believed that nobody would take a woman seriously reporting \"hard news.\" Previous \"Today Girls\" (whom Walters called \"tea pourers\") included Florence Henderson, Helen O'Connell, Estelle Parsons, and Lee Meriwether.[48][49] Within a year, she had become a reporter-at-large, developing, writing, and editing her own reports and interviews.[50] One very well-received film segment was \"A Day in the Life of a Nun.\" Another was about the daily life of a Playboy Bunny.[51] Beginning in 1971, Walters hosted her own local NBC affiliate show, Not for Women Only, which ran in the mornings after The Today Show.[52][53] Walters had a great relationship with host Hugh Downs for years. When Frank McGee was named host in 1971, he refused to do joint interviews with Walters unless he was given the first three questions.[54] She was not named co-host of the show until McGee's death in 1974 when NBC officially designated Walters as the program's first female co-host.[55] She became the first female co-host of an American news program.[56] Walters signed a five-year, $5-million contract with ABC, establishing her as the highest-paid news anchor, either male or female.[9] She and Harry Reasoner co-anchored the ABC Evening News from 1976 to 1978, making her the first American female network news anchor.[56] Reasoner had a difficult relationship with Walters because he disliked having a co-anchor, even though he worked with former CBS colleague Howard K. Smith nightly on ABC for several years. Walters said that the tension between the two was because Reasoner did not want to work with a co-anchor and also because he was unhappy at ABC, not because he disliked Walters personally.[57] In 1981, five years after the start of their short-lived ABC partnership and well after Reasoner returned to CBS News, Walters and her former co-anchor had a memorable (and cordial) 20/20 interview on the occasion of Reasoner's new book release.[58] In 1979, Walters reunited with former The Today Show host Downs as a correspondent on the ABC newsmagazine 20/20. She became Downs' co-host in 1984, and remained with the program until she retired as co-host in 2004.[59] Throughout her career at ABC, Walters appeared on ABC news specials as a commentator, including presidential inaugurations and the coverage of the September 11 attacks. She was also chosen to be the moderator for the third and final debate between candidates Jimmy Carter and Gerald Ford, held on the campus of the College of William and Mary at Phi Beta Kappa Memorial Hall in Williamsburg, Virginia, during the 1976 presidential election.[60] In 1984, she moderated a presidential debate which was held at the Dana Center for the Humanities at Saint Anselm College in Goffstown, New Hampshire.[61] Walters was known for \"personality journalism\"[62] and her \"scoop\" interviews.[63] In 1976, she first aired her highly rated, occasional, primetime Barbara Walters Specials interview program. Her first guests included a joint appearance by President-elect Jimmy Carter and Rosalynn Carter, and a separate interview with singer-actress Barbra Streisand.[64] In November 1977, she landed the first joint interview with Egyptian president Anwar Al Sadat and Israeli prime minister Menachem Begin, while they were working out the terms of the eventual Egypt\u2013Israel peace treaty.[65][66] According to The New York Times, when she competed with Walter Cronkite to interview both world leaders, at the end of Cronkite's interview, he is heard saying: \"Did Barbara get anything I didn't get?\"[67] Walters had sit-down interviews with world leaders, including the Shah of Iran, Mohammad Reza Pahlavi, and his wife, the Empress Farah Pahlavi;[68] Russia's Boris Yeltsin and Vladimir Putin;[69] China's Jiang Zemin; the UK's Margaret Thatcher;[70] Cuba's Fidel Castro,[71] as well as India's Indira Gandhi,[72] Czechoslovakia's V\u00e1clav Havel,[73] Libya's Muammar al-Gaddafi,[74] King Hussein of Jordan,[75] King Abdullah of Saudi Arabia,[76] Venezuelan President Hugo Ch\u00e1vez,[77] Iraq's Saddam Hussein and many others. Walters interviewed other influential people including pop icon Michael Jackson, Katharine Hepburn, Vogue editor Anna Wintour,[78] and Laurence Olivier in 1980.[79] Walters considered Robert Smithdas, a deaf-blind man who spent his life improving the lives of other individuals who are deaf-blind, as her most inspirational interviewee.[80] Walters was widely lampooned for asking actress Katharine Hepburn, \"If you were a tree, what kind would you be?\" On the last 20/20 television episode in which she appears, Walters showed a video of the Hepburn interview, showing the actress saying that she felt like a strong tree in her old age. Walters followed up with the question, \"What kind of a tree?\", and Hepburn responded \"an oak\" because they do not get Dutch elm disease.[81] According to Walters, for years Hepburn refused her requests for an interview. When Hepburn finally agreed she said she wanted to meet Walters first. Walters walked affably, while Hepburn was at the top of the stairs and said, \"You're late. Have you brought me chocolates?\"[82] Walters had not, but said she never showed up without them from then on. They had several other meetings later, mostly in Hepburn's living room where she would give Walters her opinions. These included that careers and marriage did not mix, as well as her feeling that combining children with careers was out of the question. Walters said Hepburn's opinions stuck with her so much, she could repeat them almost verbatim from that point onward.[33] Her television special about Cuban leader Fidel Castro aired on ABC-TV on June 9, 1977. Although the footage of her two days of interviewing Castro in Cuba showed his personality, in part, as freewheeling, charming, and humorous,[83] she pointedly said to him, \"You allow no dissent. Your newspapers, radio, television, motion pictures are under state control.\" To this, he replied, \"Barbara, our concept of freedom of the press is not yours. If you asked us if a newspaper could appear here against socialism, I can say honestly no, it cannot appear. It would not be allowed by the party, the government, or the people. In that sense we do not have the freedom of the press that you possess in the U.S. and we are very satisfied about that.\"[84] She concluded the broadcast saying, \"What we disagreed on most profoundly is the meaning of freedom\u2014and that is what truly separates us.\"[85] At the time, Walters did not mention that she had seen New York Yankees owner George Steinbrenner, pitcher Whitey Ford, and several coaches in Cuba who were there to assist Cuban ballplayers.[86] On March 3, 1999, her interview with Monica Lewinsky was seen by a record 74 million viewers, the highest rating ever for a news program.[87] Walters asked Lewinsky, \"What will you tell your children when you have them?\" Lewinsky replied, \"Mommy made a big mistake,\" at which point Walters brought the program to a dramatic conclusion, turning to the camera and saying, \"that is the understatement of the year.\"[88] Barbara Walters' 10 Most Fascinating People was aired annually starting in 1993.[89] In 2000, she quizzed pop star Ricky Martin about his sexuality years before he publicly came out. The singer later said that \"he felt violated\".[90] In 2010, Walters said that she regretted having pushed him on the issue.[91] Walters was a co-host of the daytime talk show The View; for 25 years she was also a co-executive producer of BarWall Productions alongside her business partner, Bill Geddie. Geddie and Walters were co-creators of the company. The View premiered on August 11, 1997.[92] In the original opening credits Walters said the show is a forum for women of \"different generations, backgrounds, and views.\"[93] \"Be careful what you wish for...\" was part of the opening credits of its second season. On The View, she won Daytime Emmy Awards for Best Talk Show in 2003 and Best Talk Show Host (with longtime host Joy Behar, moderator Whoopi Goldberg, Elisabeth Hasselbeck, and Sherri Shepherd) in 2009.[94] Walters retired from being a co-host on May 15, 2014.[95] She returned as a guest co-host on an intermittent basis in 2014 and 2015 even in retirement. After leaving her role as 20/20 co-host in 2004, Walters remained a part-time contributor of special programming and interviews for ABC News until 2016. On March 7, 2010, Walters announced that she would no longer hold Oscar interviews but would still work for ABC and on The View.[96] On March 28, 2013, numerous media outlets reported that Walters would retire in May 2014 and that she would make the announcement on the show four days later.[97][98][99][100] However, on the April 1 episode, she neither confirmed nor denied the retirement rumors; she said \"if and when I might have an announcement to make, I will do it on this program, I promise, and the paparazzi guys\u2014you will be the last to know\".[101][102] Six weeks later Walters confirmed that she would be retiring from television hosting and interviewing, as originally reported; she made the official announcement on the May 13, 2013, episode of The View. She also announced that she would continue as the show's executive producer for as long as it \"is on the air\".[103][104][105][106][107] On June 10, 2014, it was announced she would come out of retirement for a special 20/20 interview with Peter Rodger, the father of the perpetrator of the 2014 Isla Vista killings, Elliot Rodger.[15][108] In 2015, Walters hosted special 20/20 episodes featuring interviews with Mary Kay Letourneau[14] and Donald and Melania Trump.[16] In 2015, Walters hosted the documentary series American Scandals on Investigation Discovery.[17] Walters continued to host 10 Most Fascinating People on ABC in 2014[109] and 2015.[18] Her last on-air interview was with Donald Trump for ABC News in December 2015,[110] and she made her final public appearance in 2016.[111][112] On January 1, 2023, ABC ran a special called \"Our Barbara\" and a 20/20 senior producer noted, \"For a number of years we kept her office just as is (after 2016), the papers came every day. Outside of her office she still retained her office extension.\"[113] Walters was married four times to three different men. Her first husband was Robert Henry Katz, a business executive and former Navy lieutenant. They married on June 20, 1955, at the Plaza Hotel in New York City.[1][114] The marriage was reportedly annulled after eleven months,[115] in 1957.[116] Her second husband was Lee Guber, a theatrical producer and theater owner. They married on December 8, 1963, and divorced in 1976. After Walters had three miscarriages, the couple adopted a baby girl named Jacqueline Dena Guber (born in 1968 and adopted the same year; she was named for Walters' sister).[117] Walters' third husband was Merv Adelson who at the time was the CEO of Lorimar Television. They married in 1981 and divorced in 1984. They remarried in 1986 and divorced for the second time in 1992.[118] Walters dated lawyer[119][120] Roy Cohn in college; he said that he proposed marriage to Walters the night before her wedding to Lee Guber, but Walters denied this happened.[30] She explained her lifelong devotion to Cohn as gratitude for his help in her adoption of her daughter, Jacqueline.[121] In her autobiography, Walters says she also felt grateful to Cohn because of legal assistance he had provided to her father. According to Walters, her father was the subject of an arrest warrant for \"failure to appear\" after he failed to show up for a New York court date because the family was in Las Vegas; Cohn was able to have the charge dismissed.[122] Walters testified as a character witness at Cohn's 1986 disbarment trial.[123] Walters dated future U.S. Federal Reserve chairman Alan Greenspan in the 1970s[124] and was linked romantically to United States Senator John Warner in the 1990s.[125] In Walters's autobiography Audition, she wrote that she had an affair in the 1970s with Edward Brooke, then a married United States Senator from Massachusetts. It is not clear whether Walters also was married at the time. Walters said they ended the affair to protect their careers from scandal.[126] In 2007, she dated Pulitzer Prize\u2013winning gerontologist Robert Neil Butler.[127] Walters was a close friend of Tom Brokaw, Woody Allen, Joan Rivers, and Fox News head Roger Ailes.[128] In 2013, Walters said she regretted not having more children.[129][130] In May 2010, Walters said she would be having an open-heart operation to replace a faulty aortic valve. She had known that she was suffering from aortic stenosis, even though she was symptom-free. Four days after the operation, Walters' spokeswoman, Cindi Berger, said that the procedure to fix the faulty heart valve \"went well, and the doctors are very pleased with the outcome\".[131] Walters returned to The View and her Sirius XM satellite show, Here's Barbara, in September 2010.[132][133] Walters retired permanently from both shows four years later.[134] Walters died at her home in Manhattan on December 30, 2022, at age 93. She had been suffering from dementia in her later years.[135][9][136] Her last words were, \"No regrets \u2013 I had a great life.\" Those words were etched into her gravestone at Lakeside Memorial Park, a Jewish cemetery in Doral, Florida, where Walters is buried next to her sister and parents.[137][138][139] Her parents had spent their final years in Florida, residing in Miami, with her father a resident of Douglas Gardens Jewish Home for the Aged.[140][141] Walters began her career when the prevalent view among television executives (all of whom were male) was that women reporting news about war, politics and other important matters would be taken lightly by viewers.[65] Her success is credited with creating career opportunities for future female network anchors, including Jane Pauley, Katie Couric and Diane Sawyer. Walters often got her interviewees to speak about their perspectives and share anecdotes.[9][65] She was inducted into the Television Hall of Fame in 1989.[47] On June 15, 2007, Walters received a star on the Hollywood Walk of Fame.[142] She won Daytime and Prime Time Emmy Awards, a Women in Film Lucy Award,[143] and a GLAAD Excellence in Media award.[144] In 2008, Walters was honored with the Disney Legends award, given to those who made an outstanding contribution to The Walt Disney Company, which owns the network ABC. That same year, she received the Lifetime Achievement Award from the New York Women's Agenda. On September 21, 2009, Walters was honored with a Lifetime Achievement Award at the 30th Annual News and Documentary Emmy Awards at New York City's Lincoln Center.[145] Walters' status as a prominent figure in popular culture was reflected by Gilda Radner's gentle parody of her as \"Baba Wawa\" on Saturday Night Live in the late 1970s,[146] featuring Walters' distinctive speech including her rounded \"R's\". Her name appeared in the January 23, 1995, New York Times Monday Crossword Puzzle.[147] There was a social media campaign to have Barbara in Times Square to ring in 2020 by saying her iconic introduction to 20/20. As she was unable to appear CNN hired Cheri Oteri who was famous for her impression of Walters on Saturday Night Live to appear in her place.[148] Daytime Emmy Awards NAACP Image Award Women in Film Crystal + Lucy Awards Golden Plate Award of the American Academy of Achievement[164] In the late 1960s, Walters wrote a magazine article, \"How to Talk to Practically Anyone About Practically Anything\", which drew upon the kinds of things people said to her, which were often mistakes.[167] Shortly after the article appeared, she received a letter from Doubleday expressing interest in expanding it into a book. Walters felt that it would help \"tongue-tied, socially awkward people\u2014the many people who worry that they can't think of the right thing to say to start a conversation.\"[167] Walters published the book How to Talk with Practically Anybody About Practically Anything in 1970, with the assistance of ghostwriter June Callwood.[168] To Walters's great surprise, the book was a success. As of 2008, it had gone through eight printings, sold hundreds of thousands of copies worldwide, and had been translated into at least six languages.[167] Walters published her autobiography, Audition: A Memoir, in 2008.[169]",
      "ground_truth_chunk_ids": [
        "244_random_chunk1"
      ],
      "source_ids": [
        "S444"
      ],
      "category": "factual",
      "id": 21
    },
    {
      "question": "What is List of Cantharis species?",
      "ground_truth": "This is a list of 95 species in Cantharis, a genus of soldier beetles in the family Cantharidae.[1][2][3] Data sources: i = ITIS,[1] c = Catalogue of Life,[4] g = GBIF,[2] b = Bugguide.net,[3] f = Fauna Europaea[5]",
      "expected_answer": "This is a list of 95 species in Cantharis, a genus of soldier beetles in the family Cantharidae.[1][2][3] Data sources: i = ITIS,[1] c = Catalogue of Life,[4] g = GBIF,[2] b = Bugguide.net,[3] f = Fauna Europaea[5]",
      "ground_truth_chunk_ids": [
        "121_random_chunk1"
      ],
      "source_ids": [
        "S321"
      ],
      "category": "factual",
      "id": 22
    },
    {
      "question": "What is The Gray Nun of Belgium?",
      "ground_truth": "The Gray Nun of Belgium was a 1915 film announced for release on the Alliance Program by Dramatic Feature Films, Frank Joslyn Baum's short-lived successor to The Oz Film Manufacturing Company. Despite the advertising in Motion Picture News announcing its release date, Katharine Rogers, in L. Frank Baum: Creator of Oz, believes that Alliance found the film inferior and refused to distribute it. The exhibition copy, which may have been a work print, may have been the only copy ever struck. Baum himself thought that exchanges and exhibitors dismissed the film \"rather arbitrarily\" based on the Oz Company name.[1] In the film, Betty Pierce played a Mother Superior who aided Allied soldiers during World War I. Cathrine Countiss played the title role. It also starred David Proctor, Mae Wells, Katherine Griffith, Raymond Russell, Robert Dunbar, Harry Clements, and James Spencer.[2] Wells and Russell were prominent actors in the Oz Company, having played roles such as Mombi and Dr. Pipt, respectively. This article about a film on World War I is a stub. You can help Wikipedia by adding missing information. This article relating to \"The Wonderful Wizard of Oz\" or one of its derivative works is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "The Gray Nun of Belgium was a 1915 film announced for release on the Alliance Program by Dramatic Feature Films, Frank Joslyn Baum's short-lived successor to The Oz Film Manufacturing Company. Despite the advertising in Motion Picture News announcing its release date, Katharine Rogers, in L. Frank Baum:  Creator of Oz, believes that Alliance found the film inferior and refused to distribute it.  The exhibition copy, which may have been a work print, may have been the only copy ever struck.  Baum himself thought that exchanges and exhibitors dismissed the film \"rather arbitrarily\" based on the Oz Company name.[1] In the film, Betty Pierce played a Mother Superior who aided Allied soldiers during World War I. Cathrine Countiss played the title role.  It also starred David Proctor, Mae Wells, Katherine Griffith, Raymond Russell, Robert Dunbar, Harry Clements, and James Spencer.[2] Wells and Russell were prominent actors in the Oz Company, having played roles such as Mombi and Dr. Pipt, respectively. This article about a film on World War I is a stub. You can help Wikipedia by adding missing information. This article relating to \"The Wonderful Wizard of Oz\" or one of its derivative works is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "90_random_chunk1"
      ],
      "source_ids": [
        "S290"
      ],
      "category": "factual",
      "id": 23
    },
    {
      "question": "What is Artur Serobyan?",
      "ground_truth": "Artur Serobyan (Armenian: \u0531\u0580\u0569\u0578\u0582\u0580 \u054d\u0565\u0580\u0578\u0562\u0575\u0561\u0576, born 2 July 2003) is an Armenian professional footballer who plays as a winger or forward for Armenian Premier League club Ararat-Armenia, and the Armenia national team.[citation needed] Artur Serobyan is a graduate of the Yerevan Football Academy. At the age of 17, he already played in the senior team of Ararat-Armenia.[1] In August 2021, his loan transfer to BKMA was announced.[2] On 1 September 2023, his loan transfer to Portugal's top league club Casa Pia was announced until the end of the 2023\u201324 season.[3] On 10 January 2024, Casa Pia terminated Serobyan's loan and he returned to Ararat-Armenia.[4] On 14 January 2025, Serobyan signed for Sheriff Tiraspol on loan.[5][6] On 16 August 2025, Sheriff announced that Serobyan had left the club after his loan deal had been ended by mutual agreement.[7] Serobyan made his senior international debut for Armenia national team on 24 March 2022 in a friendly game against Montenegro, where he came in off the bench and played the last 20 minutes.[8] Sheriff Tiraspol Ararat-Armenia Individual",
      "expected_answer": "Artur Serobyan (Armenian: \u0531\u0580\u0569\u0578\u0582\u0580 \u054d\u0565\u0580\u0578\u0562\u0575\u0561\u0576, born 2 July 2003) is an Armenian professional footballer who plays as a winger or forward for Armenian Premier League club Ararat-Armenia, and the Armenia national team.[citation needed] Artur Serobyan is a graduate of the Yerevan Football Academy. At the age of 17, he already played in the senior team of Ararat-Armenia.[1] In August 2021, his loan transfer to BKMA was announced.[2] On 1 September 2023, his loan transfer to Portugal's top league club Casa Pia was announced until the end of the 2023\u201324 season.[3] On 10 January 2024, Casa Pia terminated Serobyan's loan and he returned to Ararat-Armenia.[4] On 14 January 2025, Serobyan signed for Sheriff Tiraspol on loan.[5][6] On 16 August 2025, Sheriff announced that Serobyan had left the club after his loan deal had been ended by mutual agreement.[7] Serobyan made his senior international debut for Armenia national team on 24 March 2022 in a friendly game against Montenegro, where he came in off the bench and played the last 20 minutes.[8] Sheriff Tiraspol Ararat-Armenia Individual",
      "ground_truth_chunk_ids": [
        "190_random_chunk1"
      ],
      "source_ids": [
        "S390"
      ],
      "category": "factual",
      "id": 24
    },
    {
      "question": "What is Climate change?",
      "ground_truth": "Present-day climate change includes both global warming\u2014the ongoing increase in global average temperature\u2014and its wider effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The modern-day rise in global temperatures is driven by human activities, especially fossil fuel (coal, oil and natural gas) burning since the Industrial Revolution.[3][4] Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases.[5] These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Earth's atmosphere now has roughly 50% more carbon dioxide, the main gas driving global warming, than it did at the end of the pre-industrial era, reaching levels not seen for millions of years.[6] Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common.[7] Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline.[8] Higher temperatures are also causing more intense storms, droughts, and other weather extremes.[9] Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct.[10] Even if efforts to minimize future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise.[11] Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss.[12] Human migration and conflict can also be a result.[13] The World Health Organization calls climate change one of the biggest threats to global health in the 21st century.[14] Societies and ecosystems will experience more severe risks without action to limit warming.[15] Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some",
      "expected_answer": "Present-day climate change includes both global warming\u2014the ongoing increase in global average temperature\u2014and its wider effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The modern-day rise in global temperatures is driven by human activities, especially fossil fuel (coal, oil and natural gas) burning since the Industrial Revolution.[3][4] Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases.[5] These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Earth's atmosphere now has roughly 50% more carbon dioxide, the main gas driving global warming, than it did at the end of the pre-industrial era, reaching levels not seen for millions of years.[6] Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common.[7] Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline.[8] Higher temperatures are also causing more intense storms, droughts, and other weather extremes.[9] Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct.[10] Even if efforts to minimize future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise.[11] Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss.[12] Human migration and conflict can also be a result.[13] The World Health Organization calls climate change one of the biggest threats to global health in the 21st century.[14] Societies and ecosystems will experience more severe risks without action to limit warming.[15] Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some limits to adaptation have already been reached.[16] Poorer communities are responsible for a small share of global emissions, yet have the least ability to adapt and are most vulnerable to climate change.[17][18] Many climate change impacts have been observed in the first decades of the 21st century, with 2024 the warmest on record at +1.60\u00a0\u00b0C (2.88\u00a0\u00b0F) since regular tracking began in 1850.[20][21] Additional warming will increase these impacts and can trigger tipping points, such as melting all of the Greenland ice sheet.[22] Under the 2015 Paris Agreement, nations collectively agreed to keep warming \"well under 2\u00a0\u00b0C\". However, with pledges made under the Agreement, global warming would still reach about 2.8\u00a0\u00b0C (5.0\u00a0\u00b0F) by the end of the century.[23] There is widespread support for climate action worldwide,[24][25] and most countries aim to stop emitting carbon dioxide.[26] Fossil fuels can be phased out by stopping subsidising them, conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind, solar, hydro, and nuclear power.[27] Cleanly generated electricity can replace fossil fuels for powering transportation, heating buildings, and running industrial processes.[28] Carbon can also be removed from the atmosphere, for instance by increasing forest cover and farming with methods that store carbon in soil.[29][30][31] Before the 1980s, it was unclear whether the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution. Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time.[32] In the 1980s, the terms global warming and climate change became more common, often being used interchangeably.[33][34][35] Scientifically, global warming refers only to increased global average surface temperature, while climate change describes both global warming and its effects on Earth's climate system, such as precipitation changes.[32] Climate change can also be used more broadly to include changes to the climate that have happened throughout Earth's history as result of natural processes.[36] The term anthropogenic climate change is sometimes used to describe climate change resulting from human activities.[37] Global warming\u2014used as early as 1975[38]\u2014became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate.[39] Since the 2000s, usage of climate change has increased.[40] Various scientists, politicians and media may use the terms climate crisis or climate emergency to talk about climate change, and may use the term global heating instead of global warming.[41][42] Over the last few million years the climate cycled through ice ages. One of the hotter periods was the Last Interglacial, around 125,000 years ago, where temperatures were between 0.5\u00a0\u00b0C and 1.5\u00a0\u00b0C warmer than before the start of global warming.[45] This period saw sea levels 5 to 10 metres higher than today. The most recent glacial maximum 20,000 years ago was some 5\u20137\u00a0\u00b0C colder. This period has sea levels that were over 125 metres (410\u00a0ft) lower than today.[46] Temperatures stabilized in the current interglacial period beginning 11,700 years ago.[47] This period also saw the start of agriculture.[48] Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age, did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions.[49][50] Climate information for that period comes from climate proxies, such as trees and ice cores.[51][52] Around 1850 thermometer records began to provide global coverage.[55]\nBetween the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling from sulfur dioxide emissions. Sulfur dioxide causes acid rain, but it also produces sulfate aerosols in the atmosphere, which reflect sunlight and cause global dimming. After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature.[56][57][58] Ongoing changes in climate have had no precedent for several thousand years.[59] Multiple datasets all show worldwide increases in surface temperature,[60] at a rate of around 0.2\u00a0\u00b0C per decade.[61] The 2014\u20132023 decade warmed to an average 1.19\u00a0\u00b0C [1.06\u20131.30\u00a0\u00b0C] compared to the pre-industrial baseline (1850\u20131900).[62] Not every single year was warmer than the last: internal climate variability processes can make any year 0.2\u00a0\u00b0C warmer or colder than the average.[63] From 1998 to 2013, negative phases of two such processes, Pacific Decadal Oscillation (PDO)[64] and Atlantic Multidecadal Oscillation (AMO)[65] caused a short slower period of warming called the \"global warming hiatus\".[66] After the \"hiatus\", the opposite occurred, with 2024 well above the recent average at more than +1.5\u00a0\u00b0C.[67] This is why the temperature change is defined in terms of a 20-year average, which reduces the noise of hot and cold years and decadal climate patterns, and detects the long-term signal.[68]:\u200a5\u200a[69] A wide range of other observations reinforce the evidence of warming.[70][71] The upper atmosphere is cooling, because greenhouse gases are trapping heat near the Earth's surface, and so less heat is radiating into space.[72] Warming reduces average snow cover and forces the retreat of glaciers. At the same time, warming also causes greater evaporation from the oceans, leading to more atmospheric humidity, more and heavier precipitation.[73][74] Plants are flowering earlier in spring, and thousands of animal species have been permanently moving to cooler areas.[75] Different regions of the world warm at different rates. The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global average surface temperature.[76] This is because oceans lose more heat by evaporation and oceans can store a lot of heat.[77] The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has been stored in the ocean.[78][79] The rest has heated the atmosphere, melted ice, and warmed the continents.[80] The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat.[81] Local black carbon deposits on snow and ice also contribute to Arctic warming.[82] Arctic surface temperatures are increasing between three and four times faster than in the rest of the world.[83][84] Melting of ice sheets near the poles weakens both the Atlantic and the Antarctic limb of thermohaline circulation, which further changes the distribution of heat and precipitation around the globe.[85][86][87][88] The World Meteorological Organization estimates there is almost a 50% chance of the five-year average global temperature exceeding +1.5\u00a0\u00b0C between 2024 and 2028.[91] The IPCC expects the 20-year average to exceed +1.5\u00a0\u00b0C in the early 2030s.[92] The IPCC Sixth Assessment Report (2021) included projections that by 2100 global warming is very likely to reach 1.0\u20131.8\u00a0\u00b0C under a scenario with very low emissions of greenhouse gases, 2.1\u20133.5\u00a0\u00b0C under an intermediate emissions scenario,\nor 3.3\u20135.7\u00a0\u00b0C under a very high emissions scenario.[93] The warming will continue past 2100 in the intermediate and high emission scenarios,[94][95] with future projections of global surface temperatures by year 2300 being similar to millions of years ago.[96] The remaining carbon budget for staying beneath certain temperature increases is determined by modelling the carbon cycle and climate sensitivity to greenhouse gases.[97] According to UNEP, global warming can be kept below 2.0\u00a0\u00b0C with a 50% chance if emissions after 2023 do not exceed 900 gigatonnes of CO2. This carbon budget corresponds to around 16 years of current emissions.[98] The climate system experiences various cycles on its own which can last for years, decades or even centuries. For example, El Ni\u00f1o events cause short-term spikes in surface temperature while La Ni\u00f1a events cause short term cooling.[99] Their relative frequency can affect global temperature trends on a decadal timescale.[100] Other changes are caused by an imbalance of energy from external forcings.[101] Examples of these include changes in the concentrations of greenhouse gases, solar luminosity, volcanic eruptions, and variations in the Earth's orbit around the Sun.[102] To determine the human contribution to climate change, unique \"fingerprints\" for all potential causes are developed and compared with both observed patterns and known internal climate variability.[103] For example, solar forcing\u2014whose fingerprint involves warming the entire atmosphere\u2014is ruled out because only the lower atmosphere has warmed.[104] Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes in albedo, are less impactful.[105] Greenhouse gases are transparent to sunlight, and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat, and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time.[106] While water vapour (\u224850%) and clouds (\u224825%) are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature and are therefore mostly considered to be feedbacks that change climate sensitivity. On the other hand, concentrations of gases such as CO2 (\u224820%), tropospheric ozone,[107] CFCs and nitrous oxide are added or removed independently from temperature, and are therefore considered to be external forcings that change global temperatures.[108] Before the Industrial Revolution, naturally occurring amounts of greenhouse gases caused the air near the surface to be about 33\u00a0\u00b0C warmer than it would have been in their absence.[109][110] Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal, oil, and natural gas),[111] has increased the amount of greenhouse gases in the atmosphere. In 2022, the concentrations of CO2 and methane had increased by about 50% and 164%, respectively, since 1750.[112] These CO2 levels are higher than they have been at any time during the last 14 million years.[113] Concentrations of methane are far higher than they were over the last 800,000 years.[114] Global human-caused greenhouse gas emissions in 2019 were equivalent to 59\u00a0billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane, 4% was nitrous oxide, and 2% was fluorinated gases.[115] CO2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity.[5] Additional CO2 emissions come from deforestation and industrial processes, which include the CO2 released by the chemical reactions for making cement, steel, aluminium, and fertilizer.[116][117][118][119] Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, and coal mining, as well as oil and gas extraction.[120][121] Nitrous oxide emissions largely come from the microbial decomposition of fertilizer.[122][123] While methane only lasts in the atmosphere for an average of 12 years,[124] CO2 lasts much longer. The Earth's surface absorbs CO2 as part of the carbon cycle. While plants on land and in the ocean absorb most excess emissions of CO2 every year, that CO2 is returned to the atmosphere when biological matter is digested, burns, or decays.[125] Land-surface carbon sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions.[126] The ocean has absorbed 20 to 30% of emitted CO2 over the last two decades.[127] CO2 is only removed from the atmosphere for the long term when it is stored in the Earth's crust, which is a process that can take millions of years to complete.[125] Around 30% of Earth's land area is largely unusable for humans (glaciers, deserts, etc.), 26% is forests, 10% is shrubland and 34% is agricultural land.[129] Deforestation is the main land use change contributor to global warming,[130] as the destroyed trees release CO2, and are not replaced by new trees, removing that carbon sink.[131] Between 2001 and 2018, 27% of deforestation was from permanent clearing to enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%.[132] Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink.[133] Local vegetation cover impacts how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns.[134] In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler.[133] At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains.[134] Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect.[135] Air pollution, in the form of aerosols, affects the climate on a large scale.[136] Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming,[137] and is primarily attributed to sulfate aerosols produced by the combustion of fossil fuels with heavy sulfur concentrations like coal and bunker fuel.[58] Smaller contributions come from black carbon (from combustion of fossil fuels and biomass), and from dust.[138][139][140] Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much.[141][58] Aerosols also have indirect effects on the Earth's energy budget. Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets.[142] They also reduce the growth of raindrops, which makes clouds more reflective to incoming sunlight.[143] Indirect effects of aerosols are the largest uncertainty in radiative forcing.[144] While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise.[145] Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2\u00a0\u00b0C by 2050.[146] The effect of decreasing sulfur content of fuel oil for ships since 2020[147] is estimated to cause an additional 0.05\u00a0\u00b0C increase in global mean temperature by 2050.[148] As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system.[144] Solar irradiance has been measured directly by satellites,[151] and indirect measurements are available from the early 1600s onwards.[144] Since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (the troposphere).[152] The upper atmosphere (the stratosphere) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling.[104]\nThis is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere.[153] Explosive volcanic eruptions can release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapour into the atmosphere, which adds to greenhouse gases and increases temperatures.[154] These impacts on temperature only last for several years, because both water vapour and volcanic material have low persistence in the atmosphere.[155] volcanic CO2 emissions are more persistent, but they are equivalent to less than 1% of current human-caused CO2 emissions.[156] Volcanic activity still represents the single largest natural impact (forcing) on temperature in the industrial era. Yet, like the other natural forcings, it has had negligible impacts on global temperature trends since the Industrial Revolution.[155] The climate system's response to an initial forcing is shaped by feedbacks, which either amplify or dampen the change. Self-reinforcing or positive feedbacks increase the response, while balancing or negative feedbacks reduce it.[158] The main reinforcing feedbacks are the water-vapour feedback, the ice\u2013albedo feedback, and the net cloud feedback.[159][160] The primary balancing mechanism is radiative cooling, as Earth's surface gives off more heat to space in response to rising temperature.[161] In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO2 on plant growth.[162] Feedbacks are expected to trend in a positive direction as greenhouse gas emissions continue, raising climate sensitivity.[163] These feedback processes alter the pace of global warming. For instance, warmer air can hold more moisture in the form of water vapour, which is itself a potent greenhouse gas.[159] Warmer air can also make clouds higher and thinner, and therefore more insulating, increasing climate warming.[164] The reduction of snow cover and sea ice in the Arctic is another major feedback, this reduces the reflectivity of the Earth's surface in the region and accelerates Arctic warming.[165][166] This additional warming also contributes to permafrost thawing, which releases methane and CO2 into the atmosphere.[167] Around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans.[168] This fraction is not static and if future CO2 emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%.[169] This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plants when they are warmer.[170][171] The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes in thermohaline circulation and phytoplankton distribution.[172][173][86] Uncertainty over feedbacks, particularly cloud cover,[174] is the major reason why different climate models project different magnitudes of warming for a given amount of emissions.[175] A climate model is a representation of the physical, chemical and biological processes that affect the climate system.[176] Models include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing.[177] Models are used to estimate the degree of warming future emissions will cause when accounting for the strength of climate feedbacks.[178][179] Models also predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere.[180] The physical realism of models is tested by examining their ability to simulate current or past climates.[181] Past models have underestimated the rate of Arctic shrinkage[182] and underestimated the rate of precipitation increase.[183] Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations.[184] The 2017 United States-published National Climate Assessment notes that \"climate models may still be underestimating or missing relevant feedback processes\".[185] Additionally, climate models may be unable to adequately predict short-term regional climatic shifts.[186] A subset of climate models add societal factors to a physical climate model. These models simulate how population, economic growth, and energy use affect\u2014and interact with\u2014the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change.[187][188] Depending on the socioeconomic scenario and the mitigation scenario, models produce atmospheric CO2 concentrations that range widely between 380 and 1400 ppm.[189] The environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations.[191] Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency.[192] Extremely wet or dry events within the monsoon period have increased in India and East Asia.[193] Monsoonal precipitation over the Northern Hemisphere has increased since 1980.[194] The rainfall rate and intensity of hurricanes and typhoons is likely increasing,[195] and the geographic range likely expanding poleward in response to climate warming.[196] The frequency of tropical cyclones has not increased as a result of climate change.[197] Global sea level is rising as a consequence of thermal expansion and the melting of glaciers and ice sheets. Sea level rise has increased over time, reaching 4.8\u00a0cm per decade between 2014 and 2023.[199] Over the 21st century, the IPCC projects 32\u201362\u00a0cm of sea level rise under a low emission scenario, 44\u201376\u00a0cm under an intermediate one and 65\u2013101\u00a0cm under a very high emission scenario.[200] Marine ice sheet instability processes in Antarctica may add substantially to these values,[201] including the possibility of a 2-meter sea level rise by 2100 under high emissions.[202] Climate change has led to decades of shrinking and thinning of the Arctic sea ice.[203] While ice-free summers are expected to be rare at 1.5\u00a0\u00b0C degrees of warming, they are set to occur once every three to ten years at a warming level of 2\u00a0\u00b0C.[204] Higher atmospheric CO2 concentrations cause more CO2 to dissolve in the oceans, which is making them more acidic.[205] Because oxygen is less soluble in warmer water,[206] its concentrations in the ocean are decreasing, and dead zones are expanding.[207] Greater degrees of global warming increase the risk of passing through 'tipping points'\u2014thresholds beyond which certain major impacts can no longer be avoided even if temperatures return to their previous state.[210][211] For instance, the Greenland ice sheet is already melting, but if global warming reaches levels between 1.7\u00a0\u00b0C and 2.3\u00a0\u00b0C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5\u00a0\u00b0C or less, it will still lose a lot more ice than if the warming was never allowed to reach the threshold in the first place.[212] While the ice sheets would melt over millennia, other tipping points would occur faster and give societies less time to respond. The collapse of major ocean currents like the Atlantic meridional overturning circulation (AMOC), and irreversible damage to key ecosystems like the Amazon rainforest and coral reefs can unfold in a matter of decades.[209] The collapse of the AMOC would be a severe climate catastrophe, resulting in a cooling of the Northern Hemisphere.[213] The long-term effects of climate change on oceans include further ice melt, ocean warming, sea level rise, ocean acidification and ocean deoxygenation.[214] The timescale of long-term impacts are centuries to millennia due to CO2's long atmospheric lifetime.[215] The result is an estimated total sea level rise of 2.3 metres per degree Celsius (4.2\u00a0ft/\u00b0F) after 2000 years.[216] Oceanic CO2 uptake is slow enough that ocean acidification will also continue for hundreds to thousands of years.[217] Deep oceans (below 2,000 metres (6,600\u00a0ft)) are also already committed to losing over 10% of their dissolved oxygen by the warming which occurred to date.[218] Further, the West Antarctic ice sheet appears committed to practically irreversible melting, which would increase the sea levels by at least 3.3\u00a0m (10\u00a0ft 10\u00a0in) over approximately 2000 years.[209][219][220] Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes.[221] For instance, the range of hundreds of North American birds has shifted northward at an average rate of 1.5\u00a0km/year over the past 55 years.[222] Higher atmospheric CO2 levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear.[223] A related phenomenon driven by climate change is woody plant encroachment, affecting up to 500 million hectares globally.[224] Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics.[225] The size and speed of global warming is making abrupt changes in ecosystems more likely.[226] Overall, it is expected that climate change will result in the extinction of many species.[227] The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land.[228] Just as on land, heat waves in the ocean occur more frequently due to climate change, harming a wide range of organisms such as corals, kelp, and seabirds.[229] Ocean acidification makes it harder for marine calcifying organisms such as mussels, barnacles and corals to produce shells and skeletons; and heatwaves have bleached coral reefs.[230] Harmful algal blooms enhanced by climate change and eutrophication lower oxygen levels, disrupt food webs and cause great loss of marine life.[231] Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts.[232] Plants have come under increased stress from damage by insects.[233] The effects of climate change are impacting humans everywhere in the world.[239] Impacts can be observed on all continents and ocean regions,[240] with low-latitude, less developed areas facing the greatest risk.[241] Continued warming has potentially \"severe, pervasive and irreversible impacts\" for people and ecosystems.[242] The risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries.[243] The World Health Organization calls climate change one of the biggest threats to global health in the 21st century.[14] Scientists have warned about the irreversible harms it poses.[244] Extreme weather events affect public health, and food and water security.[245][246][247] Temperature extremes lead to increased illness and death.[245][246] Climate change increases the intensity and frequency of extreme weather events.[246][247] It can affect transmission of infectious diseases, such as dengue fever and malaria.[244][245] According to the World Economic Forum, 14.5\u00a0million more deaths are expected due to climate change by 2050.[248] 30% of the global population currently live in areas where extreme heat and humidity are already associated with excess deaths.[249][250] By 2100, 50% to 75% of the global population would live in such areas.[249][251] While total crop yields have been increasing in the past 50 years due to agricultural improvements, climate change has already decreased the rate of yield growth.[247] Fisheries have been negatively affected in multiple regions.[247] While agricultural productivity has been positively affected in some high latitude areas, mid- and low-latitude areas have been negatively affected.[247] According to the World Economic Forum, an increase in drought in certain regions could cause 3.2\u00a0million deaths from malnutrition by 2050 and stunting in children.[252] With 2\u00a0\u00b0C warming, global livestock headcounts could decline by 7\u201310% by 2050, as less animal feed will be available.[253] If the emissions continue to increase for the rest of century, then over 9 million climate-related deaths would occur annually by 2100.[254] Economic damages due to climate change may be severe and there is a chance of disastrous consequences.[255] Severe impacts are expected in South-East Asia and sub-Saharan Africa, where most of the local inhabitants are dependent upon natural and agricultural resources.[256][257] Heat stress can prevent outdoor labourers from working. If warming reaches 4\u00a0\u00b0C then labour capacity in those regions could be reduced by 30 to 50%.[258] The World Bank estimates that between 2016 and 2030, climate change could drive over 120 million people into extreme poverty without adaptation.[259] Inequalities based on wealth and social status have worsened due to climate change.[260] Major difficulties in mitigating, adapting to, and recovering from climate shocks are faced by marginalized people who have less control over resources.[261][256] Indigenous people, who are subsistent on their land and ecosystems, will face endangerment to their wellness and lifestyles due to climate change.[262] An expert elicitation concluded that the role of climate change in armed conflict has been small compared to factors such as socio-economic inequality and state capabilities.[263] While women are not inherently more at risk from climate change and shocks, limits on women's resources and discriminatory gender norms constrain their adaptive capacity and resilience.[264] For example, women's work burdens, including hours worked in agriculture, tend to decline less than men's during climate shocks such as heat stress.[264] Low-lying islands and coastal communities are threatened by sea level rise, which makes urban flooding more common. Sometimes, land is permanently lost to the sea.[265] This could lead to statelessness for people in island nations, such as the Maldives and Tuvalu.[266] In some regions, the rise in temperature and humidity may be too severe for humans to adapt to.[267] With worst-case climate change, models project that areas almost one-third of humanity live in might become Sahara-like uninhabitable and extremely hot climates.[268] These factors can drive climate or environmental migration, within and between countries.[269] More people are expected to be displaced because of sea level rise, extreme weather and conflict from increased competition over natural resources. Climate change may also increase vulnerability, leading to \"trapped populations\" who are not able to move due to a lack of resources.[270] Climate change can be mitigated by reducing the rate at which greenhouse gases are emitted into the atmosphere, and by increasing the rate at which carbon dioxide is removed from the atmosphere.[276] To limit global warming to less than 2\u00a0\u00b0C global greenhouse gas emissions need to be net-zero by 2070.[277] This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry.[278] The United Nations Environment Programme estimates that countries need to triple their pledges under the Paris Agreement within the next decade to limit global warming to 2\u00a0\u00b0C.[279] With pledges made under the Paris Agreement as of 2024, there would be a 66% chance that global warming is kept under 2.8\u00a0\u00b0C by the end of the century (range: 1.9\u20133.7\u00a0\u00b0C, depending on exact implementation and technological progress). When only considering current policies, this raises to 3.1\u00a0\u00b0C.[280] Globally, limiting warming to 2\u00a0\u00b0C may result in higher economic benefits than economic costs.[281] Although there is no single pathway to limit global warming to 2\u00a0\u00b0C,[282] most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions.[283] To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in agriculture and forestry,[284] such as preventing deforestation and restoring natural ecosystems by reforestation.[285] Other approaches to mitigating climate change have a higher level of risk. Scenarios that limit global warming to 1.5\u00a0\u00b0C typically project the large-scale use of carbon dioxide removal methods over the 21st century.[286] There are concerns, though, about over-reliance on these technologies, and environmental impacts.[287] Solar radiation modification (SRM) is a proposal for reducing global warming by reflecting some sunlight away from Earth and back into space. Because it does not reduce greenhouse gas concentrations, it would not address ocean acidification[288] and is not considered mitigation.[289] SRM should be considered only as a supplement to mitigation, not a replacement for it,[290]  due to risks such as rapid warming if it were abruptly stopped and not restarted.[291] The most-studied approach is stratospheric aerosol injection.[292] SRM could reduce global warming and some of its impacts, though imperfectly.[293] It poses environmental risks, such as changes to rainfall patterns,[294] as well as political challenges, such as who would decide whether to use it.[292] Renewable energy is key to limiting climate change.[296] For decades, fossil fuels have accounted for roughly 80% of the world's energy use.[297] The remaining share has been split between nuclear power and renewables (including hydropower, bioenergy, wind and solar power and geothermal energy).[298] Fossil fuel use is expected to peak in absolute terms prior to 2030 and then to decline, with coal use experiencing the sharpest reductions.[299] Renewables represented 86% of all new electricity generation installed in 2023.[300] Other forms of clean energy, such as nuclear and hydropower, currently have a larger share of the energy supply. However, their future growth forecasts appear limited in comparison.[301] While solar panels and onshore wind are now among the cheapest forms of adding new power generation capacity in many locations,[302] green energy policies are needed to achieve a rapid transition from fossil fuels to renewables.[303] To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. Investment in coal would be eliminated and coal use nearly phased out by 2050.[304][305] Electricity generated from renewable sources would also need to become the main energy source for heating and transport.[306] Transport can switch away from internal combustion engine vehicles and towards electric vehicles, public transit, and active transport (cycling and walking).[307][308] For shipping and flying, low-carbon fuels would reduce emissions.[307] Heating could be increasingly decarbonized with technologies like heat pumps.[309] There are obstacles to the continued rapid growth of clean energy, including renewables.[310] Wind and solar produce energy intermittently and with seasonal variability. Traditionally, hydro dams with reservoirs and fossil fuel power plants have been used when variable energy production is low. Going forward, battery storage can be expanded, energy demand and supply can be matched, and long-distance transmission can smooth variability of renewable outputs.[296] Bioenergy is often not carbon-neutral and may have negative consequences for food security.[311] The growth of nuclear power is constrained by controversy around radioactive waste, nuclear weapon proliferation, and accidents.[312][313] Hydropower growth is limited by the fact that the best sites have been developed, and new projects are confronting increased social and environmental concerns.[314] Low-carbon energy improves human health by minimizing climate change as well as reducing air pollution deaths,[315] which were estimated at 7 million annually in 2016.[316] Meeting the Paris Agreement goals that limit warming to a 2\u00a0\u00b0C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5\u00a0\u00b0C could save millions and simultaneously increase energy security and reduce poverty.[317] Improving air quality also has economic benefits which may be larger than mitigation costs.[318] Reducing energy demand is another major aspect of reducing emissions.[319] If less energy is needed, there is more flexibility for clean energy development. It also makes it easier to manage the electricity grid, and minimizes carbon-intensive infrastructure development.[320] Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy.[321] Several COVID-19 related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain.[322] Strategies to reduce energy demand vary by sector. In the transport sector, passengers and freight can switch to more efficient travel modes, such as buses and trains, or use electric vehicles.[323] Industrial strategies to reduce energy demand include improving heating systems and motors, designing less energy-intensive products, and increasing product lifetimes.[324] In the building sector the focus is on better design of new buildings, and higher levels of energy efficiency in retrofitting.[325] The use of technologies like heat pumps can also increase building energy efficiency.[326] Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand.[327] A set of actions could reduce agriculture and forestry-based emissions by two-thirds from 2010 levels. These include reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production.[328] On the demand side, a key component of reducing emissions is shifting people towards plant-based diets.[329] Eliminating the production of livestock for meat and dairy would eliminate about 3/4ths of all emissions from agriculture and other land use.[330] Livestock also occupy 37% of ice-free land area on Earth and consume feed from the 12% of land area used for crops, driving deforestation and land degradation.[331] Steel and cement production are responsible for about 13% of industrial CO2 emissions. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production, so that reducing CO2 emissions requires research into alternative chemistries.[332] Where energy production or CO2-intensive heavy industries continue to produce waste CO2, technology can sometimes be used to capture and store most of the gas instead of releasing it to the atmosphere.[333] This technology, carbon capture and storage (CCS), could have a critical but limited role in reducing emissions.[333] It is relatively expensive[334] and has been deployed only to an extent that removes around 0.1% of annual greenhouse gas emissions.[333] Natural carbon sinks can be enhanced to sequester significantly larger amounts of CO2 beyond naturally occurring levels.[335] Reforestation and afforestation (planting forests where there were none before) are among the most mature sequestration techniques, although the latter raises food security concerns.[336] Farmers can promote sequestration of carbon in soils through practices such as use of winter cover crops, reducing the intensity and frequency of tillage, and using compost and manure as soil amendments.[337] Forest and landscape restoration yields many benefits for the climate, including greenhouse gas emissions sequestration and reduction.[133] Restoration/recreation of coastal wetlands, prairie plots and seagrass meadows increases the uptake of carbon into organic matter.[338][339] When carbon is sequestered in soils and in organic matter such as trees, there is a risk of the carbon being re-released into the atmosphere later through changes in land use, fire, or other changes in ecosystems.[340] The use of bioenergy in conjunction with carbon capture and storage (BECCS) can result in net negative emissions as CO2 is drawn from the atmosphere.[341] It remains highly uncertain whether carbon dioxide removal techniques will be able to play a large role in limiting warming to 1.5\u00a0\u00b0C. Policy decisions that rely on carbon dioxide removal increase the risk of global warming rising beyond international goals.[342] Adaptation is \"the process of adjustment to current or expected changes in climate and its effects\".[343]:\u200a5\u200a Without additional mitigation, adaptation cannot avert the risk of \"severe, widespread and irreversible\" impacts.[344] More severe climate change requires more transformative adaptation, which can be prohibitively expensive.[345] The capacity and potential for humans to adapt is unevenly distributed across different regions and populations, and developing countries generally have less.[346] The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basic sanitation and electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance.[347] Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding, and building flood controls. If that fails, managed retreat may be needed.[348] There are economic barriers for tackling dangerous heat impact. Avoiding strenuous work or having air conditioning is not possible for everybody.[349] In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control, and genetic improvements for increased tolerance to a changing climate.[350] Insurance allows for risk-sharing, but is often difficult to get for people on lower incomes.[351] Education, migration and early warning systems can reduce climate vulnerability.[352] Planting mangroves or encouraging other coastal vegetation can buffer storms.[353][354] Ecosystems adapt to climate change, a process that can be supported by human intervention. By increasing connectivity between ecosystems, species can migrate to more favourable climate conditions. Species can also be introduced to areas acquiring a favourable climate. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt via ecosystem-based adaptation. For instance, restoration of natural fire regimes makes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest acts as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts.[355] There are synergies but also trade-offs between adaptation and mitigation.[356] An example for synergy is increased food productivity, which has large benefits for both adaptation and mitigation.[357] An example of a trade-off is that increased use of air conditioning allows people to better cope with heat, but increases energy demand. Another trade-off example is that more compact urban development may reduce emissions from transport and construction, but may also increase the urban heat island effect, exposing people to heat-related health risks.[358] Countries that are most vulnerable to climate change have typically been responsible for a small share of global emissions. This raises questions about justice and fairness.[359] Limiting global warming makes it much easier to achieve the UN's Sustainable Development Goals, such as eradicating poverty and reducing inequalities. The connection is recognized in Sustainable Development Goal 13 which is to \"take urgent action to combat climate change and its impacts\".[360] The goals on food, clean water and ecosystem protection have synergies with climate mitigation.[361] The geopolitics of climate change is complex. It has often been framed as a free-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from switching to a low-carbon economy themselves. Sometimes mitigation also has localized benefits though. For instance, the benefits of a coal phase-out to public health and local environments exceed the costs in almost all regions.[362] Furthermore, net importers of fossil fuels win economically from switching to clean energy, causing net exporters to face stranded assets: fossil fuels they cannot sell.[363] A wide range of policies, regulations, and laws are being used to reduce emissions. As of 2019, carbon pricing covers about 20% of global greenhouse gas emissions.[364] Carbon can be priced with carbon taxes and emissions trading systems.[365] Direct global fossil fuel subsidies reached $319\u00a0billion in 2017, and $5.2\u00a0trillion when indirect costs such as air pollution are priced in.[366] Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths.[367] Money saved on fossil subsidies could be used to support the transition to clean energy instead.[368] More direct methods to reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry.[369] Several countries require utilities to increase the share of renewables in power production.[370] An Open Coalition on Compliance Carbon Markets with the aim of creating a global cap and trade system was established at COP30 (2025). According to some calculations it can increase emissions reduction seven-fold over current policies, deliver $200 billion per year for clean-energy and social programs and even close the gap between current emissions trajectory and the goals of the Paris agreement.[371][372][373] Policy designed through the lens of climate justice tries to address human rights issues and social inequality. According to proponents of climate justice, the costs of climate adaptation should be paid by those most responsible for climate change, while the beneficiaries of payments should be those suffering impacts. One way this can be addressed in practice is to have wealthy nations pay poorer countries to adapt.[374] Oxfam found that in 2023 the wealthiest 10% of people were responsible for 50% of global emissions, while the bottom 50% were responsible for just 8%.[375] Production of emissions is another way to look at responsibility: under that approach, the top 21 fossil fuel companies would owe cumulative climate reparations of $5.4\u00a0trillion over the period 2025\u20132050.[376] To achieve a just transition, people working in the fossil fuel sector would also need other jobs, and their communities would need investments.[377] Nearly all countries in the world are parties to the 1994 United Nations Framework Convention on Climate Change (UNFCCC).[379] The goal of the UNFCCC is to prevent dangerous human interference with the climate system.[380] As stated in the convention, this requires that greenhouse gas concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained.[381] The UNFCCC does not itself restrict emissions but rather provides a framework for protocols that do. Global emissions have risen since the UNFCCC was signed.[382] Its yearly conferences are the stage of global negotiations.[383] The 1997 Kyoto Protocol extended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions.[384] During the negotiations, the G77 (representing developing countries) pushed for a mandate requiring developed countries to \"[take] the lead\" in reducing their emissions,[385] since developed countries contributed most to the accumulation of greenhouse gases in the atmosphere. Per-capita emissions were also still relatively low in developing countries and developing countries would need to emit more to meet their development needs.[386] The 2009 Copenhagen Accord has been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77.[387] Associated parties aimed to limit the global temperature rise to below 2\u00a0\u00b0C.[388]  The accord set the goal of sending $100\u00a0billion per year to developing countries for mitigation and adaptation by 2020, and proposed the founding of the Green Climate Fund.[389] As of 2020[update], only 83.3\u00a0billion were delivered. Only in 2023 the target is expected to be achieved.[390] In 2015 all UN countries negotiated the Paris Agreement, which aims to keep global warming well below 2.0\u00a0\u00b0C and contains an aspirational goal of keeping warming under 1.5\u00a0\u00b0C.[391] The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, a set of procedures was made binding. Countries have to regularly set ever more ambitious goals and reevaluate these goals every five years.[392] The Paris Agreement restated that developing countries must be financially supported.[393] As of March 2025[update], 194 states and the European Union have acceded to or ratified the agreement.[394] The 1987 Montreal Protocol, an international agreement to phase out production of ozone-depleting gases, has had benefits for climate change mitigation.[395] Several ozone-depleting gases like chlorofluorocarbons are powerful greenhouse gases, so banning their production and usage may have avoided a temperature rise of 0.5\u00a0\u00b0C\u20131.0\u00a0\u00b0C,[396] as well as additional warming by preventing damage to vegetation from ultraviolet radiation.[397] It is estimated that the agreement has been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so.[398] The most recent amendment to the Montreal Protocol, the 2016 Kigali Amendment, committed to reducing the emissions of hydrofluorocarbons, which served as a replacement for banned ozone-depleting gases and are also potent greenhouse gases.[399] Should countries comply with the amendment, a warming of 0.3\u00a0\u00b0C\u20130.5\u00a0\u00b0C is estimated to be avoided.[400] In 2019, the United Kingdom parliament became the first national government to declare a climate emergency.[402] Other countries and jurisdictions followed suit.[403] That same year, the European Parliament declared a \"climate and environmental emergency\".[404] The European Commission presented its European Green Deal with the goal of making the EU carbon-neutral by 2050.[405] In 2021, the European Commission released its \"Fit for 55\" legislation package, which contains guidelines for the car industry; all new cars on the European market must be zero-emission vehicles from 2035.[406] Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon-neutral by 2050, and China by 2060.[407] While India has strong incentives for renewables, it also plans a significant expansion of coal in the country.[408] Vietnam is among very few coal-dependent, fast-developing countries that pledged to phase out unabated coal power by the 2040s or as soon as possible thereafter.[409] As of 2021, based on information from 48 national climate plans, which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5\u00a0\u00b0C or 2\u00a0\u00b0C, respectively.[410] Public debate about climate change has been strongly affected by climate change denial and misinformation, which first emerged in the United States and has since spread to other countries, particularly Canada and Australia. It originated from fossil fuel companies, industry groups, conservative think tanks, and contrarian scientists.[412] Like the tobacco industry, the main strategy of these groups has been to manufacture doubt about climate-change related scientific data and results.[413] People who hold unwarranted doubt about climate change are sometimes called climate change \"skeptics\", although \"contrarians\" or \"deniers\" are more appropriate terms.[414] There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimize the negative impacts of climate change.[415] Manufacturing uncertainty about the science later developed into a manufactured controversy: creating the belief that there is significant uncertainty about climate change within the scientific community to delay policy changes.[416] Strategies to promote these ideas include criticism of scientific institutions,[417] and questioning the motives of individual scientists.[415] An echo chamber of climate-denying blogs and media has further fomented misunderstanding of climate change.[418] Climate change came to international public attention in the late 1980s.[422] Due to media coverage in the early 1990s, people often confused climate change with other environmental issues like ozone depletion.[423] In popular culture, the climate fiction movie The Day After Tomorrow (2004) and the Al Gore documentary An Inconvenient Truth (2006) focused on climate change.[422] Significant regional, gender, age and political differences exist in both public concern for, and understanding of, climate change. More highly educated people, and in some countries, women and younger people, were more likely to see climate change as a serious threat.[424] College biology textbooks from the 2010s featured less content on climate change compared to those from the preceding decade, with decreasing emphasis on solutions.[425] Partisan gaps also exist in many countries,[426] and countries with high CO2 emissions tend to be less concerned.[427] Views on causes of climate change vary widely between countries.[428] Media coverage linked to protests has had impacts on public sentiment as well as on which aspects of climate change are focused upon.[429] Higher levels of worry are associated with stronger public support for policies that address climate change.[430] Concern has increased over time,[431] and in 2021 a majority of citizens in 30 countries expressed a high level of worry about climate change, or view it as a global emergency.[432] A 2024 survey across 125 countries found that 89% of the global population demanded intensified political action, but systematically underestimated other peoples' willingness to act.[24][25] Climate protests demand that political leaders take action to prevent climate change. They can take the form of public demonstrations, fossil fuel divestment, lawsuits and other activities.[433][434] Prominent demonstrations include the School Strike for Climate. In this initiative, young people across the globe have been protesting since 2018 by skipping school on Fridays, inspired by Swedish activist and then-teenager Greta Thunberg.[435] Mass civil disobedience actions by groups like Extinction Rebellion have protested by disrupting roads and public transport.[436] Litigation is increasingly used as a tool to strengthen climate action from public institutions and companies. Activists also initiate lawsuits which target governments and demand that they take ambitious action or enforce existing laws on climate change.[437] Lawsuits against fossil-fuel companies generally seek compensation for loss and damage.[438] On 23 July 2025, the UN's International Court of Justice issued its advisory opinion, saying explicitly that states must act to stop climate change, and if they fail to accomplish that duty, other states can sue them. This obligation includes implementing their commitments in international agreements they are parties to, such as the 2015 Paris Climate Accord.[439][440][441] Scientists in the 19th century such as Alexander von Humboldt began to foresee the effects of climate change.[443][444][445][446] In the 1820s, Joseph Fourier proposed the greenhouse effect to explain why Earth's temperature was higher than the Sun's energy alone could explain. Earth's atmosphere is transparent to sunlight, so sunlight reaches the surface where it is converted to heat. However, the atmosphere is not transparent to heat radiating from the surface, and captures some of that heat, which in turn warms the planet.[447]\nIn 1856 Eunice Newton Foote demonstrated that the warming effect of the Sun is greater for air with water vapour than for dry air, and that the effect is even greater with carbon dioxide (CO2). In \"Circumstances Affecting the Heat of the Sun's Rays\" she concluded that \"[a]n atmosphere of that gas would give to our earth a high temperature\".[448][449] Starting in 1859,[451] John Tyndall established that nitrogen and oxygen\u2014together totalling 99% of dry air\u2014are transparent to radiated heat. However, water vapour and gases such as methane and carbon dioxide absorb radiated heat and re-radiate that heat into the atmosphere. Tyndall proposed that changes in the concentrations of these gases may have caused climatic changes in the past, including ice ages.[452] Svante Arrhenius noted that water vapour in air continuously varied, but the CO2 concentration in air was influenced by long-term geological processes. Warming from increased CO2 levels would increase the amount of water vapour, amplifying warming in a positive feedback loop. In 1896, he published the first climate model of its kind, projecting that halving CO2 levels could have produced a drop in temperature initiating an ice age. Arrhenius calculated the temperature increase expected from doubling CO2 to be around 5\u20136\u00a0\u00b0C.[453] Other scientists were initially sceptical and believed that the greenhouse effect was saturated so that adding more CO2 would make no difference, and that the climate would be self-regulating.[454] Beginning in 1938, Guy Stewart Callendar published evidence that climate was warming and CO2 levels were rising,[455] but his calculations met the same objections.[454] In the 1950s, Gilbert Plass created a detailed computer model that included different atmospheric layers and the infrared spectrum. This model predicted that increasing CO2 levels would cause warming. Around the same time, Hans Suess found evidence that CO2 levels had been rising, and Roger Revelle showed that the oceans would not absorb the increase. The two scientists subsequently helped Charles Keeling to begin a record of continued increase\u2014the \"Keeling Curve\"[454]\u2014which was part of continued scientific investigation through the 1960s into possible human causation of global warming.[459] Studies such as the National Research Council's 1979 Charney Report supported the accuracy of climate models that forecast significant warming.[460] Human causation of observed global warming and dangers of unmitigated warming were publicly presented in James Hansen's 1988 testimony before a US Senate committee.[461][39] The Intergovernmental Panel on Climate Change (IPCC), set up in 1988 to provide formal advice to the world's governments, spurred interdisciplinary research.[462] As part of the IPCC reports, scientists assess the scientific discussion that takes place in peer-reviewed journal articles.[463] There is a nearly unanimous scientific consensus that the climate is warming and that this is caused by human activities.[4] No scientific body of national or international standing disagrees with this view.[464] As of 2019, agreement in recent literature reached over 99%.[457][4] The 2021 IPCC Assessment Report stated that it is \"unequivocal\" that climate change is caused by humans.[4] Consensus has further developed that action should be taken to protect people against the impacts of climate change. National science academies have called on world leaders to cut global emissions.[465] Extreme event attribution (EEA), also known as attribution science, was developed in the early decades of the 21st century.[466] EEA uses climate models to identify and quantify the role that human-caused climate change plays in the frequency, intensity, duration, and impacts of specific individual extreme weather events.[467][468] Results of attribution studies allow scientists and journalists to make statements such as, \"this weather event was made at least n times more likely by human-caused climate change\" or \"this heatwave was made m degrees hotter than it would have been in a world without global warming\" or \"this event was effectively impossible without climate change\".[469] Greater computing power in the 2000s and conceptual breakthroughs in the early to mid 2010s[470]  enabled attribution science to detect the effects of climate change on some events with high confidence.[466] Scientists use attribution methods and climate simulations that have already been peer reviewed, allowing \"rapid attribution studies\" to be published within a \"news cycle\" time frame after weather events.[470] This article incorporates text from a free content work. Licensed under CC BY-SA 3.0. Text taken from The status of women in agrifood systems \u2013 Overview\u200b, FAO, FAO. Fourth Assessment Report Fifth Assessment report Special Report: Global Warming of 1.5\u00a0\u00b0C Special Report: Climate change and Land Special Report: The Ocean and Cryosphere in a Changing Climate Sixth Assessment Report",
      "ground_truth_chunk_ids": [
        "8_fixed_chunk1"
      ],
      "source_ids": [
        "S008"
      ],
      "category": "factual",
      "id": 25
    },
    {
      "question": "What is Parabothria?",
      "ground_truth": "Parabothria is a genus of parasitic flies in the family Tachinidae. There is one described species in Parabothria, P. punoensis.[1][2] This article related to members of the fly family Tachinidae is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Parabothria is a genus of parasitic flies in the family Tachinidae. There is one described species in Parabothria, P.\u00a0punoensis.[1][2] This article related to members of the fly family Tachinidae is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "263_random_chunk1"
      ],
      "source_ids": [
        "S463"
      ],
      "category": "factual",
      "id": 26
    },
    {
      "question": "What is Ted Scherman?",
      "ground_truth": "Ted Scherman (born October 3, 1966) is a former professional tennis player from the United States. Scherman was born in San Francisco and in 1985 represented the United States in the Junior Davis Cup competition.[1] In the late 1980s he played at UC Berkeley, where he achieved All-American honors in 1987 and 1988.[2] Following his graduation in 1989 he turned professional. A right-handed player, Scherman played in the main draw of the Queensland Open in 1989, beating Grant Connell in the first round, before being eliminated in the second round by Niclas Kroon.[3] Most of his appearances at the top level of the professional tour were in doubles. He made it to 114 in the world in that format and was a semi-finalist in the ATP Tour tournament at Bordeaux in 1991, with \u0122irts Dzelde.[4] A two-time Challenger title winner, he also competed in the main draw of four Grand Slam tournaments.",
      "expected_answer": "Ted Scherman (born October 3, 1966) is a former professional tennis player from the United States. Scherman was born in San Francisco and in 1985 represented the United States in the Junior Davis Cup competition.[1] In the late 1980s he played at UC Berkeley, where he achieved All-American honors in 1987 and 1988.[2] Following his graduation in 1989 he turned professional. A right-handed player, Scherman played in the main draw of the Queensland Open in 1989, beating Grant Connell in the first round, before being eliminated in the second round by Niclas Kroon.[3] Most of his appearances at the top level of the professional tour were in doubles. He made it to 114 in the world in that format and was a semi-finalist in the ATP Tour tournament at Bordeaux in 1991, with \u0122irts Dzelde.[4] A two-time Challenger title winner, he also competed in the main draw of four Grand Slam tournaments.",
      "ground_truth_chunk_ids": [
        "50_random_chunk1"
      ],
      "source_ids": [
        "S250"
      ],
      "category": "factual",
      "id": 27
    },
    {
      "question": "What is Lisa Carlsen (basketball)?",
      "ground_truth": "Lisa Carlsen is an American women's basketball coach and former basketball player. She was recently the women's basketball head coach at Northern Illinois University. She previously served as the women's basketball head coach at Lewis University and the University of Nebraska Omaha.[1] Carlson is from Earling, Iowa.[2] She attended Northwest Missouri State University where she played college basketball and was a named all-conference four times.[1] She was named 1992 Champion NCAA Female Athlete of the Year after her senior year. She also played college softball where she earned all-conference and all-region accolades.[2] She earned a bachelor's degree in 1992 and a master's degree in 1994 from Northwest Missouri State.[1] She played basketball professionally in the Women's Basketball Association for three seasons with the Nebraska Express.[2] Prior to coaching basketball, she was a softball coach at St. Mary (Neb.) from 1994 to 1997 where she was the Midlands Collegiate Athletic Conference Coach of the Year twice.[1] At Wayne State College (Neb.) she was the softball head coach and an assistant volleyball coach from 1997 to 1998. Her overall record as a softball coach was 120\u201351.[2] Her first job as a basketball coach was in 1998 as an assistant at Omaha then in NCAA Division II.[3][1] She was promoted to head coach in 2000 where she remained for four seasons with a record of 36\u201375.[2] She was the associate head coach at Winona State for three years before taking the women's basketball head coaching position at Lewis in 2007.[2] In eight seasons at Lewis she posted an overall record of 148\u201389 with appearances in the NCAA Division II women's basketball tournament and two Great Lakes Valley Conference championships.[4] Her 2015 team went 31\u20133 and made the Division II Elite Eight.[5] They began the season 23\u20130.[6] She was named GLVC Coach of the",
      "expected_answer": "Lisa Carlsen is an American women's basketball coach and former basketball player. She was recently the women's basketball head coach at Northern Illinois University. She previously served as the women's basketball head coach at Lewis University and the University of Nebraska Omaha.[1] Carlson is from Earling, Iowa.[2] She attended Northwest Missouri State University where she played college basketball and was a named all-conference four times.[1] She was named 1992 Champion NCAA Female Athlete of the Year after her senior year. She also played college softball where she earned all-conference and all-region accolades.[2] She earned a bachelor's degree in 1992 and a master's degree in 1994 from Northwest Missouri State.[1] She played basketball professionally in the Women's Basketball Association for three seasons with the Nebraska Express.[2] Prior to coaching basketball, she was a softball coach at St. Mary (Neb.) from 1994 to 1997 where she was the Midlands Collegiate Athletic Conference Coach of the Year twice.[1] At Wayne State College (Neb.) she was the softball head coach and an assistant volleyball coach from 1997 to 1998. Her overall record as a softball coach was 120\u201351.[2] Her first job as a basketball coach was in 1998 as an assistant at Omaha then in NCAA Division II.[3][1] She was promoted to head coach in 2000 where she remained for four seasons with a record of 36\u201375.[2] She was the associate head coach at Winona State for three years before taking the women's basketball head coaching position at Lewis in 2007.[2] In eight seasons at Lewis she posted an overall record of 148\u201389 with appearances in the NCAA Division II women's basketball tournament and two Great Lakes Valley Conference championships.[4] Her 2015 team went 31\u20133 and made the Division II Elite Eight.[5] They began the season 23\u20130.[6] She was named GLVC Coach of the Year and Division II National Coach of the Year.[7] During her tenure with the Flyers the program had a 100 percent graduation rate.[8] On June 30, 2015 she was named the head coach at Northern Illinois.[9] She led them to the 2017 Women's National Invitation Tournament.[10] She resigned from NIU on March 10, 2025 after a 13\u201317 season and a 147\u2013155 record with the Huskies.[11] National champion\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Postseason invitational champion\u00a0\u00a0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Conference regular season champion\u00a0\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Conference regular season and conference tournament champion\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Division regular season champion\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Division regular season and conference tournament champion\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Conference tournament champion She and her ex-husband, Chris, have four children.[1]",
      "ground_truth_chunk_ids": [
        "29_random_chunk1"
      ],
      "source_ids": [
        "S229"
      ],
      "category": "factual",
      "id": 28
    },
    {
      "question": "What is Murder of Patricia Allen?",
      "ground_truth": "On November 13, 1991, Patricia Allen, a 31-year-old lawyer, was murdered by her estranged husband, Colin McGregor, with a crossbow in downtown Ottawa, Ontario, Canada.[1] McGregor was found guilty of first degree murder and sentenced to life in prison. The murder received national media coverage because it was the first instance, in Canada, of spousal homicide using a crossbow, which was a way to get around gun ownership laws and restrictions, and acquire a deadly weapon (though there have been several similar crossbow crimes since then).[2] Patricia Allen, the only daughter of George and Maisie Allen, grew up with two brothers. Allen earned her bachelor's degree in philosophy at university in Ottawa.[3] Allen attended the McGill University Faculty of Law, where she graduated at the top her class in 1988[4] and won the prize for the highest achievement in civil law.[5][3] In 1989 she moved to Ottawa with McGregor, where she worked as a lawyer for Revenue Canada.[6][7] Colin McGregor (born in 1961) was the eldest of three boys of a Montreal business man, owner of a travel agency, and grew up in Westmount. He attended Westmount High School and at Marianopolis College he was valedictorian and student union president.[8][9][3][4] He played junior (U20) rugby with the Westmount Rugby Club and he was a champion debater at McGill University, having won the Princeton University Debate Tournament in 1982.[9][8] The pair met at McGill University while Allen was studying law and McGregor was studying philosophy. After graduation they married and moved to Ottawa.[3][4] Their relationship was described as rocky by family and friends, as they frequently fought and McGregor was perceived as jealous and possessive.[5] By 1990 they bought a home and Allen was working as a lawyer with Revenue Canada.[5][4][10] During this time McGregor bounced between jobs, working as a",
      "expected_answer": "On November 13, 1991, Patricia Allen, a 31-year-old lawyer, was murdered by her estranged husband, Colin McGregor, with a crossbow in downtown Ottawa, Ontario, Canada.[1] McGregor was found guilty of first degree murder and sentenced to life in prison. The murder received national media coverage because it was the first instance, in Canada, of spousal homicide using a crossbow, which was a way to get around gun ownership laws and restrictions, and acquire a deadly weapon (though there have been several similar crossbow crimes since then).[2] Patricia Allen, the only daughter of George and Maisie Allen, grew up with two brothers. Allen earned her bachelor's degree in philosophy at university in Ottawa.[3] Allen attended the McGill University Faculty of Law, where she graduated at the top her class in 1988[4] and won the prize for the highest achievement in civil law.[5][3] In 1989 she moved to Ottawa with McGregor, where she worked as a lawyer for Revenue Canada.[6][7] Colin McGregor (born in 1961) was the eldest of three boys of a Montreal business man, owner of a travel agency, and grew up in Westmount. He attended Westmount High School and at Marianopolis College he was valedictorian and student union president.[8][9][3][4] He played junior (U20) rugby with the Westmount Rugby Club and he was a champion debater at McGill University, having won the Princeton University Debate Tournament in 1982.[9][8] The pair met at McGill University while Allen was studying law and McGregor was studying philosophy. After graduation they married and moved to Ottawa.[3][4] Their relationship was described as rocky by family and friends, as they frequently fought and McGregor was perceived as jealous and possessive.[5] By 1990 they bought a home and Allen was working as a lawyer with Revenue Canada.[5][4][10] During this time McGregor bounced between jobs, working as a reporter in Halifax and Montreal, as a media spokesperson for the Canadian Pharmaceutical Association and in the federal government. In the fall of 1990, McGregor started a Master of Public Administration at Carleton University[3] while Allen supported him financially.[5] In the summer of 1991 he was able to secure a prestigious co-op position as an auditor at the Department of National Defence.[5] Later in 1990, when Allen was promoted to senior policy adviser on the Goods and Services Tax, McGregor (as he told a divorce lawyer) felt a shift in their relationship.[5][11] At that time McGregor started acting bizarrely, complaining about pains in his throat, liver, eyes, pancreas and brain.[5] He was convinced he was dying from a herpes infection, believing that swallowing a cold sore had triggered the dormant virus to spread throughout his body.[5][4] After visiting a number of specialists no physical aliment was discovered; on the contrary, the doctors suggested his complaints were psychological.[5] During this time neighbours heard the couple constantly arguing, often loudly.[5] He was admitted to a psychiatric ward for a week in the spring of 1991, and after his release his condition deteriorated and he stopped working at National Defence.[5][12] After threatening suicide in August 1991, McGregor was readmitted to the psychiatric ward for three weeks.[5][13][3] It was determined that his physical symptoms were the result of stress, and he was quite probably using them to control and manipulate others. Doctors noted McGregor was very angry, full of resentment and hostility towards people.[5] During his stay in the psychiatric ward, Allen told McGregor that she wanted a divorce.[5][11][3] Before the second week of September 1991, McGregor was discharged from the psychiatric ward and a close friend picked him up, with whom he lived for a short period.[5] He would frequently call friends and ramble on topics such as suicide, his recent separation, his ailments and how his situation was unfair.[5] After his release and up until her murder, McGregor harassed Allen incessantly.[3] He called her repeatedly and tried to enter their former matrimonial home without her permission.[3] During this time Allen kept a \"diary of threats\" and had a friend stay with her.[4] Leading up to the murder they met several times to divide their possessions as the divorce neared completion.[5] In October 1991, McGregor managed to enter the house through a bathroom window using a pair of garden shears.[5][12][4] On October 23, 1991, McGregor bought a crossbow and a pack of steel-tipped hunting bolts.[4] In the following weeks he would use the walls of his room as a target practice,[4] filling them with holes and continuing until the bolts were dull. A few days prior to the murder he bought a new set of bolts.[5] On the morning of November 13, 1991, Allen drove to a dentist appointment on Argyle Avenue in downtown Ottawa.[3][7] McGregor had been monitoring her movements and covertly followed her to the dentist's office. After Allen finished and was leaving McGregor got out of his vehicle and approached Allen while she was unlocking her vehicle, with a crossbow concealed by a garbage bag.[3][7] Surprised, she asked him why he was there, at which point McGregor shot her in the chest with a steel-tipped hunting bolt, killing her.[5][3][7] McGregor went to a police station shortly after the attack and confessed to killing his wife. During questioning, which began approximately 45 minutes after the incident, he admitted that he had intentionally killed her.[5][7] In his police interview, McGregor expressed extreme distress, stating that he felt suicidal and describing himself as \u201ca monster.\u201d He said he wanted to die and had previously considered killing both his wife and himself. He also disclosed that he believed he was terminally ill due to systemic herpes. McGregor further explained that he had purchased the crossbow with the original intention of using it to take his own life, noting that it was easier to obtain than a firearm.[5][7][3] After being charged with first degree murder and making an initial appearance in court, McGregor was sent to the Royal Ottawa Hospital for a 30-day psychiatric assessment.[14] He later pleaded not guilty by reason of insanity.[12][13][4] During McGregor's trial it was discovered that he informed his psychiatrist, weeks before Allen's murder, that he had wanted to kill her.[8][13][4] In March 1993, Justice Louise Charron delivered her 149-page judgement rejecting McGregor's claim of insanity.[11] He was sentenced to life in prison. McGregor was released from prison after 29 years behind bars.[15] McGregor was sentenced to life imprisonment and served 29 years. He was granted day parole to a halfway house in 2020 and full parole in 2022. While incarcerated, he learned French and became a bilingual writer.[3] He contributed articles and maintained a blog for the online magazine The Social Eyes,[16] co-authored two books with Raymond Viger (including the Quebec Suicide Prevention Handbook),[17] and tutored other inmates. In one of his Social Eyes posts, he stated that prison had not made him bitter, citing his ability to help fellow prisoners, attend church services, and read extensively.[18] Regarding the murder of his wife, Patricia Allen, McGregor has written, \u201cI will never live down what I did.\u201d[18] In a later French-language interview (translated), he said: \u201cIt took me years to become aware of this, but I decided not to blame others and to look at myself in the mirror \u2026 I reflected on the crime, I committed the crime, and I am guilty.\u201d[19] McGregor has not publicly apologized for the killing. Patricia Allen's murder has a legacy that lives on today in several different ways and memorials. In 1992 a scholarships fund was created at Carleton University in her honour called the Patricia Allen Memorial Fund.[20] At first it started with private donations but later made use of other fundraising such as bingo nights and an annual golf tournament.[8] By 1996 it had raised over $220,000 CAD to fund scholarships for graduate students who conduct research into spousal violence.[21][8] Also in 1992 the McGill University Faculty of Law Class of 88 created an annual guest lecture in her honour named the Patricia Allen Memorial Lecture, which consists of a yearly lecture (or two) \"devoted to sensitizing and educating the legal community and others about pressing social and legal issues related to violence, especially against women.\"[6][22][23] In 1994, the Patricia Allen Memorial Fund supported a study with the Institute for Clinical Evaluative Sciences to explore when would it be necessary for doctors to report threats made by their patients.[8] The results of the study assisted medical professionals to make several recommendations, such as compelling doctors to report their patient's serious threats concerning harming a person to the police or risk disciplinary action under professional misconduct regulations.[8] Patricia Allen's name is engraved on Enclave, a monument commemorating the lives of Ottawa women who were murdered by men from 1990 to 2000.[24]",
      "ground_truth_chunk_ids": [
        "46_random_chunk1"
      ],
      "source_ids": [
        "S246"
      ],
      "category": "factual",
      "id": 29
    },
    {
      "question": "What is Goold v Collins?",
      "ground_truth": "Goold v Collins and Ors [2004] IESC 38, [2004] 7 JIC 1201 is an Irish Supreme Court case in which the Court ruled that a statutory provision's constitutionality may be reviewed only at the behest of a litigant who is contesting some current application of that provision.[1][2] Eileen Goold had been the subject of a protection order, dated 18 September 2002, compelling her to restrain from violent behaviour towards her husband. In suspected violation of this order she was twice arrested. By agreement with her husband, this order was discharged on 21 November, and, the following day, her husband wrote to the police withdrawing his complaints. On 29 January 2003, the associated criminal charges against Goold were dismissed. On 17 December 2002, Goold obtained leave to apply to the High Court (McKechnie J.) for judicial review of the constitutionality of Section 5 of the Domestic Violence Act, 1996, on the authority of which, the protection order had been issued. The State argued that these proceedings should be disallowed due to their mootness.[3] On succeeding only in part before the High Court, the State appealed to the Supreme Court. In the earlier case of DK v Crowley,[4] the Court had found other provisions of the Domestic Violence Act relating to orders barring a spouse from the family home to be unconstitutional for: \"[F]ailing to prescribe a fixed period of relatively short duration during which an interim barring order made ex parte is to continue in force, deprived the respondents to such applications of the protection of the principle of audi alteram partem in a manner and to an extent which is disproportionate, unreasonable and unnecessary.\"[4] The Court distinguished DK v Crowley on the basis that, at the time of the application for judicial review of the underlying legislation, no agreement had",
      "expected_answer": "Goold v Collins and Ors [2004] IESC 38, [2004] 7 JIC 1201 is an Irish Supreme Court case in which the Court ruled that a statutory provision's constitutionality may be reviewed only at the behest of a litigant who is contesting some current application of that provision.[1][2] Eileen Goold had been the subject of a protection order, dated 18 September 2002, compelling her to restrain from violent behaviour towards her husband. In suspected violation of this order she was twice arrested.  By agreement with her husband, this order was discharged on 21 November, and, the following day, her husband wrote to the police withdrawing his complaints.  On 29 January 2003, the associated criminal charges against Goold were dismissed.  On 17 December 2002, Goold obtained leave to apply to the High Court (McKechnie J.) for judicial review of the constitutionality of Section 5 of the Domestic Violence Act, 1996, on the authority of which, the protection order had been issued.  The State argued that these proceedings should be disallowed due to their mootness.[3]  On succeeding only in part before the High Court, the State appealed to the Supreme Court. In the earlier case of DK v Crowley,[4] the Court had found other provisions of the Domestic Violence Act relating to orders barring a spouse from the family home to be unconstitutional for: \"[F]ailing to prescribe a fixed period of relatively short duration during which an interim barring order made ex parte is to continue in force, deprived the respondents to such applications of the protection of the principle of audi alteram partem in a manner and to an extent which is disproportionate, unreasonable and unnecessary.\"[4] The Court distinguished DK v Crowley on the basis that, at the time of the application for judicial review of the underlying legislation, no agreement had been reached regarding the order in question.  Noting the 'vital social purpose' served by domestic violence legislation,[1] the Court referred to the Canadian case of Borowski v. Canada[5] where the Supreme Court of Canada held that: \"[A]n appeal is moot when a decision will not have the effect of resolving some controversy affecting or potentially affecting the rights of the parties. Such a live controversy must be present not only when the action or proceedings is commenced but also when the Court is called upon to reach a decision.\"[5] Writing for the Court, Hardiman J. allowed the State's appeal. He held that since there was not a live \"concrete dispute between the parties\"[1] the case was moot and issued an order staying the proceedings. This case was applied in the cases of LOG v Child and Family Agency  [2017] IEHC 58 and MC v Legal Aid Board [2017] IEHC 26.",
      "ground_truth_chunk_ids": [
        "12_random_chunk1"
      ],
      "source_ids": [
        "S212"
      ],
      "category": "factual",
      "id": 30
    },
    {
      "question": "What is Age of Enlightenment?",
      "ground_truth": "The Age of Enlightenment (also the Age of Reason) was a period in the history of Europe and Western civilization[1] during which the Enlightenment,[b] an intellectual[6] and cultural[6] movement, flourished, emerging in the late 17th century[6] in Western Europe[7] and reaching its peak in the 18th century, as its ideas spread more widely across Europe[7] and into the European colonies, in the Americas and Oceania.[8][9][10] Characterized by an emphasis on reason, empirical evidence, and the scientific method, the Enlightenment promoted ideals of individual liberty, religious tolerance, progress, and natural rights. Its thinkers advocated for constitutional government, the separation of church and state, and the application of rational principles to social and political reform.[11][12][13] The Enlightenment emerged from and built upon the Scientific Revolution of the 16th and 17th centuries, which had established new methods of empirical inquiry through the work of figures such as Galileo Galilei, Johannes Kepler, Francis Bacon, Pierre Gassendi, Christiaan Huygens and Isaac Newton. Philosophical foundations were laid by thinkers including Ren\u00e9 Descartes, Thomas Hobbes, Baruch Spinoza, and John Locke, whose ideas about reason, natural rights, and empirical knowledge became central to Enlightenment thought. The dating of the period of the beginning of the Enlightenment can be attributed to the publication of Descartes' Discourse on the Method in 1637, with his method of systematically disbelieving everything unless there was a well-founded reason for accepting it, and featuring his dictum, Cogito, ergo sum ('I think, therefore I am'). Others cite the publication of Newton's Principia Mathematica (1687) as the culmination of the Scientific Revolution and the beginning of the Enlightenment.[14][15][16] European historians traditionally dated its beginning with the death of Louis XIV of France in 1715 and its end with the outbreak of the French Revolution in 1789. Many historians now date the end of the Enlightenment as",
      "expected_answer": "The Age of Enlightenment (also the Age of Reason) was a period in the history of Europe and Western civilization[1] during which the Enlightenment,[b] an intellectual[6] and cultural[6] movement, flourished, emerging in the late 17th century[6] in Western Europe[7] and reaching its peak in the 18th century, as its ideas spread more widely across Europe[7] and into the European colonies, in the Americas and Oceania.[8][9][10] Characterized by an emphasis on reason, empirical evidence, and the scientific method, the Enlightenment promoted ideals of individual liberty, religious tolerance, progress, and natural rights. Its thinkers advocated for constitutional government, the separation of church and state, and the application of rational principles to social and political reform.[11][12][13] The Enlightenment emerged from and built upon the Scientific Revolution of the 16th and 17th centuries, which had established new methods of empirical inquiry through the work of figures such as Galileo Galilei, Johannes Kepler, Francis Bacon, Pierre Gassendi, Christiaan Huygens and Isaac Newton. Philosophical foundations were laid by thinkers including Ren\u00e9 Descartes, Thomas Hobbes, Baruch Spinoza, and John Locke, whose ideas about reason, natural rights, and empirical knowledge became central to Enlightenment thought. The dating of the period of the beginning of the Enlightenment can be attributed to the publication of Descartes' Discourse on the Method in 1637, with his method of systematically disbelieving everything unless there was a well-founded reason for accepting it, and featuring his dictum, Cogito, ergo sum ('I think, therefore I am'). Others cite the publication of Newton's Principia Mathematica (1687) as the culmination of the Scientific Revolution and the beginning of the Enlightenment.[14][15][16] European historians traditionally dated its beginning with the death of Louis XIV of France in 1715 and its end with the outbreak of the French Revolution in 1789. Many historians now date the end of the Enlightenment as the start of the 19th century, with the latest proposed year being the death of Immanuel Kant in 1804.[17] The movement was characterized by the widespread circulation of ideas through new institutions: scientific academies, literary salons, coffeehouses, Masonic lodges, and an expanding print culture of books, journals, and pamphlets. The ideas of the Enlightenment undermined the authority of the monarchy and religious officials and paved the way for the political revolutions of the 18th and 19th centuries. A variety of 19th-century movements, including liberalism, socialism,[18] and neoclassicism, trace their intellectual heritage to the Enlightenment.[19] The Enlightenment was marked by an increasing awareness of the relationship between the mind and the everyday media of the world,[20] and by an emphasis on the scientific method and reductionism, along with increased questioning of religious dogma\u2014an attitude captured by Kant's essay Answering the Question: What Is Enlightenment?, where the phrase sapere aude ('dare to know') can be found.[21] The central doctrines of the Enlightenment were individual liberty, representative government, the rule of law, and religious freedom, in contrast to an absolute monarchy or single party state and the religious persecution of faiths other than those formally established and often controlled outright by the State. By contrast, other intellectual currents included arguments in favour of anti-Christianity, Deism and Atheism, accompanied by demands for secular states, bans on religious education, suppression of monasteries, the suppression of the Jesuits, and the expulsion of religious orders. The Enlightenment also faced contemporary criticism, later termed the \"Counter-Enlightenment\" by Sir Isaiah Berlin, which defended traditional religious and political authorities against rationalist critique. The Age of Enlightenment was preceded by and closely associated with the Scientific Revolution.[22] Earlier philosophers whose work influenced the Enlightenment included Francis Bacon, Pierre Gassendi, Ren\u00e9 Descartes, Thomas Hobbes, Baruch Spinoza, John Locke, Pierre Bayle, and Gottfried Wilhelm Leibniz.[23][24] Some of the figures of the Enlightenment included Cesare Beccaria, George Berkeley, Denis Diderot, David Hume, Immanuel Kant, Lord Monboddo, Montesquieu, Jean-Jacques Rousseau, Adam Smith, Hugo Grotius, and Voltaire.[25] One of the most influential Enlightenment publications was the Encyclop\u00e9die (Encyclopedia). Published between 1751 and 1772 in 35 volumes, it was compiled by Diderot, Jean le Rond d'Alembert, and a team of 150 others. The Encyclop\u00e9die helped spread the ideas of the Enlightenment across Europe and beyond.[26] Other publications of the Enlightenment included Locke's A Letter Concerning Toleration (1689) and Two Treatises of Government (1689); Berkeley's A Treatise Concerning the Principles of Human Knowledge (1710), Voltaire's Letters on the English (1733) and Philosophical Dictionary (1764); Hume's A Treatise of Human Nature (1740); Montesquieu's The Spirit of the Laws (1748); Rousseau's Discourse on Inequality (1754) and The Social Contract (1762); Cesare Beccaria's On Crimes and Punishments (1764); Adam Smith's The Theory of Moral Sentiments (1759) and The Wealth of Nations (1776); and Kant's Critique of Pure Reason (1781).[citation needed] Bacon's empiricism and Descartes' rationalist philosophy laid the foundation for enlightenment thinking.[27] Descartes' attempt to construct the sciences on a secure metaphysical foundation was not as successful as his method of doubt applied to philosophy, which led to a dualistic doctrine of mind and matter. His skepticism was refined by Locke's Essay Concerning Human Understanding (1690) and Hume's writings in the 1740s. Descartes' dualism was challenged by Spinoza's uncompromising assertion of the unity of matter in his Tractatus (1670) and Ethics (1677).[28] According to Jonathan Israel, these laid down two distinct lines of Enlightenment thought: first, the moderate variety, following Descartes, Locke, and Christian Wolff, which sought accommodation between reform and the traditional systems of power and faith, and, second, the Radical Enlightenment, inspired by the philosophy of Spinoza, advocating democracy, individual liberty, freedom of expression, and eradication of religious authority.[29][30] The moderate variety tended to be deistic whereas the radical tendency separated the basis of morality entirely from theology. Both lines of thought were eventually opposed by a conservative Counter-Enlightenment which sought a return to faith.[31] In the mid-18th century, Paris became the center of philosophic and scientific activity challenging traditional doctrines and dogmas. After the Edict of Fontainebleau in 1685, the relationship between church and the absolutist government was very strong. The early enlightenment emerged in protest to these circumstances, gaining ground under the support of Madame de Pompadour, the mistress of Louis XV.[32] Called the Si\u00e8cle des Lumi\u00e8res, the philosophical movement of the Enlightenment had already started by the early 18th century, when Pierre Bayle launched the popular and scholarly Enlightenment critique of religion. As a skeptic Bayle only partially accepted the philosophy and principles of rationality. He did draw a strict boundary between morality and religion. The rigor of his Dictionnaire Historique et Critique influenced many of the Enlightenment Encyclop\u00e9distes.[33] By the mid-18th century the French Enlightenment had found a focus in the project of the Encyclop\u00e9die.[32] The philosophical movement was led by Voltaire and Rousseau, who argued for a society based upon reason rather than faith and Catholic doctrine, for a new civil order based on natural law, and for science based on experiments and observation. The political philosopher Montesquieu introduced the idea of a separation of powers in a government, a concept which was enthusiastically adopted by the authors of the United States Constitution. While the philosophes of the French Enlightenment were not revolutionaries and many were members of the nobility, their ideas played an important part in undermining the legitimacy of the Old Regime and shaping the French Revolution.[34] Francis Hutcheson, a moral philosopher and founding figure of the Scottish Enlightenment, described the utilitarian and consequentialist principle that virtue is that which provides, in his words, \"the greatest happiness for the greatest numbers.\" Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by Hutcheson's prot\u00e9g\u00e9s in Edinburgh: David Hume and Adam Smith.[35][36] Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy. Kant tried to reconcile rationalism and religious belief, individual freedom and political authority, as well as map out a view of the public sphere through private and public reason.[37] Kant's work continued to influence German intellectual life and European philosophy more broadly well into the 20th century.[38] Mary Wollstonecraft was one of England's earliest feminist philosophers.[39] She argued for a society based on reason and that women as well as men should be treated as rational beings. She is best known for her 1792 work, A Vindication of the Rights of Woman.[40] Science played an important role in Enlightenment discourse and thought. Many Enlightenment writers and thinkers had backgrounds in the sciences and associated scientific advancement with the overthrow of religion and traditional authority in favour of the development of free speech and thought.[41] There were immediate practical results. The experiments of Antoine Lavoisier were used to create the first modern chemical plants in Paris, and the experiments of the Montgolfier brothers enabled them to launch the first manned flight in a hot air balloon in 1783.[42] Broadly speaking, Enlightenment science greatly valued empiricism and rational thought and was embedded with the Enlightenment ideal of advancement and progress. The study of science, under the heading of natural philosophy, was divided into physics and a conglomerate grouping of chemistry and natural history, which included anatomy, biology, geology, mineralogy, and zoology.[43] As with most Enlightenment views, the benefits of science were not seen universally: Rousseau criticized the sciences for distancing man from nature and not operating to make people happier.[44] Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Scientific academies and societies grew out of the Scientific Revolution as the creators of scientific knowledge, in contrast to the scholasticism of the university.[45] Some societies created or retained links to universities, but contemporary sources distinguished universities from scientific societies by claiming that the university's utility was in the transmission of knowledge while societies functioned to create knowledge.[46] As the role of universities in institutionalized science began to diminish, learned societies became the cornerstone of organized science. Official scientific societies were chartered by the state to provide technical expertise.[47] Most societies were granted permission to oversee their own publications, control the election of new members and the administration of the society.[48] In the 18th century, a very large number of official academies and societies were founded in Europe; by 1789 there were over 70 official scientific societies. In reference to this growth, Bernard de Fontenelle coined the term \"the Age of Academies\" to describe the 18th century.[49] Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the Encyclop\u00e9die and the popularization of Newtonianism by Voltaire and \u00c9milie du Ch\u00e2telet. Some historians have marked the 18th century as a drab period in the history of science.[50] The century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.[citation needed] The influence of science began appearing more commonly in poetry and literature. While some societies were established with ties to universities or maintained existing ones contemporary sources often distinguished between the two, asserting that universities primarily served to transmit knowledge, whereas scientific societies were oriented toward the creation of new knowledge.[51] James Thomson penned his \"A Poem to the Memory of Sir Isaac Newton,\" which mourned the loss of Newton and praised his science and legacy.[52] Hume and other Scottish Enlightenment thinkers developed a \"science of man,\"[53] which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar, and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement,[54] and Hume's philosophical concepts that directly influenced James Madison (and thus the U.S. Constitution), and as popularised by Dugald Stewart was the basis of classical liberalism.[55] In 1776, Adam Smith published The Wealth of Nations, often considered the first work on modern economics as it had an immediate impact on British economic policy that continues into the 21st century.[56] It was immediately preceded and influenced by Anne Robert Jacques Turgot's drafts of Reflections on the Formation and Distribution of Wealth (1766). Smith acknowledged indebtedness and possibly was the original English translator.[57] Beccaria, a jurist, criminologist, philosopher, and politician and one of the great Enlightenment writers, became famous for his masterpiece Dei delitti e delle pene (Of Crimes and Punishments, 1764). His treatise, translated into 22 languages,[58] condemned torture and the death penalty and was a founding work in the field of penology and the classical school of criminology by promoting criminal justice. Francesco Mario Pagano wrote studies such as Saggi politici (Political Essays, 1783); and Considerazioni sul processo criminale (Considerations on the Criminal Trial, 1787), which established him as an international authority on criminal law.[59] The Enlightenment has long been seen as the foundation of modern Western political and intellectual culture.[60] The Enlightenment brought political modernization to the West, in terms of introducing democratic values and institutions and the creation of modern, liberal democracies. This thesis has been widely accepted by scholars and has been reinforced by the large-scale studies by Robert Darnton, Roy Porter, and, most recently, by Jonathan Israel.[61][62] Enlightenment thought was deeply influential in the political realm. European rulers such as Catherine II of Russia, Joseph II of Austria, and Frederick II of Prussia tried to apply Enlightenment thought on religious and political tolerance, which became known as enlightened absolutism.[25] Many of the major political and intellectual figures behind the American Revolution associated themselves closely with the Enlightenment: Benjamin Franklin visited Europe repeatedly and contributed actively to the scientific and political debates there and brought the newest ideas back to Philadelphia; Thomas Jefferson closely followed European ideas and later incorporated some of the ideals of the Enlightenment into the Declaration of Independence; and Madison incorporated these ideals into the U.S. Constitution during its framing in 1787.[63] Locke, one of the most influential Enlightenment thinkers,[64] based his governance philosophy on social contract theory, a subject that permeated Enlightenment political thought. English philosopher Thomas Hobbes ushered in this new debate with his work Leviathan in 1651. Hobbes also developed some of the fundamentals of European liberal thought: the right of the individual, the natural equality of all men, the artificial character of the political order (which led to the later distinction between civil society and the state), the view that all legitimate political power must be \"representative\" and based on the consent of the people, and a liberal interpretation of law which leaves people free to do whatever the law does not explicitly forbid.[65] Both Locke and Rousseau developed social contract theories in Two Treatises of Government and Discourse on Inequality, respectively. While quite different works, Locke, Hobbes, and Rousseau agreed that a social contract, in which the government's authority lies in the consent of the governed,[66] is necessary for man to live in civil society. Locke defines the state of nature as a condition in which humans are rational and follow natural law, in which all men are born equal and with the right to life, liberty, and property. However, when one citizen breaks the law of nature both the transgressor and the victim enter into a state of war, from which it is virtually impossible to break free. Therefore, Locke said that individuals enter into civil society to protect their natural rights via an \"unbiased judge\" or common authority, such as courts. In contrast, Rousseau's conception relies on the supposition that \"civil man\" is corrupted, while \"natural man\" has no want he cannot fulfill himself. Natural man is only taken out of the state of nature when the inequality associated with private property is established.[67] Rousseau said that people join into civil society via the social contract to achieve unity while preserving individual freedom. This is embodied in the sovereignty of the general will, the moral and collective legislative body constituted by citizens.[citation needed] Locke is known for his statement that individuals have a right to \"Life, Liberty, and Property,\" and his belief that the natural right to property is derived from labor. Tutored by Locke, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury, wrote in 1706: \"There is a mighty Light which spreads its self over the world especially in those two free Nations of England and Holland; on whom the Affairs of Europe now turn.\"[68] Locke's theory of natural rights has influenced many political documents, including the U.S. Declaration of Independence and the French National Constituent Assembly's Declaration of the Rights of Man and of the Citizen. Some philosophes argued that the establishment of a contractual basis of rights would lead to the market mechanism and capitalism, the scientific method, religious tolerance, and the organization of states into self-governing republics through democratic means. In this view, the tendency of the philosophes in particular to apply rationality to every problem is considered the essential change.[69] Although much of Enlightenment political thought was dominated by social contract theorists, Hume and Ferguson criticized this camp. Hume's essay Of the Original Contract argues that governments derived from consent are rarely seen and civil government is grounded in a ruler's habitual authority and force. It is precisely because of the ruler's authority over-and-against the subject that the subject tacitly consents, and Hume says that the subjects would \"never imagine that their consent made him sovereign,\" rather the authority did so.[70] Similarly, Ferguson did not believe citizens built the state, rather polities grew out of social development. In his 1767 An Essay on the History of Civil Society, Ferguson uses the four stages of progress, a theory that was popular in Scotland at the time, to explain how humans advance from a hunting and gathering society to a commercial and civil society without agreeing to a social contract. Both Rousseau's and Locke's social contract theories rest on the presupposition of natural rights, which are not a result of law or custom but are things that all men have in pre-political societies and are therefore universal and inalienable. The most famous natural right formulation comes from Locke's Second Treatise, when he introduces the state of nature. For Locke, the law of nature is grounded on mutual security or the idea that one cannot infringe on another's natural rights, as every man is equal and has the same inalienable rights. These natural rights include perfect equality and freedom, as well as the right to preserve life and property. Locke argues against indentured servitude on the basis that enslaving oneself goes against the law of nature because a person cannot surrender their own rights: freedom is absolute, and no one can take it away. Locke argues that one person cannot enslave another because it is morally reprehensible, although he introduces a caveat by saying that enslavement of a lawful captive in time of war would not go against one's natural rights. The leaders of the Enlightenment were not especially democratic, as they more often look to absolute monarchs as the key to imposing reforms designed by the intellectuals. Voltaire despised democracy and said the absolute monarch must be enlightened and must act as dictated by reason and justice\u2014in other words, be a \"philosopher-king.\"[71] In several nations, rulers welcomed leaders of the Enlightenment at court and asked them to help design laws and programs to reform the system, typically to build stronger states. These rulers are called \"enlightened despots\" by historians.[72] They included Frederick the Great of Prussia, Catherine the Great of Russia, Leopold II of Tuscany and Joseph II of Austria. Joseph was over-enthusiastic, announcing many reforms that had little support so that revolts broke out and his regime became a comedy of errors, and nearly all his programs were reversed.[73] Senior ministers Pombal in Portugal and Johann Friedrich Struensee in Denmark also governed according to Enlightenment ideals. In Poland, the model constitution of 1791 expressed Enlightenment ideals, but was in effect for only one year before the nation was partitioned among its neighbors. More enduring were the cultural achievements, which created a nationalist spirit in Poland.[74] Frederick the Great, the king of Prussia from 1740 to 1786, saw himself as a leader of the Enlightenment and patronized philosophers and scientists at his court in Berlin. Voltaire, who had been imprisoned and maltreated by the French government, was eager to accept Frederick's invitation to live at his palace. Frederick explained: \"My principal occupation is to combat ignorance and prejudice... to enlighten minds, cultivate morality, and to make people as happy as it suits human nature, and as the means at my disposal permit.\"[75] The Enlightenment has been frequently linked to the American Revolution of 1776[76] and the French Revolution of 1789\u2014both had some intellectual influence from Thomas Jefferson.[77][78]  A key aspect of this era was a profound shift from the absolute monarchies of Europe, which asserted the \"divine right\" to rule. John Locke rejected this view in his writings on the Two Treatises of Government (1689). He asserted that citizens were seen to possess natural rights, including life, liberty, and property. Therefore governments exist to protect these rights through the \"consent of the governed.\" The clash between these competing ethos often resulted in violent upheaval in differing ways. In France, Ancien r\u00e9gime, with its rigid social hierarchy and absolute monarchical power, was systematically dismantled during the French Revolution. While the American Revolution focused more on breaking free from a government - represented by King George III and Parliament - that colonists felt did not adequately represent their interests. Alexis de Tocqueville proposed the French Revolution as the inevitable result of the radical opposition created in the 18th century between the monarchy and the men of letters of the Enlightenment. These men of letters constituted a sort of \"substitute aristocracy that was both all-powerful and without real power.\" This illusory power came from the rise of \"public opinion,\" born when absolutist centralization removed the nobility and the bourgeoisie from the political sphere. The \"literary politics\" that resulted promoted a discourse of equality and was hence in fundamental opposition to the monarchical regime.[79][80] De Tocqueville \"clearly designates... the cultural effects of transformation in the forms of the exercise of power.\"[81] It does not require great art or magnificently trained eloquence, to prove that Christians should tolerate each other. I, however, am going further: I say that we should regard all men as our brothers. What? The Turk my brother? The Chinaman my brother? The Jew? The Siam? Yes, without doubt; are we not all children of the same father and creatures of the same God? Enlightenment era religious commentary was a response to the preceding century of religious conflict in Europe, especially the Thirty Years' War.[83] Theologians of the Enlightenment wanted to reform their faith to its generally non-confrontational roots and to limit the capacity for religious controversy to spill over into politics and warfare while still maintaining a true faith in God. For moderate Christians, this meant a return to simple Scripture. Locke abandoned the corpus of theological commentary in favor of an \"unprejudiced examination\" of the Word of God alone. He determined the essence of Christianity to be a belief in Christ the redeemer and recommended avoiding more detailed debate.[84] Anthony Collins, one of the English freethinkers, published his \"Essay concerning the Use of Reason in Propositions the Evidence whereof depends on Human Testimony\" (1707), in which he rejects the distinction between \"above reason\" and \"contrary to reason,\" and demands that revelation should conform to man's natural ideas of God. In the Jefferson Bible, Thomas Jefferson went further and dropped any passages dealing with miracles, visitations of angels, and the resurrection of Jesus after his death, as he tried to extract the practical Christian moral code of the New Testament.[85] Enlightenment scholars sought to curtail the political power of organized religion and thereby prevent another age of intolerant religious war.[86] Spinoza determined to remove politics from contemporary and historical theology (e.g., disregarding Judaic law).[87] Moses Mendelssohn advised affording no political weight to any organized religion but instead recommended that each person follow what they found most convincing.[88] They believed a good religion based in instinctive morals and a belief in God should not theoretically need force to maintain order in its believers, and both Mendelssohn and Spinoza judged religion on its moral fruits, not the logic of its theology.[89] Several novel ideas about religion developed with the Enlightenment, including deism and talk of atheism. According to Thomas Paine, deism is the simple belief in God the Creator with no reference to the Bible or any other miraculous source. Instead, the deist relies solely on personal reason to guide his creed,[90] which was eminently agreeable to many thinkers of the time.[91] Atheism was much discussed, but there were few proponents. Wilson and Reill note: \"In fact, very few enlightened intellectuals, even when they were vocal critics of Christianity, were true atheists. Rather, they were critics of orthodox belief, wedded rather to skepticism, deism, vitalism, or perhaps pantheism.\"[92] Some followed Pierre Bayle and argued that atheists could indeed be moral men.[93] Many others like Voltaire held that without belief in a God who punishes evil, the moral order of society was undermined; that is, since atheists gave themselves to no supreme authority and no law and had no fear of eternal consequences, they were far more likely to disrupt society.[94] Bayle observed that, in his day, \"prudent persons will always maintain an appearance of [religion],\" and he believed that even atheists could hold concepts of honor and go beyond their own self-interest to create and interact in society.[95] Locke said that if there were no God and no divine law, the result would be moral anarchy: every individual \"could have no law but his own will, no end but himself. He would be a god to himself, and the satisfaction of his own will the sole measure and end of all his actions.\"[96] The \"Radical Enlightenment\"[97][98] promoted the concept of separating church and state,[99] an idea that is often credited to Locke.[100] According to his principle of the social contract, Locke said that the government lacked authority in the realm of individual conscience, as this was something rational people could not cede to the government for it or others to control. For Locke, this created a natural right in the liberty of conscience, which he said must therefore remain protected from any government authority. These views on religious tolerance and the importance of individual conscience, along with the social contract, became particularly influential in the American colonies and the drafting of the United States Constitution.[101] In a letter to the Danbury Baptist Association in Connecticut, Thomas Jefferson calls for a \"wall of separation between church and state\" at the federal level. He previously had supported successful efforts to disestablish the Church of England in Virginia[102] and authored the Virginia Statute for Religious Freedom.[103] Jefferson's political ideals were greatly influenced by the writings of Locke, Bacon, and Newton,[104] whom he considered the three greatest men that ever lived.[105] The Enlightenment took hold in most European countries and influenced nations globally, often with a specific local emphasis. For example, in France it became associated with anti-government and anti-Church radicalism, while in Germany it reached deep into the middle classes, where it expressed a spiritualistic and nationalistic tone without threatening governments or established churches.[106] Government responses varied widely. In France, the government was hostile, and the philosophes fought against its censorship, sometimes being imprisoned or hounded into exile. The British government, for the most part, ignored the Enlightenment's leaders in England and Scotland, although it did give Newton a knighthood and a very lucrative government office. A common theme among most countries which derived Enlightenment ideas from Europe was the intentional non-inclusion of Enlightenment philosophies pertaining to slavery. Originally during the French Revolution, a revolution deeply inspired by Enlightenment philosophy, \"France's revolutionary government had denounced slavery, but the property-holding 'revolutionaries' then remembered their bank accounts.\"[107] Slavery frequently showed the limitations of the Enlightenment ideology as it pertained to European colonialism, since many colonies of Europe operated on a plantation economy fueled by slave labor. In 1791, the Haitian Revolution, a slave rebellion by emancipated slaves against French colonial rule in the colony of Saint-Domingue, broke out. European nations and the United States, despite the strong support for Enlightenment ideals, refused to \"[give support] to Saint-Domingue's anti-colonial struggle.\"[107] The very existence of an English Enlightenment has been hotly debated by scholars. The majority of textbooks on British history make little or no mention of an English Enlightenment. Some surveys of the entire Enlightenment include England and others ignore it, although they do include coverage of such major intellectuals as Joseph Addison, Edward Gibbon, John Locke, Isaac Newton, Alexander Pope, Joshua Reynolds, and Jonathan Swift.[108] Freethinking, a term describing those who stood in opposition to the institution of the Church, and the literal belief in the Bible, can be said to have begun in England no later than 1713, when Anthony Collins wrote his \"Discourse of Free-thinking,\" which gained substantial popularity. This essay attacked the clergy of all churches and was a plea for deism. Roy Porter argues that the reasons for this neglect were the assumptions that the movement was primarily French-inspired, that it was largely a-religious or anti-clerical, and that it stood in outspoken defiance to the established order.[109] Porter admits that after the 1720s England could claim thinkers to equal Diderot, Voltaire, or Rousseau. However, its leading intellectuals such as Gibbon,[110] Edmund Burke and Samuel Johnson were all quite conservative and supportive of the standing order. Porter says the reason was that Enlightenment had come early to England and had succeeded such that the culture had accepted political liberalism, philosophical empiricism, and religious toleration, positions which intellectuals on the continent had to fight against powerful odds. Furthermore, England rejected the collectivism of the continent and emphasized the improvement of individuals as the main goal of enlightenment.[111] According to Derek Hirst, the 1640s and 1650s saw a revived economy characterised by growth in manufacturing, the elaboration of financial and credit instruments, and the commercialisation of communication. The gentry found time for leisure activities, such as horse racing and bowling. In the high culture important innovations included the development of a mass market for music, increased scientific research, and an expansion of publishing. All the trends were discussed in depth at the newly established coffee houses.[112][113] In the Scottish Enlightenment, the principles of sociability, equality, and utility were disseminated in schools and universities, many of which used sophisticated teaching methods which blended philosophy with daily life.[20] Scotland's major cities created an intellectual infrastructure of mutually supporting institutions such as schools, universities, reading societies, libraries, periodicals, museums, and masonic lodges.[114] The Scottish network was \"predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment.\"[115] In France, Voltaire said \"we look to Scotland for all our ideas of civilization.\"[116] The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of William Cullen, physician and chemist; James Anderson, agronomist; Joseph Black, physicist and chemist; and James Hutton, the first modern geologist.[35][117] Several Americans, especially Benjamin Franklin and Thomas Jefferson, played a major role in bringing Enlightenment ideas to the New World and in influencing British and French thinkers.[118] Franklin was influential for his political activism and for his advances in physics.[119][120]  Franklin also broadly encouraged the individual's rights and responsibilities to serve as an educated and informed citizen.[121]  He published yearly the widely popular, Poor Richard's Almanack, filled with witty quotes encouraging disciplined self-learning, such as \"Early to bed, early to rise, makes a man healthy, wealthy and wise.\"[122] The cultural exchange during the Age of Enlightenment ran in both directions across the Atlantic. Thinkers such as Paine, Locke, and Rousseau all take Native American cultural practices as examples of natural freedom.[123] The Americans closely followed English and Scottish political ideas, as well as some French thinkers such as Montesquieu.[124] As deists, they were influenced by ideas of John Toland and Matthew Tindal. There was a great emphasis upon liberty, republicanism, and religious tolerance. There was no respect for monarchy or inherited political power. Deists reconciled science and religion by rejecting prophecies, miracles, and biblical theology. Leading deists included Thomas Paine in The Age of Reason and Thomas Jefferson in his short Jefferson Bible, from which he removed all supernatural aspects.[125] The Jewish Enlightenment, or Haskalah (Hebrew: \u05d4\u05b7\u05e9\u05b0\u05c2\u05db\u05b8\u05bc\u05dc\u05b8\u05d4, \"education\") was an intellectual movement among the Jews of Central and Eastern Europe, with a certain influence on those in Western Europe and the Muslim world. It arose as a defined ideological worldview during the 1770s, and its last stage ended around 1881, with the rise of Jewish nationalism. The movement advocated against Jewish reclusiveness, encouraged the adoption of prevalent attire over traditional dress, while also working to diminish the authority of traditional community institutions such as rabbinic courts and boards of elders. The Dutch Enlightenment began in 1640.[126] During the Early Dutch Enlightenment (1640\u20131720), many books were translated from Latin, French or English to Dutch, often at the risk of their translators and publishers.[126] By the 1720s, the Dutch Republic had also become a major center for printing and exporting banned books to France.[127] Implanted in Netherlandish culture, vernacular rationalism brought the Dutch to take advantage of the intellectual philosophy the enlightenment spread.[128]The most famous figure of the Dutch Enlightenment was Baruch Spinoza. The French Enlightenment was influenced by England[129] and in turn influenced other national enlightenments. As worded by Sharon A. Stanley, \"the French Enlightenment stands out from other national enlightenments for its unrelenting assault on church leadership and theology.\"[130] Prussia took the lead among the German states in sponsoring the political reforms that Enlightenment thinkers urged absolute rulers to adopt. There were important movements as well in the smaller states of Bavaria, Saxony, Hanover, and the Palatinate. In each case, Enlightenment values became accepted and led to significant political and administrative reforms that laid the groundwork for the creation of modern states.[131] The princes of Saxony, for example, carried out an impressive series of fundamental fiscal, administrative, judicial, educational, cultural, and general economic reforms. The reforms were aided by the country's strong urban structure and influential commercial groups and modernized pre-1789 Saxony along the lines of classic Enlightenment principles.[132][133] Before 1750, the German upper classes looked to France for intellectual, cultural, and architectural leadership, as French was the language of high society. By the mid-18th century, the Aufkl\u00e4rung (The Enlightenment) had transformed German high culture in music, philosophy, science, and literature. Christian Wolff was the pioneer as a writer who expounded the Enlightenment to German readers and legitimized German as a philosophic language.[134] Johann Gottfried von Herder broke new ground in philosophy and poetry, as a leader of the Sturm und Drang movement of proto-Romanticism. Weimar Classicism (Weimarer Klassik) was a cultural and literary movement based in Weimar that sought to establish a new humanism by synthesizing Romantic, classical, and Enlightenment ideas. The movement (from 1772 until 1805) involved Herder as well as polymath Johann Wolfgang von Goethe and Friedrich Schiller, a poet and historian. The theatre principal Abel Seyler greatly influenced the development of German theatre and promoted serious German opera, new works and experimental productions, and the concept of a national theatre.[135] Herder argued that every group of people had its own particular identity, which was expressed in its language and culture. This legitimized the promotion of German language and culture and helped shape the development of German nationalism. Schiller's plays expressed the restless spirit of his generation, depicting the hero's struggle against social pressures and the force of destiny.[136] German music, sponsored by the upper classes, came of age under composers Johann Sebastian Bach, Joseph Haydn, and Wolfgang Amadeus Mozart.[137] In remote K\u00f6nigsberg, Kant tried to reconcile rationalism and religious belief, individual freedom, and political authority. Kant's work contained basic tensions that would continue to shape German thought\u2014and indeed all of European philosophy\u2014well into the 20th century.[138] German Enlightenment won the support of princes, aristocrats, and the middle classes, and it permanently reshaped the culture.[139] However, there was a conservatism among the elites that warned against going too far.[140] In 1788, Prussia issued an \"Edict on Religion\" that forbade preaching any sermon that undermined popular belief in the Holy Trinity or the Bible. The goal was to avoid theological disputes that might impinge on domestic tranquility. Men who doubted the value of Enlightenment favoured the measure, but so too did many supporters. German universities had created a closed elite that could debate controversial issues among themselves, but spreading them to the public was seen as too risky. This intellectual elite was favoured by the state, but that might be reversed if the process of the Enlightenment proved politically or socially destabilizing.[141] During the 18th century, Austria was under Habsburg rule. The reign of Maria Theresa, the first Habsburg monarch to be considered influenced by the Enlightenment in some areas, was marked by a mix of enlightenment and conservatism. Her son Joseph II's brief reign was marked by this conflict, with his ideology of Josephinism facing opposition. Joseph II carried out numerous reforms in the spirit of the Enlightenment, which affected, for example, the school system, monasteries and the legal system. Emperor Leopold II, who was an early opponent of capital punishment, had a brief and contentious rule that was mostly marked by relations with France. Similarly, Emperor Francis II's rule was primarily marked by relations with France. The ideas of the Enlightenment also appeared in literature and theater works. Joseph von Sonnenfels was an important representative. In music, Austrian musicians such as Joseph Haydn and Wolfgang Amadeus Mozart were associated with the Enlightenment. The Modern Greek Enlightenment (Greek: \u0394\u03b9\u03b1\u03c6\u03c9\u03c4\u03b9\u03c3\u03bc\u03cc\u03c2, Diafotism\u00f3s) was the Greek expression of the Age of Enlightenment, characterized by an intellectual and philosophical movement within the Greek community. At this time, many Greeks were dispersed across the Ottoman Empire, with some residing on the Ionian Islands, in Venice, and other parts of Italy. The Hungarian Enlightenment[142] emerged during the 18th century, while Hungary was part of the Habsburg Empire. The Hungarian Enlightenment is usually said to have begun in 1772 and was greatly influenced by French Enlightenment (through Vienna).[142] The Romanian Enlightenment[143] emerged during the 18th century across the three major historical regions inhabited by Romanians: Transylvania, Wallachia, and Moldavia. At that time, Transylvania was in the Habsburg Empire while Wallachia and Moldavia were vassals of the Ottoman Empire. The Transylvanian Enlightenment was represented by the Transylvanian School, a group of thinkers who promoted a cultural revival and rights for Romanians (who were marginalized by the Habsburgs). The Wallachian Enlightenment was represented by such figures as Dinicu Golescu (1777\u20131830), while the Moldavian Englightenment was headed by prince Dimitrie Cantemir (1673-1723). The Enlightenment arrived relatively late in Switzerland, spreading from England, the Netherlands, and France toward the end of the 17th century. The movement initially took hold in Protestant regions, where it gradually replaced orthodox religious thinking. The 1712 victory of the reformed cantons of Zurich and Bern over the five Catholic cantons of central Switzerland in the Second War of Villmergen marked both a Protestant triumph and a victory for Enlightenment ideas in the economically more developed regions.[144] In Switzerland, which lacked a central court or academy, the Enlightenment spread through the intellectual elite of reformed cities, particularly pastors educated in academies and colleges with strong humanist traditions. The theological \"Helvetic triumvirate\" of Jean-Alphonse Turrettini (Geneva), Jean-Fr\u00e9d\u00e9ric Ostervald (Neuch\u00e2tel), and Samuel Werenfels (Basel) led their churches toward a humanistic Christianity beginning in 1697, creating what Paul Wernle termed \"reasoned orthodoxy\" that balanced rational thought with Christian ethics.[144] Swiss Enlightenment thinkers made significant contributions across multiple fields. The Romand school developed influential theories of natural law, with scholars like Jean Barbeyrac (Lausanne), Jean-Jacques Burlamaqui (Geneva), and Emer de Vattel (Neuch\u00e2tel) promoting concepts of inalienable rights and justified resistance to tyranny that influenced the American independence movement. In literature, Johann Jakob Bodmer and Johann Jakob Breitinger made Zurich a center of German literary innovation, while Albert von Haller's poetry represented the peak of Swiss Enlightenment literature. Jean-Jacques Rousseau, considering himself both a Genevan and Swiss citizen, developed democratic republican theories that extended Genevan models to broader European federalist principles.[144] The movement was characterized by what scholars term \"Helvetism\" \u2013 specifically Swiss aspects including a Christian conception of natural law, patriotic ethics, and philosophical approaches grounded in practical pedagogy and economics. Most distinctively, Swiss Enlightenment thought celebrated Alpine nature, viewing Switzerland as the \"land of shepherds\" whose republican and federalist traditions were shaped by its mountain environment. The movement organized through numerous societies and publications, including the Encyclop\u00e9die d'Yverdon (1770-1780), which offered a moderate alternative to the French Encyclop\u00e9die. Swiss intellectuals gained international prominence, with many serving in foreign academies, particularly in Berlin under Frederick II and in St. Petersburg under Catherine II.[144] In Italy the main centers of diffusion of the Enlightenment were Naples and Milan:[145] in both cities the intellectuals took public office and collaborated with the Bourbon and Habsburg administrations. In Naples, Antonio Genovesi, Ferdinando Galiani, and Gaetano Filangieri were active under the tolerant King Charles of Bourbon. However, the Neapolitan Enlightenment, like Vico's philosophy, remained almost always in the theoretical field.[146] Only later, many Enlighteners animated the unfortunate experience of the Parthenopean Republic. In Milan, however, the movement strove to find concrete solutions to problems. The center of discussions was the magazine Il Caff\u00e8 (1762\u20131766), founded by brothers Pietro and Alessandro Verri (famous philosophers and writers, as well as their brother Giovanni), who also gave life to the Accademia dei Pugni, founded in 1761. Minor centers were Tuscany, Veneto, and Piedmont, where among others, Pompeo Neri worked. From Naples, Genovesi influenced a generation of southern Italian intellectuals and university students. His textbook Della diceosina, o sia della Filosofia del Giusto e dell'Onesto (1766) was a controversial attempt to mediate between the history of moral philosophy on the one hand and the specific problems encountered by 18th-century commercial society on the other. It contained the greater part of Genovesi's political, philosophical, and economic thought, which became a guidebook for Neapolitan economic and social development.[147] Science flourished as Alessandro Volta and Luigi Galvani made break-through discoveries in electricity. Pietro Verri was a leading economist in Lombardy. Historian Joseph Schumpeter states he was \"the most important pre-Smithian authority on Cheapness-and-Plenty.\"[148] The most influential scholar on the Italian Enlightenment has been Franco Venturi.[149][150] Italy also produced some of the Enlightenment's greatest legal theorists, including Cesare Beccaria, Giambattista Vico, and Francesco Mario Pagano. When Charles II, the last Spanish Habsburg monarch, died his successor was from the French House of Bourbon, initiating a period of French Enlightenment influence in Spain and the Spanish Empire.[151][152] In the 18th Century, the Spanish continued to expand their empire in the Americas with the Spanish missions in California and established missions deeper inland in South America. Under Charles III, the crown began to implement serious structural changes. The monarchy curtailed the power of the Catholic Church, and established a standing military in Spanish America. Freer trade was promoted under comercio libre in which regions could trade with companies sailing from any other Spanish port, rather than the restrictive mercantile system. The crown sent out scientific expeditions to assert Spanish sovereignty over territories it claimed but did not control, but also importantly to discover the economic potential of its far-flung empire. Botanical expeditions sought plants that could be of use to the empire.[153] Charles IV gave Prussian scientist Alexander von Humboldt free rein to travel in Spanish America, usually closed to foreigners, and more importantly, access to crown officials to aid the success of his scientific expedition.[154] When Napoleon invaded Spain in 1808, Ferdinand VII abdicated and Napoleon placed his brother Joseph Bonaparte on the throne. To add legitimacy to this move, the Bayonne Constitution was promulgated, which included representation from Spain's overseas components, but most Spaniards rejected the whole Napoleonic project. A war of national resistance erupted. The Cortes de C\u00e1diz (parliament) was convened to rule Spain in the absence of the legitimate monarch, Ferdinand. It created a new governing document, the Constitution of 1812, which laid out three branches of government: executive, legislative, and judicial; put limits on the king by creating a constitutional monarchy; defined citizens as those in the Spanish Empire without African ancestry; established universal manhood suffrage; and established public education starting with primary school through university as well as freedom of expression. The constitution was in effect from 1812 until 1814, when Napoleon was defeated and Ferdinand was restored to the throne of Spain. Upon his return, Ferdinand repudiated the constitution and reestablished absolutist rule.[155] The Haitian Revolution began in 1791 and ended in 1804 and shows how Enlightenment ideas \"were part of complex transcultural flows.\"[10] Radical ideas in Paris during and after the French Revolution were mobilized in Haiti, such as by Toussaint Louverture.[10] Toussaint had read the critique of European colonialism in Guillaume Thomas Fran\u00e7ois Raynal's book Histoire des deux Indes and \"was particularly impressed by Raynal's prediction of the coming of a 'Black Spartacus.'\"[10] The revolution combined Enlightenment ideas with the experiences of the slaves in Haiti, two-thirds of whom had been born in Africa and could \"draw on specific notions of kingdom and just government from West and Central Africa, and to employ religious practices such as voodoo for the formation of revolutionary communities.\"[10] The revolution also affected France and \"forced the French National Convention to abolish slavery in 1794.\"[10] The Portuguese Enlightenment was heavily marked by the rule of Prime Minister Marquis of Pombal under King Joseph I from 1756 to 1777. Following the 1755 Lisbon earthquake which destroyed a large part of Lisbon, the Marquis of Pombal implemented important economic policies to regulate commercial activity (in particular with Brazil and England), and to standardise quality throughout the country (for example by introducing the first integrated industries in Portugal). His reconstruction of Lisbon's riverside district in straight and perpendicular streets (the Lisbon Baixa), methodically organized to facilitate commerce and exchange (for example by assigning to each street a different product or service), can be seen as a direct application of the Enlightenment ideas to governance and urbanism. His urbanistic ideas, also being the first large-scale example of earthquake engineering, became collectively known as Pombaline style, and were implemented throughout the kingdom during his stay in office. His governance was as enlightened as ruthless, see for example the T\u00e1vora affair. In literature, the first Enlightenment ideas in Portugal can be traced back to the diplomat, philosopher, and writer Ant\u00f3nio Vieira[156] who spent a considerable amount of his life in colonial Brazil denouncing discriminations against New Christians and the indigenous peoples in Brazil. During the 18th century, enlightened literary movements such as the Arc\u00e1dia Lusitana (lasting from 1756 until 1776, then replaced by the Nova Arc\u00e1dia in 1790 until 1794) surfaced in the academic medium, in particular involving former students of the University of Coimbra. A distinct member of this group was the poet Manuel Maria Barbosa du Bocage. The physician Ant\u00f3nio Nunes Ribeiro Sanches was also an important Enlightenment figure, contributing to the Encyclop\u00e9die and being part of the Russian court. The ideas of the Enlightenment influenced various economists and anti-colonial intellectuals throughout the Portuguese Empire, such as Jos\u00e9 de Azeredo Coutinho, Jos\u00e9 da Silva Lisboa, Cl\u00e1udio Manoel da Costa, and Tom\u00e1s Ant\u00f4nio Gonzaga. The Napoleonic invasion of Portugal had consequences for the Portuguese monarchy. With the aid of the British navy, the Portuguese royal family was evacuated to Brazil, its most important colony. Even though Napoleon had been defeated, the royal court remained in Brazil. The Liberal Revolution of 1820 forced the return of the royal family to Portugal. The terms by which the restored king was to rule was a constitutional monarchy under the Constitution of Portugal. Brazil declared its independence from Portugal in 1822 and became a monarchy. The existence of a Swedish Enlightenment has been debated by scholars. According to Tore Fr\u00e4ngsmyr, the Swedish Enlightenment \"never formed a truly coherent current of ideas or became a unified movement.\"[157] As worded by Max Skj\u00f6nsberg, Fr\u00e4ngsmyr's main arguments against a Swedish Enlightenment were that religious criticism in Sweden was reserved for foreign Catholicism rather than the domestic Lutheran Church and that the debate about freedom in the 1750s and 1760s focused on political economy and freedom to trade rather than freedom to 'philosophize'. The fact that political economy is now a much more important aspect of Enlightenment historiography, to a great degree thanks to research on the Scottish Enlightenment, is a clear example of why Fr\u00e4ngsmyr's case is in need of revision.[158] Between 1718 and 1772, the Swedish Enlightenment overlapped with the period of parliamentary rule known in Swedish history as the Age of Liberty. In Russia, the government began to actively encourage the proliferation of arts and sciences in the mid-18th century. This era produced the first Russian university, library, theatre, public museum, and independent press. Like other enlightened despots, Catherine the Great played a key role in fostering the arts, sciences and education. She used her own interpretation of Enlightenment ideals, assisted by notable international experts such as Voltaire (by correspondence) and in residence world class scientists such as Leonhard Euler and Peter Simon Pallas. The national Enlightenment differed from its Western European counterpart in that it promoted further modernization of all aspects of Russian life and was concerned with attacking the institution of serfdom in Russia. The Russian Enlightenment centered on the individual instead of societal enlightenment and encouraged the living of an enlightened life.[159][160] A powerful element was prosveshchenie which combined religious piety, erudition, and commitment to the spread of learning. However, it lacked the skeptical and critical spirit of the Western European Enlightenment.[161] Enlightenment ideas (o\u015bwiecenie) emerged late in Poland, as the Polish middle class was weaker and szlachta (nobility) culture (Sarmatism) together with the Polish\u2013Lithuanian Commonwealth political system (Golden Liberty) were in deep crisis. The political system was built on aristocratic republicanism, but was unable to defend itself against powerful neighbors Russia, Prussia, and Austria as they repeatedly sliced off regions until nothing was left of independent Poland. The Polish Enlightenment began in the 1730s\u201340s and especially in theatre and the arts peaked in the reign of King Stanis\u0142aw August Poniatowski (second half of the 18th century). Warsaw was a main centre after 1750, with an expansion of schools and educational institutions and the arts patronage held at the Royal Castle.[162] Leaders promoted tolerance and more education. They included King Stanislaw II August and reformers Piotr Switkowski, Antoni Poplawski, Josef Niemcewicz, and J\u00f3sef Pawlinkowski, as well as Baudouin de Cortenay, a Polonized dramatist. Opponents included Florian Jaroszewicz, Gracjan Piotrowski, Karol Wyrwicz, and Wojciech Skarszewski.[163] The movement went into decline with the Third Partition of Poland (1795) \u2013 a national tragedy inspiring a short period of sentimental writing \u2013 and ended in 1822, replaced by Romanticism.[164] Eighteenth-century China experienced \"a trend towards seeing fewer dragons and miracles, not unlike the disenchantment that began to spread across the Europe of the Enlightenment.\"[10] Furthermore, \"some of the developments that we associate with Europe's Enlightenment resemble events in China remarkably.\"[10] During this time, ideals of Chinese society were reflected in \"the reign of the Qing emperors Kangxi and Qianlong; China was posited as the incarnation of an enlightened and meritocratic society\u2014and instrumentalized for criticisms of absolutist rule in Europe.\"[10] From 1641 to 1853, the Tokugawa shogunate of Japan enforced a policy called kaikin. The policy prohibited foreign contact with most outside countries.[165] Robert Bellah found \"origins of modern Japan in certain strands of Confucian thinking, a 'functional analogue to the Protestant Ethic' that Max Weber singled out as the driving force behind Western capitalism.\"[10] Japanese Confucian and Enlightenment ideas were brought together, for example, in the work of the Japanese reformer Tsuda Mamichi in the 1870s, who said, \"Whenever we open our mouths...it is to speak of 'enlightenment.'\"[10] In Japan and much of East Asia, Confucian ideas were not replaced but \"ideas associated with the Enlightenment were instead fused with the existing cosmology\u2014which in turn was refashioned under conditions of global interaction.\"[10] In Japan in particular, the term ri, which is the Confucian idea of \"order and harmony on human society\" also came to represent \"the idea of laissez-faire and the rationality of market exchange.\"[10] By the 1880s, the slogan \"Civilization and Enlightenment\" became potent throughout Japan, China, and Korea and was employed to address challenges of globalization.[10] During this time, Korea \"aimed at isolation\" and was known as the \"hermit kingdom\" but became awakened to Enlightenment ideas by the 1890s such as with the activities of the Independence Club.[10] Korea was influenced by China and Japan but also found its own Enlightenment path with the Korean intellectual Yu Kilchun who popularized the term Enlightenment throughout Korea.[10] The use of Enlightenment ideas was a \"response to a specific situation in Korea in the 1890s, and not a belated answer to Voltaire.\"[10] In 18th-century India, Tipu Sultan was an enlightened monarch, who \"was one of the founding members of the (French) Jacobin Club in Seringapatam, had planted a liberty tree, and asked to be addressed as 'Tipu Citoyen,'\" which means Citizen Tipu.[10] In parts of India, an important movement called the \"Bengal Renaissance\" led to Enlightenment reforms beginning in the 1820s.[10] Ram Mohan Roy was a reformer who \"fused different traditions in his project of social reform that made him a proponent of a 'religion of reason.'\"[10] Eighteenth-century Egypt had \"a form of 'cultural revival' in the making\u2014specifically Islamic origins of modernization long before Napoleon's Egyptian campaign.\"[10] Napoleon's expedition into Egypt further encouraged \"social transformations that harkened back to debates about inner-Islamic reform, but now were also legitimized by referring to the authority of the Enlightenment.\"[10] A major intellectual influence on Islamic modernism and expanding the Enlightenment in Egypt, Rifa al-Tahtawi \"oversaw the publication of hundreds of European works in the Arabic language.\"[10] The Enlightenment began to influence the Ottoman Empire in the 1830s and continued into the late 19th century.[10]\nThe Tanzimat was a period of reform in the Ottoman Empire that began with the G\u00fclhane Hatt-\u0131 \u015eerif in 1839 and ended with the First Constitutional Era in 1876. Namik Kemal, a political activist and member of the Young Ottomans, drew on major Enlightenment thinkers and \"a variety of intellectual resources in his quest for social and political reform.\"[10] In 1893, Kemal responded to Ernest Renan, who had indicted the Islamic religion, with his own version of the Enlightenment, which \"was not a poor copy of French debates in the eighteenth century, but an original position responding to the exigencies of Ottoman society in the late nineteenth century.\"[10] The Arab Enlightenment, or Nahda (Arabic: \u0627\u0644\u0646\u0651\u0647\u0636\u0629, \"the awakening\"), was a cultural movement in Arab-populated regions of the Ottoman Empire such as Egypt, Lebanon, Syria, and Tunisia, during the second half of the 19th century and the early 20th century. The Nahda is often traced to the cultural shock of the French invasion of Egypt and Syria in 1798, and the reformist drive of subsequent rulers such as Muhammad Ali of Egypt. The idea of the Enlightenment has always been contested territory. According to Keith Thomas, its supporters \"hail it as the source of everything that is progressive about the modern world. For them, it stands for freedom of thought, rational inquiry, critical thinking, religious tolerance, political liberty, scientific achievement, the pursuit of happiness, and hope for the future.\"[166] Thomas adds that its detractors accuse it of shallow rationalism, na\u00efve optimism, unrealistic universalism, and moral darkness. From the start, conservative and clerical defenders of traditional religion attacked materialism and skepticism as evil forces that encouraged immorality. By 1794, they pointed to the Reign of Terror during the French Revolution as confirmation of their predictions. Romantic philosophers argued that the Enlightenment's excessive dependence on reason was a mistake that it perpetuated, disregarding the bonds of history, myth, faith, and tradition that were necessary to hold society together.[166] Ritchie Robertson portrays it as a grand intellectual and political program, offering a \"science\" of society modeled on the powerful physical laws of Newton. \"Social science\" was seen as the instrument of human improvement. It would expose truth and expand human happiness.[167] The rights of women and nonwhite people were generally overlooked in Enlightenment philosophy, which is often explicitly Eurocentric.[15] Scientific racism first emerged at this time, bringing together traditional racism and new research methods.[168][169]  During the Enlightenment, concepts of monogenism and polygenism became popular, though they would only be systematized epistemologically during the 19th century. Monogenism contends that all races have a single origin, while polygenism is the idea that each race has a separate origin. Until the 18th century, the words \"race\" and \"species\" were interchangeable.[168] The classification of non-European peoples as sub-human and irrational served to justify European dominance.[c][170]:\u200a4,\u200a10 The term \"Enlightenment\" emerged in English in the latter part of the 19th century,[171] with particular reference to French philosophy, as the equivalent of the French term Lumi\u00e8res (used first by Jean-Baptiste Dubos in 1733 and already well established by 1751). From Kant's 1784 essay \"Beantwortung der Frage: Was ist Aufkl\u00e4rung?\" (\"Answering the Question: What is Enlightenment?\"), the German term became Aufkl\u00e4rung (aufkl\u00e4ren = to illuminate; sich aufkl\u00e4ren = to clear up). However, scholars have never agreed on a definition of the Enlightenment or on its chronological or geographical extent. Terms like les Lumi\u00e8res (French), illuminismo (Italian), ilustraci\u00f3n (Spanish) and Aufkl\u00e4rung (German) referred to partly overlapping movements. Not until the late 19th century did English scholars agree they were talking about \"the Enlightenment.\"[166][172] Enlightenment historiography began in the period itself, from what Enlightenment figures said about their work. A dominant element was the intellectual angle they took. Jean le Rond d'Alembert's Preliminary Discourse of l'Encyclop\u00e9die provides a history of the Enlightenment which comprises a chronological list of developments in the realm of knowledge\u2014of which the Encyclop\u00e9die forms the pinnacle.[173] In 1783, Mendelssohn referred to Enlightenment as a process by which man was educated in the use of reason.[174][d] Kant called Enlightenment \"man's release from his self-incurred tutelage,\" tutelage being \"man's inability to make use of his understanding without direction from another.\"[175] \"For Kant, Enlightenment was mankind's final coming of age, the emancipation of the human consciousness from an immature state of ignorance.\"[176] The German scholar Ernst Cassirer called the Enlightenment \"a part and a special phase of that whole intellectual development through which modern philosophic thought gained its characteristic self-confidence and self-consciousness.\"[177] According to historian Roy Porter, the liberation of the human mind from a dogmatic state of ignorance, is the epitome of what the Age of Enlightenment was trying to capture.[176] Bertrand Russell saw the Enlightenment as a phase in a progressive development which began in antiquity and that reason and challenges to the established order were constant ideals throughout that time.[178] Russell said that the Enlightenment was ultimately born out of the Protestant reaction against the Catholic Counter-Reformation and that philosophical views such as affinity for democracy against monarchy originated among 16th-century Protestants to justify their desire to break away from the Catholic Church. Although many of these philosophical ideals were picked up by Catholics, Russell argues that by the 18th century the Enlightenment was the principal manifestation of the schism that began with Martin Luther.[178] Jonathan Israel rejects the attempts of postmodern and Marxian historians to understand the revolutionary ideas of the period purely as by-products of social and economic transformations.[179] He instead focuses on the history of ideas in the period from 1650 to the end of the 18th century and claims that it was the ideas themselves that caused the change that eventually led to the revolutions of the latter half of the 18th century and the early 19th century.[180] Israel argues that until the 1650s Western civilization \"was based on a largely shared core of faith, tradition, and authority.\"[181] There is little consensus on the beginning of the Age of Enlightenment, though several historians and philosophers argue that it was marked by Descartes' 1637 philosophy of Cogito, ergo sum (\"I think, therefore I am\"), which shifted the epistemological basis from external authority to internal certainty.[182][183][184] In France, many cited the publication of Newton's Principia Mathematica (1687),[185] which built upon the work of earlier scientists and formulated the laws of motion and universal gravitation.[186] French historians usually place the Si\u00e8cle des Lumi\u00e8res (\"Century of Enlightenments\") between 1715 and 1789: from the beginning of the reign of Louis XV until the French Revolution.[187] Most scholars use the last years of the century, often choosing the French Revolution or the beginning of the Napoleonic Wars (1804) as a convenient point in time with which to date the end of the Enlightenment.[188] In recent years, scholars have expanded the time span and global perspective of the Enlightenment by examining: (1) how European intellectuals did not work alone and other people helped spread and adapt Enlightenment ideas, (2) how Enlightenment ideas were \"a response to cross-border interaction and global integration,\" and (3) how the Enlightenment \"continued throughout the nineteenth century and beyond.\"[10] The Enlightenment \"was not merely a history of diffusion\" and \"was the work of historical actors around the world... who invoked the term... for their own specific purposes.\"[10] In their 1947 book Dialectic of Enlightenment, Frankfurt School philosophers Max Horkheimer and Theodor W. Adorno, both wartime exiles from Nazi Germany, critiqued the supposed rational basis of the modern world: Enlightenment, understood in the widest sense as the advance of thought, has always aimed at liberating human beings from fear and installing them as masters. Yet the wholly enlightened earth radiates under the sign of disaster triumphant.[189] Extending Horkheimer and Adorno's argument, intellectual historian Jason Josephson Storm argues that any idea of the Age of Enlightenment as a clearly defined period that is separate from the earlier Renaissance and later Romanticism or Counter-Enlightenment constitutes a myth. Storm points out that there are vastly different and mutually contradictory periodizations of the Enlightenment depending on nation, field of study, and school of thought; that the term and category of \"Enlightenment\" referring to the Scientific Revolution was actually applied after the fact; that the Enlightenment did not see an increase in disenchantment or the dominance of the mechanistic worldview; and that a blur in the early modern ideas of the humanities and natural sciences makes it hard to circumscribe a Scientific Revolution.[190] Storm defends his categorization of the Enlightenment as \"myth\" by noting the regulative role ideas of a period of Enlightenment and disenchantment play in modern Western culture, such that belief in magic, spiritualism, and even religion appears somewhat taboo in intellectual strata.[191] In the 1970s, study of the Enlightenment expanded to include the ways Enlightenment ideas spread to European colonies and how they interacted with indigenous cultures and how the Enlightenment took place in formerly unstudied areas such as Italy, Greece, the Balkans, Poland, Hungary, and Russia.[192][193] Intellectuals such as Robert Darnton and J\u00fcrgen Habermas have focused on the social conditions of the Enlightenment. Habermas described the creation of the \"bourgeois public sphere\" in 18th-century Europe, containing the new venues and modes of communication allowing for rational exchange. Habermas said that the public sphere was bourgeois, egalitarian, rational, and independent from the state, making it the ideal venue for intellectuals to critically examine contemporary politics and society, away from the interference of established authority. While the public sphere is generally an integral component of the social study of the Enlightenment, other historians[e] have questioned whether the public sphere had these characteristics. In contrast to the intellectual historiographical approach of the Enlightenment, which examines the various currents or discourses of intellectual thought within the European context during the 17th and 18th centuries, the cultural (or social) approach examines the changes that occurred in European society and culture. This approach studies the process of changing sociabilities and cultural practices during the Enlightenment. One of the primary elements of the culture of the Enlightenment was the rise of the public sphere, a \"realm of communication marked by new arenas of debate, more open and accessible forms of urban public space and sociability, and an explosion of print culture,\" in the late 17th century and 18th century.[194] Elements of the public sphere included that it was egalitarian, that it discussed the domain of \"common concern,\" and that argument was founded on reason.[195] Habermas uses the term \"common concern\" to describe those areas of political/social knowledge and discussion that were previously the exclusive territory of the state and religious authorities, now open to critical examination by the public sphere. The values of this bourgeois public sphere included holding reason to be supreme, considering everything to be open to criticism (the public sphere is critical), and the opposition of secrecy of all sorts.[196] The creation of the public sphere has been associated with two long-term historical trends: the rise of the modern nation state and the rise of capitalism. The modern nation state in its consolidation of public power created by counterpoint a private realm of society independent of the state, which allowed for the public sphere. Capitalism also increased society's autonomy and self-awareness, as well as an increasing need for the exchange of information. As the nascent public sphere expanded, it embraced a large variety of institutions, and the most commonly cited were coffee houses and caf\u00e9s, salons and the literary public sphere, figuratively localized in the Republic of Letters.[198][199] In France, the creation of the public sphere was helped by the aristocracy's move from the king's palace at Versailles to Paris in about 1720, since their rich spending stimulated the trade in luxuries and artistic creations, especially fine paintings.[200] The context for the rise of the public sphere was the economic and social change commonly associated with the Industrial Revolution: \"Economic expansion, increasing urbanization, rising population and improving communications in comparison to the stagnation of the previous century.\"[201] Rising efficiency in production techniques and communication lowered the prices of consumer goods and increased the amount and variety of goods available to consumers (including the literature essential to the public sphere). Meanwhile, the colonial experience (most European states had colonial empires in the 18th century) began to expose European society to extremely heterogeneous cultures, leading to the breaking down of \"barriers between cultural systems, religious divides, gender differences and geographical areas.\"[202] The word \"public\" implies the highest level of inclusivity\u2014the public sphere by definition should be open to all. However, this sphere was only public to relative degrees. Enlightenment thinkers frequently contrasted their conception of the \"public\" with that of the people: Condorcet contrasted \"opinion\" with populace, Marmontel \"the opinion of men of letters\" with \"the opinion of the multitude\" and d'Alembert the \"truly enlightened public\" with \"the blind and noisy multitude.\"[203] Additionally, most institutions of the public sphere excluded both women and the lower classes.[204] Cross-class influences occurred through noble and lower class participation in areas such as the coffeehouses and the Masonic lodges. Because of the focus on reason over superstition, the Enlightenment cultivated the arts.[205] Emphasis on learning, art, and music became more widespread, especially with the growing middle class. Areas of study such as literature, philosophy, science, and the fine arts increasingly explored subject matter to which the general public, in addition to the previously more segregated professionals and patrons, could relate.[206] As musicians depended more on public support, public concerts became increasingly popular and helped supplement performers' and composers' incomes. The concerts also helped them to reach a wider audience. Handel, for example, epitomized this with his highly public musical activities in London. He gained considerable fame there with performances of his operas and oratorios. The music of Haydn and Mozart, with their Viennese Classical styles, are usually regarded as being the most in line with the Enlightenment ideals.[207] The desire to explore, record, and systematize knowledge had a meaningful impact on music publications. Rousseau's Dictionnaire de musique (published 1767 in Geneva and 1768 in Paris) was a leading text in the late 18th century.[207] This widely available dictionary gave short definitions of words like genius and taste and was clearly influenced by the Enlightenment movement. Another text influenced by Enlightenment values was Charles Burney's A General History of Music: From the Earliest Ages to the Present Period (1776), which was a historical survey and an attempt to rationalize elements in music systematically over time.[208] Recently, musicologists have shown renewed interest in the ideas and consequences of the Enlightenment. For example, Rose Rosengard Subotnik's Deconstructive Variations (subtitled Music and Reason in Western Society) compares Mozart's Die Zauberfl\u00f6te (1791) using the Enlightenment and Romantic perspectives and concludes that the work is \"an ideal musical representation of the Enlightenment.\"[208] As the economy and the middle class expanded, there was an increasing number of amateur musicians. One manifestation of this involved women, who became more involved with music on a social level. Women were already engaged in professional roles as singers and increased their presence in the amateur performers' scene, especially with keyboard music.[206] Music publishers began to print music that amateurs could understand and play. The majority of the works that were published were for keyboard, voice and keyboard, and chamber ensemble.[206] After these initial genres were popularized, from the mid-century on, amateur groups sang choral music, which then became a new trend for publishers to capitalize on. The increasing study of the fine arts, as well as access to amateur-friendly published works, led to more people becoming interested in reading and discussing music. Music magazines, reviews, and critical works which suited amateurs as well as connoisseurs began to surface.[206] The philosophes spent a great deal of energy disseminating their ideas among educated men and women in cosmopolitan cities. They used many venues, some of them quite new. The term \"Republic of Letters\" was coined in 1664 by Pierre Bayle in his journal Nouvelles de la Republique des Lettres. Towards the end of the 18th century, the editor of Histoire de la R\u00e9publique des Lettres en France, a literary survey, described the Republic of Letters as being: In the midst of all the governments that decide the fate of men; in the bosom of so many states, the majority of them despotic\u00a0... there exists a certain realm which holds sway only over the mind\u00a0... that we honor with the name Republic, because it preserves a measure of independence, and because it is almost its essence to be free. It is the realm of talent and of thought.[209] The Republic of Letters was the sum of a number of Enlightenment ideals: an egalitarian realm governed by knowledge that could act across political boundaries and rival state power.[209] It was a forum that supported \"free public examination of questions regarding religion or legislation.\"[210] Kant considered written communication essential to his conception of the public sphere; once everyone was a part of the \"reading public,\" then society could be said to be enlightened.[210][211] The people who participated in the Republic of Letters, such as Diderot and Voltaire, are frequently known today as important Enlightenment figures. Indeed, the men who wrote Diderot's Encyclop\u00e9die arguably formed a microcosm of the larger \"republic.\"[212] Many women played an essential part in the French Enlightenment because of the role they played as salonni\u00e8res in Parisian salons, as the contrast to the male philosophes. The salon was the principal social institution of the republic[213] and \"became the civil working spaces of the project of Enlightenment.\" Women, as salonni\u00e8res, were \"the legitimate governors of [the] potentially unruly discourse\" that took place within.[214] While women were marginalized in the public culture of the Old Regime, the French Revolution destroyed the old cultural and economic restraints of patronage and corporatism (medieval guilds), opening French society to female participation, particularly in the literary sphere.[215] In France, the established men of letters (gens de lettres) had fused with the elites (les grands) of French society by the mid-18th century. This led to the creation of an oppositional literary sphere, Grub Street, the domain of a \"multitude of versifiers and would-be authors.\"[216] These men came to London to become authors only to discover that the literary market could not support large numbers of writers, who in any case were very poorly remunerated by the publishing-bookselling guilds.[217] The writers of Grub Street, the Grub Street Hacks, were left feeling bitter about the relative success of the men of letters[218] and found an outlet for their literature which was typified by the libelle. Written mostly in the form of pamphlets, the libelles \"slandered the court, the Church, the aristocracy, the academies, the salons, everything elevated and respectable, including the monarchy itself.\"[219] Le Gazetier cuirass\u00e9 by Charles Th\u00e9veneau de Morande was a prototype of the genre. It was Grub Street literature that was most read by the public during the Enlightenment.[220] According to Darnton, more importantly the Grub Street hacks inherited the \"revolutionary spirit\" once displayed by the philosophes and paved the way for the French Revolution by desacralizing figures of political, moral, and religious authority in France.[221] The increased consumption of reading materials of all sorts was one of the key features of the \"social\" Enlightenment. Developments in the Industrial Revolution allowed consumer goods to be produced in greater quantities at lower prices, encouraging the spread of books, pamphlets, newspapers, and journals \u2013 \"media of the transmission of ideas and attitudes.\" Commercial development likewise increased the demand for information, along with rising populations and increased urbanisation.[222] However, demand for reading material extended outside of the realm of the commercial and outside the realm of the upper and middle classes, as evidenced by the biblioth\u00e8que bleue. Literacy rates are difficult to gauge, but in France the rates doubled over the course of the 18th century.[223] Reflecting the decreasing influence of religion, the number of books about science and art published in Paris doubled from 1720 to 1780, while the number of books about religion dropped to just one-tenth of the total.[34] Reading underwent serious changes in the 18th century. In particular, Rolf Engelsing has argued for the existence of a reading revolution. Until 1750, reading was done intensively: people tended to own a small number of books and read them repeatedly, often to small audience. After 1750, people began to read \"extensively,\" finding as many books as they could, increasingly reading them alone.[224][225] This is supported by increasing literacy rates, particularly among women.[226] The vast majority of the reading public could not afford to own a private library, and while most of the state-run \"universal libraries\" set up in the 17th and 18th centuries were open to the public, they were not the only sources of reading material. On one end of the spectrum was the biblioth\u00e8que bleue, a collection of cheaply produced books published in Troyes, France. Intended for a largely rural and semi-literate audience these books included almanacs, retellings of medieval romances and condensed versions of popular novels, among other things. While some historians have argued against the Enlightenment's penetration into the lower classes, the biblioth\u00e8que bleue represents at least a desire to participate in Enlightenment sociability.[227] Moving up the classes, a variety of institutions offered readers access to material without needing to buy anything. Libraries that lent out their material for a small price started to appear, and occasionally bookstores would offer a small lending library to their patrons. Coffee houses commonly offered books, journals, and sometimes even popular novels to their customers. Tatler and The Spectator, two influential periodicals sold from 1709 to 1714, were closely associated with coffee house culture in London, being both read and produced in various establishments in the city.[228] This is an example of the triple or even quadruple function of the coffee house: reading material was often obtained, read, discussed, and even produced on the premises.[229] It is difficult to determine what people actually read during the Enlightenment. For example, examining the catalogs of private libraries gives an image skewed in favor of the classes wealthy enough to afford libraries and also ignores censored works unlikely to be publicly acknowledged. For this reason, a study of publishing would be much more fruitful for discerning reading habits.[230] Across continental Europe, but in France especially, booksellers and publishers had to negotiate censorship laws of varying strictness. For example, the Encyclop\u00e9die narrowly escaped seizure and had to be saved by Malesherbes, the man in charge of the French censor. Indeed, many publishing companies were conveniently located outside France so as to avoid overzealous French censors. They would smuggle their merchandise across the border, where it would then be transported to clandestine booksellers or small-time peddlers.[231] The records of clandestine booksellers may give a better representation of what literate Frenchmen might have truly read, since their clandestine nature provided a less restrictive product choice.[232] In one case, political books were the most popular category, primarily libels and pamphlets. Readers were more interested in sensationalist stories about criminals and political corruption than they were in political theory itself. The second most popular category, \"general works\" (those books \"that did not have a dominant motif and that contained something to offend almost everyone in authority\"), demonstrated a high demand for generally low-brow subversive literature. However, these works never became part of literary canon and are largely forgotten today as a result.[232] A healthy, legal publishing industry existed throughout Europe, although established publishers and book sellers occasionally ran afoul of the law. For example, the Encyclop\u00e9die, condemned by both the King and Clement XII, nevertheless found its way into print with the help of the aforementioned Malesherbes and creative use of French censorship law.[233] However, many works were sold without running into any legal trouble at all. Borrowing records from libraries in England, Germany, and North America indicate that more than 70% of books borrowed were novels. Less than 1% of the books were of a religious nature, indicating the general trend of declining religiosity.[209] A genre that greatly rose in importance was that of scientific literature. Natural history in particular became increasingly popular among the upper classes. Works of natural history include Ren\u00e9-Antoine Ferchault de R\u00e9aumur's Histoire naturelle des insectes and Jacques Gautier d'Agoty's La Myologie compl\u00e8te, ou description de tous les muscles du corps humain (1746). Outside Ancien R\u00e9gime France, natural history was an important part of medicine and industry, encompassing the fields of botany, zoology, meteorology, hydrology, and mineralogy. Students in Enlightenment universities and academies were taught these subjects to prepare them for careers as diverse as medicine and theology. As shown by Matthew Daniel Eddy, natural history in this context was a very middle class pursuit and operated as a fertile trading zone for the interdisciplinary exchange of diverse scientific ideas.[234] The target audience of natural history was French upper class, evidenced more by the specific discourse of the genre than by the generally high prices of its works. Naturalists catered to upper class desire for erudition: many texts had an explicit instructive purpose. However, natural history was often a political affair. As Emma Spary writes, the classifications used by naturalists \"slipped between the natural world and the social\u00a0... to establish not only the expertise of the naturalists over the natural, but also the dominance of the natural over the social.\"[235] The idea of taste (le go\u00fbt) was a social indicator: to truly be able to categorize nature, one had to have the proper taste, an ability of discretion shared by all members of the upper class. In this way, natural history spread many of the scientific developments of the time but also provided a new source of legitimacy for the dominant class.[236] From this basis, naturalists could then develop their own social ideals based on their scientific works.[237] The first scientific and literary journals were established during the Enlightenment. The first journal, the Parisian Journal des s\u00e7avans, appeared in 1665. However, it was not until 1682 that periodicals began to be more widely produced. French and Latin were the dominant languages of publication, but there was also a steady demand for material in German and Dutch. There was generally low demand for English publications on the continent, which was echoed by England's similar lack of desire for French works. Languages commanding less of an international market\u2014such as Danish, Spanish, and Portuguese\u2014found journal success more difficult, and a more international language was used instead. French slowly took over Latin's status as the lingua franca of learned circles. This in turn gave precedence to the publishing industry in Holland, where the vast majority of these French language periodicals were produced.[238] Jonathan Israel called the journals the most influential cultural innovation of European intellectual culture.[239] They shifted the attention of the \"cultivated public\" away from established authorities to novelty and innovation, and instead promoted the Enlightened ideals of toleration and intellectual objectivity. Being a source of knowledge derived from science and reason, they were an implicit critique of existing notions of universal truth monopolized by monarchies, parliaments, and religious authorities. They also advanced Christian Enlightenment that upheld \"the legitimacy of God-ordained authority\"\u2014the Bible\u2014in which there had to be agreement between the biblical and natural theories.[240] Although the existence of dictionaries and encyclopedias spanned into ancient times, the texts changed from defining words in a long running list to far more detailed discussions of those words in 18th-century encyclopedic dictionaries.[241] The works were part of an Enlightenment movement to systematize knowledge and provide education to a wider audience than the elite. As the 18th century progressed, the content of encyclopedias also changed according to readers' tastes. Volumes tended to focus more strongly on secular affairs, particularly science and technology, rather than matters of theology. Along with secular matters, readers also favoured an alphabetical ordering scheme over cumbersome works arranged along thematic lines.[242] Commenting on alphabetization, the historian Charles Porset has said that \"as the zero degree of taxonomy, alphabetical order authorizes all reading strategies; in this respect it could be considered an emblem of the Enlightenment.\" For Porset, the avoidance of thematic and hierarchical systems thus allows free interpretation of the works and becomes an example of egalitarianism.[243] Encyclopedias and dictionaries also became more popular during the Age of Enlightenment as the number of educated consumers who could afford such texts began to multiply.[241] In the latter half of the 18th century, the number of dictionaries and encyclopedias published by decade increased from 63 between 1760 and 1769 to approximately 148 in the decade proceeding the French Revolution.[244] Along with growth in numbers, dictionaries and encyclopedias also grew in length, often having multiple print runs that sometimes included in supplemented editions.[242] The first technical dictionary was drafted by John Harris and entitled Lexicon Technicum: Or, An Universal English Dictionary of Arts and Sciences. Harris' book avoids theological and biographical entries and instead concentrates on science and technology. Published in 1704, the Lexicon Technicum was the first book to be written in English that took a methodical approach to describing mathematics and commercial arithmetic along with the physical sciences and navigation. Other technical dictionaries followed Harris' model, including Ephraim Chambers' Cyclopaedia (1728), which included five editions and is a substantially larger work than Harris'. The folio edition of the work even included foldout engravings. The Cyclopaedia emphasized Newtonian theories, Lockean philosophy and contained thorough examinations of technologies, such as engraving, brewing, and dyeing. In Germany, practical reference works intended for the uneducated majority became popular in the 18th century. The Marperger Curieuses Natur-, Kunst-, Berg-, Gewerk- und Handlungs-Lexicon (1712) explained terms that usefully described the trades and scientific and commercial education. Jablonksi Allgemeines Lexicon (1721) was better known than the Handlungs-Lexicon and underscored technical subjects rather than scientific theory. For example, over five columns of text were dedicated to wine while geometry and logic were allocated only twenty-two and seventeen lines, respectively. The first edition of the Encyclop\u00e6dia Britannica (1771) was modelled along the same lines as the German lexicons.[245] However, the prime example of reference works that systematized scientific knowledge in the Age of Enlightenment were universal encyclopedias rather than technical dictionaries. It was the goal of universal encyclopedias to record all human knowledge in a comprehensive reference work.[246] The most well-known of these works is Diderot and d'Alembert's Encyclop\u00e9die, ou dictionnaire raisonn\u00e9 des sciences, des arts et des m\u00e9tiers. The work, which began publication in 1751, was composed of 35 volumes and over 71,000 separate entries. A great number of the entries were dedicated to describing the sciences and crafts in detail and provided intellectuals across Europe with a high-quality survey of human knowledge. In d'Alembert's Preliminary Discourse to the Encyclopedia of Diderot, the work's goal to record the extent of human knowledge in the arts and sciences is outlined: As an Encyclop\u00e9die, it is to set forth as well as possible the order and connection of the parts of human knowledge. As a Reasoned Dictionary of the Sciences, Arts, and Trades, it is to contain the general principles that form the basis of each science and each art, liberal or mechanical, and the most essential facts that make up the body and substance of each.[247] The massive work was arranged according to a \"tree of knowledge.\" The tree reflected the marked division between the arts and sciences, which was largely a result of the rise of empiricism. Both areas of knowledge were united by philosophy, or the trunk of the tree of knowledge. The Enlightenment's desacrilization of religion was pronounced in the tree's design, particularly where theology accounted for a peripheral branch, with black magic as a close neighbour.[248] As the Encyclop\u00e9die gained popularity, it was published in quarto and octavo editions after 1777. The quarto and octavo editions were much less expensive than previous editions, making the Encyclop\u00e9die more accessible to the non-elite. Robert Darnton estimates that there were approximately 25,000 copies of the Encyclop\u00e9die in circulation throughout France and Europe before the French Revolution.[249] The extensive yet affordable encyclopedia came to represent the transmission of Enlightenment and scientific education to an expanding audience.[250] One of the most important developments that the Enlightenment era brought to the discipline of science was its popularization. An increasingly literate population seeking knowledge and education in both the arts and the sciences drove the expansion of print culture and the dissemination of scientific learning. The new literate population was precipitated by a high rise in the availability of food; this enabled many people to rise out of poverty, and instead of paying more for food, they had money for education.[251] Popularization was generally part of an overarching Enlightenment ideal that endeavoured \"to make information available to the greatest number of people.\"[252] As public interest in natural philosophy grew during the 18th century, public lecture courses and the publication of popular texts opened up new roads to money and fame for amateurs and scientists who remained on the periphery of universities and academies.[253] More formal works included explanations of scientific theories for individuals lacking the educational background to comprehend the original scientific text. Newton's celebrated Philosophiae Naturalis Principia Mathematica was published in Latin and remained inaccessible to readers without education in the classics until Enlightenment writers began to translate and analyze the text in the vernacular. The first significant work that expressed scientific theory and knowledge expressly for the laity, in the vernacular and with the entertainment of readers in mind, was Bernard de Fontenelle's Conversations on the Plurality of Worlds (1686). The book was produced specifically for women with an interest in scientific writing and inspired a variety of similar works.[254] These popular works were written in a discursive style, which was laid out much more clearly for the reader than the complicated articles, treatises, and books published by the academies and scientists. Charles Leadbetter's Astronomy (1727) was advertised as \"a Work entirely New\" that would include \"short and easie  [sic] Rules and Astronomical Tables.\"[255] The first French introduction to Newtonianism and the Principia was El\u00e9ments de la philosophie de Newton, published by Voltaire in 1738.[256] \u00c9milie du Ch\u00e2telet's translation of the Principia, published after her death in 1756, also helped to spread Newton's theories beyond scientific academies and the university.[257] Writing for a growing female audience, Francesco Algarotti published Il Newtonianism per le dame, which was a tremendously popular work and was translated from Italian into English by Elizabeth Carter. A similar introduction to Newtonianism for women was produced by Henry Pemberton. His A View of Sir Isaac Newton's Philosophy was published by subscription. Extant records of subscribers show that women from a wide range of social standings purchased the book, indicating the growing number of scientifically inclined female readers among the middling class.[258] During the Enlightenment, women also began producing popular scientific works. Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years in eleven editions.[259] Most work on the Enlightenment emphasizes the ideals discussed by intellectuals, rather than the actual state of education at the time. Leading educational theorists like England's John Locke and Switzerland's Jean Jacques Rousseau both emphasized the importance of shaping young minds early. By the late Enlightenment, there was a rising demand for a more universal approach to education, particularly after the American Revolution and the French Revolution. The predominant educational psychology from the 1750s onward, especially in northern European countries, was associationism: the notion that the mind associates or dissociates ideas through repeated routines. In addition to being conducive to Enlightenment ideologies of liberty, self-determination, and personal responsibility, it offered a practical theory of the mind that allowed teachers to transform longstanding forms of print and manuscript culture into effective graphic tools of learning for the lower and middle orders of society.[260] Children were taught to memorize facts through oral and graphic methods that originated during the Renaissance.[261] Many of the leading universities associated with Enlightenment progressive principles were located in northern Europe, with the most renowned being the universities of Leiden, G\u00f6ttingen, Halle, Montpellier, Uppsala, and Edinburgh. These universities, especially Edinburgh, produced professors whose ideas had a significant impact on Britain's North American colonies and later the American Republic. Within the natural sciences, Edinburgh's medical school also led the way in chemistry, anatomy, and pharmacology.[234] In other parts of Europe, the universities and schools of France and most of Europe were bastions of traditionalism and were not hospitable to the Enlightenment. In France, the major exception was the medical university at Montpellier.[262] The history of Academies in France during the Enlightenment begins with the Academy of Science, founded in 1666 in Paris. It was closely tied to the French state, acting as an extension of a government seriously lacking in scientists. It helped promote and organize new disciplines and it trained new scientists. It also contributed to the enhancement of scientists' social status, considering them to be the \"most useful of all citizens.\" Academies demonstrate the rising interest in science along with its increasing secularization, as evidenced by the small number of clerics who were members (13%).[264] The presence of the French academies in the public sphere cannot be attributed to their membership, as although the majority of their members were bourgeois, the exclusive institution was only open to elite Parisian scholars. They perceived themselves as \"interpreters of the sciences for the people.\" For example, it was with this in mind that academicians took it upon themselves to disprove the popular pseudo-science of mesmerism.[265] The strongest contribution of the French Academies to the public sphere comes from the concours acad\u00e9miques (roughly translated as \"academic contests\") they sponsored throughout France. These academic contests were perhaps the most public of any institution during the Enlightenment.[266] The practice of contests dated back to the Middle Ages and was revived in the mid-17th century. The subject matter had previously been generally religious and/or monarchical, featuring essays, poetry, and painting. However, by roughly 1725 this subject matter had radically expanded and diversified, including \"royal propaganda, philosophical battles, and critical ruminations on the social and political institutions of the Old Regime.\" Topics of public controversy were also discussed such as the theories of Newton and Descartes, the slave trade, women's education, and justice in France.[267] More importantly, the contests were open to all, and the enforced anonymity of each submission guaranteed that neither gender nor social rank would determine the judging. Indeed, although the \"vast majority\" of participants belonged to the wealthier strata of society (\"the liberal arts, the clergy, the judiciary and the medical profession\"), there were some cases of the popular classes submitting essays and even winning.[268] Similarly, a significant number of women participated\u2014and won\u2014the competitions. Of a total of 2,300 prize competitions offered in France, women won 49\u2014perhaps a small number by modern standards but very significant in an age in which very few women had any academic training. Indeed, the majority of the winning entries were for poetry competitions, a genre commonly stressed in women's education.[269] In England, the Royal Society of London played a significant role in the public sphere and the spread of Enlightenment ideas. It was founded by a group of independent scientists and given a royal charter in 1662.[270] The society played a large role in spreading Robert Boyle's experimental philosophy around Europe and acted as a clearinghouse for intellectual correspondence and exchange.[271] Boyle was \"a founder of the experimental world in which scientists now live and operate\" and his method based knowledge on experimentation, which had to be witnessed to provide proper empirical legitimacy. This is where the Royal Society came into play: witnessing had to be a \"collective act\" and the Royal Society's assembly rooms were ideal locations for relatively public demonstrations.[272] However, not just any witness was considered to be credible: \"Oxford professors were accounted more reliable witnesses than Oxfordshire peasants.\" Two factors were taken into account: a witness's knowledge in the area and a witness's \"moral constitution.\" In other words, only civil society were considered for Boyle's public.[273] Salons were places where philosophes were reunited and discussed old, actual, or new ideas. This led to salons being the birthplace of intellectual and enlightened ideas. Coffeehouses were especially important to the spread of knowledge during the Enlightenment because they created a unique environment in which people from many different walks of life gathered and shared ideas. They were frequently criticized by nobles who feared the possibility of an environment in which class and its accompanying titles and privileges were disregarded. Such an environment was especially intimidating to monarchs who derived much of their power from the disparity between classes of people. If the different classes joined together under the influence of Enlightenment thinking, they might recognize the all-encompassing oppression and abuses of their monarchs and because of the numbers of their members might be able to successfully revolt. Monarchs also resented the idea of their subjects convening as one to discuss political matters, especially matters of foreign affairs. Rulers thought political affairs were their business only, a result of their divine right to rule.[274] Coffeeshops became homes away from home for many who sought to engage in discourse with their neighbors and discuss intriguing and thought-provoking matters, from philosophy to politics. Coffeehouses were essential to the Enlightenment, for they were centers of free-thinking and self-discovery. Although many coffeehouse patrons were scholars, many were not. Coffeehouses attracted a diverse set of people, including the educated wealthy and bourgeois as well as the lower classes. Patrons, being doctors, lawyers, merchants, represented almost all classes, so the coffeeshop environment sparked fear in those who wanted to preserve class distinction. One of the most popular critiques of the coffeehouse said that it \"allowed promiscuous association among people from different rungs of the social ladder, from the artisan to the aristocrat\" and was therefore compared to Noah's Ark, receiving all types of animals, clean and unclean.[275] This unique culture served as a catalyst for journalism, when Joseph Addison and Richard Steele recognized its potential as an audience. Together, Steele and Addison published The Spectator (1711), a daily publication which aimed, through fictional narrator Mr. Spectator, to both entertain and provoke discussion on serious philosophical matters. The first English coffeehouse opened in Oxford in 1650. Brian Cowan said that Oxford coffeehouses developed into \"penny universities,\" offering a locus of learning that was less formal than at structured institutions. These penny universities occupied a significant position in Oxford academic life, as they were frequented by those consequently referred to as the virtuosi, who conducted their research on some of the premises. According to Cowan, \"the coffeehouse was a place for like-minded scholars to congregate, to read, as well as learn from and to debate with each other, but was emphatically not a university institution, and the discourse there was of a far different order than any university tutorial.\"[276] The Caf\u00e9 Procope was established in Paris in 1686, and by the 1720s there were around 400 caf\u00e9s in the city. The Caf\u00e9 Procope in particular became a center of Enlightenment, welcoming such celebrities as Voltaire and Rousseau. The Caf\u00e9 Procope was where Diderot and D'Alembert decided to create the Encyclop\u00e9die.[277] The caf\u00e9s were one of the various \"nerve centers\" for bruits publics, public noise or rumour. These bruits were allegedly a much better source of information than were the actual newspapers available at the time.[278] The debating societies are an example of the public sphere during the Enlightenment.[279] Their origins include: In the late 1770s, popular debating societies began to move into more \"genteel\" rooms, a change which helped establish a new standard of sociability.[281] The backdrop to these developments was \"an explosion of interest in the theory and practice of public elocution.\" The debating societies were commercial enterprises that responded to this demand, sometimes very successfully. Some societies welcomed from 800 to 1,200 spectators per night.[282] The debating societies discussed an extremely wide range of topics. Before the Enlightenment, most intellectual debates revolved around \"confessional\"\u2014that is, Catholic, Lutheran, Reformed (Calvinist) or Anglican issues, debated primarily to establish which bloc of faith ought to have the \"monopoly of truth and a God-given title to authority.\"[283] After Enlightenment, everything that previously had been rooted in tradition was questioned, and often replaced by new concepts. After the second half of the 17th century and during the 18th century, a \"general process of rationalization and secularization set in\" and confessional disputes were reduced to a secondary status in favor of the \"escalating contest between faith and incredulity.\"[283] In addition to debates on religion, societies discussed issues such as politics and the role of women. However, the critical subject matter of these debates did not necessarily translate into opposition to the government; the results of the debate quite frequently upheld the status quo.[284] From a historical standpoint, one of the most important features of the debating society was their openness to the public, as women attended and even participated in almost every debating society, which were likewise open to all classes providing they could pay the entrance fee. Once inside, spectators were able to participate in a largely egalitarian form of sociability that helped spread Enlightenment ideas.[285] Historians have debated the extent to which the secret network of Freemasonry was a main factor in the Enlightenment.[286] Leaders of the Enlightenment included Freemasons such as Diderot, Montesquieu, Voltaire, Lessing, Pope,[287] Horace Walpole, Robert Walpole, Mozart, Goethe, Frederick the Great, Benjamin Franklin[288] and George Washington.[289] Norman Davies said Freemasonry was a powerful force on behalf of liberalism in Europe from about 1700 to the twentieth century. It expanded during the Enlightenment, reaching practically every country in Europe. It was especially attractive to powerful aristocrats and politicians as well as intellectuals, artists, and political activists.[290] During the Enlightenment, Freemasons comprised an international network of like-minded men, often meeting in secret in ritualistic programs at their lodges. They promoted the ideals of the Enlightenment and helped diffuse these values across Britain, France, and other places. Freemasonry as a systematic creed with its own myths, values, and rituals originated in Scotland c.\u20091600 and spread to England and then across the Continent in the 18th century. They fostered new codes of conduct\u2014including a communal understanding of liberty and equality inherited from guild sociability\u2014\"liberty, fraternity, and equality.\"[291] Scottish soldiers and Jacobite Scots brought to the Continent ideals of fraternity, which reflected not the local system of Scottish customs, but the institutions and ideals originating in the English Revolution against royal absolutism.[292] Freemasonry was particularly prevalent in France\u2014by 1789, there were perhaps as many as 100,000 French Masons, making Freemasonry the most popular of all Enlightenment associations.[293] The Freemasons displayed a passion for secrecy and created new degrees and ceremonies. Similar societies, partially imitating Freemasonry, emerged in France, Germany, Sweden, and Russia. One example was the Illuminati, founded in Bavaria in 1776, which was copied after the Freemasons, but was never part of the movement. The name itself translates to \"enlightened,\" chosen to reflect their original intent to promote the values of the movement. The Illuminati was an overtly political group, which most Masonic lodges decidedly were not.[294] Masonic lodges created a private model for public affairs. They \"reconstituted the polity and established a constitutional form of self-government, complete with constitutions and laws, elections, and representatives.\" In other words, the micro-society set up within the lodges constituted a normative model for society as a whole. This was especially true on the continent: when the first lodges began to appear in the 1730s, their embodiment of British values was often seen as threatening by state authorities. For example, the Parisian lodge that met in the mid 1720s was composed of English Jacobite exiles.[295] Furthermore, freemasons across Europe explicitly linked themselves to the Enlightenment as a whole. For example, in French lodges the line \"As the means to be enlightened I search for the enlightened\" was a part of their initiation rites. British lodges assigned themselves the duty to \"initiate the unenlightened.\" This did not necessarily link lodges to the irreligious, but neither did this exclude them from the occasional heresy. In fact, many lodges praised the Grand Architect, the masonic terminology for the deistic divine being who created a scientifically ordered universe.[296] German historian Reinhart Koselleck claimed: \"On the Continent there were two social structures that left a decisive imprint on the Age of Enlightenment: the Republic of Letters and the Masonic lodges.\"[297] Scottish professor Thomas Munck argues that \"although the Masons did promote international and cross-social contacts which were essentially non-religious and broadly in agreement with enlightened values, they can hardly be described as a major radical or reformist network in their own right.\"[298] Many of the Masons values seemed to greatly appeal to Enlightenment values and thinkers. Diderot discusses the link between Freemason ideals and the enlightenment in D'Alembert's Dream, exploring masonry as a way of spreading enlightenment beliefs.[299] Historian Margaret Jacob stresses the importance of the Masons in indirectly inspiring enlightened political thought.[300] On the negative side, Daniel Roche contests claims that Masonry promoted egalitarianism and he argues the lodges only attracted men of similar social backgrounds.[301] The presence of noble women in the French \"lodges of adoption\" that formed in the 1780s was largely due to the close ties shared between these lodges and aristocratic society.[302] The major opponent of Freemasonry was the Catholic Church so in countries with a large Catholic element, such as France, Italy, Spain, and Mexico, much of the ferocity of the political battles involve the confrontation between what Davies calls the reactionary Church and enlightened Freemasonry.[303][304] Even in France, Masons did not act as a group.[305] American historians, while noting that Benjamin Franklin and George Washington were indeed active Masons, have downplayed the importance of Freemasonry in causing the American Revolution because the Masonic order was non-political and included both Patriots and their enemy the Loyalists.[306] At the same time, the Classical art of Greece and Rome became interesting to people again, since archaeological teams discovered Pompeii and Herculaneum.[307] For up to Descartes ... a particular sub-iectum ... lies at the foundation of its own fixed qualities and changing circumstances. The superiority of a sub-iectum ... arises out of the claim of man to a ... self-supported, unshakeable foundation of truth, in the sense of certainty. Why and how does this claim acquire its decisive authority? The claim originates in that emancipation of man in which he frees himself from obligation to Christian revelational truth and Church doctrine to a legislating for himself that takes its stand upon itself.",
      "ground_truth_chunk_ids": [
        "130_fixed_chunk1"
      ],
      "source_ids": [
        "S130"
      ],
      "category": "factual",
      "id": 31
    },
    {
      "question": "What is Decision theory?",
      "ground_truth": "Decision theory or the theory of rational choice is a branch of probability, economics, and analytic philosophy that uses expected utility and probability to model how individuals would behave rationally under uncertainty.[1][2] It differs from the cognitive and behavioral sciences in that it is mainly prescriptive and concerned with identifying optimal decisions for a rational agent, rather than describing how people actually make decisions. Despite this, the field is important to the study of real human behavior by social scientists, as it lays the foundations to mathematically model and analyze individuals in fields such as sociology, economics, criminology, cognitive science, moral philosophy and political science.[citation needed] The roots of decision theory lie in probability theory, developed by Blaise Pascal and Pierre de Fermat in the 17th century, which was later refined by others like Christiaan Huygens. These developments provided a framework for understanding risk and uncertainty, which are central to decision-making. In the 18th century, Daniel Bernoulli introduced the concept of \"expected utility\" in the context of gambling, which was later formalized by John von Neumann and Oskar Morgenstern in the 1940s. Their work on Game Theory and Expected Utility Theory helped establish a rational basis for decision-making under uncertainty. After World War II, decision theory expanded into economics, particularly with the work of economists like Milton Friedman and others, who applied it to market behavior and consumer choice theory. This era also saw the development of Bayesian decision theory, which incorporates Bayesian probability into decision-making models. By the late 20th century, scholars like Daniel Kahneman and Amos Tversky challenged the assumptions of rational decision-making. Their work in behavioral economics highlighted cognitive biases and heuristics that influence real-world decisions, leading to the development of prospect theory, which modified expected utility theory by accounting for psychological factors. Normative decision theory is",
      "expected_answer": "Decision theory or the theory of rational choice is a branch of probability, economics, and analytic philosophy that uses expected utility and probability to model how individuals would behave rationally under uncertainty.[1][2] It differs from the cognitive and behavioral sciences in that it is mainly prescriptive and concerned with identifying optimal decisions for a rational agent, rather than describing how people actually make decisions. Despite this, the field is important to the study of real human behavior by social scientists, as it lays the foundations to mathematically model and analyze individuals in fields such as sociology, economics, criminology, cognitive science, moral philosophy and political science.[citation needed] The roots of decision theory lie in probability theory, developed by Blaise Pascal and Pierre de Fermat in the 17th century, which was later refined by others like Christiaan Huygens. These developments provided a framework for understanding risk and uncertainty, which are central to decision-making. In the 18th century, Daniel Bernoulli introduced the concept of \"expected utility\" in the context of gambling, which was later formalized by John von Neumann and Oskar Morgenstern in the 1940s. Their work on Game Theory and Expected Utility Theory helped establish a rational basis for decision-making under uncertainty. After World War II, decision theory expanded into economics, particularly with the work of economists like Milton Friedman and others, who applied it to market behavior and consumer choice theory. This era also saw the development of Bayesian decision theory, which incorporates Bayesian probability into decision-making models. By the late 20th century, scholars like Daniel Kahneman and Amos Tversky challenged the assumptions of rational decision-making. Their work in behavioral economics highlighted cognitive biases and heuristics that influence real-world decisions, leading to the development of prospect theory, which modified expected utility theory by accounting for psychological factors. Normative decision theory is concerned with identification of optimal decisions where optimality is often determined by considering an ideal decision maker who is able to calculate with perfect accuracy and is in some sense fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis and is aimed at finding tools, methodologies, and software (decision support systems) to help people make better decisions.[3][4] In contrast, descriptive decision theory is concerned with describing observed behaviors often under the assumption that those making decisions are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework (e.g. stochastic transitivity axioms), reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).[3][4] Prescriptive decision theory is concerned with predictions about behavior that positive decision theory produces to allow for further tests of the kind of decision-making that occurs in practice. In recent decades, there has also been increasing interest in \"behavioral decision theory\", contributing to a re-evaluation of what useful decision-making requires.[5][6] The area of choice under uncertainty represents the heart of decision theory. Known from the 17th century (Blaise Pascal invoked it in his famous wager, which is contained in his Pens\u00e9es, published in 1670), the idea of expected value is that, when faced with a number of actions, each of which could give rise to more than one possible outcome with different probabilities, the rational procedure is to identify all possible outcomes, determine their values (positive or negative) and the probabilities that will result from each course of action, and multiply the two to give an \"expected value\", or the average expectation for an outcome; the action to be chosen should be the one that gives rise to the highest total expected value. In 1738, Daniel Bernoulli published an influential paper entitled Exposition of a New Theory on the Measurement of Risk, in which he uses the St. Petersburg paradox to show that expected value theory must be normatively wrong. He gives an example in which a Dutch merchant is trying to decide whether to insure a cargo being sent from Amsterdam to St. Petersburg in winter. In his solution, he defines a utility function and computes expected utility rather than expected financial value.[7] In the 20th century, interest was reignited by Abraham Wald's 1939 paper pointing out that the two central procedures of sampling-distribution-based statistical-theory, namely hypothesis testing and parameter estimation, are special cases of the general decision problem.[8] Wald's paper renewed and synthesized many concepts of statistical theory, including loss functions, risk functions, admissible decision rules, antecedent distributions, Bayesian procedures, and minimax procedures. The phrase \"decision theory\" itself was used in 1950 by E. L. Lehmann.[9] The revival of subjective probability theory, from the work of Frank Ramsey, Bruno de Finetti, Leonard Savage and others, extended the scope of expected utility theory to situations where subjective probabilities can be used. At the time, von Neumann and Morgenstern's theory of expected utility[10] proved that expected utility maximization followed from basic postulates about rational behavior. The work of Maurice Allais and Daniel Ellsberg showed that human behavior has systematic and sometimes important departures from expected-utility maximization (Allais paradox and Ellsberg paradox).[11] The prospect theory of Daniel Kahneman and Amos Tversky renewed the empirical study of economic behavior with less emphasis on rationality presuppositions. It describes a way by which people make decisions when all of the outcomes carry a risk.[12] Kahneman and Tversky found three regularities \u2013 in actual human decision-making, \"losses loom larger than gains\"; people focus more on changes in their utility-states than they focus on absolute utilities; and the estimation of subjective probabilities is severely biased by anchoring. Intertemporal choice is concerned with the kind of choice where different actions lead to outcomes that are realized at different stages over time.[13] It is also described as cost-benefit decision making since it involves the choices between rewards that vary according to magnitude and time of arrival.[14] If someone received a windfall of several thousand dollars, they could spend it on an expensive holiday, giving them immediate pleasure, or they could invest it in a pension scheme, giving them an income at some time in the future. What is the optimal thing to do? The answer depends partly on factors such as the expected rates of interest and inflation, the person's life expectancy, and their confidence in the pensions industry. However even with all those factors taken into account, human behavior again deviates greatly from the predictions of prescriptive decision theory, leading to alternative models in which, for example, objective interest rates are replaced by subjective discount rates.[citation needed] Some decisions are difficult because of the need to take into account how other people in the situation will respond to the decision that is taken. The analysis of such social decisions is often treated under decision theory, though it involves mathematical methods. In the emerging field of socio-cognitive engineering, the research is especially focused on the different types of distributed decision-making in human organizations, in normal and abnormal/emergency/crisis situations.[15] Other areas of decision theory are concerned with decisions that are difficult simply because of their complexity, or the complexity of the organization that has to make them. Individuals making decisions are limited in resources (i.e. time and intelligence) and are therefore boundedly rational; the issue is thus, more than the deviation between real and optimal behavior, the difficulty of determining the optimal behavior in the first place. Decisions are also affected by whether options are framed together or separately; this is known as the distinction bias.[citation needed] Heuristics are procedures for making a decision without working out the consequences of every option. Heuristics decrease the amount of evaluative thinking required for decisions, focusing on some aspects of the decision while ignoring others.[16] While quicker than step-by-step processing, heuristic thinking is also more likely to involve fallacies or inaccuracies.[17] One example of a common and erroneous thought process that arises through heuristic thinking is the gambler's fallacy \u2014 believing that an isolated random event is affected by previous isolated random events. For example, if flips of a fair coin give repeated tails, the coin still has the same probability (i.e., 0.5) of tails in future turns, though intuitively it might seems that heads becomes more likely.[18] In the long run, heads and tails should occur equally often; people commit the gambler's fallacy when they use this heuristic to predict that a result of heads is \"due\" after a run of tails.[19] Another example is that decision-makers may be biased towards preferring moderate alternatives to extreme ones. The compromise effect operates under a mindset that the most moderate option carries the most benefit. In an incomplete information scenario, as in most daily decisions, the moderate option will look more appealing than either extreme, independent of the context, based only on the fact that it has characteristics that can be found at either extreme.[20] A highly controversial issue is whether one can replace the use of probability in decision theory with something else. Advocates for the use of probability theory point to: The proponents of fuzzy logic, possibility theory, Dempster\u2013Shafer theory, and info-gap decision theory maintain that probability is only one of many alternatives and point to many examples where non-standard alternatives have been implemented with apparent success. Notably, probabilistic decision theory can sometimes be sensitive to assumptions about the probabilities of various events, whereas non-probabilistic rules, such as minimax, are robust in that they do not make such assumptions. A general criticism of decision theory based on a fixed universe of possibilities is that it considers the \"known unknowns\", not the \"unknown unknowns\":[21] it focuses on expected variations, not on unforeseen events, which some argue have outsized impact and must be considered \u2013 significant events may be \"outside model\". This[which?] line of argument, called the ludic fallacy, is that there are inevitable imperfections in modeling the real world by particular models, and that unquestioning reliance on models blinds one to their limits.",
      "ground_truth_chunk_ids": [
        "163_fixed_chunk1"
      ],
      "source_ids": [
        "S163"
      ],
      "category": "factual",
      "id": 32
    },
    {
      "question": "What is Tanya Plibersek?",
      "ground_truth": "Tanya Joan Pliber\u0161ek[a] (born 2 December 1969) is an Australian politician who has served as Minister for Social Services since 2025 and the member of parliament (MP) for the New South Wales division of Sydney since 1998. Previously, she served as the Minister for the Environment and Water from 2022 to 2025, deputy leader of the Labor Party from 2013 to 2019, and held ministerial offices in the Rudd and Gillard governments. Plibersek was born in Sydney to Slovenian immigrant parents and grew up in Sutherland Shire. She has degrees from the University of Technology Sydney and Macquarie University, and worked in the NSW Government's Domestic Violence Unit before entering parliament. Plibersek was elected to the Division of Sydney at the 1998 federal election, aged 28. She joined the shadow cabinet in 2004, and when Labor won the 2007 election was made Minister for Housing and Minister for the Status of Women. In a cabinet reshuffle in 2010, Plibersek was made Minister for Human Services and Minister for Social Inclusion. She was promoted to Minister for Health the following year, and held that position until Labor's defeat at the 2013 election. Plibersek was then elected as deputy to new ALP leader Bill Shorten. Plibersek served as deputy opposition leader until Labor's defeat at the 2019 Australian federal election. She was subsequently made shadow minister for education under new opposition leader Anthony Albanese. Upon Labor's victory at the 2022 Australian federal election, she was appointed Minister for the Environment and Water. After the 2025 federal election she was made Minister for Social Services in the Second Albanese ministry. She is a senior figure in the Labor Left faction. Plibersek was born in Sydney, the youngest of three children born to Joseph and Rose Plibersek. Her elder brother Ray is a lawyer,",
      "expected_answer": "Tanya Joan Pliber\u0161ek[a] (born 2 December 1969) is an Australian politician who has served as Minister for Social Services since 2025 and the member of parliament (MP) for the New South Wales division of Sydney since 1998. Previously, she served as the Minister for the Environment and Water from 2022 to 2025, deputy leader of the Labor Party from 2013 to 2019, and held ministerial offices in the Rudd and Gillard governments. Plibersek was born in Sydney to Slovenian immigrant parents and grew up in Sutherland Shire. She has degrees from the University of Technology Sydney and Macquarie University, and worked in the NSW Government's Domestic Violence Unit before entering parliament. Plibersek was elected to the Division of Sydney at the 1998 federal election, aged 28. She joined the shadow cabinet in 2004, and when Labor won the 2007 election was made Minister for Housing and Minister for the Status of Women. In a cabinet reshuffle in 2010, Plibersek was made Minister for Human Services and Minister for Social Inclusion. She was promoted to Minister for Health the following year, and held that position until Labor's defeat at the 2013 election. Plibersek was then elected as deputy to new ALP leader Bill Shorten. Plibersek served as deputy opposition leader until Labor's defeat at the 2019 Australian federal election. She was subsequently made shadow minister for education under new opposition leader Anthony Albanese. Upon Labor's victory at the 2022 Australian federal election, she was appointed Minister for the Environment and Water. After the 2025 federal election she was made Minister for Social Services in the Second Albanese ministry. She is a senior figure in the Labor Left faction. Plibersek was born in Sydney, the youngest of three children born to Joseph and Rose Plibersek. Her elder brother Ray is a lawyer, and her eldest brother Phillip (d. 1997) was a geologist. Her parents were born in small Slovenian villages, arriving in Australia unknown to each other as part of the post-war immigration scheme. Her mother (n\u00e9e Rosalija Repi\u010d) was born in Podvinci, and came to Australia via Italy. Her father (n\u00e9 Jo\u017ee Pliber\u0161ek) was born in Ko\u010dno pri Polskavi, and came to Australia via Austria. He found work as a labourer on the Snowy Mountains Scheme, and later spent 20 years working for Qantas as a plumber and gas fitter.[1][2] Plibersek grew up in the suburb of Oyster Bay in Sydney's Sutherland Shire. She attended Oyster Bay Public School and Jannali Girls High School, where she was the dux. She joined the Labor Party at the age of 15.[1] Plibersek studied journalism at the University of Technology Sydney, graduating with a Bachelor of Arts in communications. She then took a Masters in Public Policy and Politics at Macquarie University.[3][4] After a failed attempt to secure a cadetship with the Australian Broadcasting Corporation (ABC), she found work with the Domestic Violence Unit at the New South Wales Government's Office for the Status and Advancement of Women.[3] She found working with the state women's minister Kerry Chikarovski \"demoralising\" and later criticised her for focusing on the glass ceiling rather than other women's issues. Plibersek subsequently joined the office of Senator Bruce Childs, before switching to work for Senator George Campbell as a research officer.[5] Plibersek was elected to the House of Representatives at the 1998 federal election, aged 28,[6] retaining the Division of Sydney for the ALP following the retirement of Peter Baldwin. With the support of George Campbell's \"hard left\" faction, she won preselection for the seat against twelve other candidates, including ten other women. In the lead-up to the ballot she \"wrote to each branch member three or four times, attended branch meetings virtually every night, gave talks to community groups, and contributed to three candidates' debates\".[5] Plibersek supported Kim Beazley's unsuccessful candidacies in the 2003 ALP leadership votes, where he initially lost to Simon Crean and then later to Mark Latham.[7] In July 2003 she and Anthony Albanese publicly criticised Crean for his rejection of the party's policy on a Second Sydney Airport.[8] After the 2004 election, Plibersek was elected to Latham's shadow ministry and allocated three portfolios \u2013 youth; the status of women; and work and family, community and early childhood education. In June 2005, after Latham was succeeded as opposition leader by Beazley, she retained the youth and status of women portfolios and was given responsibility for childcare.[6] Upon the release of The Latham Diaries she described him as \"a negative and critical person\".[9] Plibersek publicly supported Beazley against Kevin Rudd in the 2006 leadership spill, though was retained in Rudd's shadow ministry after his defeat of Beazley, with the portfolios of youth; the status of women; and human services and housing.[6] Following the 2007 federal election, Plibersek was appointed Minister for Housing and Minister for the Status of Women in the First Rudd Ministry. Following the 2010 federal election, Plibersek was appointed Minister for Human Services and Minister for Social Inclusion. Her appointment took effect following the birth of Plibersek's youngest son Louis, and soon afterwards Plibersek directed the Human Services response to the 2010\u201311 Queensland floods. As Minister for Human Services, Plibersek established emergency and recovery centres to provide urgent support to flood-affected communities.[citation needed] As Minister for Housing, Plibersek established the National Rental Affordability Scheme to build 50,000 affordable rental homes, invested $6\u00a0billion in social housing to build 21,600 new homes and repair 80,000 homes, and provided $550\u00a0million for homelessness services. The new housing was built ahead of time and under budget. She also established the Housing Affordability Fund and First Home Saver Accounts.[10] In December 2008, along with Kevin Rudd, Prime Minister at that time, Plibersek released the Government's White Paper on Homelessness, The Road Home, which expressed a goal of halving homelessness by 2020.[11] This goal was abandoned by the incoming Abbott government which cut homelessness funding and ended the National Rental Affordability Scheme and First Home Saver Accounts.[citation needed] As Minister for the Status of Women, Plibersek convened the National Council to Reduce Violence against Women and their Children in May 2008, and released the National Council's Plan for Australia to Reduce Violence Against Women and their Children in March 2009 \u2013 the first in Australian history.[12][13] Plibersek also addressed the 2009 United Nations International Women's Day event, attended by United Nations Secretary General Ban Ki-moon, and announced Australia's formal accession to the United Nations Optional Protocol to the Convention on the Elimination of All Forms of Discrimination Against Women (CEDAW).[14] Plibersek said that acceding to the Optional Protocol \"will send a strong message that Australia is serious about promoting gender equality and that we are prepared to be judged by international human rights standards.\"[15] Plibersek played a key role in designing and securing Australia's first paid parental leave scheme. Plibersek also strengthened Australia's anti-people trafficking strategy, with a focus on better supporting victims including women in the sex industry.[16] As Shadow Minister for Women, Plibersek also committed the Labor Opposition to expanding access to abortions by making termination services legal in all taxpayer-funded hospitals. This policy was later dumped from Labor's election commitments under Albanese.[17] In Opposition, Plibersek supported women's economic security by committing the Labor Opposition to the removal of the $450 per month minimum income threshold for eligibility for the superannuation guarantee, a policy later adopted by the Liberal Government.[18] Plibersek also committed to forcing companies with over one thousand employees to disclose the gap in salaries between male and female staff. This was dismissed by the Morrison Liberal Government at the time as setting \"one set of employees against another\".[19] As Minister for Health Plibersek established Grow Up Smiling, a $4\u00a0billion package to support better dental care for children, which expanded Medicare-subsided dental check-ups for children from age 2 to 17. She introduced free Gardasil vaccinations, previously only available for girls, for boys to protect against cancers caused by HPV \u2013 a world first.[20] Along with the previous Health Minister, Nicola Roxon, Plibersek also implemented world-leading plain packaging of tobacco laws which saw smoking rates drop to 13%.[citation needed] Plibersek also added the RU486 abortion pill to the PBS, improving access to reproductive healthcare.[21] When Plibersek was Minister for Health, Australia achieved the best 5-year cancer survival rates in the world. Plibersek also delivered 1,300 more hospital beds and 60,000 additional doctors, nurses and allied health professionals. She also oversaw the funding, construction and/or opening of a number of new facilities, including the Chris O\u2019Brien Lifehouse Cancer Centre, the Kinghorn Cancer Centre, the Launceston Multi-Purpose Health Centre, and a new medical and dental school as well as new facilities for the South Australian Health and Medical Research Institute (SAHMRI) in Adelaide.[citation needed] Plibersek further added\u202f87 new medicines\u202fto the\u202fPharmaceutical Benefits Scheme (PBS)\u202fbetween\u202f2012\u20132013. introduced the\u202fLiving Longer Living Better reforms,[22] allocating\u202f$1.2 billion\u202fto improve\u202faged care services\u202fbetween\u202f2012\u20132013, approved the\u202ffirst rapid HIV test\u202ffor point-of-care use in Australia, providing results within an hour.[23] Plibersek was unanimously elected deputy leader of the Labor Party (and thus Deputy Leader of the Opposition) on 14 October 2013, following the leadership election that had seen Bill Shorten succeed Kevin Rudd as leader.[24] She was Shadow Minister for Foreign Affairs and International Development until July 2016. Following the 2016 election, she was made Shadow Minister for Education and Shadow Minister for Women.[25] Shorten said handing Plibersek the education portfolio was \"about putting a great policy thinker on the political frontline\".[26] Plibersek was re-elected at the 2019 election with a swing of 5.7 points to the Labor Party. Following the party's defeat at the federal election of 2019 and Bill Shorten's immediate resignation as party leader, Plibersek made it known that she was interested in standing in the leadership election, and was supported by Shorten and former prime minister Julia Gillard; however, she concluded that \"now is not my time\", citing family responsibilities.[27][28][29] After Anthony Albanese's victory in the leadership contest, Plibersek was appointed Shadow Minister for Education and Training in his new shadow cabinet.[30] In January 2021 Albanese also appointed her Shadow Minister for Women.[31] Plibersek was appointed Minister for the Environment and Water in the first Albanese ministry.[32] Upon Peter Dutton's election as Leader of the Opposition, Plibersek compared Dutton to Lord Voldemort. In a radio interview, she stated: \"I think there will be a lot of children who have watched a lot of Harry Potter films who will be very frightened of what they are seeing on TV at night. I am saying he looks a bit like Voldemort. We will see whether he can do what he promised he would do when he was last running for leader, which is smile more.\"[33] Plibersek later apologised. In a radio interview, Dutton called the claims \"unfortunate\" but \"water off a duck's back\", also noting that he wasn't \"bald by choice\" and was diagnosed with a skin condition several years ago.[33] Plibersek's apology was welcomed by newly elected Prime Minister Anthony Albanese in an interview with ABC News.[33] In November 2024, Plibersek reached an agreement with the Greens and independent senator David Pocock to pass legislation to create a national environment protection agency. However, the legislation was scrapped at the last minute after intervention from Prime Minister Albanese.[34][35][36] After the re-election of the Albanese government in the 2025 federal election, Plibersek was moved to the role of Minister for Social Services in the second Albanese ministry.[37] Plibersek is a member of the Labor Party's left faction. Plibersek has argued that government should actively invest in the economy to promote growth and equality, calling for a federal commitment to a policy of full employment where \"Australians who can work, can get a job\". During the coronavirus recession of 2020, she has advocated government stimulus over tax cuts for high income earners. In September 2020, she explained her priorities for economic recovery: \"We need to be building things in Australia to support both the skilled trades people and the apprentices that we should be training right now. We need to build things. We need to make things. We need to care for people. We need secure jobs with decent pay\". Education Plibersek is a long-term supporter of investing in education. As the shadow Minister for Education and Training she has developed policies across schools, universities, TAFE and vocational education. Prior to the 2019 election, these policies included increasing school funding by $14 billion over a decade, to the creation of a new Evidence Institute for Schools and instituting a review of the country's NAPLAN testing system. In higher education Plibersek promised to reintroduce the demand driven system of university funding, creating an extra 200,000 places for students. Energy Plibersek is a strong supporter of renewable energy and transitioning towards clean energy production. She has argued that the renewables industry is key to promoting new jobs, assisting local manufacturing, lowering carbon emissions and reducing power prices. She has also endorsed programs to help households install solar panels on their homes \u2013 which have been adopted by over two and a half million Australia households. In 2018, Plibersek argued against providing federal subsidies for new coal fired power plants. Housing\nAfter the 2007 election, Plibersek was made Federal Minister for Housing in the Rudd government. As part of the government's response to the 2008 financial crisis, Plibersek implemented several policies that both grew the housing stock and stimulated the Australian economy. These policies included the First Home Owners Boost, providing up to $21,000 for people buying new dwellings, the National Rental Affordability Scheme, providing incentives for investors to build properties for low and middle income Australians, and $6 billion for the construction, repair and improvement of social housing. Plibersek also released the Homelessness White Paper, which set out a comprehensive national plan to tackle homelessness in Australia with significant funding attached. Plibersek has argued that significant new investment in social and public housing should be part of Australia's response to the coronavirus economic downturn. Welfare Plibersek supports an increase to Newstart, Australia's then-unemployment benefit, arguing that the current rate is too low, \"trapping people in poverty\" who are \"just surviving\" on an allowance of $40 a day. Women's reproductive rights Plibersek is pro-choice. As Minister for Health, Plibersek approved listing the abortion drug RU-486 on the Pharmaceutical Benefits Scheme. Plibersek described the provision of the medicine as \"a good thing in the situation where women are faced with one of the most difficult decisions that they will ever make\".[38] Anti-abortion groups criticised the move, with one campaigner, Margaret Tighe, labelling it a \"gross abuse of power.\"[39] Other commentators, including Clementine Ford, labelled the decision \"progressive\".[40][41] First Nations people Plibersek supports instituting an Indigenous Voice to Parliament, based on the recommendations made in the Uluru Statement from the Heart. As Deputy Leader of the Labor Party, she stated that implementing the Indigenous Voice was her party's priority in Indigenous affairs, alongside \u2018closing the gap\u2019, particularly in health and education. An Australian republic Plibersek supports Australia becoming a Republic. As a first term Member of Parliament in 1999, she campaigned for the yes vote in the constitutional referendum to replace the Queen and Governor-General with a President appointed by a two-thirds majority of the members of the Commonwealth Parliament. LGBT rights From the 1990s onward, Plibersek campaigned for the removal of discrimination against same-sex de facto couples from federal legislation, raising the issue formally in Parliament on multiple occasions during her parliamentary career (including 1999,[42] 2006,[43] and 2008[44]). In her regular paid advertisement in the South Sydney Herald, Plibersek wrote that \"The passing of these reforms to federal legislation was one of the proudest moments of my time in the Australian Parliament\"[45] and she has marched in the Sydney Gay and Lesbian Mardi Gras Parade almost every year for three decades.[46] As deputy leader, Plibersek led the push to make support for same-sex marriage binding Labor policy[47] which resulted in many Labor MPs speaking out publicly in support of same-sex marriage. Plibersek seconded a private members bill to legalise same sex marriage, moved by Labor leader Bill Shorten. She opposed the 2017 postal plebiscite, arguing it was unnecessary and divisive, but campaigned strongly for the \u2018yes\u2019 vote during the plebiscite campaign. Multiculturalism and citizenship As an Australian with Slovenian heritage, Plibersek is vocal advocate for multiculturalism. On an episode of Q&A in 2018 she clashed with conservative Senator Matthew Canavan on the topic of immigration, arguing against Canavan's claim that immigrants congregated through \u2018ghettoisation\u2019. Before the 2019 election, Plibersek pledged $8\u00a0million towards community language schools, to expand language training for children. In January 2020 Plibersek aroused controversy in an Australia Day speech, calling for children to learn the Australian citizenship pledge at school.[48][49] In the speech, Plibersek argued that progressives should feel more comfortable with the concept of patriotism: \"You can be proud of your citizenship and dedicated to progress. You can cherish this nation and want to make it better. You can be a progressive and love your country: I certainly do.\" Foreign aid As Shadow Minister for Foreign Affairs, Plibersek opposed the cuts to foreign aid made by the Abbott Liberal government. At the 2016 election Labor promised to reverse those cuts if elected. Iraq Plibersek opposed the 2003 invasion of Iraq.[50] In 2003, when then-US President George W. Bush visited Australia, Tanya presented national security adviser Condoleezza Rice with a letter, signed by 43 Labor MPs, explaining why Labor parliamentarians opposed Australia invading Iraq without United Nations approval.[51] She also stated in Parliament, \"I do not support an attack on Iraq. I particularly do not support a pre-emptive first strike. Nor do I support any action that is initiated by the US alone rather than being sanctioned by the United Nations.\"[50] East Timor While Shadow Minister for Foreign Affairs in 2016, Plibersek proposed that Australia redraw its maritime border with East Timor. According to the Sydney Morning Herald, \"Ms Plibersek lamented that Australia's pivotal role in securing East Timor's independence \u2013 \"a proud moment\" \u2013 was being tarnished by its refusal to negotiate a new, permanent maritime boundary with East Timor. \"The maritime boundary dispute has poisoned relations with our newest neighbour. This must change for their sake and ours,\" Ms Plibersek said.\"[52]\nThis position was later adopted by the Liberal government, and a new border agreement treaty was signed in 2018. Israel Speaking in the House of Representatives on 17 September 2002, Plibersek said: \"I can think of a rogue state which consistently ignores UN resolutions, whose ruler is a war criminal responsible for the massacres of civilians in refugee camps outside its borders. The US supports and funds this country. This year it gave it a blank cheque to continue its repression of its enemies. It uses US military hardware to bulldoze homes and kill civilians. It is called Israel, and the war criminal is Ariel Sharon. Needless to say, the US does not mention the UN resolutions that Israel has ignored for 30 years; it just continues sending the money...\"[53] Plibersek's remarks again gained prominence in October 2013, after she and Bill Shorten were elected as deputy leader and leader of the Labor Party, respectively. After choosing to take on the foreign affairs portfolio while in opposition, Liberal Party MP Julie Bishop, then Minister for Foreign Affairs, said Plibersek should \"publicly retract those statements\". The Australian noted that Plibersek's appointment was likely to be criticised by the Jewish community in Australia.[54] But the Executive Council of Australian Jewry expressed satisfaction in Plibersek's elevation to the deputy leadership, noting that she had \u2018developed friendly relations with the Jewish Community\u2019. Plibersek visited Israel and the State of Palestine in February 2014, meeting with the Prime Minister of Palestine, Rami Hamdallah.[55] Plibersek has held the following portfolios and parliamentary party positions since her election in 1998 (both shadow and government appointments are listed):[6] Plibersek lives in Sydney with her husband Michael Coutts-Trotter, who is a senior NSW public servant, and three children.[56] Following the 2010 federal election, when Labor retained government with the support of the Australian Greens and independents, parliamentary numbers were finely balanced. After some controversy, Plibersek was granted a pair by the Coalition so that her absence from the House of Representatives while on maternity leave did not affect the result of votes.[57] She gave birth to her son later that year.[58][59] In September 2016, her older brother Ray Plibersek was elected to Sutherland Shire council representing C Ward for the Australian Labor Party.[60] Plibersek is fond of bushwalking and Jane Austen[61] novels.",
      "ground_truth_chunk_ids": [
        "24_random_chunk1"
      ],
      "source_ids": [
        "S224"
      ],
      "category": "factual",
      "id": 33
    },
    {
      "question": "What is World Wide Web?",
      "ground_truth": "The World Wide Web (also known as WWW, W3, or simply the Web)[2] is a public interconnected information system that enables content sharing over the Internet.[3] It facilitates access to documents and other web resources according to specific rules of the Hypertext Transfer Protocol (HTTP).[4] The Web was invented by English computer scientist Tim Berners-Lee while at CERN in 1989 and opened to the public in 1993. It was conceived as a \"universal linked information system\".[5][6][7] Documents and other media content are made available to the network through web servers and can be accessed by programs such as web browsers. Servers and resources on the World Wide Web are identified and located through a character string called uniform resource locator (URL). The original and still very common document type is a web page formatted in Hypertext Markup Language (HTML). This markup language supports plain text, images, embedded video and audio contents, and scripts (short programs) that implement complex user interaction. The HTML language also supports hyperlinks (embedded URLs), which provide immediate access to other web resources. Web navigation, or web surfing, is the common practice of following such hyperlinks across multiple websites. Web applications are web pages that function as application software. The information on the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common domain name make up a website. A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organisations, government agencies, and individual users; and comprises an enormous amount of educational, entertainment, commercial, and government information. The World Wide Web has become the world's dominant information systems platform.[8][9][10][11] It is the primary tool that billions of",
      "expected_answer": "The World Wide Web (also known as WWW, W3, or simply the Web)[2] is a public interconnected information system that enables content sharing over the Internet.[3] It facilitates access to documents and other web resources according to specific rules of the Hypertext Transfer Protocol (HTTP).[4] The Web was invented by English computer scientist Tim Berners-Lee while at CERN in 1989 and opened to the public in 1993. It was conceived as a \"universal linked information system\".[5][6][7] Documents and other media content are made available to the network through web servers and can be accessed by programs such as web browsers. Servers and resources on the World Wide Web are identified and located through a character string called uniform resource locator (URL). The original and still very common document type is a web page formatted in Hypertext Markup Language (HTML). This markup language supports plain text, images, embedded video and audio contents, and scripts (short programs) that implement complex user interaction. The HTML language also supports hyperlinks (embedded URLs), which provide immediate access to other web resources. Web navigation, or web surfing, is the common practice of following such hyperlinks across multiple websites. Web applications are web pages that function as application software. The information on the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common domain name make up a website. A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organisations, government agencies, and individual users; and comprises an enormous amount of educational, entertainment, commercial, and government information. The World Wide Web has become the world's dominant information systems platform.[8][9][10][11] It is the primary tool that billions of people worldwide use to interact with the Internet.[4] The Web was invented by English computer scientist Tim Berners-Lee while working at CERN.[12][13] He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organisation, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common tree structure approach, used for instance in the existing CERNDOC documentation system and in the Unix filesystem, as well as approaches that relied on tagging files with keywords, as in the VAX/NOTES system. Instead, he adopted concepts he had put into practice with his private ENQUIRE system (1980), built at CERN. When he became aware of Ted Nelson's hypertext model (1965), in which documents can be linked in unconstrained ways through hyperlinks associated with \"hot spots\" embedded in the text, it helped to confirm the validity of his concept.[14][15] The model was later popularised by Apple's HyperCard system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to mutable data files, or even fire up programs on their server computer. He also conceived \"gateways\" that would allow access through the new system to documents organised in other ways (such as traditional computer file systems or the Usenet). Moreover, he insisted that the system should be decentralised, without any central control or coordination over the creation of links.[6][16][12][13] Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name.[6] He got a working system implemented by the end of 1990, including a browser called  WorldWideWeb (which became the name of the project and of the network) and an HTTP server running at CERN. As part of that development, he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format.[17] The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN and began to spread to other scientific and academic institutions. Within the next two years, there were 50 websites created.[18][19] CERN made the Web protocol and code available royalty free on 30 April 1993, enabling its widespread use.[20][21][22] After the NCSA released the Mosaic web browser later that year, the Web's popularity grew rapidly as thousands of websites sprang up in less than a year.[23][24] Mosaic was a graphical browser that could display inline images and submit forms that  were processed by the HTTPd server.[25][26] Marc Andreessen and Jim Clark founded Netscape the following year and released the Navigator browser, which introduced Java and JavaScript to the Web. It quickly became the dominant browser. Netscape became a public company in 1995, which triggered a frenzy for the Web and started the dot-com bubble.[27] Microsoft responded by developing its own browser, Internet Explorer, starting the browser wars. By bundling it with Windows, it became the dominant browser for 14 years.[28] Berners-Lee founded the World Wide Web Consortium (W3C) which created XML in 1996 and recommended replacing HTML with stricter XHTML.[29] In the meantime, developers began exploiting an IE feature called XMLHttpRequest to make Ajax applications and launched the Web 2.0 revolution. Mozilla, Opera, and Apple rejected XHTML and created the WHATWG which developed HTML5.[30] In 2009, the W3C conceded and abandoned XHTML.[31] In 2019, it ceded control of the HTML specification to the WHATWG.[32] The World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet.[33][34][35][11] Gopher was run by the University of Minnesota and the alternative to the World Wide Web. Tim Berners-Lee states that World Wide Web is officially spelled as three separate words, each capitalised, with no intervening hyphens.[44] Use of the www prefix has been declining, especially when web applications sought to brand their domain names and make them easily pronounceable. As the mobile web grew in popularity,[45] services like Gmail.com, Outlook.com, Myspace.com, Facebook.com and Twitter.com are most often mentioned without adding \"www.\" (or, indeed, \".com\") to the domain.[46] In English, www is usually read as double-u double-u double-u.[47] Some users pronounce it dub-dub-dub, particularly in New Zealand.[48] Stephen Fry, in his \"Podgrams\" series of podcasts, pronounces it wuh wuh wuh.[49] The English writer Douglas Adams once quipped in The Independent on Sunday (1999): \"The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for\".[50] The terms Internet and World Wide Web are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of computer networks interconnected through telecommunications and optical networking. In contrast, the World Wide Web is a global collection of documents and other resources, linked by hyperlinks and URIs. Web resources are accessed using HTTP or HTTPS, which are application-level Internet protocols that use the Internet transport protocols.[4] Viewing a web page on the World Wide Web normally begins either by typing the URL of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages\u2014and to move from one web page to another through hyperlinks\u2014came to be known as 'browsing,' 'web surfing' (after channel surfing), or 'navigating the Web'. Early studies of this new behaviour investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation, and targeted navigation.[51] The following example demonstrates the functioning of a web browser when accessing a page at the URL http://example.org/home.html. The browser resolves the server name of the URL (example.org) into an Internet Protocol address using the globally distributed Domain Name System (DNS). This lookup returns an IP address such as 203.0.113.4 or 2001:db8:2e::7334. The browser then requests the resource by sending an HTTP request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service, so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses port number 80 and, for HTTPS, it normally uses port number 443. The content of the HTTP request can be as simple as two lines of text: The computer receiving the HTTP request delivers it to the web server software listening for requests on port 80. If the web server can fulfil the request, it sends an HTTP response back to the browser indicating success: Followed by the content of the requested page. Hypertext Markup Language (HTML) for a basic web page might look like this: The web browser parses the HTML and interprets the markup (<title>, <p> for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media, scripts that affect page behaviour, and Cascading Style Sheets that affect page layout. The browser makes additional HTTP requests to the web server for these other Internet media types. As it receives its content from the web server, the browser progressively renders the page onto the screen as specified by its HTML and these additional resources. Hypertext Markup Language (HTML) is the standard markup language for creating web pages and web applications. With Cascading Style Sheets (CSS) and JavaScript, it forms a triad of cornerstone technologies for the World Wide Web.[52] Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document. HTML elements are the building blocks of HTML pages. With HTML constructs, images and other objects such as interactive forms may be embedded into the rendered page. HTML provides a means to create structured documents by denoting structural semantics for text such as headings, paragraphs, lists, links, quotes, and other items. HTML elements are delineated by tags, written using angle brackets. Tags such as <img /> and <input /> directly introduce content into the page. Other tags, such as <p>, surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page. HTML can embed programs written in a scripting language such as JavaScript, which affects the behaviour and content of web pages. Inclusion of CSS defines the look and layout of content. The World Wide Web Consortium (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997.[update][53] Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions, and other web resources. In the underlying HTML, a hyperlink is coded like this:\n<a href=\"http://example.org/home.html\">Example.org Homepage</a>. Such a collection of useful, related resources interconnected via hypertext links is dubbed a web of information. Publication on the Internet created what Tim Berners-Lee first called the WorldWideWeb (in its original CamelCase, which was subsequently discarded) in November 1990.[54] The hyperlink structure of the web is described by the webgraph: the nodes of the web graph correspond to the web pages (or URLs), the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called \"dead\" links. The ephemeral nature of the Web has prompted many efforts to archive websites. The Internet Archive, active since 1996, is the best known of such efforts. Many hostnames used for the World Wide Web begin with www because of the long-standing practice of naming Internet hosts according to the services they provide. The hostname of a web server is often www, in the same way that it may be ftp for an FTP server, and news or nntp for a Usenet news server. These hostnames appear as Domain Name System (DNS) or subdomain names, as in www.example.com. The use of www is not required by any technical or policy standard and many websites do not use it; the first web server was nxoc01.cern.ch.[55] According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of www as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending www to an institution's website domain name was subsequently copied.[56][better\u00a0source\u00a0needed] Many established websites still use the prefix, or they employ other subdomain names such as www2, secure or en for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the www subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different websites. The use of a subdomain name is useful for load balancing incoming web traffic by creating a CNAME record that points to a cluster of web servers. Since, currently[as of?], only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root.[57][dubious \u2013 discuss] When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix \"www\" to the beginning of it and possibly \".com\", \".org\" and \".net\" at the end, depending on what might be missing. For example, entering \"microsoft\" may be transformed to http://www.microsoft.com/ and \"openoffice\" to http://www.openoffice.org. This feature started appearing in early versions of Firefox, when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as Lynx.[58][unreliable source?] It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices.[59] The scheme specifiers http:// and https:// at the start of a web URI refer to Hypertext Transfer Protocol or HTTP Secure, respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.[citation needed] A web page (also written as webpage) is a document that is suitable for the World Wide Web and web browsers. A web browser displays a web page on a monitor or mobile device. The term web page usually refers to what is visible, but may also refer to the contents of the computer file itself, which is usually a text file containing hypertext written in HTML or a comparable markup language. Typical web pages provide hypertext for browsing to other web pages via hyperlinks, often referred to as links. Web browsers will frequently have to access multiple web resource elements, such as reading style sheets, scripts, and images, while presenting each web page. On a network, a web browser can retrieve a web page from a remote web server. The web server may restrict access to a private network, such as a corporate intranet. The web browser uses the Hypertext Transfer Protocol (HTTP) to make such requests to the web server. A static web page is delivered exactly as stored, as web content in the web server's file system. In contrast, a dynamic web page is generated by a web application, usually driven by server-side software. Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email, etc. A static web page (sometimes called a flat page/stationary page) is a web page that is delivered to the user exactly as stored, in contrast to dynamic web pages which are generated by a web application. Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a web server to negotiate content-type or language of the document where such versions are available and the server is configured to do so. A server-side dynamic web page is a web page whose construction is controlled by an application server processing server-side scripts. In server-side scripting, parameters determine how the assembly of every new web page proceeds, including the setting up of more client-side processing. A client-side dynamic web page processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via Document Object Model, or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way. A dynamic web page is then reloaded by the user or by a computer program to change some variable content. The updated information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a dynamic web page update using Ajax technologies will neither create a page to go back to nor truncate the web browsing history forward of the displayed page. Using Ajax technologies, the end user gets one dynamic page managed as a single page in the web browser while the actual web content rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM, the DOM, for its client, from an application server. Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not static web pages, though it has fallen out of common use since the popularisation of AJAX, a term which is now itself rarely used. Client-side scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.[citation needed] JavaScript is a scripting language that was initially developed in 1995 by Brendan Eich, then of Netscape, for use within web pages.[60] The standardised version is ECMAScript.[60] To make web pages more interactive, some web applications also use JavaScript techniques such as Ajax (asynchronous JavaScript and XML). Client-side script is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly poll the server to check whether new information is available.[61] A website[62] is a collection of related web resources including web pages, multimedia content, typically identified with a common domain name, and published on at least one web server. Notable examples are wikipedia.org, google.com, and amazon.com. A website may be accessible via a public Internet Protocol (IP) network, such as the Internet, or a private local area network (LAN), by referencing a uniform resource locator (URL) that identifies the site. Websites can have many functions and can be used in various fashions; a website can be a personal website, a corporate website for a company, a government website, an organisation website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and social networking to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an intranet. Web pages, which are the building blocks of websites, are documents, typically composed in plain text interspersed with formatting instructions of Hypertext Markup Language (HTML, XHTML). They may incorporate elements from other websites with suitable markup anchors. Web pages are accessed and transported with the Hypertext Transfer Protocol (HTTP), which may optionally employ encryption (HTTP Secure, HTTPS) to provide security and privacy for the user. The user's application, often a web browser, renders the page content according to its HTML markup instructions onto a display terminal. Hyperlinking between web pages conveys to the reader the site structure and guides the navigation of the site, which often starts with a home page containing a directory of the site web content. Some websites require user registration or subscription to access content. Examples of subscription websites include many business sites, news websites, academic journal websites, gaming websites, file-sharing websites, message boards, web-based email, social networking websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services. End users can access websites on a range of devices, including desktop and laptop computers, tablet computers, smartphones, and smart TVs. A web browser (commonly referred to as a browser) is a software user agent for accessing information on the World Wide Web. To connect to a website's server and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer. In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into websites. The most popular browsers are Chrome, Safari, Edge, Samsung Internet and Firefox.[63] A Web server is server software, or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over HTTP and several other related protocols. The primary function of a web server is to store, process and deliver web pages to clients.[64] The communication between client and server takes place using the Hypertext Transfer Protocol (HTTP). Pages delivered are most frequently HTML documents, which may include images, style sheets and scripts in addition to the text content. A user agent, commonly a web browser or web crawler, initiates communication by making a request for a specific resource using HTTP and the server responds with the content of that resource or an error message if unable to do so. The resource is typically a real file on the server's secondary storage, but this is not necessarily the case and depends on how the web server is implemented. While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting web forms, including uploading of files. Many generic web servers also support  scripting using Active Server Pages (ASP), PHP (Hypertext Preprocessor), or other scripting languages. This means that the behaviour of the web server can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents dynamically (\"on-the-fly\") as opposed to returning static documents. The former is primarily used for retrieving or modifying information from databases. The latter is typically much faster and more easily cached but cannot deliver dynamic content. Web servers can also frequently be found embedded in devices such as printers, routers, webcams and serving only a local network. The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most operating systems). Optical networking is a sophisticated infrastructure that utilises optical fibre to transmit data over long distances, connecting countries, cities, and even private residences. The technology uses optical microsystems like tunable lasers, filters, attenuators, switches, and wavelength-selective switches to manage and operate these networks.[65][66] The large quantity of optical fibre installed throughout the world at the end of the twentieth century set the foundation of the Internet as it is used today. The information highway relies heavily on optical networking, a method of sending messages encoded in light to relay information in various telecommunication networks.[67] The Advanced Research Projects Agency Network (ARPANET) was one of the first iterations of the Internet, created in collaboration with universities and researchers in 1969.[68][69][70][71] However, access to the ARPANET was limited to researchers, and in 1985, the National Science Foundation founded the National Science Foundation Network (NSFNET), a program that provided supercomputer access to researchers.[71] Limited public access to the Internet led to pressure from consumers and corporations to privatise the network. In 1993, the US passed the National Information Infrastructure Act, which dictated that the National Science Foundation must hand over control of the optical capabilities to commercial operators.[72][73] The privatisation of the Internet and the release of the World Wide Web to the public in 1993 led to an increased demand for Internet capabilities. This spurred developers to seek solutions to reduce the time and cost of laying new fibre and increase the amount of information that can be sent on a single fibre, to meet the growing needs of the public.[74][75][76][77] In 1994, Pirelli S.p.A.'s optical components division introduced a wavelength-division multiplexing (WDM) system to meet growing demand for increased data transmission. This four-channel WDM technology allowed more information to be sent simultaneously over a single optical fibre, effectively boosting network capacity.[78][79] Pirelli wasn't the only company that developed a WDM system; another company, the Ciena Corporation (Ciena), created its own technology to transmit data more efficiently. David Huber, an optical networking engineer and entrepreneur Kevin Kimberlin founded Ciena in 1992.[80][81][82] Drawing on laser technology from Gordon Gould and William Culver of Optelecom, Inc., the company focused on utilising optical amplifiers to transmit data via light.[83][84][85] Under chief executive officer Pat Nettles, Ciena developed a dual-stage optical amplifier for dense wavelength-division multiplexing (DWDM), patented in 1997 and deployed on the Sprint network in 1996.[86][87][88][89][90] An HTTP cookie (also called web cookie, Internet cookie, browser cookie, or simply cookie) is a small piece of data sent from a website and stored on the user's computer by the user's web browser while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons, logging in, or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields, such as names, addresses, passwords, and credit card numbers. Cookies perform essential functions in the modern web. Perhaps most importantly, authentication cookies are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's web browser, and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a hacker, used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see cross-site scripting and cross-site request forgery for examples).[91] Tracking cookies, and especially third-party tracking cookies, are commonly used as ways to compile long-term records of individuals' browsing histories \u2013 a potential privacy concern that prompted European[92] and U.S. lawmakers to take action in 2011.[93][94] European law requires that all websites targeting European Union member states gain \"informed consent\" from users before storing non-essential cookies on their device. Google Project Zero researcher Jann Horn describes ways cookies can be read by intermediaries, like Wi-Fi hotspot providers. When in such circumstances, he recommends using the browser in private browsing mode (widely known as Incognito mode in Google Chrome).[95] A web search engine or Internet search engine is a software system that is designed to carry out web search (Internet search), which means to search the World Wide Web in a systematic way for particular information specified in a web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). The information may be a mix of web pages, images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories. Unlike web directories, which are maintained only by human editors, search engines also maintain real-time information by running an algorithm on a web crawler. Internet content that is not capable of being searched by a web search engine is generally described as the deep web. In 1990, Archie, the world's first search engine, was released. The technology was originally an index of File Transfer Protocol (FTP) sites, which was a method for moving files between a client and a server network.[96][97] This early search tool was superseded by more advanced engines like Yahoo! in 1995 and Google in 1998.[98][99] The deep web,[100] invisible web,[101] or hidden web[102] are parts of the World Wide Web whose contents are not indexed by standard web search engines. The opposite term to the deep web is the surface web, which is accessible to anyone using the Internet.[103] Computer scientist Michael K. Bergman is credited with coining the term deep web in 2001 as a search indexing term.[104] The content of the deep web is hidden behind HTTP forms,[105][106] and includes many very common uses such as web mail, online banking, and services that users must pay for, and which is protected by a paywall, such as video on demand, some online magazines and newspapers, among others. The content of the deep web can be located and accessed by a direct URL or IP address and may require a password or other security access past the public website page. A web cache is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a browser cache by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in online banking, or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Website designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise firewalls often cache Web resources requested by one user for the benefit of many users. Some search engines store cached content of frequently accessed websites. For criminals, the Web has become a venue to spread malware and engage in a range of cybercrime, including (but not limited to) identity theft, fraud, espionage, and intelligence gathering.[107] Web-based vulnerabilities now outnumber traditional computer security concerns,[108][109] and as measured by Google, about one in ten web pages may contain malicious code.[110] Most web-based attacks take place on legitimate websites, and most, as measured by Sophos, are hosted in the United States, China and Russia.[111] The most common of all malware threats is SQL injection attacks against websites.[112] Through HTML and URIs, the Web was vulnerable to attacks like cross-site scripting (XSS) that came with the introduction of JavaScript[113] and were exacerbated to some degree by Web 2.0 and Ajax web design that favours the use of scripts.[114] In one 2007 estimate, 70% of all websites are open to XSS attacks on their users.[115] Phishing is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5\u00a0billion in 2012.[116] Two of the well-known phishing methods are Covert Redirect and Open Redirect. Proposed solutions vary. Large security companies like McAfee already design governance and compliance suites to meet post-9/11 regulations,[117] and some, like Finjan Holdings have recommended active real-time inspection of programming code and all content regardless of its source.[107] Some have argued that for enterprises to see Web security as a business opportunity rather than a cost centre,[118] while others call for \"ubiquitous, always-on digital rights management\" enforced in the infrastructure to replace the hundreds of companies that secure data and networks.[119] Jonathan Zittrain has said users sharing responsibility for computing safety is far preferable to locking down the Internet.[120] Every time a client requests a web page, the server can identify the request's IP address. Web servers usually log IP addresses in a log file. Also, unless set not to do so, most web browsers record requested web pages in a viewable history feature, and usually cache much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide personally identifiable information is by using a virtual private network. A VPN encrypts traffic between the client and VPN server, and masks the original IP address, lowering the chance of user identification. When a web page asks for, and the user supplies, personally identifiable information\u2014such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses HTTP cookies, username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organisation can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their demographic profile. These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's terms and conditions and the local laws that apply, information from these profiles may be sold, shared, or passed to other organisations without the user being informed. For many ordinary people, this means little more than some unexpected emails in their inbox or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web. Social networking sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified with an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences. Online bullies may make use of personal information to harass or stalk users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners.[121] Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential facial recognition technology, it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web. Web standards include many interdependent standards and specifications, some of which govern aspects of the Internet, not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and web services. Considerations include the interoperability, accessibility and usability of web pages and websites. Web standards, in the broader sense, consist of the following: Web standards are not fixed sets of rules but are constantly evolving sets of finalised technical specifications of web technologies.[128] Web standards are developed by standards organisations\u2014groups of interested and often competing parties chartered with the task of standardisation\u2014not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of W3C specifications, the highest maturity level). There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with disabilities. These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change.[129] The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and equal opportunity to people with disabilities.[130] Tim Berners-Lee once noted, \"The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect.\"[129] Many countries regulate web accessibility as a requirement for websites.[131] International co-operation in the W3C Web Accessibility Initiative led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using assistive technology.[129][132] The W3C Internationalisation Activity assures that web technology works in all languages, scripts, and cultures.[133] Beginning in 2004 or 2005, Unicode gained ground and eventually in December 2007 surpassed both ASCII and Western European as the Web's most frequently used character map.[134] Originally RFC\u00a03986 allowed resources to be identified by URI in a subset of US-ASCII. RFC\u00a03987 allows more characters\u2014any character in the Universal Character Set\u2014and now a resource can be identified by IRI in any language.[135]",
      "ground_truth_chunk_ids": [
        "42_fixed_chunk1"
      ],
      "source_ids": [
        "S042"
      ],
      "category": "factual",
      "id": 34
    },
    {
      "question": "What is Longwood Bowl?",
      "ground_truth": "The Longwood Bowl[1] was a men's and women's tennis tournament first played at the Longwood Cricket Club courts at Brookline, Massachusetts, United States from 1882 to 1949. The men's tournament was also known as the Longwood Challenge Bowl.[2] The first women's event was the Longwood Tennis Cup it later became known as the Longwood Bowl Invitational. In 1877 the Longwood Cricket Club was founded.[3] In 1881 the club held its first tennis tournament.[4] In 1882 the club held its first important tennis event the Longwood Cricket Club Tournament it was the precursor event to the Longwood Bowl also known as the Longwood Challenge Bowl tournament founded in 1891.[5] The men's event was held through till 1942 when it was discontinued, and the women's event continued on till 1949 before it was also abolished. The tournament was played for the entire time at Brookline, Massachusetts where Longwood Cricket Club's tennis courts are located.[6] In 1922 the club house and administrative center was moved to Newton, Massachusetts.[7] The winners of the men's tournament retain a permanent replica of the Longwood Bowl Trophy if they win it three times.[8] Incomplete roll included:[9] Incomplete roll",
      "expected_answer": "The Longwood Bowl[1] was a men's and women's tennis tournament first played at the Longwood Cricket Club courts at Brookline, Massachusetts, United States from 1882 to 1949. The men's tournament was also known as the Longwood Challenge Bowl.[2] The first women's event was the Longwood Tennis Cup it later became known as the Longwood Bowl Invitational. In 1877 the Longwood Cricket Club was founded.[3] In 1881 the club held its first tennis tournament.[4] In 1882 the club held its first important tennis event the Longwood Cricket Club Tournament it was the precursor event to the Longwood Bowl also known as the Longwood Challenge Bowl tournament founded in 1891.[5] The men's event was held through till 1942 when it was discontinued, and the women's event continued on till 1949 before it was also abolished. The tournament was played for the entire time at Brookline, Massachusetts where Longwood Cricket Club's tennis courts are located.[6] In 1922 the club house and administrative center was moved to Newton, Massachusetts.[7] The winners of the men's tournament retain a permanent replica of the Longwood Bowl Trophy if they win it three times.[8] Incomplete roll included:[9] Incomplete roll",
      "ground_truth_chunk_ids": [
        "79_random_chunk1"
      ],
      "source_ids": [
        "S279"
      ],
      "category": "factual",
      "id": 35
    },
    {
      "question": "What is French Revolution?",
      "ground_truth": "The French Revolution[a] was a period of political and societal change in France that began with the Estates General of 1789 and ended with the Coup of 18 Brumaire on 9 November 1799. Many of the revolution's ideas are considered fundamental principles of liberal democracy,[1] and its values remain central to modern French political discourse.[2] It was caused by a combination of social, political, and economic factors which the existing regime proved unable to manage. Financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614. The representatives of the Third Estate broke away and re-constituted themselves as a National Assembly in June. The Storming of the Bastille in Paris on 14 July led to a series of radical measures by the Assembly, including the abolition of feudalism, state control over the Catholic Church in France, and issuing the Declaration of the Rights of Man and of the Citizen. The next three years were dominated by a struggle for political control. King Louis XVI's attempted flight to Varennes in June 1791 further discredited the monarchy, and military defeats after the outbreak of the French Revolutionary Wars in April 1792 led to the insurrection of 10 August 1792. As a result, the monarchy was replaced by the French First Republic in September, followed by the execution of Louis XVI himself in January 1793. After another revolt in June 1793, the constitution was suspended, and political power passed from the National Convention to the Committee of Public Safety, dominated by radical Jacobins led by Maximilien Robespierre. About 16,000 people were sentenced by the Revolutionary Tribunal and executed in the Reign of Terror, which ended in July 1794 with the Thermidorian Reaction. Weakened by external threats and internal opposition, the Committee of",
      "expected_answer": "The French Revolution[a] was a period of political and societal change in France that began with the Estates General of 1789 and ended with the Coup of 18 Brumaire on 9 November 1799. Many of the revolution's ideas are considered fundamental principles of liberal democracy,[1] and its values remain central to modern French political discourse.[2] It was caused by a combination of social, political, and economic factors which the existing regime proved unable to manage. Financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614. The representatives of the Third Estate broke away and re-constituted themselves as a National Assembly in June. The Storming of the Bastille in Paris on 14 July led to a series of radical measures by the Assembly, including the abolition of feudalism, state control over the Catholic Church in France, and issuing the Declaration of the Rights of Man and of the Citizen. The next three years were dominated by a struggle for political control. King Louis XVI's attempted flight to Varennes in June 1791 further discredited the monarchy, and military defeats after the outbreak of the French Revolutionary Wars in April 1792 led to the insurrection of 10 August 1792. As a result, the monarchy was replaced by the French First Republic in September, followed by the execution of Louis XVI himself in January 1793. After another revolt in June 1793, the constitution was suspended, and political power passed from the National Convention to the Committee of Public Safety, dominated by radical Jacobins led by Maximilien Robespierre. About 16,000 people were sentenced by the Revolutionary Tribunal and executed in the Reign of Terror, which ended in July 1794 with the Thermidorian Reaction. Weakened by external threats and internal opposition, the Committee of Public Safety was replaced in November 1795 by the Directory. Its instability ended in 1799 with the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul. The Revolution resulted from multiple long-term and short-term factors, culminating in a social, economic, financial and political crisis in the late 1780s.[3][4][5] Combined with resistance to reform by the ruling elite and indecisive policy by Louis XVI and his ministers, the result was a crisis the state was unable to manage.[6][7] Between 1715 and 1789, the French population grew from 21 to 28 million, 20% of whom lived in towns or cities, Paris alone having over 600,000 inhabitants.[8] This was accompanied by a tripling in the size of the middle class, which comprised almost 10% of the population by 1789.[9] Despite increases in overall prosperity, its benefits were largely restricted to the rentier and mercantile classes, while the living standards fell for wage labourers and peasant farmers who rented their land.[10][11] Economic recession from 1785, combined with bad harvests in 1787 and 1788, led to high unemployment and food prices, causing a financial and political crisis.[3][12][13][14] While the state also experienced a debt crisis, the level of debt itself was not high compared with Britain's.[15] A significant problem was that tax rates varied widely from one region to another, were often different from the official amounts, and were collected inconsistently. The complexity and lack of accountability caused resentment among all taxpayers.[16][b] Attempts to simplify the system were blocked by the regional Parlements which approved financial policy. The resulting impasse led to the calling of the Estates General of 1789, which became radicalised by the struggle for control of public finances.[18] Louis XVI was willing to consider reforms, but he often backed down when faced with opposition from conservative elements within the nobility. Enlightenment critiques of social institutions were widely discussed among the educated French elite. At the same time, the American Revolution and the European revolts of the 1780s inspired public debate on issues such as patriotism, liberty, equality, and democracy. These shaped the response of the educated public to the crisis,[19] while scandals such as the Affair of the Diamond Necklace fuelled widespread anger at the court, nobility, and church officials.[20] France faced a series of budgetary crises during the 18th century as revenues failed to keep pace with expenditure.[21][22] Despite solid economic growth, the use of tax farmers meant this was not reflected in a proportional growth in state tax income.[21] As the nobility and Church benefited from a variety of exemptions, the tax burden fell mainly on the lower classes.[23] Reform was difficult because new tax laws had to be registered with regional judicial bodies or parlements that were able to block them. The king could impose laws by decree, but this risked open conflict with the parlements, the nobility, and those subject to new taxes.[24] France primarily used loans to fund the 1778 to 1783 Anglo-French War. Even after it ended, the monarchy continued to borrow heavily, and by 1788, half of state revenue went on servicing its debt.[25] In 1786, the French finance minister, Calonne, proposed reforms including a universal land tax, the abolition of grain controls and internal tariffs, and new provincial assemblies appointed by the king. The new taxes were rejected, first by a hand-picked Assembly of Notables dominated by the nobility, then by the parlements when submitted by Calonne's successor Brienne. The notables and parlements argued that the proposed taxes could only be approved by an Estates-General, a representative body that last met in 1614.[26] The conflict between the Crown and the parlements became a national political crisis. Both sides issued a series of public statements, the government arguing that it was combating privilege, and the parlement defending the ancient rights of the nation. Public opinion was firmly on the side of the parlements, and riots broke out in several towns. Brienne's attempts to raise new loans failed, and on 8 August 1788, he announced that the king would summon an Estates-General to convene the following May. Brienne resigned and was replaced by Jacques Necker.[27] In September 1788, the Parlement of Paris ruled that the Estates-General should convene in the same form as in 1614, meaning that the three estates would meet and vote separately, with votes counted by estate rather than by head. As a result, the clergy and nobility could combine to outvote the Third Estate, despite representing less than 5% of the population.[28][29] With the relaxation of censorship and laws against political clubs, a group of liberal nobles and middle class activists known as the Society of Thirty launched a campaign for the doubling of Third Estate representation and individual voting. The public debate sparked an average of 25 new political pamphlets published each week from 25 September 1788.[30] One of the most influential was written by Abb\u00e9 Siey\u00e8s. Titled What Is the Third Estate?, it denounced the privilege of the clergy and nobility, and argued the Third Estate represented the nation and should sit alone as a National Assembly. Activists such as Jean Joseph Mounier, Antoine Barnave and Maximilien Robespierre organised regional meetings, petitions and literature in support of these demands.[31] In December, the king agreed to double the representation of the Third Estate, but left the question of counting votes for the Estates-General to decide.[32] The Catholic Church in France was wealthy, owning nearly 10% of all land, as well as receiving annual tithes.[33] However, three-quarters of the 303 clergy elected were parish priests, many of whom earned less than unskilled labourers and had more in common with their poor parishioners than with the bishops of the first estate.[34] The Second Estate elected 322 deputies, representing about 400,000 men and women, who owned about 25% of the land and collected seigneurial dues and rents from their tenants. Most delegates were town-dwelling members of the noblesse d'\u00e9p\u00e9e, or traditional aristocracy. Courtiers and representatives of the noblesse de robe (those who derived rank from judicial or administrative posts) were underrepresented.[35] Of the 610 deputies of the Third Estate, about two-thirds held legal qualifications and almost half were venal office holders. Less than 100 were in trade or industry, and none were peasants or artisans.[36] To assist delegates, each region completed a list of grievances, known as Cahiers de dol\u00e9ances.[37] Tax inequality and seigneurial dues (feudal payments owed to landowners) headed the grievances in the cahiers de doleances for the estate.[38] On 5 May 1789, the Estates-General convened at Versailles, with Necker reiterating that each estate should decide separately how and when it would meet and vote in common with the other estates. On the following day, each estate was to separately verify the credentials of their representatives. The Third Estate, however, voted to invite the other estates to join them in verifying all the representatives of the Estates-General in common, and to agree that votes should be counted by head. Negotiations continued until 12 June when the Third Estate unilaterally began verifying its own members. On 17th, the Third Estate declared itself to be the National Assembly of France and that all existing taxes were illegal.[39] By 19 June, they had been joined by more than 100 members of the clergy.[40] Shaken by this challenge to his authority, the king agreed to a reform package he would present personally to the Estates-General. The Salle des \u00c9tats was closed to prepare for the joint session, but the members of the Estates-General were not informed in advance. Finding their meeting place closed next day, they took the so-called Tennis Court Oath, undertaking not to disperse until a constitution had been agreed.[41] At the royal session, Louis XVI announced a series of reforms and stated no new taxes or loans would be implemented without the consent of the Estates-General. However, he then undermined this by re-stating his original demand for all three to sit and vote separately. The Third Estate refused to leave the hall and reiterated their oath not to disperse until a constitution had been agreed. Over the next days more members of the clergy joined the National Assembly. On 27 June, faced with popular demonstrations and mutinies in his French Guards, Louis XVI commanded the members of the first and second estates to join the third in the National Assembly.[42] Even the limited reforms the king had announced went too far for Marie Antoinette and Louis' younger brother the Comte d'Artois. On their advice, Louis dismissed Necker again as chief minister on 11 July.[43] On 12 July, the Assembly went into a non-stop session following rumours that the king was planning to use the Swiss Guards to force it to close. The news brought crowds of protestors into the streets, and soldiers of the elite Gardes Fran\u00e7aises refused to disperse them.[44] On 14 July many of these soldiers joined a crowd attacking the Bastille, a royal fortress with large stores of arms and ammunition. Its governor, Bernard-Ren\u00e9 de Launay, surrendered after several hours of fighting that cost the lives of 83 attackers. Launay was taken to the H\u00f4tel de Ville, where he was killed and his head placed on a pike and paraded around the city. Although rumoured to hold many prisoners, the Bastille held only seven: four forgers, a lunatic, a failed assassin, and a deviant nobleman. Nevertheless, it was a potent symbol of the Ancien R\u00e9gime and it was demolished in the following weeks.[45] Bastille Day has become the French national holiday.[46] Alarmed by the prospect of losing control of the capital, Louis appointed the Marquis de Lafayette commander of the National Guard, with Jean-Sylvain Bailly as head of a new administrative structure known as the Commune. On 17 July, Louis visited Paris accompanied by 100 deputies, where he was greeted by Bailly and accepted a tricolore cockade to loud cheers. However, it was clear power had shifted from his court; he was welcomed as 'Louis XVI, father of the French and king of a free people.'[47] The short-lived unity enforced on the Assembly by a common threat quickly dissipated. Deputies argued over constitutional forms, while civil authority rapidly deteriorated. On 22 July, former Finance Minister Joseph Foullon and his son were lynched by a Parisian mob, and neither Bailly nor Lafayette could prevent it. In rural areas, wild rumours and paranoia resulted in the formation of militia and an agrarian insurrection known as the Great Fear.[48] The breakdown of law and order and frequent attacks on aristocratic property led much of the nobility to flee abroad. These \u00e9migr\u00e9s funded reactionary forces within France and urged foreign monarchs to back a counter-revolution.[49] In response, the Assembly published the August Decrees which abolished feudalism. Over 25% of French farmland was subject to feudal dues, providing the nobility with most of their income; these were now cancelled, along with church tithes. While their former tenants were supposed to pay them compensation, collecting it proved impossible, and the obligation was annulled in 1793.[50] Other decrees included equality before the law, opening public office to all, freedom of worship, and cancellation of special privileges held by provinces and towns.[51] With the suspension of the 13 regional parlements in November, the key institutional pillars of the old regime had all been abolished in less than four months. From its early stages, the Revolution therefore displayed signs of its radical nature; what remained unclear was the constitutional mechanism for turning intentions into practical applications.[52] On 9 July, the National Assembly declared itself the National Constituent Assembly[53] and appointed a committee to draft a constitution and statement of rights.[54] Twenty drafts were submitted, which were used by a sub-committee to create a Declaration of the Rights of Man and of the Citizen, with Mirabeau being the most prominent member.[55] The declaration was approved by the Assembly and published on 26 August as a statement of principle.[56] The Assembly now concentrated on the constitution. Mounier and his monarchist supporters advocated a bicameral system, with an upper house appointed by the king, who would also have the right to appoint ministers and veto legislation. On 10 September, the majority of the Assembly, led by Siey\u00e8s and Talleyrand, voted in favour of a single body, and the following day approved a \"suspensive veto\" for the king, meaning Louis could delay implementation of a law but not block it indefinitely. In October, the Assembly voted to restrict political rights, including voting rights, to \"active citizens\", defined as French males over the age of 25 who paid direct taxes equal to three days' labour. The remainder were designated \"passive citizens\", restricted to \"civil rights\", a distinction opposed by a significant minority, including the Jacobin clubs.[57][58] By mid-1790, the main elements of a constitutional monarchy were in place, although the constitution was not accepted by Louis until 1791.[59] Food shortages and the worsening economy caused frustration at the lack of progress and led to popular unrest in Paris. This came to a head in late September 1789, when the Flanders Regiment arrived in Versailles to reinforce the royal bodyguard and were welcomed with a formal banquet as was common practice. The radical press described this as a 'gluttonous orgy' and claimed the tricolour cockade had been abused, while the Assembly viewed their arrival as an attempt to intimidate them.[60] On 5 October, crowds of women assembled outside the H\u00f4tel de Ville, agitating against high food prices and shortages.[61] These protests quickly turned political, and after seizing weapons stored at the H\u00f4tel de Ville, some 7,000 of them marched on Versailles, where they entered the Assembly to present their demands. They were followed to Versailles by 15,000 members of the National Guard under Lafayette, who was virtually \"a prisoner of his own troops\".[62] When the National Guard arrived later that evening, Lafayette persuaded Louis that the safety of his family required their relocation to Paris. Next morning, some of the protestors broke into the royal apartments, searching for Marie Antoinette, who had escaped. They ransacked the palace, killing several guards. Order was eventually restored, and the royal family and Assembly left for Paris, escorted by the National Guard.[63] Louis had announced his acceptance of the August Decrees and the declaration, and his official title changed from 'King of France' to 'King of the French'.[64] Historian John McManners argues \"in eighteenth-century France, throne and altar were commonly spoken of as in close alliance; their simultaneous collapse ... would one day provide the final proof of their interdependence.\" One suggestion is that after a century of persecution, some French Protestants actively supported an anti-Catholic regime, a resentment fuelled by Enlightenment thinkers such as Voltaire.[65] Jean-Jacques Rousseau, considered a philosophical founder of the revolution,[66][67][68] wrote it was \"manifestly contrary to the law of nature... that a handful of people should gorge themselves with superfluities, while the hungry multitude goes in want of necessities.\"[69] The Revolution caused a massive shift of power from the Catholic Church to the state; although the extent of religious belief has been questioned, elimination of tolerance for religious minorities meant by 1789 being French also meant being Catholic.[70] The church was the largest individual landowner in France, controlling nearly 10% of all estates and levied tithes, effectively a 10% tax on income, collected from peasant farmers in the form of crops. In return, it provided a minimal level of social support.[71] The August Decrees abolished tithes, and on 2 November the Assembly confiscated all church property, the value of which was used to back a new paper currency known as assignats. In return, the state assumed responsibilities such as paying the clergy and caring for the poor, the sick and the orphaned.[72] On 13 February 1790, religious orders and monasteries were dissolved, while monks and nuns were encouraged to return to private life.[73] The Civil Constitution of the Clergy of 12 July 1790 made them employees of the state, established rates of pay, and developed a system for electing priests and bishops. Pope Pius VI and many French Catholics objected to this since it denied the authority of the Pope over the French church. In October, 30 bishops wrote a declaration denouncing the law, further fuelling opposition.[74] When clergy were required to swear loyalty to the Civil Constitution in November, it split the church between the 24% who complied and the majority who refused.[75] This stiffened popular resistance against state interference, especially in traditionally Catholic areas such as Normandy, Brittany and the Vend\u00e9e, where only a few priests took the oath and the civilian population turned against the revolution.[74] The result was state-led persecution of \"refractory clergy\", many of whom were forced into exile, deported, or executed.[76] The period from October 1789 to spring 1791 is usually seen as one of relative tranquility, when some of the most important legislative reforms were enacted. However, conflict over the source of legitimate authority was more apparent in the provinces, where officers of the Ancien R\u00e9gime had been swept away but not yet replaced by new structures. This was less obvious in Paris, since the National Guard made it the best policed city in Europe, but disorder in the provinces inevitably affected members of the Assembly.[77] Centrists led by Siey\u00e8s, Lafayette, Mirabeau and Bailly created a majority by forging consensus with monarchiens like Mounier, and independents including Adrien Duport, Barnave and Alexandre Lameth. At one end of the political spectrum, reactionaries like Cazal\u00e8s and Maury denounced the Revolution in all its forms, with radicals like Maximilien Robespierre at the other. He and Jean-Paul Marat opposed the criteria for \"active citizens\", gaining them substantial support among the Parisian proletariat, many of whom had been disenfranchised by the measure.[78] On 14 July 1790, celebrations were held throughout France commemorating the fall of the Bastille, with participants swearing an oath of fidelity to \"the nation, the law and the king.\" The F\u00eate de la F\u00e9d\u00e9ration in Paris was attended by the royal family, with Talleyrand performing a mass. Despite this show of unity, the Assembly was increasingly divided, while external players like the Paris Commune and National Guard competed for power. One of the most significant was the Jacobin club; originally a forum for general debate, by August 1790 it had over 150 members, split into different factions.[79] The Assembly continued to develop new institutions; in September 1790, the regional Parlements were abolished and their legal functions replaced by a new independent judiciary, with jury trials for criminal cases. However, moderate deputies were uneasy at popular demands for universal suffrage, labour unions and cheap bread, and over the winter of 1790 and 1791, they passed a series of measures intended to disarm popular radicalism. These included exclusion of poorer citizens from the National Guard, limits on use of petitions and posters, and the June 1791 Le Chapelier Law suppressing trade guilds and any form of worker organisation.[80] The traditional force for preserving law and order was the army, which was increasingly divided between officers, who largely came from the nobility, and ordinary soldiers. In August 1790, the loyalist General Bouill\u00e9 suppressed a serious mutiny at Nancy; although congratulated by the Assembly, he was criticised by Jacobin radicals for the severity of his actions. Growing disorder meant many professional officers either left or became \u00e9migr\u00e9s, further destabilising the institution.[81] Held in the Tuileries Palace under virtual house arrest, Louis XVI was urged by his brother and wife to re-assert his independence by taking refuge with Bouill\u00e9, who was based at Montm\u00e9dy with 10,000 soldiers considered loyal to the Crown.[82] The royal family left the palace in disguise on the night of 20 June 1791; late the next day, Louis was recognised as he passed through Varennes, arrested and taken back to Paris. The attempted escape had a profound impact on public opinion; since it was clear Louis had been seeking refuge in Austria, the Assembly now demanded oaths of loyalty to the regime and began preparing for war, while fear of 'spies and traitors' became pervasive.[83] Despite calls to replace the monarchy with a republic, Louis retained his position but was generally regarded with acute suspicion and forced to swear allegiance to the constitution. A new decree stated retracting this oath, making war upon the nation, or permitting anyone to do so in his name would be considered abdication. However, radicals led by Jacques Pierre Brissot prepared a petition demanding his deposition, and on 17 July, an immense crowd gathered in the Champ de Mars to sign. Led by Lafayette, the National Guard was ordered to \"preserve public order\" and responded to a barrage of stones by firing into the crowd, killing between 13 and 50 people.[84] The massacre badly damaged Lafayette's reputation; the authorities responded by closing radical clubs and newspapers, while their leaders went into exile or hiding, including Marat.[85] On 27 August, Emperor Leopold II and King Frederick William II of Prussia issued the Declaration of Pillnitz declaring their support for Louis and hinting at an invasion of France on his behalf. In reality, the meeting between Leopold and Frederick was primarily to discuss the partitions of Poland; the declaration was intended to satisfy Comte d'Artois and other French \u00e9migr\u00e9s, but the threat rallied popular support behind the regime.[86] Based on a motion proposed by Robespierre, existing deputies were barred from elections held in September for the French Legislative Assembly. Although Robespierre was one of those excluded, his support in the clubs gave him a political power base not available to Lafayette and Bailly, who resigned respectively as head of the National Guard and the Paris Commune. The new laws were gathered together in the 1791 Constitution, and submitted to Louis XVI, who pledged to defend it \"from enemies at home and abroad\". On 30 September, the Constituent Assembly was dissolved, and the Legislative Assembly convened the next day.[87] The Legislative Assembly is often dismissed by historians as an ineffective body, compromised by divisions over the role of the monarchy, an issue exacerbated when Louis attempted to prevent or reverse limitations on his powers.[88] At the same time, restricting the vote to those who paid a minimal amount of tax disenfranchised a significant proportion of the 6\u00a0million Frenchmen over 25, while only 10% of those able to vote actually did so. Finally, poor harvests and rising food prices led to unrest among the urban class known as sans-culottes, who saw the new regime as failing to meet their demands for bread and work.[89] This meant the new constitution was opposed by significant elements inside and outside the Assembly, itself split into three main groups. 264 members were affiliated with Barnave's Feuillants, constitutional monarchists who considered the Revolution had gone far enough, while another 136 were Jacobin leftists who supported a republic, led by Brissot and usually referred to as Brissotins.[90] The remaining 345 belonged to La Plaine, a centrist faction who switched votes depending on the issue, but many of whom shared doubts as to whether Louis was committed to the Revolution.[90] After he officially accepted the new Constitution, one recorded response was \"Vive le roi, s'il est de bon foi!\", or \"Long live the king \u2013 if he keeps his word\".[91] Although a minority in the Assembly, control of key committees allowed the Brissotins to provoke Louis into using his veto. They first managed to pass decrees confiscating \u00e9migr\u00e9 property and threatening them with the death penalty.[92] This was followed by measures against non-juring priests, whose opposition to the Civil Constitution led to a state of near civil war in southern France, which Barnave tried to defuse by relaxing the more punitive provisions. On 29 November, the Assembly approved a decree giving refractory clergy eight days to comply, or face charges of 'conspiracy against the nation', an act opposed even by Robespierre.[93] When Louis vetoed both, his opponents were able to portray him as opposed to reform in general.[94] Brissot accompanied this with a campaign for war against Austria and Prussia, often interpreted as a mixture of calculation and idealism. While exploiting popular anti-Austrianism, it reflected a genuine belief in exporting the values of political liberty and popular sovereignty.[95] Simultaneously, conservatives headed by Marie Antoinette also favoured war, seeing it as a way to regain control of the military, and restore royal authority. In December 1791, Louis made a speech in the Assembly giving foreign powers a month to disband the \u00e9migr\u00e9s or face war, an act greeted with enthusiasm by supporters, but suspicion from opponents.[96] Barnave's inability to build a consensus in the Assembly resulted in the appointment of a new government, chiefly composed of Brissotins. On 20 April 1792, the French Revolutionary Wars began when French armies attacked Austrian and Prussian forces along their borders, before suffering a series of disastrous defeats. In an effort to mobilise popular support, the government ordered non-juring priests to swear the oath or be deported, dissolved the Constitutional Guard and replaced it with 20,000 f\u00e9d\u00e9r\u00e9s; Louis agreed to disband the Guard, but vetoed the other two proposals, while Lafayette called on the Assembly to suppress the clubs.[97] Popular anger increased when details of the Brunswick Manifesto reached Paris on 1 August, threatening 'unforgettable vengeance' should any oppose the Allies in seeking to restore the power of the monarchy. On the morning of 10 August, a combined force of the Paris National Guard and provincial f\u00e9d\u00e9r\u00e9s attacked the Tuileries Palace, killing many of the Swiss Guards protecting it.[98] Louis and his family took refuge with the Assembly and shortly after 11:00 am, the deputies present voted to 'temporarily relieve the king', effectively suspending the monarchy.[99] In late August, elections were held for the National Convention. Restrictions on the franchise meant the number of votes cast fell to 3.3\u00a0million, versus 4\u00a0million in 1791, while intimidation was widespread.[100] The Brissotins split between moderate Girondins led by Brissot, and radical Montagnards, headed by Robespierre, Georges Danton and Jean-Paul Marat. While loyalties constantly shifted, voting patterns suggest roughly 160 of the 749 deputies can generally be categorised as Girondists, with another 200 Montagnards. The remainder were part of a centrist faction known as La Plaine, headed by Bertrand Bar\u00e8re, Pierre Joseph Cambon and Lazare Carnot.[101] In the September Massacres, between 1,100 and 1,600 prisoners held in Parisian jails were summarily executed, the vast majority being common criminals.[102] A response to the capture of Longwy and Verdun by Prussia, the perpetrators were largely National Guard members and f\u00e9d\u00e9r\u00e9s on their way to the front. While responsibility is still disputed, even moderates expressed sympathy for the action, which soon spread to the provinces. One suggestion is that the killings stemmed from concern over growing lawlessness, rather than political ideology.[103] On 20 September, the French defeated the Prussians at the Battle of Valmy, in what was the first major victory by the army of France during the Revolutionary Wars. Emboldened by this, on 22 September the Convention replaced the monarchy with the French First Republic and introduced a new calendar, with 1792 becoming \"Year One\".[104] The next few months were taken up with the trial of Citoyen Louis Capet, formerly Louis XVI. While evenly divided on the question of his guilt, members of the convention were increasingly influenced by radicals based within the Jacobin clubs and Paris Commune. The Brunswick Manifesto made it easy to portray Louis as a threat to the Revolution, especially when extracts from his personal correspondence showed him conspiring with Royalist exiles.[105] On 17 January 1793, Louis was sentenced to death for \"conspiracy against public liberty and general safety\". 361 deputies were in favour, 288 against, while another 72 voted to execute him, subject to delaying conditions. The sentence was carried out on 21 January on the Place de la R\u00e9volution, now the Place de la Concorde.[106] Conservatives across Europe called for the destruction of revolutionary France, and in February the Convention responded by declaring war on Britain and the Dutch Republic. Together with Austria and Prussia, these two countries were later joined by Spain, Portugal, Naples, and Tuscany in the War of the First Coalition.[107] The Girondins hoped war would unite the people behind the government and provide an excuse for rising prices and food shortages. Instead, they found themselves the target of popular anger and in what proved a disastrous strategic move, many left Paris for the provinces. The first conscription measure or lev\u00e9e en masse on 24 February sparked riots in the capital and other regional centres. Already unsettled by changes imposed on the church, in March the traditionally conservative and royalist Vend\u00e9e rose in revolt. On 18th, General Charles Fran\u00e7ois Dumouriez was defeated at Neerwinden and defected to the Austrians. Uprisings followed in Bordeaux, Lyon, Toulon, Marseille and Caen. The Republic seemed on the verge of collapse.[108] The crisis led to the creation on 6 April 1793 of the Committee of Public Safety, an executive committee accountable to the convention.[109] The Girondins made a fatal political error by indicting Marat before the Revolutionary Tribunal for allegedly directing the September massacres; he was quickly acquitted, further isolating the Girondins from the sans-culottes. When Jacques H\u00e9bert called for a popular revolt against the \"henchmen of Louis Capet\" on 24 May, he was arrested by the Commission of Twelve, a Girondin-dominated tribunal set up to expose 'plots'. In response to protests by the Commune, the Commission warned \"if by your incessant rebellions something befalls the representatives of the nation, Paris will be obliterated\".[108] Growing discontent allowed the clubs to mobilise against the Girondins. Backed by the Commune and elements of the National Guard, on 31 May they attempted to seize power in a coup. Although the coup failed, on 2 June the convention was surrounded by a crowd of up to 80,000, demanding cheap bread, unemployment pay and political reforms, including restriction of the vote to the sans-culottes, and the right to remove deputies at will.[110] Ten members of the commission and another twenty-nine members of the Girondin faction were arrested, and on 10 June, the Montagnards took over the Committee of Public Safety.[111] Meanwhile, a committee led by Robespierre's close ally Louis Antoine de Saint-Just was tasked with preparing a new constitution. Completed in only eight days, it was ratified by the convention on 24 June and contained radical reforms, including universal male suffrage. However, normal legal processes were suspended following the assassination of Marat on 13 July by the Girondist Charlotte Corday, which the Committee of Public Safety used as an excuse to take control. The 1793 Constitution was suspended indefinitely in October.[112] Key areas of focus for the new government included creating a new state ideology, economic regulation and winning the war.[113] They were helped by divisions among their internal opponents; while areas like the Vend\u00e9e and Brittany wanted to restore the monarchy, most supported the Republic but opposed the regime in Paris. On 17 August, the Convention voted a second lev\u00e9e en masse; despite initial problems in equipping and supplying such large numbers, by mid-October Republican forces had re-taken Lyon, Marseille and Bordeaux, while defeating Coalition armies at Hondschoote and Wattignies.[114] The new class of military leaders included a young colonel named Napoleon Bonaparte, who was appointed commander of artillery at the siege of Toulon thanks to his friendship with Augustin Robespierre. His success in that role resulted in promotion to the Army of Italy in April 1794, and the beginning of his rise to military and political power.[115] Although intended to bolster revolutionary fervour, the Reign of Terror rapidly degenerated into the settlement of personal grievances. At the end of July, the Convention set price controls on a wide range of goods, with the death penalty for hoarders. On 9 September, 'revolutionary groups' were established to enforce these controls, while the Law of Suspects on 17 September approved the arrest of suspected \"enemies of freedom\". This initiated what has become known as the \"Terror\". From September 1793 to July 1794, around 300,000 were arrested,[116] with some 16,600 people executed on charges of counter-revolutionary activity, while another 40,000 may have been summarily executed, or died awaiting trial.[117] Price controls made farmers reluctant to sell their produce in Parisian markets, and by early September the city was suffering acute food shortages. At the same time, the war increased public debt, which the Assembly tried to finance by selling confiscated property. However, few would buy assets that might be repossessed by their former owners, a concern that could only be alleviated by military victory. This meant the financial position worsened as threats to the Republic increased, while printing assignats to deal with the deficit further increased inflation.[118] On 10 October, the Convention recognised the Committee of Public Safety as the supreme Revolutionary Government and suspended the constitution until peace was achieved.[112] In mid-October, Marie Antoinette was convicted of a long list of crimes and guillotined; two weeks later, the Girondist leaders arrested in June were also executed, along with Philippe \u00c9galit\u00e9. The \"Terror\" was not confined to Paris, with over 2,000 killed in Lyons after its recapture.[119] At Cholet on 17 October, the Republican army won a decisive victory over the Vend\u00e9e rebels, and the survivors escaped into Brittany. A defeat at Le Mans on 23 December ended the rebellion as a major threat, although the insurgency continued until 1796. The extent of the repression that followed has been debated by French historians since the mid-19th century.[120] Between November 1793 and February 1794, over 4,000 were drowned in the Loire at Nantes under the supervision of Jean-Baptiste Carrier. Historian Reynald Secher claims that as many as 117,000 died between 1793 and 1796. Although those numbers have been challenged, Fran\u00e7ois Furet concludes it \"not only revealed massacre and destruction on an unprecedented scale, but a zeal so violent that it has bestowed as its legacy much of the region's identity.\"[121][c] At the height of the Terror, not even its supporters were immune from suspicion, leading to divisions within the Montagnard faction between radical H\u00e9bertists and moderates led by Danton.[d] Robespierre saw their dispute as de-stabilising the regime, and, as a deist, objected to the anti-religious policies advocated by the atheist H\u00e9bert, who was arrested and executed on 24 March with 19 of his colleagues.[125] To retain the loyalty of the remaining H\u00e9bertists, Danton was arrested and executed on 5 April with Camille Desmoulins, after a show trial that arguably did more damage to Robespierre than any other act in this period.[126] The Law of 22 Prairial (10 June) denied \"enemies of the people\" the right to defend themselves. Those arrested in the provinces were sent to Paris for judgment; from March to July, executions in Paris increased from 5 to 26 per day.[127] Many Jacobins ridiculed the festival of the Cult of the Supreme Being on 8 June, a lavish and expensive ceremony led by Robespierre, who was also accused of circulating claims he was a second Messiah. Relaxation of price controls and rampant inflation caused increasing unrest among the sans-culottes, but the improved military situation reduced fears the Republic was in danger. Fearing their own survival depended on Robespierre's removal, on 29 June three members of the Committee of Public Safety openly accused him of being a dictator.[128] Robespierre responded by refusing to attend Committee meetings, allowing his opponents to build a coalition against him. In a speech made to the convention on 26 July, he claimed certain members were conspiring against the Republic, an almost certain death sentence if confirmed. When he refused to provide names, the session broke up in confusion. That evening he repeated these claims at the Jacobins club, where it was greeted with demands for execution of the 'traitors'. Fearing the consequences if they did not act first, his opponents attacked Robespierre and his allies in the Convention next day. When Robespierre attempted to speak, his voice failed, one deputy crying \"The blood of Danton chokes him!\"[129] After the Convention authorised his arrest, he and his supporters took refuge in the Hotel de Ville, which was defended by elements of the National Guard. Other units loyal to the Convention stormed the building that evening and detained Robespierre, who severely injured himself attempting suicide. He was executed on 28 July with 19 colleagues, including Saint-Just and Georges Couthon, followed by 83 members of the Commune.[130] The Law of 22 Prairial was repealed, any surviving Girondists reinstated as deputies, and the Jacobin Club was closed and banned.[131] There are various interpretations of the Terror and the violence with which it was conducted. Furet argues that the intense ideological commitment of the revolutionaries and their utopian goals required the extermination of any opposition.[132] A middle position suggests violence was not inevitable but the product of a series of complex internal events, exacerbated by war.[133] The bloodshed did not end with the death of Robespierre; southern France saw a wave of revenge killings, directed against alleged Jacobins, Republican officials and Protestants. Although the victors of Thermidor asserted control over the Commune by executing their leaders, some of those closely involved in the \"Terror\" retained their positions. They included Paul Barras, later chief executive of the French Directory, and Joseph Fouch\u00e9, director of the killings in Lyon who served as Minister of Police under the Directory, the Consulate and Empire.[134] Despite his links to Augustin Robespierre, military success in Italy meant Bonaparte escaped censure.[135] The December 1794 Treaty of La Jaunaye ended the Chouannerie in western France by allowing freedom of worship and the return of non-juring priests.[136] This was accompanied by military success; in January 1795, French forces helped the Dutch Patriots set up the Batavian Republic, securing their northern border.[137] The war with Prussia was concluded in favour of France by the Peace of Basel in April 1795, while Spain made peace shortly thereafter.[138] However, the Republic still faced a crisis at home. Food shortages arising from a poor 1794 harvest were exacerbated in northern France by the need to supply the army in Flanders, while the winter was the worst since 1709.[139] By April 1795, people were starving, and the assignat was worth only 8% of its face value; in desperation, the Parisian poor rose again.[140] They were quickly dispersed and the main impact was another round of arrests, while Jacobin prisoners in Lyon were summarily executed.[141] A committee drafted the Constitution of the Year III, approved by plebiscite on 23 September 1795 and put into place on 27 September.[142] Largely designed by Pierre Daunou and Boissy d'Anglas, it established a bicameral legislature, intended to slow down the legislative process, ending the wild swings of policy under the previous unicameral systems. The Council of 500 was responsible for drafting legislation, which was reviewed and approved by the Council of Ancients, an upper house containing 250 men over the age of 40. Executive power was in the hands of five directors, selected by the Council of Ancients from a list provided by the lower house, with a five-year mandate.[143] Deputies were chosen by indirect election, a total franchise of around 5 million voting in primaries for 30,000 electors, or 0.6% of the population. Since they were also subject to stringent property qualification, it guaranteed the return of conservative or moderate deputies. In addition, rather than dissolving the previous legislature as in 1791 and 1792, the so-called 'law of two-thirds' ruled only 150 new deputies would be elected each year. The remaining 600 Conventionnels kept their seats, a move intended to ensure stability.[144] Jacobin sympathisers viewed the French Directory as a betrayal of the Revolution, while Bonapartists later justified Napoleon's coup by emphasising its corruption.[145] The regime also faced internal unrest, a weak economy, and an expensive war, while the Council of 500 could block legislation at will. Since the directors had no power to call new elections, the only way to break a deadlock was rule by decree or use force. As a result, the directory was characterised by \"chronic violence, ambivalent forms of justice, and repeated recourse to heavy-handed repression.\"[146] Retention of the Conventionnels ensured the Thermidorians held a majority in the legislature and three of the five directors, but they were increasingly challenged by the right. On 5 October, Convention troops led by Napoleon put down a royalist rising in Paris; when the first legislative elections were held two weeks later, over 100 of the 150 new deputies were royalists of some sort.[147] The power of the Parisian sans-culottes had been broken by the suppression of the May 1795 revolt; relieved of pressure from below, the Jacobin clubs became supporters of the directory, largely to prevent restoration of the monarchy.[148] Removal of price controls and a collapse in the value of the assignat led to inflation and soaring food prices. By April 1796, over 500,000 Parisians were unemployed, resulting in the May insurrection known as the Conspiracy of the Equals. Led by the revolutionary Fran\u00e7ois-No\u00ebl Babeuf, their demands included immediate implementation of the 1793 Constitution, and a more equitable distribution of wealth. Despite support from sections of the military, the revolt was easily crushed, while Babeuf and other leaders were executed.[149] Nevertheless, by 1799 the economy had been stabilised, and important reforms made allowing steady expansion of French industry. Many of these remained in place for much of the 19th century.[150] Prior to 1797, three of the five directors were firmly Republican; Barras, R\u00e9velli\u00e8re-L\u00e9peaux and Jean-Fran\u00e7ois Rewbell, as were around 40% of the legislature. The same percentage were broadly centrist or unaffiliated, along with two directors, \u00c9tienne-Fran\u00e7ois Letourneur and Lazare Carnot. Although only 20% were committed Royalists, many centrists supported the restoration of the exiled Louis XVIII in the belief this would bring peace.[151] The elections of May 1797 resulted in significant gains for the right, with Royalists Jean-Charles Pichegru elected president of the Council of 500, and Barth\u00e9lemy appointed a director.[152] With Royalists apparently on the verge of power, Republicans attempted a pre-emptive coup on 4 September. Using troops from Napoleon's Army of Italy under Pierre Augereau, the Council of 500 was forced to approve the arrest of Barth\u00e9lemy, Pichegru and Carnot. The elections were annulled, 63 leading Royalists deported to French Guiana, and laws were passed against \u00e9migr\u00e9s, Royalists and ultra-Jacobins. The removal of his conservative opponents opened the way for direct conflict between Barras and those on the left.[153] Fighting continued despite general war weariness, and the 1798 elections resulted in a resurgence in Jacobin strength. Napoleon's invasion of Egypt in July 1798 confirmed European fears of French expansionism, and the War of the Second Coalition began in November. Without a majority in the legislature, the directors relied on the army to enforce decrees and extract revenue from conquered territories. Generals like Napoleon and Barth\u00e9lemy Catherine Joubert became central to the political process, while both the army and directory became notorious for their corruption.[154] It has been suggested the directory collapsed because by 1799, many 'preferred the uncertainties of authoritarian rule to the continuing ambiguities of parliamentary politics'.[155] The architect of its end was Siey\u00e8s, who when asked what he had done during the Terror allegedly answered \"I survived\". Nominated to the directory, his first action was to remove Barras, with the help of allies including Talleyrand, and Napoleon's brother Lucien, president of the Council of 500.[156] On 9 November 1799, the coup of 18 Brumaire replaced the five directors with the French Consulate, which consisted of three members, Napoleon, Siey\u00e8s, and Roger Ducos. Most historians consider this the end point of the French Revolution.[157] The role of ideology in the Revolution is controversial with Jonathan Israel stating that the \"radical Enlightenment\" was the primary driving force of the Revolution.[158] Cobban, however, argues \"[t]he actions of the revolutionaries were most often prescribed by the need to find practical solutions to immediate problems, using the resources at hand, not by pre-conceived theories.\"[159] The identification of ideologies is complicated by the profusion of revolutionary clubs, factions and publications, absence of formal political parties, and individual flexibility in the face of changing circumstances.[160] In addition, although the Declaration of the Rights of Man was a fundamental document for all revolutionary factions, its interpretation varied widely.[161] While all revolutionaries professed their devotion to liberty in principle, \"it appeared to mean whatever those in power wanted.\"[162] For example, the liberties specified in the Rights of Man were limited by law when they might \"cause harm to others, or be abused\". Prior to 1792, Jacobins and others frequently opposed press restrictions on the grounds these violated a basic right.[163] However, the radical National Convention passed laws in September 1793 and July 1794 imposing the death penalty for offences such as \"disparaging the National Convention\", and \"misleading public opinion.\"[164] While revolutionaries also endorsed the principle of equality, few advocated equality of wealth since property was also viewed as a right.[165] The National Assembly opposed equal political rights for women,[166] while the abolition of slavery in the colonies was delayed until February 1794 because it conflicted with the property rights of slave owners, and many feared it would disrupt trade.[167] Political equality for male citizens was another divisive issue, with the 1791 constitution limiting the right to vote and stand for office to males over 25 who met a property qualification, so-called \"active citizens\". This restriction was opposed by many activists, including Robespierre, the Jacobins, and Cordeliers.[168] The principle that sovereignty resided in the nation was a key concept of the Revolution.[169] However, Israel argues this obscures ideological differences over whether the will of the nation was best expressed through representative assemblies and constitutions, or direct action by revolutionary crowds, and popular assemblies such as the sections of the Paris commune.[170] Many considered constitutional monarchy as incompatible with the principle of popular sovereignty,[171] but prior to 1792, there was a strong bloc with an ideological commitment to such a system, based on the writings of Thomas Hobbes, John Locke, Montesquieu and Voltaire.[172] Israel argues the nationalisation of church property and the establishment of the Constitutional Church reflected an ideological commitment to secularism, and a determination to undermine a bastion of old regime privilege.[173] While Cobban agrees the Constitutional Church was motivated by ideology, he sees its origins in the anti-clericalism of Voltaire and other Enlightenment figures.[174] Jacobins were hostile to formal political parties and factions which they saw as a threat to national unity and the general will, with \"political virtue\" and \"love of country\" key elements of their ideology.[175][176] They viewed the ideal revolutionary as selfless, sincere, free of political ambition, and devoted to the nation.[177] The disputes leading to the departure first of the Feuillants, then later the Girondists, were conducted in terms of the relative political virtue and patriotism of the disputants. In December 1793, all members of the Jacobin clubs were subject to a \"purifying scrutiny\", to determine whether they were \"men of virtue\".[178] The Revolution initiated a series of conflicts that began in 1792 and ended with Napoleon's defeat at Waterloo in 1815. In its early stages, this seemed unlikely; the 1791 Constitution specifically disavowed \"war for the purpose of conquest\", and although traditional tensions between France and Austria re-emerged in the 1780s, Emperor Joseph II cautiously welcomed the reforms. Austria was at war with the Ottomans, as were the Russians, while both were negotiating with Prussia over partitioning Poland. Most importantly, Britain preferred peace, and as Emperor Leopold II stated after the Declaration of Pillnitz, \"without England, there is no case\".[179] In late 1791, factions within the Assembly came to see war as a way to unite the country and secure the Revolution by eliminating hostile forces on its borders and establishing its \"natural frontiers\".[180] France declared war on Austria in April 1792 and issued the first conscription orders, with recruits serving for twelve months. By the time peace finally came in 1815, the conflict had involved every major European power as well as the United States, redrawn the map of Europe and expanded into the Americas, the Middle East, and the Indian Ocean.[181] From 1701 to 1801, the population of Europe grew from 118 to 187\u00a0million; combined with new mass production techniques, this allowed belligerents to support large armies, requiring the mobilisation of national resources. It was a different kind of war, fought by nations rather than kings, intended to destroy their opponents' ability to resist, but also to implement deep-ranging social change. While all wars are political to some degree, this period was remarkable for the emphasis placed on reshaping boundaries and the creation of entirely new European states.[182] In April 1792, French armies invaded the Austrian Netherlands but suffered a series of setbacks before victory over an Austrian-Prussian army at Valmy in September. After defeating a second Austrian army at Jemappes on 6 November, they occupied the Netherlands, areas of the Rhineland, Nice and Savoy. Emboldened by this success, in February 1793 France declared war on the Dutch Republic, Spain and Britain, beginning the War of the First Coalition.[183] However, the expiration of the 12-month term for the 1792 recruits forced the French to relinquish their conquests. In August, new conscription measures were passed, and by May 1794 the French army had between 750,000 and 800,000 men.[184] Despite high rates of desertion, this was large enough to manage multiple internal and external threats; for comparison, the combined Prussian-Austrian army was less than 90,000.[185] By February 1795, France had annexed the Austrian Netherlands, established their frontier on the left bank of the Rhine and replaced the Dutch Republic with the Batavian Republic, a satellite state. These victories led to the collapse of the anti-French coalition; Prussia made peace in April 1795, followed soon after by Spain, leaving Britain and Austria as the only major powers still in the war.[186] In October 1797, a series of defeats by Bonaparte in Italy led Austria to agree to the Treaty of Campo Formio, in which they formally ceded the Netherlands and recognised the Cisalpine Republic.[187] Fighting continued for two reasons; first, French state finances had come to rely on indemnities levied on their defeated opponents. Second, armies were primarily loyal to their generals, for whom the wealth achieved by victory and the status it conferred became objectives in themselves. Leading soldiers like Lazare Hoche, Jean-Charles Pichegru and Lazare Carnot wielded significant political influence and often set policy; Campo Formio was approved by Bonaparte, not the Directory, which strongly objected to terms it considered too lenient.[187] Despite these concerns, the Directory never developed a realistic peace programme, fearing the destabilising effects of peace and the consequent demobilisation of hundreds of thousands of young men. As long as the generals and their armies stayed away from Paris, they were happy to allow them to continue fighting, a key factor behind sanctioning Bonaparte's invasion of Egypt. This resulted in aggressive and opportunistic policies, leading to the War of the Second Coalition in November 1798.[188] In 1789, the most populous French colonies were Saint-Domingue (today Haiti), Martinique, Guadeloupe, the \u00cele Bourbon (R\u00e9union) and the \u00cele\u00a0de la France. These colonies produced commodities such as sugar, coffee and cotton for exclusive export to France. There were about 700,000 slaves in the colonies, of which about 500,000 were in Saint-Domingue. Colonial products accounted for about a third of France's exports.[189] In February 1788, the Society of the Friends of the Blacks was formed in France with the aim of abolishing slavery in the empire. In August 1789, colonial slave owners and merchants formed the rival Club de Massiac to represent their interests. When the Constituent Assembly adopted the Declaration of the Rights of Man and of the Citizen in August 1789, delegates representing the colonial landowners successfully argued that the principles should not apply in the colonies as they would bring economic ruin and disrupt trade. Colonial landowners also gained control of the Colonial Committee of the Assembly from where they exerted a powerful influence against abolition.[190][191] People of colour also faced social and legal discrimination in mainland France and its colonies, including a bar on their access to professions such as law, medicine and pharmacy.[192][e] In 1789\u201390, a delegation of free coloureds, led by Vincent Og\u00e9 and Julien Raimond, unsuccessfully lobbied the Assembly to end discrimination against this group. Og\u00e9 left for Saint-Domingue where an uprising against white landowners broke out in October 1790. The revolt failed, and Og\u00e9 was killed.[196][191] In May 1791, the National Assembly granted full political rights to coloureds born of two free parents but left the rights of freed slaves to be determined by the colonial assemblies. The assemblies refused to implement the decree and fighting broke out between the coloured population of Saint-Domingue and white colonists, each side recruiting slaves to their forces. A major slave revolt followed in August.[197] In March 1792, the Legislative Assembly responded to the revolt by granting citizenship to all free coloureds and sending two commissioners, L\u00e9ger-F\u00e9licit\u00e9 Sonthonax and \u00c9tienne Polverel, and 6,000 troops to Saint-Domingue to enforce the decree. On arrival in September, the commissioners announced that slavery would remain in force. Over 72,000 slaves were still in revolt, mostly in the north.[198] Brissot and his supporters envisaged an eventual abolition of slavery but their immediate concern was securing trade and the support of merchants for the revolutionary wars. After Brissot's fall, the new constitution of June 1793 included a new Declaration of the Rights of Man and the Citizen but excluded the colonies from its provisions. In any event, the new constitution was suspended until France was at peace.[199] In early 1793, royalist planters from Guadeloupe and Saint-Domingue formed an alliance with Britain. The Spanish supported insurgent slaves, led by Jean-Fran\u00e7ois Papillon and Georges Biassou, in the north of Saint-Domingue. White planters loyal to the republic sent representatives to Paris to convince the Jacobin controlled Convention that those calling for the abolition of slavery were British agents and supporters of Brissot, hoping to disrupt trade.[200] In June, the commissioners in Saint-Domingue freed 10,000 slaves fighting for the republic. As the royalists and their British and Spanish supporters were also offering freedom for slaves willing to fight for their cause, the commissioners outbid them by abolishing slavery in the north in August, and throughout the colony in October. Representatives were sent to Paris to gain the approval of the convention for the decision.[200][201] The Convention voted for the abolition of slavery in the colonies on 4 February 1794 and decreed that all residents of the colonies had the full rights of French citizens irrespective of colour.[202] An army of 1,000 sans-culottes led by Victor Hugues was sent to Guadeloupe to expel the British and enforce the decree. The army recruited former slaves and eventually numbered 11,000, capturing Guadeloupe and other smaller islands. Abolition was also proclaimed on Guyane. Martinique remained under British occupation, while colonial landowners in R\u00e9union and the \u00celes Mascareignes repulsed the republicans.[203] Black armies drove the Spanish out of Saint-Domingue in 1795, and the British troops withdrew in 1798.[204] In republican controlled areas from 1793 to 1799, freed slaves were required to work on their former plantations or for their former masters if they were in domestic service. They were paid a wage and gained property rights. Black and coloured generals were effectively in control of large areas of Guadeloupe and Saint-Domingue, including Toussaint Louverture in the north of Saint-Domingue, and Andr\u00e9 Rigaud in the south. Historian Fr\u00e9deric R\u00e9gent states that the restrictions on the freedom of employment and movement of former slaves meant that, \"only whites, persons of color already freed before the decree, and former slaves in the army or on warships really benefited from general emancipation.\"[203] Newspapers and pamphlets played a central role in stimulating and defining the Revolution. Prior to 1789, there have been a small number of heavily censored newspapers that needed a royal licence to operate, but the Estates General created an enormous demand for news, and over 130 newspapers appeared by the end of the year. Among the most significant were Marat's L'Ami du peuple and Elys\u00e9e Loustallot's Revolutions de Paris\u00a0[fr].[205] Over the next decade, more than 2,000 newspapers were founded, 500 in Paris alone. Most lasted only a matter of weeks but they became the main communication medium, combined with the very large pamphlet literature.[206] Newspapers were read aloud in taverns and clubs and circulated hand to hand. There was a widespread assumption that writing was a vocation, not a business, and the role of the press was the advancement of civic republicanism.[207] By 1793 the radicals were most active but initially the royalists flooded the country with their publication the \"L'Ami du Roi\u00a0[fr]\" (Friends of the King) until they were suppressed.[208] To illustrate the differences between the new Republic and the old regime, the leaders needed to implement a new set of symbols to be celebrated instead of the old religious and monarchical symbols. To this end, symbols were borrowed from historic cultures and redefined, while those of the old regime were either destroyed or reattributed acceptable characteristics. These revised symbols were used to instil in the public a new sense of tradition and reverence for the Enlightenment and the Republic.[209] \"La Marseillaise\" (French pronunciation: [la ma\u0281s\u025bj\u025b\u02d0z]) became the national anthem of France. The song was written and composed in 1792 by Claude Joseph Rouget de Lisle, and was originally titled \"Chant de guerre pour l'Arm\u00e9e du Rhin\". The French National Convention adopted it as the First Republic's anthem in 1795. It acquired its nickname after being sung in Paris by volunteers from Marseille marching on the capital. The song is the first example of the \"European march\" anthemic style, while the evocative melody and lyrics led to its widespread use as a song of revolution and incorporation into many pieces of classical and popular music. De Lisle was instructed to 'produce a hymn which conveys to the soul of the people the enthusiasm which it (the music) suggests.'[211] The guillotine remains \"the principal symbol of the Terror in the French Revolution.\"[212] Invented by a physician during the Revolution as a quicker, more efficient and more distinctive form of execution, the guillotine became a part of popular culture and historic memory. It was celebrated on the left as the people's avenger, for example in the revolutionary song La guillotine permanente,[213] and cursed as the symbol of the Terror by the right.[214] Its operation became a popular entertainment that attracted great crowds of spectators. Vendors sold programmes listing the names of those scheduled to die. Many people came day after day and vied for the best locations from which to observe the proceedings; knitting women (tricoteuses) formed a cadre of hardcore regulars, inciting the crowd. Parents often brought their children. By the end of the Terror, the crowds had thinned drastically. Repetition had staled even this most grisly of entertainments, and audiences grew bored.[215] Cockades were widely worn by revolutionaries beginning in 1789. They pinned the blue-and-red cockade of Paris onto the white cockade of the Ancien R\u00e9gime. Camille Desmoulins asked his followers to wear green cockades on 12 July 1789. The Paris militia, formed on 13 July, adopted a blue and red cockade. Blue and red are the traditional colours of Paris, and they are used on the city's coat of arms. Cockades with various colour schemes were used during the storming of the Bastille on 14 July.[216] The Liberty cap, also known as the Phrygian cap, or pileus, is a brimless, felt cap that is conical in shape with the tip pulled forward. It reflects Roman republicanism and liberty, alluding to the Roman ritual of manumission, in which a freed slave receives the bonnet as a symbol of his newfound liberty.[217] In August and October 1793, revolutionary authorities ordered the exhumation of the remains of members of the royal family buried at the Saint-Denis basilica, which had served as the principal burial site of French royalty since the early Middle Ages.[218][page\u00a0needed]The remains were reburied in mass graves.[219][page\u00a0needed] These acts reflected revolutionary hostility toward royal authority and the symbolic power of dynastic memory.[220][page\u00a0needed] Deprived of political rights by the Ancien R\u00e9gime, the Revolution initially allowed women to participate, although only to a limited degree. Activists included Girondists like Olympe de Gouges, author of the Declaration of the Rights of Woman and of the Female Citizen, and Charlotte Corday, killer of Marat. Others like Th\u00e9roigne de M\u00e9ricourt, Pauline L\u00e9on and the Society of Revolutionary Republican Women supported the Jacobins, staged demonstrations in the National Assembly and took part in the October 1789 March to Versailles. Despite this, the 1791 and 1793 constitutions denied them political rights and democratic citizenship.[221] In 1793, the Society of Revolutionary Republican Women campaigned for strict price controls on bread, and a law that would compel all women to wear the tricolour cockade. Although both demands were successful, in October the male-dominated Jacobins who then controlled the government denounced the Society as dangerous rabble-rousers and made all women's clubs and associations illegal. Organised women were permanently shut out of the French Revolution after 30 October 1793.[222] At the same time, especially in the provinces, women played a prominent role in resisting social changes introduced by the Revolution. This was particularly so in terms of the reduced role of the Catholic Church; for those living in rural areas, closing of the churches meant a loss of normality.[223] This sparked a counter-revolutionary movement led by women; while supporting other political and social changes, they opposed the dissolution of the Catholic Church and revolutionary cults like the Cult of the Supreme Being.[224] Olwen Hufton argues some wanted to protect the Church from heretical changes enforced by revolutionaries, viewing themselves as \"defenders of faith\".[225] Olympe de Gouges was an author whose publications emphasised that while women and men were different, this should not prevent equality under the law. In her Declaration of the Rights of Woman and of the Female Citizen she insisted women deserved rights, especially in areas concerning them directly, such as divorce and recognition of illegitimate children. Along with other Girondists, she was executed in November 1793 during the Terror. Madame Roland, also known as Manon or Marie Roland, was another important female activist whose political focus was not specifically women but other aspects of the government. A Girondist, her personal letters to leaders of the Revolution influenced policy; in addition, she often hosted political gatherings of the Brissotins, a political group which allowed women to join. She too was executed in November 1793.[226] The Revolution abolished many economic constraints imposed by the Ancien R\u00e9gime, including church tithes and feudal dues although tenants often paid higher rents and taxes.[227] All church lands were nationalised, along with those owned by Royalist exiles, which were used to back paper currency known as assignats, and the feudal guild system eliminated.[228] It also abolished the highly inefficient system of tax farming, whereby private individuals would collect taxes for a hefty fee. The government seized the foundations that had been set up (starting in the 13th century) to provide an annual stream of revenue for hospitals, poor relief, and education. The state sold the lands but typically local authorities did not replace the funding and so most of the nation's charitable and school systems were massively disrupted.[229] Between 1790 and 1796, industrial and agricultural output dropped, foreign trade plunged, and prices soared, forcing the government to finance expenditure by issuing ever increasing quantities assignats. When this resulted in escalating inflation, the response was to impose price controls and persecute private speculators and traders, creating a black market. Between 1789 and 1793, the annual deficit increased from 10% to 64% of gross national product, while annual inflation reached 3,500% after a poor harvest in 1794 and the removal of price controls. The assignats were withdrawn in 1796 but inflation continued until the introduction of the gold-based Franc germinal in 1803.[230] The French Revolution had a major impact on western history by ending feudalism in France and creating a path for advances in individual freedoms throughout Europe.[231][2] The revolution represented the most significant challenge to political absolutism up to that point in history and spread democratic ideals throughout Europe and ultimately the world.[232] Its impact on French nationalism was profound, while also stimulating nationalist movements throughout Europe.[233] Some modern historians argue the concept of the nation state was a direct consequence of the revolution.[234] As such, the revolution is often seen as marking the start of modernity and the modern period.[235] The long-term impact on France was profound, shaping politics, society, religion and ideas, and polarising politics for more than a century. Historian Fran\u00e7ois Aulard writes: \"From the social point of view, the Revolution consisted in the suppression of what was called the feudal system, in the emancipation of the individual, in greater division of landed property, the abolition of the privileges of noble birth, the establishment of equality, the simplification of life.... The French Revolution differed from other revolutions in being not merely national, for it aimed at benefiting all humanity.\"[236][title\u00a0missing] The revolution permanently crippled the power of the aristocracy and drained the wealth of the Church, although the two institutions survived. Hanson suggests the French underwent a fundamental transformation in self-identity, evidenced by the elimination of privileges and their replacement by intrinsic human rights.[237] After the collapse of the First French Empire in 1815, the French public lost many of the rights and privileges earned since the revolution, but remembered the participatory politics that characterised the period. According to Paul Hanson, \"Revolution became a tradition, and republicanism an enduring option.\"[238] The Revolution meant an end to arbitrary royal rule and held out the promise of rule by law under a constitutional order. Napoleon as emperor set up a constitutional system and the restored Bourbons were forced to retain one. After the abdication of Napoleon III in 1871, the French Third Republic was launched with a deep commitment to upholding the ideals of the Revolution.[239][240] The Vichy regime (1940\u20131944) tried to undo the revolutionary heritage but retained the republic. However, there were no efforts by the Bourbons, Vichy or any other government to restore the privileges that had been stripped away from the nobility in 1789. France permanently became a society of equals under the law.[238] Agriculture was transformed by the Revolution. With the breakup of large estates controlled by the Church and the nobility and worked by hired hands, rural France became more a land of small independent farms. Harvest taxes were ended, such as the tithe and seigneurial dues. Primogeniture was ended both for nobles and peasants, thereby weakening the family patriarch, and led to a fall in the birth rate since all children had a share in the family property.[241] Cobban argues the Revolution bequeathed to the nation \"a ruling class of landowners.\"[242] Economic historians are divided on the economic impact of the Revolution. One suggestion is the resulting fragmentation of agricultural holdings had a significant negative impact in the early years of 19th century, then became positive in the second half of the century because it facilitated the rise in human capital investments.[243] Others argue the redistribution of land had an immediate positive impact on agricultural productivity, before the scale of these gains gradually declined over the course of the 19th century.[244] In the cities, entrepreneurship on a small scale flourished, as restrictive monopolies, privileges, barriers, rules, taxes and guilds gave way. However, the British blockade virtually ended overseas and colonial trade, hurting the cities and their supply chains. Overall, the Revolution did not greatly change the French business system, and probably helped freeze in place the horizons of the small business owner. The typical businessman owned a small store, mill or shop, with family help and a few paid employees; large-scale industry was less common than in other industrialising nations.[245] Historians often see the impact of the Revolution as through the institutions and ideas exported by Napoleon. Economic historians Dan Bogart, Mauricio Drelichman, Oscar Gelderblom, and Jean-Laurent Rosenthal describe Napoleon's codified law as the French Revolution's \"most significant export.\"[246] According to Daron Acemoglu, Davide Cantoni, Simon Johnson, and James A. Robinson the French Revolution had long-term effects in Europe. They suggest that \"areas that were occupied by the French and that underwent radical institutional reform experienced more rapid urbanization and economic growth, especially after 1850. There is no evidence of a negative effect of French invasion.\"[247] The Revolution sparked intense debate in Britain. The Revolution Controversy was a \"pamphlet war\" set off by the publication of A Discourse on the Love of Our Country, a speech given by Richard Price to the Revolution Society on 4 November 1789, supporting the French Revolution. Edmund Burke responded in November 1790 with his own pamphlet, Reflections on the Revolution in France, attacking the French Revolution as a threat to the aristocracy of all countries.[248][249] William Coxe opposed Price's premise that one's country is principles and people, not the State itself.[250] Conversely, two seminal political pieces of political history were written in Price's favour, supporting the general right of the French people to replace their State. One of the first of these \"pamphlets\" into print was A Vindication of the Rights of Men by Mary Wollstonecraft. Wollstonecraft's title was echoed by Thomas Paine's Rights of Man, published a few months later. In 1792 Christopher Wyvill published Defence of Dr. Price and the Reformers of England, a plea for reform and moderation.[251] This exchange of ideas has been described as \"one of the great political debates in British history\".[252] In Ireland, the effect was to transform what had been an attempt by Protestant settlers to gain some autonomy into a mass movement led by the Society of United Irishmen involving Catholics and Protestants. It stimulated the demand for further reform throughout Ireland, especially in Ulster, and led to the Irish Rebellion of 1798, which was brutally suppressed by government troops.[253] The German reaction to the Revolution swung from favourable to antagonistic. At first it brought liberal and democratic ideas, the end of guilds, serfdom and the Jewish ghetto. It brought economic freedoms and agrarian and legal reform. Above all the antagonism helped stimulate and shape German nationalism.[254] France invaded Switzerland and turned it into the \"Helvetic Republic\" (1798\u20131803), a French puppet state. French interference with localism and traditions was deeply resented in Switzerland, although some reforms took hold and survived in the later period of restoration.[255][256] France invaded and occupied the region now known as Belgium between 1794 and 1814. The new government enforced reforms, incorporating the region into France. Resistance was strong in every sector, as Belgian nationalism emerged to oppose French rule. The French legal system, however, was adopted, with its equal legal rights, and abolition of class distinctions.[257] The Kingdom of Denmark adopted liberalising reforms in line with those of the French Revolution. Reform was gradual and the regime itself carried out agrarian reforms that had the effect of weakening absolutism by creating a class of independent peasant freeholders. Much of the initiative came from well-organised liberals who directed political change in the first half of the 19th century.[258] The Constitution of Norway of 1814 was inspired by the French Revolution[259] and was considered to be one of the most liberal and democratic constitutions at the time.[260] Initially, most people in the Province of Quebec were favourable toward the revolutionaries' aims. The Revolution took place against the background of an ongoing campaign for constitutional reform in the colony by Loyalist emigrants from the United States.[261] Public opinion began to shift against the Revolution after the Flight to Varennes and further soured after the September Massacres and the subsequent execution of Louis XVI.[262] French migration to the Canadas experienced a substantial decline during and after the Revolution. Only a limited number of artisans, professionals, and religious emigres were allowed to settle in the region during this period.[263] Most emigres settled in Montreal or Quebec City.[263] The influx of religious emigres also revitalised the local Catholic Church, with exiled priests establishing a number of parishes across the Canadas.[263] In the United States, the French Revolution deeply polarised American politics, and this polarisation led to the creation of the First Party System. In 1793, as war broke out in Europe, the Democratic-Republican Party led by former American minister to France Thomas Jefferson favored revolutionary France and pointed to the 1778 treaty that was still in effect. George Washington and his unanimous cabinet, including Jefferson, decided that the treaty did not bind the United States to enter the war. Washington proclaimed neutrality instead.[264] The first writings on the French revolution were near contemporaneous with events and mainly divided along ideological lines. These included Edmund Burke's conservative critique Reflections on the Revolution in France (1790) and Thomas Paine's response Rights of Man (1791).[265] From 1815, narrative histories dominated, often based on first-hand experience of the revolutionary years. By the mid-nineteenth century, more scholarly histories appeared, written by specialists and based on original documents and a more critical assessment of contemporary accounts.[266] Dupuy identifies three main strands in nineteenth century historiography of the Revolution. The first is represented by reactionary writers who rejected the revolutionary ideals of popular sovereignty, civil equality, and the promotion of rationality, progress and personal happiness over religious faith. The second stream is those writers who celebrated its democratic, and republican values. The third were liberals like Germaine de Sta\u00ebl and Guizot, who accepted the necessity of reforms establishing a constitution and the rights of man, but rejected state interference with private property and individual rights, even when supported by a democratic majority.[267] Jules Michelet was a leading 19th-century historian of the democratic republican strand, and Thiers, Mignet and Tocqueville were prominent in the liberal strand.[268] Hippolyte Taine's Origins of Contemporary France (1875\u20131894) was modern in its use of departmental archives, but Dupuy sees him as reactionary, given his contempt for the crowd, and Revolutionary values.[269] The broad distinction between conservative, democratic-republican and liberal interpretations of the Revolution persisted in the 20th-century, although historiography became more nuanced, with greater attention to critical analysis of documentary evidence.[269][270] Alphonse Aulard (1849\u20131928) was the first professional historian of the Revolution; he promoted graduate studies, scholarly editions, and learned journals.[271][272] His major work, The French Revolution, a Political History, 1789\u20131804 (1905), was a democratic and republican interpretation of the Revolution.[273] Socio-economic analysis and a focus on the experiences of ordinary people dominated French studies of the Revolution from the 1930s.[274] Georges Lefebvre elaborated a Marxist socio-economic analysis of the revolution with detailed studies of peasants, the rural panic of 1789, and the behaviour of revolutionary crowds.[275][276] Albert Soboul, also writing in the Marxist-Republican tradition, published a major study of the sans-culottes in 1958.[277] Alfred Cobban challenged Jacobin-Marxist social and economic explanations of the revolution in two important works, The Myth of the French Revolution (1955) and Social Interpretation of the French Revolution (1964). He argued the Revolution was primarily a political conflict, which ended in a victory for conservative property owners, a result which retarded economic development.[278][279] In their 1965 work, La Revolution fran\u00e7aise, Fran\u00e7ois Furet and Denis Richet also argued for the primacy of political decisions, contrasting the reformist period of 1789 to 1790 with the following interventions of the urban masses which led to radicalisation and an ungovernable situation.[280] From the 1990s, Western scholars largely abandoned Marxist interpretations of the revolution in terms of bourgeoisie-proletarian class struggle as anachronistic. However, no new explanatory model has gained widespread support.[235][281] The historiography of the Revolution has expanded into areas such as cultural and regional histories, visual representations, transnational interpretations, and decolonisation.[280]",
      "ground_truth_chunk_ids": [
        "22_fixed_chunk1"
      ],
      "source_ids": [
        "S022"
      ],
      "category": "factual",
      "id": 36
    },
    {
      "question": "What is List of census-designated places in West Virginia?",
      "ground_truth": "The United States Census Bureau separates places by incorporation for statistical purposes during its decennial census. To incorporate, communities may need to meet statutory requirements made by their respective state, such as thresholds in population or specificities relative to location.[a] Federally, the Census Bureau defines incorporated places as areas, whose boundaries do not cross state lines, that \"provide governmental functions for a concentration of people\", as opposed to \"minor civil [divisions], which generally ... provide services or administer an area without regard, necessarily, to population\".[5] Unincorporated communities, classified as census-designated places (CDPs), lack elected municipal officers and boundaries with legal status.[5] The Bureau identified 205 CDPs in the state of West Virginia at the 2020 census. The Municipal Code of West Virginia, which governs incorporation, requires applicant municipal corporations (places for incorporation) that cover an area more than 1 square mile (2.6 km2) to have a minimum of 500 inhabitants or freeholders per square mile, and those under 1 square mile to have at least 100 inhabitants or freeholders. Applicant areas must not reside within a municipality \"urban in character\", nor claim an area \"disproportionate to its number of inhabitants\".[6] Upon approval, the state classifies municipal corporations as a Class I city, with a population of more than fifty thousand, a Class II city, with a population between ten thousand and fifty thousand, a Class III city, with a population between two thousand and ten thousand, or a Class IV town or village, with a population of less than two thousand.[7] All municipalities can \"use a common seal\", defend, maintain, or institute a proceeding in court, and hold, take, purchase, or lease, as lessee, property for municipal purposes.[8] Of the fifty-five counties in West Virginia, Logan is home to the most CDPs, with twenty-five, followed by Fayette, with twenty, and",
      "expected_answer": "The United States Census Bureau separates places by incorporation for statistical purposes during its decennial census. To incorporate, communities may need to meet statutory requirements made by their respective state, such as thresholds in population or specificities relative to location.[a] Federally, the Census Bureau defines incorporated places as areas, whose boundaries do not cross state lines, that \"provide governmental functions for a concentration of people\", as opposed to \"minor civil [divisions], which generally ... provide services or administer an area without regard, necessarily, to population\".[5] Unincorporated communities, classified as census-designated places (CDPs), lack elected municipal officers and boundaries with legal status.[5] The Bureau identified 205 CDPs in the state of West Virginia at the 2020 census. The Municipal Code of West Virginia, which governs incorporation, requires applicant municipal corporations (places for incorporation) that cover an area more than 1 square mile (2.6\u00a0km2) to have a minimum of 500 inhabitants or freeholders per square mile, and those under 1 square mile to have at least 100 inhabitants or freeholders. Applicant areas must not reside within a municipality \"urban in character\", nor claim an area \"disproportionate to its number of inhabitants\".[6] Upon approval, the state classifies municipal corporations as a Class I city, with a population of more than fifty thousand, a Class II city, with a population between ten thousand and fifty thousand, a Class III city, with a population between two thousand and ten thousand, or a Class IV town or village, with a population of less than two thousand.[7] All municipalities can \"use a common seal\", defend, maintain, or institute a proceeding in court, and hold, take, purchase, or lease, as lessee, property for municipal purposes.[8] Of the fifty-five counties in West Virginia, Logan is home to the most CDPs, with twenty-five, followed by Fayette, with twenty, and Raleigh, with eighteen. The largest CDP by population is Teays Valley, with 14,350 residents, while Bowden, with 0 residents over 0.12 square miles (0.31\u00a0km2), represents the state's smallest CDP by both population and area.[9]",
      "ground_truth_chunk_ids": [
        "235_random_chunk1"
      ],
      "source_ids": [
        "S435"
      ],
      "category": "factual",
      "id": 37
    },
    {
      "question": "What is Nutrition?",
      "ground_truth": "Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life. The intake of these substances provides organisms with nutrients (divided into macro- and micro-) which can be metabolized to create energy and chemical structures; too much or too little of an essential nutrient can cause malnutrition. Nutritional science, the study of nutrition as a hard science, typically emphasizes human nutrition. The type of organism determines what nutrients it needs and how it obtains them. Organisms obtain nutrients by consuming organic matter, consuming inorganic matter, absorbing light, or some combination of these. Some can produce nutrients internally by consuming basic elements, while others must consume other organisms to obtain pre-existing nutrients. All forms of life require carbon, energy, and water as well as various other molecules. Animals require complex nutrients such as carbohydrates, lipids, and proteins, obtaining them by consuming other organisms. Humans have developed agriculture and cooking to replace foraging and advance human nutrition. Plants acquire nutrients through the soil and the atmosphere. Fungi absorb nutrients around them by breaking them down and absorbing them through the mycelium. Scientific analysis of food and nutrients began during the chemical revolution in the late 18th century. Chemists in the 18th and 19th centuries experimented with different elements and food sources to develop theories of nutrition.[1] Modern nutrition science began in the 1910s as individual micronutrients began to be identified. The first vitamin to be chemically identified was thiamine in 1926, and vitamin C was identified as a protection against scurvy in 1932.[2] The role of vitamins in nutrition was studied in the following decades. The first recommended dietary allowances for humans were developed to address fears of disease caused by food deficiencies during the Great Depression and the Second World War.[3] Due to",
      "expected_answer": "Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life. The intake of these substances provides organisms with nutrients (divided into macro- and micro-) which can be metabolized to create energy and chemical structures; too much or too little of an essential nutrient can cause malnutrition. Nutritional science, the study of nutrition as a hard science, typically emphasizes human nutrition. The type of organism determines what nutrients it needs and how it obtains them. Organisms obtain nutrients by consuming organic matter, consuming inorganic matter, absorbing light, or some combination of these. Some can produce nutrients internally by consuming basic elements, while others must consume other organisms to obtain pre-existing nutrients. All forms of life require carbon, energy, and water as well as various other molecules. Animals require complex nutrients such as carbohydrates, lipids, and proteins, obtaining them by consuming other organisms. Humans have developed agriculture and cooking to replace foraging and advance human nutrition. Plants acquire nutrients through the soil and the atmosphere. Fungi absorb nutrients around them by breaking them down and absorbing them through the mycelium. Scientific analysis of food and nutrients began during the chemical revolution in the late 18th century. Chemists in the 18th and 19th centuries experimented with different elements and food sources to develop theories of nutrition.[1] Modern nutrition science began in the 1910s as individual micronutrients began to be identified. The first vitamin to be chemically identified was thiamine in 1926, and vitamin C was identified as a protection against scurvy in 1932.[2] The role of vitamins in nutrition was studied in the following decades. The first recommended dietary allowances for humans were developed to address fears of disease caused by food deficiencies during the Great Depression and the Second World War.[3] Due to its importance in human health, the study of nutrition has heavily emphasized human nutrition and agriculture, while ecology is a secondary concern.[4] Nutrients are substances that provide energy and physical components to the organism, allowing it to survive, grow, and reproduce. Nutrients can be basic elements or complex macromolecules. Approximately 30 elements are found in organic matter, with nitrogen, carbon, and phosphorus being the most important.[5] Macronutrients are the primary substances required by an organism, and micronutrients are substances required by an organism in trace amounts. Organic micronutrients are classified as vitamins, and inorganic micronutrients are classified as minerals. Over-nutrition of macronutrients is a major cause of obesity and increases the risk of developing various non-communicable diseases (NCDs), including type 2 diabetes, stroke, hypertension, coronary heart disease, osteoporosis, and some forms of cancer.[6] Nutrients can also be classified as essential or nonessential, with essential meaning the body cannot synthesize the nutrient on its own.[7] Nutrients are absorbed by the cells and used in metabolic biochemical reactions. These include fueling reactions that create precursor metabolites and energy, biosynthetic reactions that convert precursor metabolites into building block molecules, polymerizations that combine these molecules into macromolecule polymers, and assembly reactions that use these polymers to construct cellular structures.[5] Organisms can be classified by how they obtain carbon and energy. Heterotrophs are organisms that obtain nutrients by consuming the carbon of other organisms, while autotrophs are organisms that produce their own nutrients from the carbon of inorganic substances like carbon dioxide. Mixotrophs are organisms that can be heterotrophs and autotrophs, including some plankton and carnivorous plants. Phototrophs obtain energy from light, while chemotrophs obtain energy by consuming chemical energy from matter. Organotrophs consume other organisms to obtain electrons, while lithotrophs obtain electrons from inorganic substances, such as water, hydrogen sulfide, dihydrogen, iron(II), sulfur, or ammonium.[8] Prototrophs can create essential nutrients from other compounds, while auxotrophs must consume preexisting nutrients.[9] In nutrition, the diet of an organism is the sum of the foods it eats.[10] A healthy diet improves the physical and mental health of an organism. This requires ingestion and absorption of vitamins, minerals, essential amino acids from protein and essential fatty acids from fat-containing food. Carbohydrates, protein and fat play major roles in ensuring the quality of life, health and longevity of the organism.[11] Some cultures and religions have restrictions on what is acceptable for their diet.[12] A nutrient cycle is a biogeochemical cycle involving the movement of inorganic matter through a combination of soil, organisms, air or water, where they are exchanged in organic matter.[13] Energy flow is a unidirectional and noncyclic pathway, whereas the movement of mineral nutrients is cyclic. Mineral cycles include the carbon cycle, sulfur cycle, nitrogen cycle, water cycle, phosphorus cycle, and oxygen cycle, among others that continually recycle along with other mineral nutrients into productive ecological nutrition.[13] Biogeochemical cycles that are performed by living organisms and natural processes are water, carbon, nitrogen, phosphorus, and sulfur cycles.[14] Nutrient cycles allow these essential elements to return to the environment after being absorbed or consumed.[15] Without proper nutrient cycling, there would be a risk of change in oxygen levels, climate, and ecosystem function.[citation needed] Foraging is the process of seeking out nutrients in the environment. It may also be defined to include the subsequent use of the resources. Some organisms, such as animals and bacteria, can navigate to find nutrients, while others, such as plants and fungi, extend outward to find nutrients. Foraging may be random, in which the organism seeks nutrients without method, or it may be systematic, in which the organism can go directly to a food source.[16] Organisms are able to detect nutrients through taste or other forms of nutrient sensing, allowing them to regulate nutrient intake.[17] Optimal foraging theory is a model that explains foraging behavior as a cost\u2013benefit analysis in which an animal must maximize the gain of nutrients while minimizing the amount of time and energy spent foraging. It was created to analyze the foraging habits of animals, but it can also be extended to other organisms.[18] Some organisms are specialists that are adapted to forage for a single food source, while others are generalists that can consume a variety of food sources.[19] Nutrient deficiencies, known as malnutrition, occur when an organism does not have the nutrients that it needs. A deficiency is not the same as a nutrient inadequacy which occurs when the intake of nutrients is above the level of deficiency, but below the recommended dietary level. This may lead to hidden symptoms of nutrient deficiency that are difficult to identify.[20] Nutrient deficiency may be caused by a sudden decrease in nutrient intake or by an inability to absorb essential nutrients. Not only is malnutrition the result of a lack of necessary nutrients,[21] but it can also be a result of other illnesses and health conditions. When this occurs, an organism will adapt by reducing energy consumption and expenditure to prolong the use of stored nutrients. It will use stored energy reserves until they are depleted.[22] A balanced diet includes appropriate amounts of all essential and non-essential nutrients. These can vary by age, weight, sex, physical activity levels, and more. A lack of just one essential nutrient can cause bodily harm, just as an overabundance can cause toxicity. The Daily Reference Values keep the majority of people from nutrient deficiencies.[23] DRVs are not recommendations but a combination of nutrient references to educate professionals and policymakers on what the maximum and minimum nutrient intakes are for the average person.[24] Food labels also use DRVs as a reference to create safe nutritional guidelines for the average healthy person.[25] Animals are heterotrophs that consume other organisms to obtain nutrients. Herbivores are animals that eat plants, carnivores are animals that eat other animals, and omnivores are animals that eat both plants and other animals.[26] Many herbivores rely on bacterial fermentation to create digestible nutrients from indigestible plant cellulose, while obligate carnivores must eat animal meats to obtain certain vitamins or nutrients their bodies cannot otherwise synthesize. Animals generally have a higher requirement of energy in comparison to plants.[27] The macronutrients essential to animal life are carbohydrates, amino acids, and fatty acids.[7][28] All macronutrients except water are required by the body for energy, however, this is not their sole physiological function. The energy provided by macronutrients in food is measured in kilocalories, usually called Calories, where 1 Calorie is the amount of energy required to raise 1 kilogram of water by 1 degree Celsius.[29] Carbohydrates are molecules that store significant amounts of energy. Animals digest and metabolize carbohydrates to obtain this energy. Carbohydrates are typically synthesized by plants during metabolism, and animals have to obtain most carbohydrates from nature, as they have only a limited ability to generate them. They include sugars, oligosaccharides, and polysaccharides. Glucose is the simplest form of carbohydrate.[30] Carbohydrates are broken down to produce glucose and short-chain fatty acids, and they are the most abundant nutrients for herbivorous land animals.[31] Carbohydrates contain 4 calories per gram. Lipids provide animals with fats and oils. They are not soluble in water, and they can store energy for an extended period of time. They can be obtained from many different plant and animal sources. Most dietary lipids are triglycerides, composed of glycerol and fatty acids. Phospholipids and sterols are found in smaller amounts.[32] An animal's body will reduce the amount of fatty acids it produces as dietary fat intake increases, while it increases the amount of fatty acids it produces as carbohydrate intake increases.[33] Fats contain 9 calories per gram. Protein consumed by animals is broken down to amino acids, which would be later used to synthesize new proteins. Protein is used to form cellular structures, fluids,[34] and enzymes (biological catalysts). Enzymes are essential to most metabolic processes, as well as DNA replication, repair, and transcription.[35] Protein contains 4 calories per gram. Much of animal behavior is governed by nutrition. Migration patterns and seasonal breeding take place in conjunction with food availability, and courtship displays are used to display an animal's health.[36] Animals develop positive and negative associations with foods that affect their health, and they can instinctively avoid foods that have caused toxic injury or nutritional imbalances through a conditioned food aversion. Some animals, such as rats, do not seek out new types of foods unless they have a nutrient deficiency.[37] Early human nutrition consisted of foraging for nutrients, like other animals, but it diverged at the beginning of the Holocene with the Neolithic Revolution, in which humans developed agriculture to produce food. The Chemical Revolution in the 18th century allowed humans to study the nutrients in foods and develop more advanced methods of food preparation. Major advances in economics and technology during the 20th century allowed mass production and food fortification to better meet the nutritional needs of humans.[38] Human behavior is closely related to human nutrition, making it a subject of social science in addition to biology. Nutrition in humans is balanced with eating for pleasure, and optimal diet may vary depending on the demographics and health concerns of each person.[39] Social determinants of health (SDOH) and structural factors drive nutrition and diet-related health disparities.[40] Humans are omnivores that eat a variety of foods. Cultivation of cereals and production of bread has made up a key component of human nutrition since the beginning of agriculture. Early humans hunted animals for meat, and modern humans domesticate animals to consume their meat and eggs. The development of animal husbandry has also allowed humans in some cultures to consume the milk of other animals and process it into foods such as cheese. Other foods eaten by humans include nuts, seeds, fruits, and vegetables. Access to domesticated animals as well as vegetable oils has caused a significant increase in human intake of fats and oils. Humans have developed advanced methods of food processing that prevent contamination of pathogenic microorganisms and simplify the production of food. These include drying, freezing, heating, milling, pressing, packaging, refrigeration, and irradiation. Most cultures add herbs and spices to foods before eating to add flavor, though most do not significantly affect nutrition. Other additives are also used to improve the safety, quality, flavor, and nutritional content of food.[41] Humans obtain most carbohydrates as starch from cereals, though sugar has grown in importance.[30] Lipids can be found in animal fat, butterfat, vegetable oil, and leaf vegetables, and they are also used to increase flavor in foods.[32] Protein can be found in virtually all foods, as it makes up cellular material, though certain methods of food processing may reduce the amount of protein in a food.[42] Humans can also obtain energy from ethanol, which is both a food and a drug, but it provides relatively few essential nutrients and is associated with nutritional deficiencies and other health risks.[43] In humans, poor nutrition can cause deficiency-related diseases, such as blindness, anemia, scurvy, preterm birth, stillbirth and cretinism,[44] or nutrient-excess conditions, such as obesity[45] and metabolic syndrome.[46] Other conditions possibly affected by nutrition disorders include cardiovascular diseases,[47] diabetes,[48][49] and osteoporosis.[50] Undernutrition can lead to wasting in acute cases, and stunting of marasmus in chronic cases of malnutrition.[44] In domesticated animals, such as pets, livestock, and working animals, as well as other animals in captivity, nutrition is managed by humans through animal feed. Fodder and forage are provided to livestock. Specialized pet food has been manufactured since 1860, and subsequent research and development have addressed the nutritional needs of pets. Dog food and cat food in particular are heavily studied and typically include all essential nutrients for these animals. Cats are sensitive to some common nutrients, such as taurine, and require additional nutrients derived from meat. Large-breed puppies are susceptible to overnutrition, as small-breed dog food is more energy dense than they can absorb.[51] Most plants obtain nutrients through inorganic substances absorbed from the soil or the atmosphere. Carbon, hydrogen, oxygen, nitrogen, and sulfur are essential nutrients that make up organic material in a plant and allow enzymic processes. These are absorbed ions in the soil, such as bicarbonate, nitrate, ammonium, and sulfate, or they are absorbed as gases, such as carbon dioxide, water, oxygen gas, and sulfur dioxide. Phosphorus, boron, and silicon are used for esterification. They are obtained through the soil as phosphates, boric acid, and silicic acid, respectively. Other nutrients used by plants are potassium, sodium, calcium, magnesium, manganese, chlorine, iron, copper, zinc, and molybdenum.[52] Plants uptake essential elements from the soil through their roots and from the air (consisting of mainly nitrogen and oxygen) through their leaves. Nutrient uptake in the soil is achieved by cation exchange, wherein root hairs pump hydrogen ions (H+) into the soil through proton pumps. These hydrogen ions displace cations attached to negatively charged soil particles so that the cations are available for uptake by the root. In the leaves, stomata open to take in carbon dioxide and expel oxygen.[53] Although nitrogen is plentiful in the Earth's atmosphere, very few plants can use this directly. Most plants, therefore, require nitrogen compounds to be present in the soil in which they grow. This is made possible by the fact that largely inert atmospheric nitrogen is changed in a nitrogen fixation process to biologically usable forms in the soil by bacteria.[54] As these nutrients do not provide the plant with energy, they must obtain energy by other means. Green plants absorb energy from sunlight with chloroplasts and convert it to usable energy through photosynthesis.[55] Fungi are chemoheterotrophs that consume external matter for energy. Most fungi absorb matter through the root-like mycelium, which grows through the organism's source of nutrients and can extend indefinitely. The fungus excretes extracellular enzymes to break down surrounding matter and then absorbs the nutrients through the cell wall. Fungi can be parasitic, saprophytic, or symbiotic. Parasitic fungi attach and feed on living hosts, such as animals, plants, or other fungi. Saprophytic fungi feed on dead and decomposing organisms. Symbiotic fungi grow around other organisms and exchange nutrients with them.[56] Protists include all eukaryotes that are not animals, plants, or fungi, resulting in great diversity between them. Algae are photosynthetic protists that can produce energy from light. Several types of protists use mycelium similar to those of fungi. Protozoa are heterotrophic protists, and different protozoa seek nutrients in different ways. Flagellate protozoa use a flagellum to assist in hunting for food, and some protozoa travel via infectious spores to act as parasites.[57] Many protists are mixotrophic, having both phototrophic and heterotrophic characteristics. Mixotrophic protists will typically depend on one source of nutrients while using the other as a supplemental source or a temporary alternative when its primary source is unavailable.[58] Prokaryotes, including bacteria and archaea, vary greatly in how they obtain nutrients across nutritional groups. Prokaryotes can only transport soluble compounds across their cell envelopes, but they can break down chemical components around them. Some lithotrophic prokaryotes are extremophiles that can survive in nutrient-deprived environments by breaking down inorganic matter.[59] Phototrophic prokaryotes, such as cyanobacteria and Chloroflexia, can engage in photosynthesis to obtain energy from sunlight. This is common among bacteria that form in mats atop geothermal springs. Phototrophic prokaryotes typically obtain carbon from assimilating carbon dioxide through the Calvin cycle.[60] Some prokaryotes, such as Bdellovibrio and Ensifer, are predatory and feed on other single-celled organisms. Predatory prokaryotes seek out other organisms through chemotaxis or random collision, merge with the organism, degrade it, and absorb the released nutrients. Predatory strategies of prokaryotes include attaching to the outer surface of the organism and degrading it externally, entering the cytoplasm of the organism, or by entering the periplasmic space of the organism. Groups of predatory prokaryotes may forgo attachment by collectively producing hydrolytic enzymes.[61]",
      "ground_truth_chunk_ids": [
        "69_fixed_chunk1"
      ],
      "source_ids": [
        "S069"
      ],
      "category": "factual",
      "id": 38
    },
    {
      "question": "What is Abbas Shareef?",
      "ground_truth": "Abbas Shareef (Dhivehi: \u07a2\u07a6\u0787\u07b0\u0784\u07a7\u0790\u07b0 \u079d\u07a6\u0783\u07a9\u078a\u07b0; born 19??) is a Maldivian lawyer who is currently serving as Prosecutor General since September 2024 and had previously served as the Secretary to the President on Legal Affairs under president Mohamed Muizzu and a Judge at the High Court of the Maldives from March 2011 to October 2015. Abbas Shareef was born in Mal\u00e9, Maldives. He studied Bachelor of Laws at the Australian University of Tasmania in 2000.[1] Shareef was appointed a Judge at the High Court of the Maldives by then-president Mohamed Nasheed on 26 March 2011.[2] Prior to his appointment as a High Court judge, he held positions such as Assistant Legal Officer, Legal Officer and Assistant Director General at the President's Office. He further served as a member of the Judicial Service Commission appointed by the President, Deputy President of the Judicial Service Commission and Vice President of the Appeals Committee of the Football Association of Maldives.[3] On 22 June 2015, Shareef along with Azmiralda Zahir and Shuaib Hussain Zakariyya were transferred to the southern branch of the appellate court.[4] In October 2015, Shareef retired as a High Court judge.[5][6] Shareef was appointed as Secretary to the President on Legal Affairs on 17 November 2023.[7] On 12 September 2024, following the resignation of Hussain Shameem, President Mohamed Muizzu submitted the name of Shareef to the People's Majlis for parliamentary approval to appoint him as the Prosecutor General of the Maldives.[8][9] Shareef was approved by the parliament on 16 September 2024.[10][11][12] On 18 September 2024, Abbas was appointed as the Prosecutor General by President Mohamed Muizzu.[13][14]",
      "expected_answer": "Abbas Shareef (Dhivehi: \u07a2\u07a6\u0787\u07b0\u0784\u07a7\u0790\u07b0 \u079d\u07a6\u0783\u07a9\u078a\u07b0; born 19??) is a Maldivian lawyer who is currently serving as Prosecutor General since September 2024 and had previously served as the Secretary to the President on Legal Affairs under president Mohamed Muizzu and a Judge at the High Court of the Maldives from March 2011 to October 2015. Abbas Shareef was born in Mal\u00e9, Maldives. He studied Bachelor of Laws at the Australian University of Tasmania in 2000.[1] Shareef was appointed a Judge at the High Court of the Maldives by then-president Mohamed Nasheed on 26 March 2011.[2] Prior to his appointment as a High Court judge, he held positions such as Assistant Legal Officer, Legal Officer and Assistant Director General at the President's Office. He further served as a member of the Judicial Service Commission appointed by the President, Deputy President of the Judicial Service Commission and Vice President of the Appeals Committee of the Football Association of Maldives.[3] On 22 June 2015, Shareef along with Azmiralda Zahir and Shuaib Hussain Zakariyya were transferred to the southern branch of the appellate court.[4] In October 2015, Shareef retired as a High Court judge.[5][6] Shareef was appointed as Secretary to the President on Legal Affairs on 17 November 2023.[7] On 12 September 2024, following the resignation of Hussain Shameem, President Mohamed Muizzu submitted the name of Shareef to the People's Majlis for parliamentary approval to appoint him as the Prosecutor General of the Maldives.[8][9] Shareef was approved by the parliament on 16 September 2024.[10][11][12] On 18 September 2024, Abbas was appointed as the Prosecutor General by President Mohamed Muizzu.[13][14]",
      "ground_truth_chunk_ids": [
        "28_random_chunk1"
      ],
      "source_ids": [
        "S228"
      ],
      "category": "factual",
      "id": 39
    },
    {
      "question": "What is Compiler?",
      "ground_truth": "In computing, a compiler is software that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.[1][2]: p1 [3] There are many different types of compilers which produce output in different useful forms. A cross-compiler produces code for a different CPU or operating system than the one on which the cross-compiler itself runs. A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimized compiler for a language. Related software include decompilers, programs that translate from low-level languages to higher level ones; programs that translate between high-level languages, usually called source-to-source compilers or transpilers; language rewriters, usually programs that translate the form of expressions without a change of language; and compiler-compilers, compilers that produce compilers (or parts of them), often in a generic and reusable way so as to be able to produce many differing compilers. A compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and machine specific code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[4] With respect to making source code runnable, an interpreter provides a similar function as a compiler, but via a different mechanism. An interpreter executes code without converting it to",
      "expected_answer": "In computing, a compiler is software that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.[1][2]:\u200ap1\u200a[3] There are many different types of compilers which produce output in different useful forms. A cross-compiler  produces code for a different CPU or operating system than the one on which the cross-compiler itself runs.  A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimized compiler for a language. Related software include decompilers, programs that translate from low-level languages to higher level ones; programs that translate between high-level languages, usually called source-to-source compilers or transpilers; language rewriters, usually programs that translate the form of expressions without a change of language; and compiler-compilers, compilers that produce compilers (or parts of them), often in a generic and reusable way so as to be able to produce many differing compilers. A compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and machine specific code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[4] With respect to making source code runnable, an interpreter provides a similar function as a compiler, but via a different mechanism. An interpreter executes code without converting it to machine code.[2]:\u200ap2\u200a Therefore, some interpreters execute source code while others execute an intermediate form such as bytecode. Hence a program compiled to native code tends to run faster than when interpreted. Environments with a bytecode-intermediate-form tends toward intermediate-speed. While Just-in-time compilation allows for native execution speed with a one-time startup processing time cost. For low-level programming languages, such as assembly and C, it is typical that they are compiled, especially when speed is a significant concern, rather than being cross-platform supported. So that for such languages, there are more one-to-one correspondences between the source code and the resulting machine code, making it easier for programmers to control the use of hardware. In theory; a programming language can be used via either a compiler or an interpreter, but in practice, each language tends to be used with only one or the other. Nonetheless, it is possible to write a compiler for a language that is commonly interpreted. For example, Common Lisp can be compiled to Java bytecode (and then interpreted by the Java virtual machine), as well as C code (then compiled to native machine code), or directly to native code. Theoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures.[5] Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process. It is usually more productive for a programmer to use a high-level language, so the development of high-level languages followed naturally from the capabilities offered by digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include: The sentences in a language may be defined by a set of rules called a grammar.[6] Backus\u2013Naur form (BNF) describes the syntax of \"sentences\" of a language. It was developed by John Backus and used for the syntax of Algol 60.[7] The ideas derive from the context-free grammar concepts by linguist Noam Chomsky.[8] \"BNF and its extensions have become standard tools for describing the syntax of programming notations. In many cases, parts of compilers are generated automatically from a BNF description.\"[9] Between 1942 and 1945, Konrad Zuse designed the first (algorithmic) programming language for computers called Plankalk\u00fcl (\"Plan Calculus\").  Zuse also envisioned a Planfertigungsger\u00e4t (\"Plan assembly device\") to automatically translate the mathematical formulation of a program into machine-readable punched film stock.[10] While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s.[11] APL is a language for mathematical computations. Between 1949 and 1951, Heinz Rutishauser proposed Superplan, a high-level language and automatic translator.[12] His ideas were later refined by Friedrich L. Bauer and Klaus Samelson.[13] High-level language design during the formative years of digital computing provided useful programming tools for a variety of applications: Compiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with the analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.[17] Some early milestones in the development of compiler technology: Early operating systems and software were written in assembly language. In the 1960s and early 1970s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C. BCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool.[30] Several compilers have been implemented, Richards' book provides insights to the language and its compiler.[31] BCPL was not only an influential systems programming language that is still used in research[32] but also provided a basis for the design of B and C languages. BLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W. A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970. Multics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbat\u00f3 from MIT.[33] Multics was written in the PL/I language developed by IBM and IBM User Group.[34] IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented.[35] For the first few years of the Multics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs.[36] EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.[37] Bell Labs left the Multics project in 1969, and developed a system programming language B based on BCPL concepts, written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix. Bell Labs started the development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs.[38] Initially, a front-end program to Bell Labs' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.[39][40] Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science.[41] Bell Labs became interested in OOP with the development of C++.[42] C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983.[43] The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew. In many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex. DARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf's CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target.[44] PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator. PQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure.[45] The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation.[46] Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the (since 1995, object-oriented) programming language Ada. The Ada STONEMAN document[a] formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with the American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the STONEMAN document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overall effort on Ada development.[47] Other Ada compiler efforts got underway in Britain at the University of York and in Germany at the University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation.[48] There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment. High-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.[49] \"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security.\"[50] The \"Compiler Research: The Next 50 Years\" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets. A compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end-to-end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets. In the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once. A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process. Classifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing much work and early computers did not have enough memory to contain one program that did all of this work. As a result, compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations. The ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal). In some cases, the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass. The disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once. Splitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program. Regardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end. This front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end.[51] Practical examples of this approach are the GNU Compiler Collection, Clang (LLVM-based C/C++ compiler),[52] and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends. The front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope. While the frontend can be a single monolithic function or program, as in a scannerless parser, it was traditionally implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly, the frontend is broken into three phases: lexical analysis (also known as lexing or scanning), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases, these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare. The main phases of the front end include the following: The middle end, also known as optimizer, performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code.[56] The middle end contains those optimizations that are independent of the CPU architecture being targeted. The main phases of the middle end include the following: Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation. The scope of compiler analysis and optimizations vary greatly; their scope may range from operating within a basic block, to whole procedures, or even the whole program.  There is a trade-off between the granularity of the optimizations and the cost of compilation.  For example, peephole optimizations are fast to perform during compilation but only affect a small local fragment of the code, and can be performed independently of the context in which the code fragment appears.  In contrast, interprocedural optimization requires more compilation time and memory space, but enable optimizations that are only possible by considering the behavior of multiple functions simultaneously. Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes. Due to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled. The back end is responsible for the CPU architecture specific optimizations and for code generation.[56] The main phases of the back end include the following: Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification.[58] Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler. Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language \u2013 for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.[59] Interpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a set of directly executed machine instructions is needed somewhere at the bottom of the execution stack (see machine language). Furthermore, for optimization compilers can contain interpreter functionality, and interpreters may include ahead of time compilation techniques. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further. Meta-tracing is an automated compiler synthesis approach which takes this further and can be used to synthesize a compiler from a language interpreter. Some language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4,[60] and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself. One classification of compilers is by the platform on which their generated code executes. This is known as the target platform. A native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment. The output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason, such compilers are not usually classified as native or cross compilers. The lower level language that is the target of a compiler may itself be a high-level programming language. C, viewed by some as a sort of portable assembly language, is frequently the target language of such compilers. For example, Cfront, the original compiler for C++, used C as its target language. The C code generated by such a compiler is usually not intended to be readable and maintained by humans, so indent style and creating pretty C intermediate code are ignored. Some of the features of C that make it a good target language include the #line directive, which can be generated by the compiler to support debugging of the original source, and the wide platform support available with C compilers. While a common compiler type outputs machine code, there are many other types: Assemblers, which translate human readable assembly language to the machine code instructions executed by hardware, are not considered compilers.[69][b] (The inverse program that translates machine code to assembly language is called a disassembler.)",
      "ground_truth_chunk_ids": [
        "152_fixed_chunk1"
      ],
      "source_ids": [
        "S152"
      ],
      "category": "factual",
      "id": 40
    },
    {
      "question": "What is Pterostylis stricta?",
      "ground_truth": "Pterostylis stricta, commonly known as the northern greenhood, is a species of orchid endemic to Queensland. It has a rosette of leaves and when flowering a single translucent white flower with green lines, a reddish-brown tip and a curved, protruding labellum. Pterostylis stricta is a terrestrial, perennial, deciduous, herb with an underground tuber and a rosette of wrinkled leaves. Each leaf is 15\u201360 mm (0.6\u20132 in) long and 10\u201320 mm (0.4\u20130.8 in) wide. When flowering, there is a single white flower with green lines and a reddish-brown tip, 20\u201325 mm (0.8\u20131 in) long and 10\u201312 mm (0.4\u20130.5 in) wide which is borne on a flowering spike 100\u2013300 mm (4\u201310 in) high. The dorsal sepal and petals are fused to form a hood or \"galea\" over the column, the dorsal sepal about the same length as the petals, all with a sharp point. There is a wide gap at each side of the flower between the petals and lateral sepals. The lateral sepals are erect with a tapering tip 12\u201315 mm (0.5\u20130.6 in) long and there is a broad, bulging sinus between them. The labellum is 14\u201316 mm (0.55\u20130.63 in) long, about 4 mm (0.2 in) wide, dark reddish-brown and curved, protruding above the sinus. Flowering occurs from March to July.[2][3] Pterostylis stricta was first described in 1972 by Stephen Clemesha and Bruce Gray and the description was published in The Orchadian from a specimen collected near Ravenshoe.[4] The specific epithet (stricta) is a Latin word meaning \"draw together\", \"hold in check\" or \"bind\".[5] The northern greenhood grows with grasses and in sheltered gullies in forest between Mount Finnigan and Paluma at altitudes of between 800 and 1,250 m (2,600 and 4,100 ft).[2][3]",
      "expected_answer": "Pterostylis stricta, commonly known as the northern greenhood, is a species of orchid endemic to Queensland. It has a rosette of leaves and when flowering a single translucent white flower with green lines, a reddish-brown tip and a curved, protruding labellum. Pterostylis stricta is a terrestrial, perennial, deciduous, herb with an underground tuber and a rosette of wrinkled leaves. Each leaf is 15\u201360\u00a0mm (0.6\u20132\u00a0in) long and 10\u201320\u00a0mm (0.4\u20130.8\u00a0in) wide. When flowering, there is a single white flower with green lines and a reddish-brown tip, 20\u201325\u00a0mm (0.8\u20131\u00a0in) long and 10\u201312\u00a0mm (0.4\u20130.5\u00a0in) wide which is borne on a flowering spike 100\u2013300\u00a0mm (4\u201310\u00a0in) high. The dorsal sepal and petals are fused to form a hood or \"galea\" over the column, the dorsal sepal about the same length as the petals, all with a sharp point. There is a wide gap at each side of the flower between the petals and lateral sepals. The lateral sepals are erect with a tapering tip 12\u201315\u00a0mm (0.5\u20130.6\u00a0in) long and there is a broad, bulging sinus between them. The labellum is 14\u201316\u00a0mm (0.55\u20130.63\u00a0in) long, about 4\u00a0mm (0.2\u00a0in) wide, dark reddish-brown and curved, protruding above the sinus. Flowering occurs from March to July.[2][3] Pterostylis stricta was first described in 1972 by  Stephen Clemesha and Bruce Gray and the description was published in The Orchadian from a specimen collected near Ravenshoe.[4] The specific epithet (stricta) is a Latin word meaning \"draw together\", \"hold in check\" or \"bind\".[5] The northern greenhood grows with grasses and in sheltered gullies in forest between Mount Finnigan and Paluma at altitudes of between 800 and 1,250\u00a0m (2,600 and 4,100\u00a0ft).[2][3]",
      "ground_truth_chunk_ids": [
        "170_random_chunk1"
      ],
      "source_ids": [
        "S370"
      ],
      "category": "factual",
      "id": 41
    },
    {
      "question": "What is Bernstadt, Kentucky?",
      "ground_truth": "Bernstadt is an unincorporated community located in Laurel County, Kentucky, located about 6 miles (9.7 km) west of London, Kentucky, the county seat. It is on Kentucky Route 1956, which exits the national forest here.[2] Its population was estimated to be 400 in 1997 and is the location of the First Evangelical Reformed Church, listed on the National Register of Historic Places. The community is known for its Swiss population.[3] On November 14, 2007, an EF0 tornado struck Bernstadt, traveling southwest of Pittsburg.[4] The Omni Broadcasting Network hosted the OBN affiliate WBON-LD in the community in December 2003.[5] This Laurel County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Bernstadt is an unincorporated community located in Laurel County, Kentucky, located about 6 miles (9.7\u00a0km) west of London, Kentucky, the county seat. It is on Kentucky Route 1956, which exits the national forest here.[2] Its population was estimated to be 400 in 1997 and is the location of the First Evangelical Reformed Church, listed on the National Register of Historic Places. The community is known for its Swiss population.[3] On November 14, 2007, an EF0 tornado struck Bernstadt, traveling southwest of Pittsburg.[4] The Omni Broadcasting Network hosted the OBN affiliate WBON-LD in the community in December 2003.[5] This Laurel County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "18_random_chunk1"
      ],
      "source_ids": [
        "S218"
      ],
      "category": "factual",
      "id": 42
    },
    {
      "question": "What is Containerization (computing)?",
      "ground_truth": "In software engineering, containerization is operating-system-level virtualization or application-level virtualization over multiple network resources so that software applications can run in isolated user spaces called containers in any cloud or non-cloud environment, regardless of type or vendor.[1] The term \"container\" has different meanings in different contexts, and it is important to ensure that the intended definition aligns with the audience's understanding.[2][3] Each container is basically a fully functional and portable cloud or non-cloud computing environment surrounding the application and keeping it independent of other environments running in parallel.[4] Individually, each container simulates a different software application and runs isolated processes[5] by bundling related configuration files, libraries and dependencies.[6] But, collectively, multiple containers share a common operating system kernel (OS).[7] In recent times, containerization technology has been widely adopted by cloud computing platforms like Amazon Web Services, Microsoft Azure, Google Cloud Platform, and IBM Cloud.[8] Containerization has also been pursued by the U.S. Department of Defense as a way of more rapidly developing and fielding software updates, with first application in its F-22 air superiority fighter.[9] Container orchestration or container management is mostly used in the context of application containers.[10] Implementations providing such orchestration include Kubernetes and Docker swarm. Container clusters need to be managed. This includes functionality to create a cluster, to upgrade the software or repair it, balance the load between existing instances, scale by starting or stopping instances to adapt to the number of users, to log activities and monitor produced logs or the application itself by querying sensors. Open-source implementations of such software include OKD and Rancher. Quite a number of companies provide container cluster management as a managed service, like Alibaba, Amazon, Google, and Microsoft. This software-engineering-related article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "In software engineering, containerization is operating-system-level virtualization or application-level virtualization over multiple network resources so that software applications can run in isolated user spaces called containers in any cloud or non-cloud environment, regardless of type or vendor.[1] The term \"container\" has different meanings in different contexts, and it is important to ensure that the intended definition aligns with the audience's understanding.[2][3] Each container is basically a fully functional and portable cloud or non-cloud computing environment surrounding the application and keeping it independent of other environments running in parallel.[4] Individually, each container simulates a different software application and runs isolated processes[5] by bundling related configuration files, libraries and dependencies.[6] But, collectively, multiple containers share a common operating system kernel (OS).[7] In recent times, containerization technology has been widely adopted by cloud computing platforms like Amazon Web Services, Microsoft Azure,  Google Cloud Platform, and IBM Cloud.[8] Containerization has also been pursued by the U.S. Department of Defense as a way of more rapidly developing and fielding software updates, with first application in its F-22 air superiority fighter.[9] Container orchestration or container management is mostly used in the context of application containers.[10] Implementations providing such orchestration include Kubernetes and Docker swarm. Container clusters need to be managed. This includes functionality to create a cluster, to upgrade the software or repair it, balance the load between existing instances, scale by starting or stopping instances to adapt to the number of users, to log activities and monitor produced logs or the application itself by querying sensors. Open-source implementations of such software include OKD and Rancher. Quite a number of companies provide container cluster management as a managed service, like Alibaba, Amazon, Google, and Microsoft. This software-engineering-related article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "155_fixed_chunk1"
      ],
      "source_ids": [
        "S155"
      ],
      "category": "factual",
      "id": 43
    },
    {
      "question": "What is Climate model?",
      "ground_truth": "Numerical climate models (or climate system models) are mathematical models that can simulate the interactions of important drivers of climate. These drivers are the atmosphere, oceans, land surface and ice. Scientists use climate models to study the dynamics of the climate system and to make projections of future climate and of climate change. Climate models can also be qualitative (i.e. not numerical) models and contain narratives, largely descriptive, of possible futures.[1] Climate models take account of incoming energy from the Sun as well as outgoing energy from Earth. An imbalance results in a change in temperature. The incoming energy from the Sun is in the form of short wave electromagnetic radiation, chiefly visible and short-wave (near) infrared. The outgoing energy is in the form of long wave (far) infrared electromagnetic energy. These processes are part of the greenhouse effect. Climate models vary in complexity. For example, a simple radiant heat transfer model treats the Earth as a single point and averages outgoing energy. This can be expanded vertically (radiative-convective models) and horizontally. More complex models are the coupled atmosphere\u2013ocean\u2013sea ice global climate models. These types of models solve the full equations for mass transfer, energy transfer and radiant exchange. In addition, other types of models can be interlinked. For example Earth System Models include also land use as well as land use changes. This allows researchers to predict the interactions between climate and ecosystems. Climate models are systems of differential equations based on the basic laws of physics, fluid motion, and chemistry. Scientists divide the planet into a 3-dimensional grid and apply the basic equations to those grids. Atmospheric models calculate winds, heat transfer, radiation, relative humidity, and surface hydrology within each grid and evaluate interactions with neighboring points. These are coupled with oceanic models to simulate climate variability and",
      "expected_answer": "Numerical climate models (or climate system models) are mathematical models that can simulate the interactions of important drivers of climate. These drivers are the atmosphere, oceans, land surface and ice. Scientists use climate models to study the dynamics of the climate system and to make projections of future climate and of climate change. Climate models can also be qualitative (i.e. not numerical) models and contain narratives, largely descriptive, of possible futures.[1] Climate models take account of incoming energy from the Sun as well as outgoing energy from Earth. An imbalance results in a change in temperature. The incoming energy from the Sun is in the form of short wave electromagnetic radiation, chiefly visible and short-wave (near) infrared. The outgoing energy is in the form of long wave (far) infrared electromagnetic energy. These processes are part of the greenhouse effect. Climate models vary in complexity. For example, a simple radiant heat transfer model treats the Earth as a single point and averages outgoing energy. This can be expanded vertically (radiative-convective models) and horizontally. More complex models are the coupled atmosphere\u2013ocean\u2013sea ice global climate models. These types of models solve the full equations for mass transfer, energy transfer and radiant exchange. In addition, other types of models can be interlinked. For example Earth System Models include also land use as well as land use changes. This allows researchers to predict the interactions between climate and ecosystems. Climate models are systems of differential equations based on the basic laws of physics, fluid motion, and chemistry. Scientists divide the planet into a 3-dimensional grid and apply the basic equations to those grids. Atmospheric models calculate winds, heat transfer, radiation, relative humidity, and surface hydrology within each grid and evaluate interactions with neighboring points.  These are coupled with oceanic models to simulate climate variability and change that occurs on different timescales due to shifting ocean currents and the much larger heat storage capacity of the global ocean.  External drivers of change may also be applied. Including an ice-sheet model better accounts for long term effects such as sea level rise. Complex climate models enable extreme event attribution, which is the science of identifying and quantifying the role that human-caused climate change plays in the frequency, intensity and impacts of extreme weather events.[2][3] Attribution science aims to determine the degree to which such events can be explained by or linked to human-caused global warming, and are not simply due to random climate variability[4] or natural weather patterns.[5] There are three major types of institution where climate models are developed, implemented and used: Big climate models are essential but they are not perfect.\u00a0Attention still needs to be given to the real world (what is happening and why). The global models are essential to assimilate all the observations, especially from space (satellites) and produce comprehensive analyses of what is happening, and then they can be used to make predictions/projections. Simple models have a role to play that is widely abused and fails to recognize the simplifications such as not including a water cycle.[6][7] A general circulation model (GCM) is a type of climate model. It employs a mathematical model of the general circulation of a planetary atmosphere or ocean. It uses the Navier\u2013Stokes equations on a rotating sphere with thermodynamic terms for various energy sources (radiation, latent heat). These equations are the basis for computer programs used to simulate the Earth's atmosphere or oceans. Atmospheric and oceanic GCMs (AGCM and OGCM) are key components along with sea ice and land-surface components. GCMs and global climate models are used for weather forecasting, understanding the climate, and forecasting climate change. Atmospheric GCMs (AGCMs) model the atmosphere and impose sea surface temperatures as boundary conditions. Coupled atmosphere-ocean GCMs (AOGCMs, e.g. HadCM3, EdGCM, GFDL CM2.X, ARPEGE-Climat)[9] combine the two models. The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory[10] AOGCMs represent the pinnacle of complexity in climate models and internalise as many processes as possible. However, they are still under development and uncertainties remain.  They may be coupled to models of other processes, such as the carbon cycle, so as to better model feedback effects. Such integrated multi-system models are sometimes referred to as either \"earth system models\" or \"global climate models.\" Versions designed for decade to century time scale climate applications were created by Syukuro Manabe and Kirk Bryan at the Geophysical Fluid Dynamics Laboratory (GFDL) in Princeton, New Jersey.[8] These models are based on the integration of a variety of fluid dynamical, chemical and sometimes biological equations. Simulation of the climate system in full 3-D space and time was impractical prior to the establishment of large computational facilities starting in the 1960s.  In order to begin to understand which factors may have changed Earth's paleoclimate states, the constituent and dimensional complexities of the system needed to be reduced.  A simple quantitative model that balanced incoming/outgoing energy was first developed for the atmosphere in the late 19th century.[11]  Other EBMs similarly seek an economical description of surface temperatures by applying the conservation of energy constraint to individual columns of the Earth-atmosphere system.[12] Essential features of EBMs include their relative conceptual simplicity and their ability to sometimes produce analytical solutions.[13]:\u200a19\u200a Some models account for effects of ocean, land, or ice features on the surface budget.   Others include interactions with parts of the water cycle or carbon cycle.  A variety of these and other reduced system models can be useful for specialized tasks that supplement GCMs, particularly to bridge gaps between simulation and understanding.[14][15] Zero-dimensional models consider Earth as a point in space, analogous to the pale blue dot viewed by Voyager 1 or an astronomer's view of very distant objects.    This dimensionless view while highly limited is still useful in that the laws of physics are applicable in a bulk fashion to unknown objects,  or in an appropriate lumped manner if some major properties of the object are known.   For example, astronomers know that most planets in our own solar system feature some kind of solid/liquid surface surrounded by a gaseous atmosphere. A very simple model of the radiative equilibrium of the Earth is where The constant parameters include The constant \n\n\n\n\u03c0\n\n\nr\n\n2\n\n\n\n\n{\\displaystyle \\pi \\,r^{2}}\n\n can be factored out, giving a nildimensional equation for the equilibrium where The remaining variable parameters which are specific to the planet include This very simple model is quite instructive. For example, it shows the temperature sensitivity to changes in the solar constant, Earth albedo, or effective Earth emissivity.  The effective emissivity also gauges the strength of the atmospheric greenhouse effect,  since it is the ratio of the thermal emissions escaping to space versus those emanating from the surface.[19] The calculated emissivity can be compared to available data. Terrestrial surface emissivities are all in the range of 0.96 to 0.99[20][21] (except for some small desert areas which may be as low as 0.7). Clouds, however, which cover about half of the planet's surface, have an average emissivity of about 0.5[22] (which must be reduced by the fourth power of the ratio of cloud absolute temperature to average surface absolute temperature) and an average cloud temperature of about 258\u00a0K (\u221215\u00a0\u00b0C; 5\u00a0\u00b0F).[23]  Taking all this properly into account results in an effective earth emissivity of about 0.64 (earth average temperature 285\u00a0K (12\u00a0\u00b0C; 53\u00a0\u00b0F)).[citation needed] Dimensionless models have also been constructed with functionally distinct atmospheric layers from the surface.   The simplest of these is the zero-dimensional, one-layer model,[24]  which may be readily extended to an arbitrary number of atmospheric layers.   The surface and atmospheric layer(s) are each characterized by a corresponding temperature and emissivity value, but no thickness.  Applying radiative equilibrium (i.e conservation of energy) at the idealized interfaces between layers produces a set of coupled equations which are solvable.[25] These multi-layered EBMs are examples of multi-compartment models.  They can estimate average temperatures closer to those observed for Earth's surface and troposphere.[26]  They likewise further illustrate the radiative heat transfer processes which underlie the greenhouse effect.  Quantification of this phenomenon using a version of the one-layer model was first published by Svante Arrhenius in year 1896.[11] Water vapor is a main determinant of the emissivity of Earth's atmosphere.  It both influences the flows of radiation and is influenced by convective flows of heat in a manner that is consistent with its equilibrium concentration and temperature as a function of elevation (i.e. relative humidity distribution).  This has been shown by refining the zero dimension model in the vertical to a one-dimensional radiative-convective model which considers two processes of energy transport:[27] Radiative-convective models typically use a distributed model of the atmosphere versus elevation.  This has advantages over the lumped models and also lays a foundation for more complex models.[28]   They can estimate both surface temperature and the temperature variation with elevation in a more realistic manner.  In particular, they properly simulate the observed decline in upper atmospheric temperature and the rise in surface temperature when trace amounts of other non-condensible greenhouse gases such as carbon dioxide are included.[27] Other parameters are sometimes included to simulate localized effects in other dimensions and to address the factors that move energy about Earth.  For example, the effect of ice-albedo feedback on global climate sensitivity has been investigated using a one-dimensional radiative-convective climate model.[29][30] The zero-dimensional model may be expanded to consider the energy transported horizontally in the atmosphere. This kind of model may well be zonally averaged. This model has the advantage of allowing a rational dependence of local albedo and emissivity on temperature \u2013 the poles can be allowed to be icy and the equator warm \u2013 but the lack of true dynamics means that horizontal transports have to be specified.[31] Early examples include research of Mikhail Budyko and William D. Sellers who worked on the Budyko-Sellers model.[32][33] This work also showed the role of positive feedback in the climate system and has been considered foundational for the energy balance models since its publication in 1969.[12][34] Depending on the nature of questions asked and the pertinent time scales, there are, on the one extreme,  conceptual, more inductive models, and, on the other extreme, general circulation models operating at the highest spatial and temporal resolution currently feasible. Models of intermediate complexity bridge the gap. One example is the Climber-3 model. Its atmosphere is a 2.5-dimensional statistical-dynamical model with 7.5\u00b0 \u00d7 22.5\u00b0 resolution and time step of half a day;  the ocean is MOM-3 (Modular Ocean Model) with a 3.75\u00b0 \u00d7 3.75\u00b0 grid and 24 vertical levels.[35] Box models are simplified versions of complex systems, reducing them to boxes linked by fluxes. The boxes contain reservoirs (i.e. inventories) of species of matter and energy that are assumed to be mixed homogeneously. The concentration of any species is therefore uniform at any time within a box. However, the  abundance of a species within a given box may vary as a function of time due to input flows or output flows; and may also vary due to the production, consumption or transformation of a species within the box.[36][37] Simple box models, i.e. box model with a small number of boxes whose properties (e.g. their volume) do not change with time, are often useful to derive analytical formulas describing the dynamical and steady-state abundances of a species.  The formulae are called governing equations and are derived from conservation laws (e.g. conservation of energy,  conservation of mass, etc.).  Larger sets of interacting species and equations are evaluated with numerical techniques to describe behavior of the system.[38] Box models are used extensively to simulate environmental systems and ecosystems.  In 1961 Henry Stommel was the first to use a simple 2-box model to study the stability of large-scale ocean circulation.[39]  A more complex model has examined interactions between ocean circulation and the carbon cycle.[40] The field of complex networks has emerged as an important area of science to generate novel insights into nature of complex systems [41][42] The application of network theory to climate science is a young and emerging field.[43][44][45] To identify and analyze patterns in global climate, scientists model climate data as complex networks. Unlike most real-world networks where nodes and edges are well defined, in climate networks, nodes are identified as the sites in a spatial grid of the underlying global climate data set, which can be represented at various resolutions. Two nodes are connected by an edge depending on the degree of statistical similarity (that may be related to dependence) between the corresponding pairs of time-series taken from climate records.[44][46]\nThe climate network approach enables novel insights into the dynamics of the climate system over different spatial and temporal scales.[44] In 1956, Norman Phillips developed a mathematical model that realistically depicted monthly and seasonal patterns in the troposphere. This was the first successful climate model.[47][48] Several groups then began working to create general circulation models.[49] The first general circulation climate model combined oceanic and atmospheric processes and was developed in the late 1960s at the Geophysical Fluid Dynamics Laboratory, a component of the U.S. National Oceanic and Atmospheric Administration.[50] By 1975, Manabe and Wetherald had developed a three-dimensional global climate model that gave a roughly accurate representation of the current climate. Doubling CO2 in the model's atmosphere gave a roughly 2\u00a0\u00b0C rise in global temperature.[51] Several other kinds of computer models gave similar results: it was impossible to make a model that gave something resembling the actual climate and not have the temperature rise when the CO2 concentration was increased. By the early 1980s, the U.S. National Center for Atmospheric Research had developed the Community Atmosphere Model (CAM), which can be run by itself or as the atmospheric component of the Community Climate System Model. The latest update (version 3.1) of the standalone CAM was issued on 1 February 2006.[52][53][54] In 1986, efforts began to initialize and model soil and vegetation types, resulting in more realistic forecasts.[55] Coupled ocean-atmosphere climate models, such as the Hadley Centre for Climate Prediction and Research's HadCM3 model, are being used as inputs for climate change studies.[49] Meta-analyses of past climate change models show that they have generally been accurate, albeit conservative, under-predicting levels of warming.[56][57] The Coupled Model Intercomparison Project (CMIP) has been a leading effort to foster improvements in GCMs and climate change understanding since 1995.[59][60] The IPCC stated in 2010 it has increased confidence in forecasts coming from climate models: \"There is considerable confidence that climate models provide credible quantitative estimates of future climate change, particularly at continental scales and above. This confidence comes from the foundation of the models in accepted physical principles and from their ability to reproduce observed features of current climate and past climate changes. Confidence in model estimates is higher for some climate variables (e.g., temperature) than for others (e.g., precipitation). Over several decades of development, models have consistently provided a robust and unambiguous picture of significant climate warming in response to increasing greenhouse gases.\"[61] The World Climate Research Programme (WCRP), hosted by the World Meteorological Organization (WMO), coordinates research activities on climate modelling worldwide. A 2012 U.S. National Research Council report discussed how the large and diverse U.S. climate modeling enterprise could evolve to become more unified.[62] Efficiencies could be gained by developing a common software infrastructure shared by all U.S. climate researchers, and holding an annual climate modeling forum, the report found.[63] Cloud-resolving climate models are nowadays run on high intensity super-computers which have a high power consumption and thus cause CO2 emissions.[64]\u00a0They require exascale computing (billion billion \u2013 i.e., a quintillion \u2013 calculations per second). For example, the Frontier exascale supercomputer consumes 29 MW.[65] It can simulate a year\u2019s worth of climate at cloud resolving scales in a day.[66] Techniques that could lead to energy savings, include for example: \"reducing floating point precision computation; developing machine learning algorithms to avoid unnecessary computations; and creating a new generation of scalable numerical algorithms that would enable higher throughput in terms of simulated years per wall clock day.\"[64] Parametrization (or parameterization) in an atmospheric model (either weather model or climate model) is a method of replacing processes that are too small-scale or complex to be physically represented in the model by a simplified process. This can be contrasted with other processes\u2014e.g., large-scale flow of the atmosphere\u2014that are explicitly resolved within the models.  Associated with these parametrizations are various parameters used in the simplified processes. Examples include the descent rate of raindrops, convective clouds, simplifications of the atmospheric radiative transfer on the basis of atmospheric radiative transfer codes, and cloud microphysics.  Radiative parametrizations are important to both atmospheric and oceanic modeling alike.  Atmospheric emissions from different sources within individual grid boxes also need to be parametrized to determine their impact on air quality. Climate models on the web:",
      "ground_truth_chunk_ids": [
        "178_fixed_chunk1"
      ],
      "source_ids": [
        "S178"
      ],
      "category": "factual",
      "id": 44
    },
    {
      "question": "What is Walchensee Forever?",
      "ground_truth": "Walchensee Forever is a 2020 German documentary film directed by Janna Ji Wonders [de].[1][2] Wonders chronicles the life of four generations of women from her family.[3] The film won the Grand Prix at the 2020 Biografilm Festival in Bologna.[4] This article related to a German film of the 2020s is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Walchensee Forever is a 2020 German documentary film directed by Janna Ji Wonders\u00a0[de].[1][2] Wonders chronicles the life of four generations of women from her family.[3] The film won the Grand Prix at the 2020 Biografilm Festival in Bologna.[4] This article related to a German film of the 2020s is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "198_random_chunk1"
      ],
      "source_ids": [
        "S398"
      ],
      "category": "factual",
      "id": 45
    },
    {
      "question": "What is Systems engineering?",
      "ground_truth": "Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. The individual outcome of such efforts, an engineered system, can be defined as a combination of components that work in synergy to collectively perform a useful function. Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability, and many other disciplines, aka \"ilities\", necessary for successful system design, development, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, production systems engineering, process systems engineering, mechanical engineering, manufacturing engineering, production engineering, control engineering, software engineering, electrical engineering, cybernetics, aerospace engineering, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered and integrated into a whole. The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high-quality outputs with minimum cost and time. The systems engineering process must begin by discovering the real problems that need to be resolved and identifying the most probable or highest-impact failures that can occur. Systems engineering involves finding solutions to these problems. The term systems engineering can be traced back to Bell Telephone Laboratories in the 1940s.[1] The need to identify and manipulate the properties of a system as a whole, which in complex engineering projects may greatly differ from the sum of the parts' properties, motivated various industries, especially those developing systems for",
      "expected_answer": "Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. The individual outcome of such efforts, an engineered system, can be defined as a combination of components that work in synergy to collectively perform a useful function. Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability, and many other disciplines, aka \"ilities\", necessary for successful system design, development, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, production systems engineering, process systems engineering, mechanical engineering, manufacturing engineering, production engineering, control engineering, software engineering, electrical engineering, cybernetics, aerospace engineering, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered and integrated into a whole. The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high-quality outputs with minimum cost and time. The systems engineering process must begin by discovering the real problems that need to be resolved and identifying the most probable or highest-impact failures that can occur. Systems engineering involves finding solutions to these problems. The term systems engineering can be traced back to Bell Telephone Laboratories in the 1940s.[1] The need to identify and manipulate the properties of a system as a whole, which in complex engineering projects may greatly differ from the sum of the parts' properties, motivated various industries, especially those developing systems for the U.S. military, to apply the discipline.[2][3] When it was no longer possible to rely on design evolution to improve upon a system and the existing tools were not sufficient to meet growing demands, new methods began to be developed that addressed the complexity directly.[4] The continuing evolution of systems engineering comprises the development and identification of new methods and modeling techniques. These methods aid in a better comprehension of the design and developmental control of engineering systems as they grow more complex. Popular tools that are often used in the systems engineering context were developed during these times, including Universal Systems Language (USL), Unified Modeling Language (UML), Quality function deployment (QFD), and Integration Definition (IDEF). In 1990, a professional society for systems engineering, the National Council on Systems Engineering (NCOSE), was founded by representatives from a number of U.S. corporations and organizations. NCOSE was created to address the need for improvements in systems engineering practices and education. As a result of growing involvement from systems engineers outside of the U.S., the name of the organization was changed to the International Council on Systems Engineering (INCOSE) in 1995.[5] Schools in several countries offer graduate programs in systems engineering, and continuing education options are also available for practicing engineers.[6] Systems engineering signifies only an approach and, more recently, a discipline in engineering. The aim of education in systems engineering is to formalize various approaches simply and in doing so, identify new methods and research opportunities similar to that which occurs in other fields of engineering. As an approach, systems engineering is holistic and interdisciplinary in flavor. The traditional scope of engineering embraces the conception, design, development, production, and operation of physical systems. Systems engineering, as originally conceived, falls within this scope. \"Systems engineering\", in this sense of the term, refers to the building of engineering concepts. The use of the term \"systems engineer\" has evolved over time to embrace a wider, more holistic concept of \"systems\" and of engineering processes. This evolution of the definition has been a subject of ongoing controversy,[13] and the term continues to apply to both the narrower and a broader scope. Traditional systems engineering was seen as a branch of engineering in the classical sense, that is, as applied only to physical systems, such as spacecraft and aircraft. More recently, systems engineering has evolved to take on a broader meaning especially when humans were seen as an essential component of a system. Peter Checkland, for example, captures the broader meaning of systems engineering by stating that 'engineering' \"can be read in its general sense; you can engineer a meeting or a political agreement.\"[14]:\u200a10 Consistent with the broader scope of systems engineering, the Systems Engineering Body of Knowledge (SEBoK)[15] has defined three types of systems engineering: Systems engineering focuses on analyzing and eliciting customer needs and required functionality early in the development cycle, documenting requirements, then proceeding with design synthesis and system validation while considering the complete problem, the system lifecycle. This includes fully understanding all of the stakeholders involved. Oliver et al. claim that the systems engineering process can be decomposed into: Within Oliver's model, the goal of the Management Process is to organize the technical effort in the lifecycle, while the Technical Process includes assessing available information, defining effectiveness measures, to create a behavior model, create a structure model, perform trade-off analysis, and create sequential build & test plan.[16] Depending on their application, although there are several models that are used in the industry, all of them aim to identify the relation between the various stages mentioned above and incorporate feedback. Examples of such models include the Waterfall model and the VEE model (also called the V model).[17] System development often requires contribution from diverse technical disciplines.[18] By providing a systems (holistic) view of the development effort, systems engineering helps mold all the technical contributors into a unified team effort, forming a structured development process that proceeds from concept to production to operation and, in some cases, to termination and disposal. In an acquisition, the holistic integrative discipline combines contributions and balances tradeoffs among cost, schedule, and performance while maintaining an acceptable level of risk covering the entire life cycle of the item.[19] This perspective is often replicated in educational programs, in that systems engineering courses are taught by faculty from other engineering departments, which helps create an interdisciplinary environment.[20][21] The need for systems engineering arose with the increase in complexity of systems and projects, in turn exponentially increasing the possibility of component friction, and therefore the unreliability of the design. When speaking in this context, complexity incorporates not only engineering systems but also the logical human organization of data. At the same time, a system can become more complex due to an increase in size as well as with an increase in the amount of data, variables, or the number of fields that are involved in the design. The International Space Station is an example of such a system. The development of smarter control algorithms, microprocessor design, and analysis of environmental systems also come within the purview of systems engineering. Systems engineering encourages the use of tools and methods to better comprehend and manage complexity in systems. Some examples of these tools can be seen here:[22] Taking an interdisciplinary approach to engineering systems is inherently complex since the behavior of and interaction among system components is not always immediately well defined or understood. Defining and characterizing such systems and subsystems and the interactions among them is one of the goals of systems engineering. In doing so, the gap that exists between informal requirements from users, operators, marketing organizations, and technical specifications is successfully bridged. [23] The principles of systems engineering\u00a0\u2013 holism, emergent behavior, boundary, et al.\u00a0\u2013 can be applied to any system, complex or otherwise, provided systems thinking is employed at all levels.[24] Besides defense and aerospace, many information and technology-based companies, software development firms, and industries in the field of electronics & communications require systems engineers as part of their team.[25] An analysis by the INCOSE Systems Engineering Center of Excellence (SECOE) indicates that optimal effort spent on systems engineering is about 15\u201320% of the total project effort.[26] At the same time, studies have shown that systems engineering essentially leads to a reduction in costs among other benefits.[26] However, no quantitative survey at a larger scale encompassing a wide variety of industries has been conducted until recently. Such studies are underway to determine the effectiveness and quantify the benefits of systems engineering.[27][28] Systems engineering encourages the use of modeling and simulation to validate assumptions or theories on systems and the interactions within them.[29][30] Use of methods that allow early detection of possible failures, in safety engineering, are integrated into the design process. At the same time, decisions made at the beginning of a project whose consequences are not clearly understood can have enormous implications later in the life of a system, and it is the task of the modern systems engineer to explore these issues and make critical decisions. No method guarantees today's decisions will still be valid when a system goes into service years or decades after first conceived. However, there are techniques that support the process of systems engineering. Examples include soft systems methodology, Jay Wright Forrester's System dynamics method, and the Unified Modeling Language (UML)\u2014all currently being explored, evaluated, and developed to support the engineering decision process. Education in systems engineering is often seen as an extension to the regular engineering courses,[31] reflecting the industry attitude that engineering students need a foundational background in one of the traditional engineering disciplines (e.g. aerospace engineering, civil engineering, electrical engineering, mechanical engineering, manufacturing engineering, industrial engineering, chemical engineering)\u2014plus practical, real-world experience to be effective as systems engineers. Undergraduate university programs explicitly in systems engineering are growing in number but remain uncommon, the degrees including such material are most often presented as a BS in Industrial Engineering. Typically programs (either by themselves or in combination with interdisciplinary study) are offered beginning at the graduate level in both academic and professional tracks, resulting in the grant of either a MS/MEng or Ph.D./EngD degree. INCOSE, in collaboration with the Systems Engineering Research Center at Stevens Institute of Technology maintains a regularly updated directory of worldwide academic programs at suitably accredited institutions.[6] As of 2017, it lists over 140 universities in North America offering more than 400 undergraduate and graduate programs in systems engineering. Widespread institutional acknowledgment of the field as a distinct subdiscipline is quite recent; the 2009 edition of the same publication reported the number of such schools and programs at only 80 and 165, respectively. Education in systems engineering can be taken as systems-centric or domain-centric: Both of these patterns strive to educate the systems engineer who is able to oversee interdisciplinary projects with the depth required of a core engineer.[32] Systems engineering tools are strategies, procedures, and techniques that aid in performing systems engineering on a project or product. The purpose of these tools varies from database management, graphical browsing, simulation, and reasoning, to document production, neutral import/export, and more.[33] There are many definitions of what a system is in the field of systems engineering. Below are a few authoritative definitions: Systems engineering processes encompass all creative, manual, and technical activities necessary to define the product and which need to be carried out to convert a system definition to a sufficiently detailed system design specification for product manufacture and deployment. Design and development of a system can be divided into four stages, each with different definitions:[41] Depending on their application, tools are used for various stages of the systems engineering process:[23] Models play important and diverse roles in systems engineering. A model can be defined in several\nways, including:[42] Together, these definitions are broad enough to encompass physical engineering models used in the verification of a system design, as well as schematic models like a functional flow block diagram and mathematical (i.e. quantitative) models used in the trade study process. This section focuses on the last.[42] The main reason for using mathematical models and diagrams in trade studies is to provide estimates of system effectiveness, performance or technical attributes, and cost from a set of known or estimable quantities. Typically, a collection of separate models is needed to provide all of these outcome variables. The heart of any mathematical model is a set of meaningful quantitative relationships among its inputs and outputs. These relationships can be as simple as adding up constituent quantities to obtain a total, or as complex as a set of differential equations describing the trajectory of a spacecraft in a gravitational field. Ideally, the relationships express causality, not just correlation.[42] Furthermore, key to successful systems engineering activities are also the methods with which these models are efficiently and effectively managed and used to simulate the systems. However, diverse domains often present recurring problems of modeling and simulation for systems engineering, and new advancements are aiming to cross-fertilize methods among distinct scientific and engineering communities, under the title of 'Modeling & Simulation-based Systems Engineering'.[43][page\u00a0needed] Initially, when the primary purpose of a systems engineer is to comprehend a complex problem, graphic representations of a system are used to communicate a system's functional and data requirements.[44] Common graphical representations include: A graphical representation relates the various subsystems or parts of a system through functions, data, or interfaces. Any or each of the above methods is used in an industry based on its requirements. For instance, the N2 chart may be used where interfaces between systems are important. Part of the design phase is to create structural and behavioral models of the system. Once the requirements are understood, it is now the responsibility of a systems engineer to refine them and to determine, along with other engineers, the best technology for a job. At this point starting with a trade study, systems engineering encourages the use of weighted choices to determine the best option. A decision matrix, or Pugh method, is one way (QFD is another) to make this choice while considering all criteria that are important. The trade study in turn informs the design, which again affects graphic representations of the system (without changing the requirements). In an SE process, this stage represents the iterative step that is carried out until a feasible solution is found. A decision matrix is often populated using techniques such as statistical analysis, reliability analysis, system dynamics (feedback control), and optimization methods. Systems Modeling Language (SysML), a modeling language used for systems engineering applications, supports the specification, analysis, design, verification and validation of a broad range of complex systems.[45] Lifecycle Modeling Language (LML), is an open-standard modeling language designed for systems engineering that supports the full lifecycle: conceptual, utilization, support, and retirement stages.[46] Many related fields may be considered tightly coupled to systems engineering. The following areas have contributed to the development of systems engineering as a distinct entity: Cognitive systems engineering (CSE) is a specific approach to the description and analysis of human-machine systems or sociotechnical systems.[47] The three main themes of CSE are how humans cope with complexity, how work is accomplished by the use of artifacts, and how human-machine systems and socio-technical systems can be described as joint cognitive systems. CSE has since its beginning become a recognized scientific discipline, sometimes also referred to as cognitive engineering. The concept of a Joint Cognitive System (JCS) has in particular become widely used as a way of understanding how complex socio-technical systems can be described with varying degrees of resolution. The more than 20 years of experience with CSE has been described extensively.[48][49] Like systems engineering, configuration management as practiced in the defense and aerospace industry is a broad systems-level practice. The field parallels the taskings of systems engineering; where systems engineering deals with requirements development, allocation to development items and verification, configuration management deals with requirements capture, traceability to the development item, and audit of development item to ensure that it has achieved the desired functionality and outcomes that systems engineering and/or Test and Verification Engineering have obtained and proven through objective testing. Control engineering and its design and implementation of control systems, used extensively in nearly every industry, is a large sub-field of systems engineering. The cruise control on an automobile and the guidance system for a ballistic missile are two examples. Control systems theory is an active field of applied mathematics involving the investigation of solution spaces and the development of new methods for the analysis of the control process. Industrial engineering is a branch of engineering that concerns the development, improvement, implementation, and evaluation of integrated systems of people, money, knowledge, information, equipment, energy, material, and process. Industrial engineering draws upon the principles and methods of engineering analysis and synthesis, as well as mathematical, physical, and social sciences together with the principles and methods of engineering analysis and design to specify, predict, and evaluate results obtained from such systems. Production Systems Engineering (PSE) is an emerging branch of Engineering intended to uncover fundamental principles of production systems and utilize them for analysis, continuous improvement, and design.[50] Interface design and its specification are concerned with assuring that the pieces of a system connect and inter-operate with other parts of the system and with external systems as necessary. Interface design also includes assuring that system interfaces are able to accept new features, including mechanical, electrical, and logical interfaces, including reserved wires, plug-space, command codes, and bits in communication protocols. This is known as extensibility. Human-Computer Interaction (HCI) or Human-Machine Interface (HMI) is another aspect of interface design and is a critical aspect of modern systems engineering. Systems engineering principles are applied in the design of communication protocols for local area networks and wide area networks. Mechatronic engineering, like systems engineering, is a multidisciplinary field of engineering that uses dynamic systems modeling to express tangible constructs. In that regard, it is almost indistinguishable from Systems Engineering, but what sets it apart is the focus on smaller details rather than larger generalizations and relationships. As such, both fields are distinguished by the scope of their projects rather than the methodology of their practice. Operations research supports systems engineering. Operations research, briefly, is concerned with the optimization of a process under multiple constraints.[51][52] Performance engineering is the discipline of ensuring a system meets customer expectations for performance throughout its life. Performance is usually defined as the speed with which a certain operation is executed or the capability of executing a number of such operations in a unit of time. Performance may be degraded when operations queued to execute are throttled by limited system capacity. For example, the performance of a packet-switched network is characterized by the end-to-end packet transit delay or the number of packets switched in an hour. The design of high-performance systems uses analytical or simulation modeling, whereas the delivery of high-performance implementation involves thorough performance testing. Performance engineering relies heavily on statistics, queueing theory, and probability theory for its tools and processes. Program management (or project management) has many similarities with systems engineering, but has broader-based origins than the engineering ones of systems engineering. Project management is also closely related to both program management and systems engineering. Both include scheduling as engineering support tool in assessing interdisciplinary concerns under management process. In particular, the direct relationship of resources, performance features, and risk to the duration of a task or the dependency links among tasks and impacts across the system lifecycle are systems engineering concerns. Proposal engineering is the application of scientific and mathematical principles to design, construct, and operate a cost-effective proposal development system. Basically, proposal engineering uses the \"systems engineering process\" to create a cost-effective proposal and increase the odds of a successful proposal. Reliability engineering is the discipline of ensuring a system meets customer expectations for reliability throughout its life (i.e. it does not fail more frequently than expected). Next to the prediction of failure, it is just as much about the prevention of failure. Reliability engineering applies to all aspects of the system. It is closely associated with maintainability, availability (dependability or RAMS preferred by some), and integrated logistics support. Reliability engineering is always a critical component of safety engineering, as in failure mode and effects analysis (FMEA) and hazard fault tree analysis, and of security engineering. Risk management, the practice of assessing and dealing with risk is one of the interdisciplinary parts of Systems Engineering. In development, acquisition, or operational activities, the inclusion of risk in tradeoffs with cost, schedule, and performance features, involves the iterative complex configuration management of traceability and evaluation to the scheduling and requirements management across domains and for the system lifecycle that requires the interdisciplinary technical approach of systems engineering. Systems Engineering has Risk Management define, tailor, implement, and monitor a structured process for risk management which is integrated into the overall effort.[53] The techniques of safety engineering may be applied by non-specialist engineers in designing complex systems to minimize the probability of safety-critical failures. The \"System Safety Engineering\" function helps to identify \"safety hazards\" in emerging designs and may assist with techniques to \"mitigate\" the effects of (potentially) hazardous conditions that cannot be designed out of systems. Security engineering can be viewed as an interdisciplinary field that integrates the community of practice for control systems design, reliability, safety, and systems engineering. It may involve such sub-specialties as authentication of system users, system targets, and others: people, objects, and processes. From its beginnings, software engineering has helped shape modern systems engineering practice. The techniques used in the handling of the complexities of large software-intensive systems have had a major effect on the shaping and reshaping of the tools, methods, and processes of Systems Engineering.",
      "ground_truth_chunk_ids": [
        "140_fixed_chunk1"
      ],
      "source_ids": [
        "S140"
      ],
      "category": "factual",
      "id": 46
    },
    {
      "question": "What is Nanotechnology?",
      "ground_truth": "Nanotechnology is the manipulation of matter with at least one dimension sized from 1 to 100 nanometers (nm). At this scale, commonly known as the nanoscale, surface area and quantum mechanical effects become important in describing properties of matter. This definition of nanotechnology includes all types of research and technologies that deal with these special properties. It is common to see the plural form \"nanotechnologies\" as well as \"nanoscale technologies\" to refer to research and applications whose common trait is scale.[1] An earlier understanding of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabricating macroscale products, now referred to as molecular nanotechnology.[2] Nanotechnology defined by scale includes fields of science such as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage,[3][4] engineering,[5] microfabrication,[6] and molecular engineering.[7] The associated research and applications range from extensions of conventional device physics to molecular self-assembly,[8] from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale. Nanotechnology may be able to create new materials and devices with diverse applications, such as in nanomedicine, nanoelectronics, agricultural sectors,[citation needed] biomaterials energy production, and consumer products. However, nanotechnology raises issues, including concerns about the toxicity and environmental impact of nanomaterials,[9] and their potential effects on global economics, as well as various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted. The concepts that seeded nanotechnology were first discussed in 1959 by physicist Richard Feynman in his talk There's Plenty of Room at the Bottom, in which he described the possibility of synthesis via direct manipulation of atoms. The term \"nano-technology\" was first used by Norio Taniguchi in 1974, though it was not widely known. Inspired by Feynman's concepts, K. Eric",
      "expected_answer": "Nanotechnology is the manipulation of matter with at least one dimension sized from 1 to 100 nanometers (nm). At this scale, commonly known as the nanoscale, surface area and quantum mechanical effects become important in describing properties of matter. This definition of nanotechnology includes all types of research and technologies that deal with these special properties. It is common to see the plural form \"nanotechnologies\" as well as \"nanoscale technologies\" to refer to research and applications whose common trait is scale.[1] An earlier understanding of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabricating macroscale products, now referred to as molecular nanotechnology.[2] Nanotechnology defined by scale includes fields of science such as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage,[3][4] engineering,[5] microfabrication,[6] and molecular engineering.[7] The associated research and applications range from extensions of conventional device physics to molecular self-assembly,[8] from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale. Nanotechnology may be able to create new materials and devices with diverse applications, such as in nanomedicine, nanoelectronics, agricultural sectors,[citation needed] biomaterials energy production, and consumer products. However, nanotechnology raises issues, including concerns about the toxicity and environmental impact of nanomaterials,[9] and their potential effects on global economics, as well as various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted. The concepts that seeded nanotechnology were first discussed in 1959 by physicist Richard Feynman in his talk There's Plenty of Room at the Bottom, in which he described the possibility of synthesis via direct manipulation of atoms. The term \"nano-technology\" was first used by Norio Taniguchi in 1974, though it was not widely known. Inspired by Feynman's concepts, K. Eric Drexler used the term \"nanotechnology\" in his 1986 book Engines of Creation: The Coming Era of Nanotechnology, which achieved popular success and helped thrust nanotechnology into the public sphere.[10]  In it he proposed the idea of a nanoscale \"assembler\" that would be able to build a copy of itself and of other items of arbitrary complexity with atom-level control. Also in 1986, Drexler co-founded The Foresight Institute to increase public awareness and understanding of nanotechnology concepts and implications. The emergence of nanotechnology as a field in the 1980s occurred through the convergence of Drexler's theoretical and public work, which developed and popularized a conceptual framework, and experimental advances that drew additional attention to the prospects[citation needed]. In the 1980s, two breakthroughs helped to spark the growth of nanotechnology. First, the invention of the scanning tunneling microscope in 1981 enabled visualization of individual atoms and bonds, and was successfully used to manipulate individual atoms in 1989. The microscope's developers Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory received a Nobel Prize in Physics in 1986.[11][12] Binnig, Quate and Gerber also invented the analogous atomic force microscope that year. Second, fullerenes (buckyballs) were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry.[13][14] C60 was not initially described as nanotechnology; the term was used regarding subsequent work with related carbon nanotubes (sometimes called graphene tubes or Bucky tubes) which suggested potential applications for nanoscale electronics and devices. The discovery of carbon nanotubes is attributed to Sumio Iijima of NEC in 1991,[15] for which Iijima won the inaugural 2008 Kavli Prize in Nanoscience. In the early 2000s, the field garnered increased scientific, political, and commercial attention that led to both controversy and progress. Controversies emerged regarding the definitions and potential implications of nanotechnologies, exemplified by the Royal Society's report on nanotechnology.[16] Challenges were raised regarding the feasibility of applications envisioned by advocates of molecular nanotechnology, which culminated in a public debate between Drexler and Smalley in 2001 and 2003.[17] Meanwhile, commercial products based on advancements in nanoscale technologies began emerging. These products were limited to bulk applications of nanomaterials and did not involve atomic control of matter. Some examples include the Silver Nano platform for using silver nanoparticles as an antibacterial agent, nanoparticle-based sunscreens, carbon fiber strengthening using silica nanoparticles, and carbon nanotubes for stain-resistant textiles.[18][19] Governments moved to promote and fund research into nanotechnology, such as American the National Nanotechnology Initiative, which formalized a size-based definition of nanotechnology and established research funding, and in Europe via the European Framework Programmes for Research and Technological Development. By the mid-2000s scientific attention began to flourish. Nanotechnology roadmaps centered on atomically precise manipulation of matter and discussed existing and projected capabilities, goals, and applications.[20][21] Nanotechnology is the science and engineering of functional systems at the molecular scale. In its original sense, nanotechnology refers to the projected ability to construct items from the bottom up making complete, high-performance products. One nanometer (nm) is one billionth, or 10\u22129, of a meter. By comparison, typical carbon\u2013carbon bond lengths, or the spacing between these atoms in a molecule, are in the range 0.12\u20130.15 nm, and DNA's diameter is around 2\u00a0nm. On the other hand, the smallest cellular life forms, the bacteria of the genus Mycoplasma, are around 200\u00a0nm in length. By convention, nanotechnology is taken as the scale range 1 to 100 nm, following the definition used by the American National Nanotechnology Initiative. The lower limit is set by the size of atoms (hydrogen has the smallest atoms, which have an approximately ,25\u00a0nm kinetic diameter). The upper limit is more or less arbitrary, but is around the size below which phenomena not observed in larger structures start to become apparent and can be made use of.[22] These phenomena make nanotechnology distinct from devices that are merely miniaturized versions of an equivalent macroscopic device; such devices are on a larger scale and come under the description of microtechnology.[23] To put that scale in another context, the comparative size of a nanometer to a meter is the same as that of a marble to the size of the earth.[24] Two main approaches are used in nanotechnology. In the \"bottom-up\" approach, materials and devices are built from molecular components which assemble themselves chemically by principles of molecular recognition.[25] In the \"top-down\" approach, nano-objects are constructed from larger entities without atomic-level control.[26] Areas of physics such as nanoelectronics, nanomechanics, nanophotonics and nanoionics have evolved to provide nanotechnology's scientific foundation. Several phenomena become pronounced as system size. These include statistical mechanical effects, as well as quantum mechanical effects, for example, the \"quantum size effect\" in which the electronic properties of solids alter along with reductions in particle size. Such effects do not apply at macro or micro dimensions. However, quantum effects can become significant when nanometer scales. Additionally, physical (mechanical, electrical, optical, etc.) properties change versus macroscopic systems. One example is the increase in surface area to volume ratio altering mechanical, thermal, and catalytic properties of materials. Diffusion and reactions can be different as well. Systems with fast ion transport are referred to as nanoionics. The mechanical properties of nanosystems are of interest in research. Modern synthetic chemistry can prepare small molecules of almost any structure. These methods are used to manufacture a wide variety of useful chemicals such as pharmaceuticals or commercial polymers. This ability raises the question of extending this kind of control to the next-larger level, seeking methods to assemble single molecules into supramolecular assemblies consisting of many molecules arranged in a well-defined manner. These approaches utilize the concepts of molecular self-assembly and/or supramolecular chemistry to automatically arrange themselves into a useful conformation through a bottom-up approach. The concept of molecular recognition is important: molecules can be designed so that a specific configuration or arrangement is favored due to non-covalent intermolecular forces. The Watson\u2013Crick basepairing rules are a direct result of this, as is the specificity of an enzyme targeting a single substrate, or the specific folding of a protein. Thus, components can be designed to be complementary and mutually attractive so that they make a more complex and useful whole. Such bottom-up approaches should be capable of producing devices in parallel and be much cheaper than top-down methods, but could potentially be overwhelmed as the size and complexity of the desired assembly increases. Most useful structures require complex and thermodynamically unlikely arrangements of atoms. Nevertheless, many examples of self-assembly based on molecular recognition in exist in biology, most notably Watson\u2013Crick basepairing and enzyme-substrate interactions. Molecular nanotechnology, sometimes called molecular manufacturing, concerns engineered nanosystems (nanoscale machines) operating on the molecular scale. Molecular nanotechnology is especially associated with molecular assemblers, machines that can produce a desired structure or device atom-by-atom using the principles of mechanosynthesis. Manufacturing in the context of productive nanosystems is not related to conventional technologies used to manufacture nanomaterials such as carbon nanotubes and nanoparticles. When Drexler independently coined and popularized the term \"nanotechnology\", he envisioned manufacturing technology based on molecular machine systems. The premise was that molecular-scale biological analogies of traditional machine components demonstrated molecular machines were possible: biology was full of examples of sophisticated, stochastically optimized biological machines. Drexler and other researchers[27] have proposed that advanced nanotechnology ultimately could be based on mechanical engineering principles, namely, a manufacturing technology based on the mechanical functionality of these components (such as gears, bearings, motors, and structural members) that would enable programmable, positional assembly to atomic specification.[28] The physics and engineering performance of exemplar designs were analyzed in Drexler's book Nanosystems: Molecular Machinery, Manufacturing, and Computation.[2] In general, assembling devices on the atomic scale requires positioning atoms on other atoms of comparable size and stickiness. Carlo Montemagno's view is that future nanosystems will be hybrids of silicon technology and biological molecular machines.[29] Richard Smalley argued that mechanosynthesis was impossible due to difficulties in mechanically manipulating individual molecules.[30] This led to an exchange of letters in the American Chemical Society publication Chemical & Engineering News in 2003.[31] Though biology clearly demonstrates that molecular machines are possible, non-biological molecular machines remained in their infancy. Alex Zettl and colleagues at Lawrence Berkeley Laboratories and UC Berkeley[32] constructed at least three molecular devices whose motion is controlled via changing voltage: a nanotube nanomotor, a molecular actuator,[33] and a nanoelectromechanical relaxation oscillator.[34] Ho and Lee at Cornell University in 1999 used a scanning tunneling microscope to move an individual carbon monoxide molecule (CO) to an individual iron atom (Fe) sitting on a flat silver crystal and chemically bound the CO to the Fe by applying a voltage.[35] Many areas of science develop or study materials having unique properties arising from their nanoscale dimensions.[38] The bottom-up approach seeks to arrange smaller components into more complex assemblies. These seek to create smaller devices by using larger ones to direct their assembly. Functional approaches seek to develop useful components without regard to how they might be assembled. These subfields seek to anticipate what inventions nanotechnology might yield, or attempt to propose an agenda along which inquiry could progress. These often take a big-picture view, with more emphasis on societal implications than engineering details. Nanomaterials can be classified in 0D, 1D, 2D and 3D nanomaterials. Dimensionality plays a major role in determining the characteristic of nanomaterials including physical, chemical, and biological characteristics. With the decrease in dimensionality, an increase in surface-to-volume ratio is observed. This indicates that smaller dimensional nanomaterials have higher surface area compared to 3D nanomaterials. Two dimensional (2D) nanomaterials have been extensively investigated for electronic, biomedical, drug delivery and biosensor applications. The atomic force microscope (AFM) and the scanning tunneling microscope (STM) are two versions of scanning probes that are used for nano-scale observation. Other types of scanning probe microscopy have much higher resolution, since they are not limited by the wavelengths of sound or light. The tip of a scanning probe can also be used to manipulate nanostructures (positional assembly). Feature-oriented scanning may be a promising way to implement these nano-scale manipulations via an automatic algorithm.[56][57] However, this is still a slow process because of low velocity of the microscope. The top-down approach anticipates nanodevices that must be built piece by piece in stages, much as manufactured items are made. Scanning probe microscopy is an important technique both for characterization and synthesis. Atomic force microscopes and scanning tunneling microscopes can be used to look at surfaces and to move atoms around. By designing different tips for these microscopes, they can be used for carving out structures on surfaces and to help guide self-assembling structures. By using, for example, feature-oriented scanning approach, atoms or molecules can be moved around on a surface with scanning probe microscopy techniques.[56][57] Various techniques of lithography, such as optical lithography, X-ray lithography, dip pen lithography, electron beam lithography or nanoimprint lithography offer top-down fabrication techniques where a bulk material is reduced to a nano-scale pattern. Another group of nano-technological techniques include those used for fabrication of nanotubes and nanowires, those used in semiconductor fabrication such as deep ultraviolet lithography, electron beam lithography, focused ion beam machining, nanoimprint lithography, atomic layer deposition, and molecular vapor deposition, and further including molecular self-assembly techniques such as those employing di-block copolymers.[58] In contrast, bottom-up techniques build or grow larger structures atom by atom or molecule by molecule. These techniques include chemical synthesis, self-assembly and positional assembly. Dual-polarization interferometry is one tool suitable for characterization of self-assembled thin films. Another variation of the bottom-up approach is molecular-beam epitaxy or MBE. Researchers at Bell Telephone Laboratories including John R. Arthur. Alfred Y. Cho, and Art C. Gossard developed and implemented MBE as a research tool in the late 1960s and 1970s. Samples made by MBE were key to the discovery of the fractional quantum Hall effect for which the 1998 Nobel Prize in Physics was awarded. MBE lays down atomically precise layers of atoms and, in the process, build up complex structures. Important for research on semiconductors, MBE is also widely used to make samples and devices for the newly emerging field of spintronics. Therapeutic products based on responsive nanomaterials, such as the highly deformable, stress-sensitive transfersome vesicles, are approved for human use in some countries.[59] As of August 21, 2008, the Project on Emerging Nanotechnologies estimated that over 800 manufacturer-identified nanotech products were publicly available, with new ones hitting the market at a pace of 3\u20134 per week.[19] Most applications are \"first generation\" passive nanomaterials that includes titanium dioxide in sunscreen, cosmetics, surface coatings,[60] and some food products; Carbon allotropes used to produce gecko tape; silver in food packaging, clothing, disinfectants, and household appliances; zinc oxide in sunscreens and cosmetics, surface coatings, paints and outdoor furniture varnishes; and cerium oxide as a fuel catalyst.[18] In the electric car industry, single wall carbon nanotubes (SWCNTs) address key lithium-ion battery challenges, including energy density, charge rate, service life, and cost. SWCNTs connect electrode particles during charge/discharge process, preventing battery premature degradation. Their exceptional ability to wrap active material particles enhanced electrical conductivity and physical properties, setting them apart multi-walled carbon nanotubes and carbon black.[61][62][63] Further applications allow tennis balls to last longer, golf balls to fly straighter, and bowling balls to become more durable. Trousers and socks have been infused with nanotechnology to last longer and lower temperature in the summer. Bandages are infused with silver nanoparticles to heal cuts faster.[64] Video game consoles and personal computers may become cheaper, faster, and contain more memory thanks to nanotechnology.[65] Also, to build structures for on chip computing with light, for example on chip optical quantum information processing, and picosecond transmission of information.[66] Nanotechnology may have the ability to make existing medical applications cheaper and easier to use in places like the doctors' offices and at homes.[67] Cars use nanomaterials in such ways that car parts require fewer metals during manufacturing and less fuel to operate in the future.[68] Nanoencapsulation involves the enclosure of active substances within carriers. Typically, these carriers offer advantages, such as enhanced bioavailability, controlled release, targeted delivery, and protection of the encapsulated substances. In the medical field, nanoencapsulation plays a significant role in drug delivery. It facilitates more efficient drug administration, reduces side effects, and increases treatment effectiveness. Nanoencapsulation is particularly useful for improving the bioavailability of poorly water-soluble drugs, enabling controlled and sustained drug release, and supporting the development of targeted therapies. These features collectively contribute to advancements in medical treatments and patient care.[69][70] Nanotechnology may play role in tissue engineering. When designing scaffolds, researchers attempt to mimic the nanoscale features of a cell's microenvironment to direct its differentiation down a suitable lineage.[71] For example, when creating scaffolds to support bone growth, researchers may mimic osteoclast resorption pits.[72] Researchers used DNA origami-based nanobots capable of carrying out logic functions to target drug delivery in cockroaches.[73] A nano bible (a .5mm2 silicon chip) was created by the Technion in order to increase youth interest in nanotechnology.[74] One concern is the effect that industrial-scale manufacturing and use of nanomaterials will have on human health and the environment, as suggested by nanotoxicology research. For these reasons, some groups advocate that nanotechnology be regulated. However, regulation might stifle scientific research and the development of beneficial innovations. Public health research agencies, such as the National Institute for Occupational Safety and Health research potential health effects stemming from exposures to nanoparticles.[75][76] Nanoparticle products may have unintended consequences. Researchers have discovered that bacteriostatic silver nanoparticles used in socks to reduce foot odor are released in the wash.[77] These particles are then flushed into the wastewater stream and may destroy bacteria that are critical components of natural ecosystems, farms, and waste treatment processes.[78] Public deliberations on risk perception in the US and UK carried out by the Center for Nanotechnology in Society found that participants were more positive about nanotechnologies for energy applications than for health applications, with health applications raising moral and ethical dilemmas such as cost and availability.[79] Experts, including director of the Woodrow Wilson Center's Project on Emerging Nanotechnologies David Rejeski, testified[80] that commercialization depends on adequate oversight, risk research strategy, and public engagement. As of 206 Berkeley, California was the only US city to regulate nanotechnology.[81] Inhaling airborne nanoparticles and nanofibers may contribute to pulmonary diseases, e.g. fibrosis.[82] Researchers found that when rats breathed in nanoparticles, the particles settled in the brain and lungs, which led to significant increases in biomarkers for inflammation and stress response[83] and that nanoparticles induce skin aging through oxidative stress in hairless mice.[84][85] A two-year study at UCLA's School of Public Health found lab mice consuming nano-titanium dioxide showed DNA and chromosome damage to a degree \"linked to all the big killers of man, namely cancer, heart disease, neurological disease and aging\".[86] A Nature Nanotechnology study suggested that some forms of carbon nanotubes could be as harmful as asbestos if inhaled in sufficient quantities. Anthony Seaton of the Institute of Occupational Medicine in Edinburgh, Scotland, who contributed to the article on carbon nanotubes said \"We know that some of them probably have the potential to cause mesothelioma. So those sorts of materials need to be handled very carefully.\"[87] In the absence of specific regulation forthcoming from governments, Paull and Lyons (2008) have called for an exclusion of engineered nanoparticles in food.[88] A newspaper article reports that workers in a paint factory developed serious lung disease and nanoparticles were found in their lungs.[89][90][91][92] Calls for tighter regulation of nanotechnology have accompanied a debate related to human health and safety risks.[93] Some regulatory agencies cover some nanotechnology products and processes \u2013 by \"bolting on\" nanotechnology to existing regulations \u2013 leaving clear gaps.[94] Davies proposed a road map describing steps to deal with these shortcomings.[95] Andrew Maynard, chief science advisor to the Woodrow Wilson Center's Project on Emerging Nanotechnologies, reported insufficient funding for human health and safety research, and as a result inadequate understanding of human health and safety risks.[96] Some academics called for stricter application of the precautionary principle, slowing marketing approval, enhanced labelling and additional safety data.[97] A Royal Society report identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that \"manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure\".[16]",
      "ground_truth_chunk_ids": [
        "142_fixed_chunk1"
      ],
      "source_ids": [
        "S142"
      ],
      "category": "factual",
      "id": 47
    },
    {
      "question": "What is Bernic Lake?",
      "ground_truth": "Bernic Lake is a lake in the eastern part of the province of Manitoba, Canada. It is located just southwest of Nopiming Provincial Park, and just north of Whiteshell Provincial Park. The Tanco mine is located on the northwestern shore of the river. The mine is an active producer of caesium, tantalum and spodumene from a large pegmatite ore body. This Manitoba location article is a stub. You can help Wikipedia by adding missing information. This article related to a lake in Canada is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Bernic Lake is a lake in the eastern part of the province of Manitoba, Canada. It is located just southwest of Nopiming Provincial Park, and just north of Whiteshell Provincial Park. The Tanco mine is located on the northwestern shore of the river. The mine is an active producer of caesium, tantalum and spodumene from a large pegmatite ore body. This Manitoba location article is a stub. You can help Wikipedia by adding missing information. This article related to a lake in Canada is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "127_random_chunk1"
      ],
      "source_ids": [
        "S327"
      ],
      "category": "factual",
      "id": 48
    },
    {
      "question": "What is Water cycle?",
      "ground_truth": "The water cycle (or hydrologic cycle or hydrological cycle) is a biogeochemical cycle that involves the continuous change in form of water on, above and below the surface of the Earth across different reservoirs. The mass of water on Earth remains fairly constant over time.[2] However, the partitioning of the water into the major reservoirs of ice, fresh water, salt water and atmospheric water is variable and depends on climatic variables. The water moves from one reservoir to another, such as from river to ocean, or from the ocean to the atmosphere due to a variety of physical and chemical processes. The processes that drive these movements, or fluxes, are evaporation, transpiration, condensation, precipitation, sublimation, infiltration, surface runoff, and subsurface flow. In doing so, the water goes through different phases: liquid, solid (ice) and vapor. The ocean plays a key role in the water cycle as it is the source of 86% of global evaporation.[3] The water cycle is driven by energy exchanges in the form of heat transfers between different phases. The energy released or absorbed during a phase change can result in temperature changes.[4] Heat is absorbed as water transitions from the liquid to the vapor phase through evaporation. This heat is also known as the latent heat of vaporization.[5] Conversely, when water condenses or melts from solid ice it releases energy and heat. On a global scale, water plays a critical role in transferring heat from the tropics to the poles via ocean circulation.[6] The evaporative phase of the cycle also acts as a purification process by separating water molecules from salts and other particles that are present in its liquid phase.[7] The condensation phase in the atmosphere replenishes the land with freshwater. The flow of liquid water transports minerals across the globe. It also reshapes the",
      "expected_answer": "The water cycle (or hydrologic cycle or hydrological cycle) is a biogeochemical cycle that involves the continuous change in form of water on, above and below the surface of the Earth across different reservoirs. The mass of water on Earth remains fairly constant over time.[2] However, the partitioning of the water into the major reservoirs of ice, fresh water, salt water and atmospheric water is variable and depends on climatic variables. The water moves from one reservoir to another, such as from river to ocean, or from the ocean to the atmosphere due to a variety of physical and chemical processes. The processes that drive these movements, or fluxes, are evaporation, transpiration, condensation, precipitation, sublimation, infiltration, surface runoff, and subsurface flow. In doing so, the water goes through different phases: liquid, solid (ice) and vapor. The ocean plays a key role in the water cycle as it is the source of 86% of global evaporation.[3] The water cycle is driven by energy exchanges in the form of heat transfers between different phases. The energy released or absorbed during a phase change can result in temperature changes.[4] Heat is absorbed as water transitions from the liquid to the vapor phase through evaporation. This heat is also known as the latent heat of vaporization.[5] Conversely, when water condenses or melts from solid ice it releases energy and heat. On a global scale, water plays a critical role in transferring heat from the tropics to the poles via ocean circulation.[6] The evaporative phase of the cycle also acts as a purification process by separating water molecules from salts and other particles that are present in its liquid phase.[7] The condensation phase in the atmosphere replenishes the land with freshwater. The flow of liquid water transports minerals across the globe. It also reshapes the geological features of the Earth, through processes of weathering, erosion, and deposition. The water cycle is also essential for the maintenance of most life and ecosystems on the planet. Human actions are greatly affecting the water cycle. Activities such as deforestation, urbanization, and the extraction of groundwater are altering natural landscapes (land use changes) all have an effect on the water cycle.[8]:\u200a1153\u200a On top of this, climate change is leading to an intensification of the water cycle. Research has shown that global warming is causing shifts in precipitation patterns, increased frequency of extreme weather events, and changes in the timing and intensity of rainfall.[9]:\u200a85\u200a These water cycle changes affect ecosystems, water availability, agriculture, and human societies. The water cycle is powered by the energy emitted from the sun. There are several ways in which this is accomplished, one of the first ways is through evaporation where the energy from the sun heats the water in oceans, lakes, streams, rivers, seas, ponds, etc. and that water goes through a phase change to become a gas (water vapor) that goes up into the atmosphere. Two other ways that water gets into the atmosphere is through snow and ice sublimating into water vapor and through evapotranspiration which is water transpired from plants and evaporated from the soil. Clouds form because water molecules have a smaller molecular mass than the major gas components of the atmosphere (oxygen, O2; and nitrogen, N2); this smaller molecular mass leads to water having a lower density which drives the water molecules higher up in the atmosphere due to buoyancy. However, as altitude increases, air pressure decreases which causes a drop in temperature. The lower temperature forces the water vapor to go through another phase change, this time it forces it to condense into liquid water droplets which are supported by an updraft; if there is enough of these water droplets over a large area, it is considered a cloud. Condensation of the water vapour closer to the ground level is referred to as fog. Atmospheric circulation moves water vapor around the globe; cloud particles collide, grow, and fall out of the upper atmospheric layers as precipitation. Some precipitation falls as snow, hail, or sleet, and can accumulate in ice caps and glaciers, which can store frozen water for thousands of years. Most water falls as rain back into the ocean or onto land, where the water flows over the ground as surface runoff. A portion of this runoff enters rivers, with streamflow moving water towards the oceans. Runoff and water emerging from the ground (groundwater) may be stored as freshwater in lakes. Not all runoff flows into rivers; much of it soaks into the ground as infiltration. Some water infiltrates deep into the ground and replenishes aquifers, which can store freshwater for long periods of time. Some infiltration stays close to the land surface and can seep back into surface-water bodies (and the ocean) as groundwater discharge or be taken up by plants and transferred back to the atmosphere as water vapor by transpiration. Some groundwater finds openings in the land surface and emerges as freshwater springs. In river valleys and floodplains, there is often continuous water exchange between surface water and ground water in the hyporheic zone. Over time, the water returns to the ocean, to continue the water cycle. The ocean plays a key role in the water cycle. The ocean holds \"97% of the total water on the planet; 78% of global precipitation occurs over the ocean, and it is the source of 86% of global evaporation\".[3] Important physical processes within the water cycle include (in alphabetical order): The residence time of a reservoir within the hydrologic cycle is the average time a water molecule will spend in that reservoir (see table). It is a measure of the average age of the water in that reservoir. Groundwater can spend over 10,000 years beneath Earth's surface before leaving.[23] Particularly old groundwater is called fossil water. Water stored in the soil remains there very briefly, because it is spread thinly across the Earth, and is readily lost by evaporation, transpiration, stream flow, or groundwater recharge. After evaporating, the residence time in the atmosphere is about 9 days before condensing and falling to the Earth as precipitation. The major ice sheets \u2013 Antarctica and Greenland \u2013 store ice for very long periods. Ice from Antarctica has been reliably dated to 800,000 years before present, though the average residence time is shorter.[24] In hydrology, residence times can be estimated in two ways.[25][26] The more common method relies on the principle of conservation of mass (water balance) and assumes the amount of water in a given reservoir is roughly constant. With this method, residence times are estimated by dividing the volume of the reservoir by the rate by which water either enters or exits the reservoir. Conceptually, this is equivalent to timing how long it would take the reservoir to become filled from empty if no water were to leave (or how long it would take the reservoir to empty from full if no water were to enter). An alternative method to estimate residence times, which is gaining in popularity for dating groundwater, is the use of isotopic techniques. This is done in the subfield of isotope hydrology. The water cycle describes the processes that drive the movement of water throughout the hydrosphere. However, much more water is \"in storage\" (or in \"pools\") for long periods of time than is actually moving through the cycle. The storehouses for the vast majority of all water on Earth are the oceans. It is estimated that of the 1,386,000,000\u00a0km3 of the world's water supply, about 1,338,000,000\u00a0km3 is stored in oceans, or about 97%. It is also estimated that the oceans supply about 90% of the evaporated water that goes into the water cycle.[28] The Earth's ice caps, glaciers, and permanent snowpack store another 24,064,000\u00a0km3, accounting for only 1.7% of the planet's total water volume. However, this quantity of water is 68.7% of all fresh water on the planet.[29] Human activities can alter the water cycle at the local or regional level. This happens due to changes in land use and land cover. Such changes affect \"precipitation, evaporation, flooding, groundwater, and the availability of freshwater for a variety of uses\".[8]:\u200a1153 Examples of common land use changes include urbanization, agricultural expansion, and deforestation. These changes can increase soil compaction and impervious surface cover which decrease the infiltration capacity of soils and result in greater surface runoff rates.[30] Deforestation has local and regional effects; at the local level it reduces soil moisture, evaporation, rainfall, and snowfall; at the regional level it can cause temperature changes that affect that affect rainfall patterns.[8]:\u200a1153 Water management structures such as dams, stormwater drains, and sewage pipes can also alter local hydrologic conditions. Dams can alter natural flow rates, decrease water quality, and lead to a loss of habitat for aquatic species.[31] Stormwater drains function to decrease runoff rates, regulate flow rates, and increase groundwater recharge.[32] Leakage from sewage pipes may artificially contribute to groundwater recharge, resulting in higher stream baseflow conditions and groundwater contamination.[33] Groundwater depletion, however, remains an ongoing concern as groundwater is being pumped at unsustainable rates to meet municipal, industrial, and agricultural water demands.[34] Since the middle of the 20th century, human-caused climate change has resulted in observable changes in the global water cycle.[9]:\u200a85\u200a The IPCC Sixth Assessment Report in 2021 predicted that these changes will continue to grow significantly at the global and regional level.[9]:\u200a85\u200a These findings are a continuation of scientific consensus expressed in the IPCC Fifth Assessment Report from 2007 and other special reports by the Intergovernmental Panel on Climate Change which had already stated that the water cycle will continue to intensify throughout the 21st century.[8] The effects of climate change on the water cycle are profound and have been described as an intensification or a strengthening of the water cycle (also called the hydrologic cycle).[36]:\u200a1079\u200a This effect has been observed since at least 1980.[36]:\u200a1079\u200a One example is when heavy rain events become even stronger. The effects of climate change on the water cycle have important negative effects on the availability of freshwater resources, as well as other water reservoirs such as oceans, ice sheets, the atmosphere and soil moisture. The water cycle is essential to life on Earth and plays a large role in the global climate system and ocean circulation. The warming of our planet is expected to be accompanied by changes in the water cycle for various reasons.[37] For example, a warmer atmosphere can contain more water vapor which has effects on evaporation and rainfall. The underlying cause of the intensifying water cycle is the increased amount of greenhouse gases in the atmosphere, which lead to a warmer atmosphere through the greenhouse effect.[37] Fundamental laws of physics explain how the saturation vapor pressure in the atmosphere increases by 7% when temperature rises by 1\u00a0\u00b0C.[38] This relationship is known as the Clausius-Clapeyron equation. The strength of the water cycle and its changes over time are of considerable interest, especially as the climate changes.[39] The hydrological cycle is a system whereby the evaporation of moisture in one place leads to precipitation (rain or snow) in another place. For example, evaporation always exceeds precipitation over the oceans. This allows moisture to be transported by the atmosphere from the oceans onto land where precipitation exceeds evapotranspiration. The runoff from the land flows into streams and rivers and discharges into the ocean, which completes the global cycle.[39] The water cycle is a key part of Earth's energy cycle through the evaporative cooling at the surface which provides latent heat to the atmosphere, as atmospheric systems play a primary role in moving heat upward.[39] While the water cycle is itself a biogeochemical cycle, flow of water over and beneath the Earth is a key component of the cycling of other biogeochemicals.[40] Runoff is responsible for almost all of the transport of eroded sediment and phosphorus from land to waterbodies.[41] The salinity of the oceans is derived from erosion and transport of dissolved salts from the land. Cultural eutrophication of lakes is primarily due to phosphorus, applied in excess to agricultural fields in fertilizers, and then transported overland and down rivers. Both runoff and groundwater flow play significant roles in transporting nitrogen from the land to waterbodies.[42] The dead zone at the outlet of the Mississippi River is a consequence of nitrates from fertilizer being carried off agricultural fields and funnelled down the river system to the Gulf of Mexico. Runoff also plays a part in the carbon cycle, again through the transport of eroded rock and soil.[43] The hydrodynamic wind within the upper portion of a planet's atmosphere allows light chemical elements such as Hydrogen to move up to the exobase, the lower limit of the exosphere, where the gases can then reach escape velocity, entering outer space without impacting other particles of gas. This type of gas loss from a planet into space is known as planetary wind.[44] Planets with hot lower atmospheres could result in humid upper atmospheres that accelerate the loss of hydrogen.[45] In ancient times, it was widely thought that the land mass floated on a body of water, and that most of the water in rivers has its origin under the earth. Examples of this belief can be found in the works of Homer (c.\u2009800 BCE). In Works and Days (ca. 700 BC), the Greek poet Hesiod outlines the idea of the water cycle: \"[Vapour] is drawn from the ever-flowing rivers and is raised high above the earth by windstorm, and sometimes it turns to rain towards evening, and sometimes to wind when Thracian Boreas huddles the thick clouds.\"[46] In the ancient Near East, Hebrew scholars observed that even though the rivers ran into the sea, the sea never became full. Some scholars conclude that the water cycle was described completely during this time in this passage: \"The wind goeth toward the south, and turneth about unto the north; it whirleth about continually, and the wind returneth again according to its circuits. All the rivers run into the sea, yet the sea is not full; unto the place from whence the rivers come, thither they return again\" (Ecclesiastes 1:6-7).[47] Furthermore, it was also observed that when the clouds were full, they emptied rain on the earth (Ecclesiastes 11:3). In the Adityahridayam (a devotional hymn to the Sun God) of Ramayana, a Hindu epic dated to the 4th century BCE, it is mentioned in the 22nd verse that the Sun heats up water and sends it down as rain. By roughly 500 BCE, Greek scholars were speculating that much of the water in rivers can be attributed to rain. The origin of rain was also known by then. These scholars maintained the belief, however, that water rising up through the earth contributed a great deal to rivers. Examples of this thinking included Anaximander (570 BCE) (who also speculated about the evolution of land animals from fish[48]) and Xenophanes of Colophon (530 BCE).[49] Warring States period Chinese scholars such as Chi Ni Tzu (320 BCE) and Lu Shih Ch'un Ch'iu (239 BCE) had similar thoughts.[50] The idea that the water cycle is a closed cycle can be found in the works of Anaxagoras of Clazomenae (460 BCE) and Diogenes of Apollonia (460 BCE). Both Plato (390 BCE) and Aristotle (350 BCE) speculated about percolation as part of the water cycle. Aristotle correctly hypothesized that the sun played a role in the Earth's hydraulic cycle in his book Meteorology, writing \"By it [the sun's] agency the finest and sweetest water is everyday carried up and is dissolved into vapor and rises to the upper regions, where it is condensed again by the cold and so returns to the earth,\" and believed that clouds were composed of cooled and condensed water vapor.[51][52] Much like the earlier Aristotle, the Eastern Han Chinese scientist Wang Chong (27\u2013100 AD) accurately described the water cycle of Earth in his Lunheng but was dismissed by his contemporaries.[53] Up to the time of the Renaissance, it was wrongly assumed that precipitation alone was insufficient to feed rivers, for a complete water cycle, and that underground water pushing upwards from the oceans were the main contributors to river water. Bartholomew of England held this view (1240 CE), as did Leonardo da Vinci (1500 CE) and Athanasius Kircher (1644 CE). The first published thinker to assert that rainfall alone was sufficient for the maintenance of rivers was Bernard Palissy (1580 CE), who is often credited as the discoverer of the modern theory of the water cycle. Palissy's theories were not tested scientifically until 1674, in a study commonly attributed to Pierre Perrault. Even then, these beliefs were not accepted in mainstream science until the early nineteenth century.[54]",
      "ground_truth_chunk_ids": [
        "176_fixed_chunk1"
      ],
      "source_ids": [
        "S176"
      ],
      "category": "factual",
      "id": 49
    },
    {
      "question": "What is Mughal Empire?",
      "ground_truth": "The Mughal Empire was an early modern empire that ruled most of the Indian subcontinent. At its peak, the empire stretched from the outer fringes of the Indus River Basin in the west, northern Afghanistan in the northwest, and Kashmir in the north, to the highlands of present-day Assam and Bangladesh in the east, and the uplands of the Deccan Plateau in South India.[9][10] The Mughal Empire is conventionally said to have been founded in 1526 by Babur, a ruler from what is now Uzbekistan, who with the help of the neighbouring Safavid and Ottoman Empires[11] defeated the sultan of Delhi, Ibrahim Lodi, in the First Battle of Panipat and swept down the plains of North India. The Mughal imperial structure, however, is sometimes dated to 1600, to the rule of Babur's grandson, Akbar.[12] This imperial structure lasted until 1720, shortly after the death of the last major emperor, Aurangzeb,[13][14] during whose reign the empire also achieved its maximum geographical extent. Reduced subsequently to the region in and around Old Delhi by 1760, the empire was formally dissolved by the British Raj after the Indian Rebellion of 1857. Although the Mughal Empire was created and sustained by military warfare,[15][16][17] it did not vigorously suppress the cultures and peoples it came to rule; rather, it equalised and placated them through new administrative practices,[18][19] and diverse ruling elites, leading to more efficient, centralised, and standardised rule.[20] The basis of the empire's collective wealth was agricultural taxes, implemented by the third Mughal emperor, Akbar.[21][22] These taxes, which amounted to well over half the output of a peasant cultivator,[23] were paid in the well-regulated silver currency,[20] and allowed peasants and artisans to enter larger markets.[24] The relative peace maintained by the empire during much of the 17th century was a factor in India's economic",
      "expected_answer": "The Mughal Empire was an early modern empire that ruled most of the Indian subcontinent. At its peak, the empire stretched from the outer fringes of the Indus River Basin in the west, northern Afghanistan in the northwest, and Kashmir in the north, to the highlands of present-day Assam and Bangladesh in the east, and the uplands of the Deccan Plateau in South India.[9][10] The Mughal Empire is conventionally said to have been founded in 1526 by Babur, a ruler from what is now Uzbekistan, who with the help of the neighbouring Safavid and Ottoman Empires[11] defeated the sultan of Delhi, Ibrahim Lodi, in the First Battle of Panipat and swept down the plains of North India. The Mughal imperial structure, however, is sometimes dated to 1600, to the rule of Babur's grandson, Akbar.[12] This imperial structure lasted until 1720, shortly after the death of the last major emperor, Aurangzeb,[13][14] during whose reign the empire also achieved its maximum geographical extent. Reduced subsequently to the region in and around Old Delhi by 1760, the empire was formally dissolved by the British Raj after the Indian Rebellion of 1857. Although the Mughal Empire was created and sustained by military warfare,[15][16][17] it did not vigorously suppress the cultures and peoples it came to rule; rather, it equalised and placated them through new administrative practices,[18][19] and diverse ruling elites, leading to more efficient, centralised, and standardised rule.[20] The basis of the empire's collective wealth was agricultural taxes, implemented by the third Mughal emperor, Akbar.[21][22] These taxes, which amounted to well over half the output of a peasant cultivator,[23] were paid in the well-regulated silver currency,[20] and allowed peasants and artisans to enter larger markets.[24] The relative peace maintained by the empire during much of the 17th century was a factor in India's economic expansion.[25] The burgeoning European presence in the Indian Ocean and an increasing demand for Indian raw and finished products generated much wealth for the Mughal court.[26] There was conspicuous consumption among the Mughal elite,[27] resulting in greater patronage of painting, literary forms, textiles, and architecture, especially during the reign of Shah Jahan.[28] Among the Mughal UNESCO World Heritage Sites in South Asia are Agra Fort, Fatehpur Sikri, Red Fort, Humayun's Tomb, Lahore Fort, Shalamar Gardens, and the Taj Mahal, which has been described as \"the jewel of Muslim art in India, and one of the universally admired masterpieces of the world's heritage\".[29] The word Mughal (also spelled Mogul[30] or Moghul in English) is the Indo-Persian form of Mongol. However, the Mughal dynasty's early followers were Chagatai Turks and not Mongols.[31][32] The term Mughal was applied to them in India by association with the Mongols and to distinguish them from the Afghan elite who ruled the Delhi Sultanate.[31] In the West, the term Mughal was used for the emperor and, by extension, the empire as a whole.[33] The term remains disputed by Indologists.[34] In Marshall Hodgson's view, the dynasty should be called Timurid/Timuri or Indo-Timurid.[31] The closest to an official name for the empire was Hindustan, which was documented in the Ain-i-Akbari.[35] Mughal administrative records also refer to the empire as \"dominion of Hindustan\" (Wil\u0101yat-i-Hindust\u0101n),[36] \"country of Hind\" (Bil\u0101d-i-Hind), \"Sultanate of Al-Hind\" (Sal\u1e6danat(i) al-Hind\u012byyah) as observed in the epitaph of Emperor Aurangzeb[37] or endonymous identification from emperor Bahadur Shah Zafar as \"Land of Hind\" (Hindost\u0101n) in Hindustani.[38][39] Contemporary Chinese chronicles referred to the empire as Hindustan (H\u00e9nd\u016bs\u012bt\u01cen).[40] The Mughal designation for their dynasty was Gurkani (G\u016brk\u0101niy\u0101n), a reference to their descent from the Turco-Mongol conqueror Timur, who took the title G\u016brk\u0101n 'son-in-law' after his marriage to a Chinggisid princess.[41] The Mughal Empire was founded by Babur (reigned 1526\u20131530), a Central Asian ruler who was descended from the Turco-Mongol conqueror Timur (the founder of the Timurid Empire) on his father's side, and from Genghis Khan on his mother's side.[43] Paternally, Babur belonged to the Turkicised Barlas tribe of Mongol origin.[44] Ousted from his ancestral domains in Central Asia, Babur turned to India to satisfy his ambitions.[45] He established himself in Kabul and then pushed steadily southward into India from Afghanistan through the Khyber Pass.[43] Babur's forces defeated Ibrahim Lodi, Sultan of Delhi, in the First Battle of Panipat in 1526. Through his use of firearms and cannons, he was able to shatter Ibrahim's armies despite being at a numerical disadvantage,[46][47] expanding his dominion up to the mid Indo-Gangetic Plain.[48] After the battle, the centre of Mughal power shifted to Agra.[46] In the decisive Battle of Khanwa, fought near Agra a year later, the Timurid forces of Babur defeated the combined Rajput armies of Rana Sanga of Mewar, with his native cavalry employing traditional flanking tactics.[46][47] The preoccupation with wars and military campaigns, however, did not allow the new emperor to consolidate the gains he had made in India.[49] The instability of the empire became evident under his son, Humayun (reigned 1530\u20131556), who was forced into exile in Persia by the rebellious Sher Shah Suri (reigned 1540\u20131545).[43] Humayun's exile in Persia established diplomatic ties between the Safavid and Mughal courts and led to increasing Persian cultural influence in the later restored Mughal Empire.[50] Humayun's triumphant return from Persia in 1555 restored Mughal rule in some parts of India, but he died in an accident the next year.[51] Akbar (reigned 1556\u20131605) was born Jalal-ud-din Muhammad[52] in the Umarkot Fort,[53] to Humayun and his wife Hamida Banu Begum, a Persian princess.[54] Akbar succeeded to the throne under a regent, Bairam Khan, who helped consolidate the Mughal Empire in India.[55] Through warfare, Akbar was able to extend the empire in all directions and controlled almost the entire Indian subcontinent north of the Godavari River.[56] He created a new ruling elite loyal to him, implemented a modern administration, and encouraged cultural developments. He increased trade with European trading companies.[43] India developed a strong and stable economy, leading to commercial expansion and economic development.[citation needed] Akbar allowed freedom of religion at his court and attempted to resolve socio-political and cultural differences in his empire by establishing a new religion, Din-i-Ilahi, with strong characteristics of a ruler cult.[43] He left his son an internally stable state, which was in the midst of its golden age, but before long signs of political weakness would emerge.[43] Jahangir (born Salim,[57] reigned 1605\u20131627) was born to Akbar and his wife Mariam-uz-Zamani, an Indian princess.[58] Salim was named after the Indian Sufi saint, Salim Chishti.[59][60] He \"was addicted to opium, neglected the affairs of the state, and came under the influence of rival court cliques\".[43] Jahangir distinguished himself from Akbar by making substantial efforts to gain the support of the Islamic religious establishment. One way he did this was by bestowing many more madad-i-ma'ash (tax-free personal land revenue grants given to religiously learned or spiritually worthy individuals) than Akbar had.[61] In contrast to Akbar, Jahangir came into conflict with non-Muslim religious leaders, notably the Sikh guru Arjan, whose execution was the first of many conflicts between the Mughal Empire and the Sikh community.[62][63][64] Shah Jahan (reigned 1628\u20131658) was born to Jahangir and his wife Jagat Gosain.[57] His reign ushered in the golden age of Mughal architecture.[65] During the reign of Shah Jahan, the splendour of the Mughal court reached its peak, as exemplified by the Taj Mahal. The cost of maintaining the court, however, began to exceed the revenue coming in.[43] Shah Jahan extended the Mughal Empire to the Deccan by ending the Ahmadnagar Sultanate and forcing the Adil Shahis and Qutb Shahis to pay tribute.[66] Shah Jahan's eldest son, the liberal Dara Shikoh, became regent in 1658, as a result of his father's illness.[43] Dara championed a syncretistic Hindu-Muslim culture, emulating his great-grandfather Akbar.[67] With the support of the Islamic orthodoxy, however, a younger son of Shah Jahan, Aurangzeb (r.\u20091658\u20131707), seized the throne. Aurangzeb defeated Dara in 1659 and had him executed.[43] Although Shah Jahan fully recovered from his illness, Aurangzeb kept Shah Jahan imprisoned until he died in 1666.[68] Aurangzeb brought the empire to its greatest territorial extent,[69] and oversaw an increase in the Islamicisation of the Mughal state. He encouraged conversion to Islam, reinstated the jizya on non-Muslims, and compiled the Fatawa 'Alamgiri, a collection of Islamic law. Aurangzeb also ordered the execution of the Sikh guru Tegh Bahadur, leading to the militarisation of the Sikh community.[70][63][64] From the imperial perspective, conversion to Islam integrated local elites into the king's vision of a network of shared identity that would join disparate groups throughout the empire in obedience to the Mughal emperor.[71] He led campaigns from 1682 in the Deccan,[72] annexing its remaining Muslim powers of Bijapur and Golconda,[73][72] though engaged in a prolonged conflict in the region which had a ruinous effect on the empire.[74] The campaigns took a toll on the Mughal treasury, and Aurangzeb's absence led to a severe decline in governance, while stability and economic output in the Mughal Deccan plummeted.[74] Aurangzeb is considered the most controversial Mughal emperor,[75] with some historians arguing his religious conservatism and intolerance undermined the stability of Mughal society,[43] while other historians question this, noting that he financed or patronised the building of non-Muslim institutions,[76] employed significantly more Hindus in his imperial bureaucracy than his predecessors did, and opposed bigotry against Hindus and Shia Muslims.[77] Aurangzeb's son, Bahadur Shah I, repealed the religious policies of his father and attempted to reform the administration. \"However, after he died in 1712, the Mughal dynasty began to sink into chaos and violent feuds. In 1719 alone, four emperors successively ascended the throne\",[43] as figureheads under the rule of a brotherhood of nobles belonging to the Indian Muslim caste known as the Sadaat-e-Bara, whose leaders, the Sayyid Brothers, became the de facto sovereigns of the empire.[78][79] During the reign of Muhammad Shah (reigned 1719\u20131748), the empire began to break up, and vast tracts of central India passed from Mughal to Maratha hands. As the Mughals tried to suppress the independence of Nizam-ul-Mulk, Asaf Jah I in the Deccan, he encouraged the Marathas to invade central and northern India.[80][81][82] The Indian campaign of Nader Shah, who had previously reestablished Iranian suzerainty over most of West Asia, the Caucasus, and Central Asia, culminated with the Sack of Delhi shattering the remnants of Mughal power and prestige, and taking off all the accumulated Mughal treasury. The Mughals could no longer finance the huge armies with which they had formerly enforced their rule. Many of the empire's elites now sought to control their affairs and broke away to form independent kingdoms.[83] But lip service continued to be paid to the Mughal Emperor as the highest manifestation of sovereignty. Not only the Muslim gentry, but the Maratha, Hindu, and Sikh leaders took part in ceremonial acknowledgements of the emperor as the sovereign of India.[84] Meanwhile, some regional polities within the increasingly fragmented Mughal Empire involved themselves and the state in global conflicts, leading only to defeat and loss of territory during conflicts such as the Carnatic wars and Bengal War.[citation needed] The Mughal Emperor Shah Alam II (1759\u20131806) made futile attempts to reverse the Mughal decline. Delhi was sacked by the Afghans, and when the Third Battle of Panipat was fought between the Maratha Empire and the Afghans (led by Ahmad Shah Durrani) in 1761, in which the Afghans were victorious, the emperor had ignominiously taken temporary refuge with the British to the east. In 1771, the Marathas recaptured Delhi from the Rohillas, and in 1784 the Marathas officially became the protectors of the emperor in Delhi,[85] a state of affairs that continued until the Second Anglo-Maratha War. Thereafter, the British East India Company became the protectors of the Mughal dynasty in Delhi.[84] The British East India Company took control of the former Mughal province of Bengal-Bihar in 1793 after it abolished local rule (Nizamat) that lasted until 1858, marking the beginning of the British colonial era over the Indian subcontinent. By 1857 a considerable part of former Mughal India was under the East India Company's control. After a crushing defeat in the Indian Rebellion of 1857 which he nominally led, the last Mughal emperor, Bahadur Shah Zafar, was deposed by the British East India Company and exiled in 1858 to Rangoon, Burma.[86] Historians have offered numerous accounts of the several factors involved in the rapid collapse of the Mughal Empire between 1707 and 1720, after a century of growth and prosperity. A succession of short-lived incompetent and weak rulers, and civil wars over the succession, created political instability at the centre. The Mughals appeared virtually unassailable during the 17th century, but, once gone, their imperial overstretch became clear, and the situation could not be recovered. The seemingly innocuous European trading companies, such as the British East Indies Company, played no real part in the initial decline; they were still racing to get permission from the Mughal rulers to establish trades and factories in India.[87] In fiscal terms, the throne lost the revenues needed to pay its chief officers, the emirs (nobles) and their entourages. The emperor lost authority as the widely scattered imperial officers lost confidence in the central authorities and made their deals with local men of influence. The imperial army bogged down in long, futile wars against the more aggressive Marathas, and lost its fighting spirit. Finally came a series of violent political feuds over control of the throne. After the execution of Emperor Farrukhsiyar in 1719, local Mughal successor states took power in region after region.[88] The Mughal Empire had a highly centralised, bureaucratic government, most of which was instituted during the rule of the third Mughal emperor, Akbar.[89][72] The central government was headed by the Mughal emperor; immediately beneath him were four ministries. The finance/revenue ministry, headed by an official called a diwan, was responsible for controlling revenues from the empire's territories, calculating tax revenues, and using this information to distribute assignments. The ministry of the military (army/intelligence) was headed by an official titled mir bakhshi, who was in charge of military organisation, messenger service, and the mansabdari system. The ministry in charge of law/religious patronage was the responsibility of the sadr as-sudr, who appointed judges and managed charities and stipends. Another ministry was dedicated to the imperial household and public works, headed by the mir saman. Of these ministers, the diwan held the most importance, and typically acted as the wazir (prime minister) of the empire.[86][89][90] The empire was divided into Subah (provinces), each of which was headed by a provincial governor called a subadar. The structure of the central government was mirrored at the provincial level; each suba had its own bakhshi, sadr as-sudr, and finance minister that reported directly to the central government rather than the subahdar. Subas were subdivided into administrative units known as sarkars, which were further divided into groups of villages known as parganas. The Mughal government in the pargana consisted of a Muslim judge and local tax collector.[86][89] Parganas were the basic administrative unit of the Mughal Empire.[91] Mughal administrative divisions were not static. Territories were often rearranged and reconstituted for better administrative control, and to extend cultivation. For example, a sarkar could turn into a subah, and Parganas were often transferred between sarkars. The hierarchy of division was ambiguous sometimes, as a territory could fall under multiple overlapping jurisdictions. Administrative divisions were also vague in their geography\u2014the Mughal state did not have enough resources or authority to undertake detailed land surveys, and hence the geographical limits of these divisions were not formalised and maps were not created. The Mughals instead recorded detailed statistics about each division, to assess the territory's capacity for revenue, based on simpler land surveys.[92] The Mughals had multiple imperial capitals, established throughout their rule. These were the cities of Agra, Delhi, Lahore, and Fatehpur Sikri. Power often shifted back and forth between these capitals.[93] Sometimes this was necessitated by political and military demands, but shifts also occurred for ideological reasons (for example, Akbar's establishment of Fatehpur Sikri), or even simply because the cost of establishing a new capital was marginal.[94] Situations where two simultaneous capitals existed happened multiple times in Mughal history. Certain cities also served as short-term, provincial capitals, as was the case with Aurangzeb's shift to Aurangabad in the Deccan.[93] Kabul was the summer capital of Mughals from 1526 to 1681.[95] The imperial camp, used for military expeditions and royal tours, also served as a kind of mobile, \"de facto\" administrative capital. From the time of Akbar, Mughal camps were huge in scale, accompanied by numerous personages associated with the royal court, as well as soldiers and labourers. All administration and governance were carried out within them. The Mughal Emperors spent a significant portion of their ruling period within these camps.[96] After Aurangzeb, the Mughal capital definitively became the walled city of Shahjahanabad (Old Delhi).[97] The Mughal Empire's legal system was context-specific and evolved throughout the empire's rule. Being a Muslim state, the empire employed fiqh (Islamic jurisprudence) and therefore the fundamental institutions of Islamic law such as those of the qadi (judge), mufti (jurisconsult), and muhtasib (censor and market supervisor) were well-established in the Mughal Empire. However, the dispensation of justice also depended on other factors, such as administrative rules, local customs, and political convenience. This was due to Persianate influences on Mughal ideology and the fact that the Mughal Empire governed a non-Muslim majority.[98] Scholar Mouez Khalfaoui notes that legal institutions in the Mughal Empire systemically suffered from the corruption of local judges.[99] The Mughal Empire followed the Sunni Hanafi system of jurisprudence. In its early years, the empire relied on Hanafi legal references inherited from its predecessor, the Delhi Sultanate. These included the al-Hidayah (the best guidance) and the Fatawa al-Tatarkhaniyya (religious decisions of the Emire Tatarkhan). During the Mughal Empire's peak, the Fatawa 'Alamgiri was commissioned by Emperor Aurangzeb. This compendium of Hanafi law sought to serve as a central reference for the Mughal state that dealt with the specifics of the South Asian context.[99] The Mughal Empire also drew on Persian notions of kingship. Particularly, this meant that the Mughal emperor was considered the supreme authority on legal affairs.[98] Various kinds of courts existed in the Mughal Empire. One such court was that of the qadi. The Mughal qadi was responsible for dispensing justice; this included settling disputes, judging people for crimes, and dealing with inheritances and orphans. The qadi also had additional importance in documents, as the seal of the qadi was required to validate deeds and tax records. Qadis did not constitute a single position, but made up a hierarchy. For example, the most basic kind was the pargana (district) qadi. More prestigious positions were those of the qadi al-quddat (judge of judges) who accompanied the mobile imperial camp, and the qadi-yi lashkar (judge of the army).[98] Qadis were usually appointed by the emperor or the sadr-us-sudr (chief of charities).[98][100] The jurisdiction of the qadi was availed by Muslims and non-Muslims alike.[101] The jagirdar (local tax collector) was another kind of official approach, especially for high-stakes cases. Subjects of the Mughal Empire also took their grievances to the courts of superior officials, who held more authority and punitive power than the local qadi. Such officials included the kotwal (local police), the faujdar (an officer controlling multiple districts and troops of soldiers), and the most powerful, the subahdar (provincial governor). In some cases, the emperor dispensed justice directly.[98] Jahangir was known to have installed a \"chain of justice\" in the Agra Fort that any aggrieved subject could shake to get the attention of the emperor and bypass the inefficacy of officials.[102] Self-regulating tribunals operating at the community or village level were common, but sparse documentation of them exists. For example, it is unclear how panchayats (village councils) operated in the Mughal era.[98] The Mughal economy was large and prosperous.[103][104] India was producing 24.5% of the world's manufacturing output up until 1750.[105][104] Mughal India's economy has been described as a form of proto-industrialisation, like that of 18th-century Western Europe before the Industrial Revolution.[106] Modern historians and researchers generally agree that the character of the Mughal Empire's economic policy resembles the laissez-faire system in dealing with trade and billions to achieve the economic ends.[107][108][109][110] The Mughals were responsible for building an extensive road system and creating a uniform currency.[111] The empire had an extensive road network, which was vital to the economic infrastructure, built by a public works department set up by the Mughals which designed, constructed and maintained roads linking towns and cities across the empire, making trade easier to conduct.[103] The main base of the empire's collective wealth was agricultural taxes, instituted by the third Mughal emperor, Akbar.[21][22] These taxes, which amounted to well over half the output of a peasant cultivator,[23] were paid in the well-regulated silver currency,[20] and caused peasants and artisans to enter larger markets.[24] In circa 1595, Modern historians estimated the state's annual revenues of the Mughal Empire were around 99,000,000 rupees.[112] The Mughals adopted and standardised the rupee (rupiya, or silver) and dam (copper) currencies introduced by Sur Emperor Sher Shah Suri during his brief rule.[113] The Mughals minted coins with high purity, never dropping below 96%, and without debasement until the 1720s.[114] Despite India having its stocks of gold and silver, the Mughals produced minimal gold of their own but mostly minted coins from imported bullion, as a result of the empire's strong export-driven economy, with global demand for Indian agricultural and industrial products drawing a steady stream of precious metals into India.[115] The historian Shireen Moosvi estimates that in terms of contributions to the Mughal economy, in the late 16th century, the primary sector contributed 52%, the secondary sector 18% and the tertiary sector 29%; the secondary sector contributed a higher percentage than in early 20th-century British India, where the secondary sector only contributed 11% to the economy.[116] In terms of the urban-rural divide, 18% of Mughal India's labour force were urban and 82% were rural, contributing 52% and 48% to the economy, respectively.[117] According to Moosvi, Mughal India had a per-capita income, in terms of wheat, 1.24% higher in the late 16th century than British India did in the early 20th century.[118] This income, however, would have to be revised downwards if manufactured goods, like clothing, would be considered. Compared to food per capita, expenditure on clothing was much smaller though, so relative income between 1595 and 1596 should be comparable to 1901\u20131910.[119] However, in a system where wealth was hoarded by elites, wages were depressed for manual labour.[120] While slavery also existed, it was limited largely to household servants.[120] Indian agricultural production increased under the Mughal Empire.[103] A variety of crops were grown, including food crops such as wheat, rice, and barley, and non-food cash crops such as cotton, indigo and opium. By the mid-17th century, Indian cultivators began to extensively grow two new crops from the Americas, maize and tobacco.[103] The Mughal administration emphasised the agrarian reform that began under the non-Mughal emperor Sher Shah Suri, which Akbar adopted and furthered with more reforms. The civil administration was organised hierarchically based on merit, with promotions based on performance.[121] The Mughal government funded the building of irrigation systems across the empire, which produced much higher crop yields and increased the net revenue base, leading to increased agricultural production.[103] A major Mughal reform introduced by Akbar was a new land revenue system called zabt. He replaced the tribute system, previously common in India and used by Tokugawa Japan at the time, with a monetary tax system based on a uniform currency.[114] The revenue system was biased in favour of higher value cash crops such as cotton, indigo, sugar cane, tree crops, and opium, providing state incentives to grow cash crops, in addition to rising market demand.[115] Under the zabt system, the Mughals also conducted extensive cadastral surveying to assess the area of land under plough cultivation, with the Mughal state encouraging greater land cultivation by offering tax-free periods to those who brought new land under cultivation.[114] The expansion of agriculture and cultivation continued under later Mughal emperors, including Aurangzeb.[122] Mughal agriculture was in some ways advanced compared to European agriculture at the time, exemplified by the common use of the seed drill among Indian peasants before its adoption in Europe.[123] Geared sugar rolling mills first appeared in Mughal India, using the principle of rollers as well as worm gearing, by the 17th century.[124] South Asia during the Mughal's rule was a very fertile ground for manufacturing technologies coveted by the Europeans before the Industrial Revolution.[125] Up until 1750, India produced about 25% of the world's industrial output.[126] Manufactured goods and cash crops from the Mughal Empire were sold throughout the world.[103] The growth of manufacturing industries in the Indian subcontinent during the Mughal era in the 17th\u201318th centuries has been referred to as a form of proto-industrialisation, similar to 18th-century Western Europe before the Industrial Revolution.[106] In early modern Europe, there was significant demand for products from Mughal India, particularly cotton textiles, as well as goods such as spices, peppers, indigo, silks, and saltpetre (for use in munitions).[103] European fashion, for example, became increasingly dependent on Mughal Indian textiles and silks.[127] The largest manufacturing industry in the Mughal Empire was textile manufacturing, particularly cotton textile manufacturing, which included the production of piece goods, calicos, and muslins. The cotton textile industry was responsible for a large part of the empire's international trade.[103] India had a 25% share of the global textile trade in the early 18th century,[128] and it represented the most important manufactured goods in world trade in the 18th century.[129] The most important centre of cotton production was the Bengal province, particularly around its capital city of Dhaka.[130] The production of cotton was advanced by the diffusion of the spinning wheel across India shortly before the Mughal era, lowering the costs of yarn and helping to increase demand for cotton. The diffusion of the spinning wheel and the incorporation of the worm gear and crank handle into the roller cotton gin led to greatly expanded Indian cotton textile production during the Mughal era.[131] The Bengal Subah province was especially prosperous from the time of its takeover by the Mughals in 1590 until the British East India Company seized control in 1757.[132] Historian C. A. Bayly wrote that it was probably the Mughal Empire's wealthiest province.[133] Domestically, much of India depended on Bengali products such as rice, silks and cotton textiles. Overseas, Europeans depended on Bengali products such as cotton textiles, silks, and opium.[127] The province was a leading producer of grains, salt, fruits, liquors and wines, precious metals and ornaments.[134] After 150 years of rule by Mughal viceroys, Bengal gained de facto independence as a dominion under Murshid Quli Khan, the first Nawab of Bengal in 1717.[135] The Nawabs permitted European companies to set up trading posts across the region, which regarded Bengal as the richest place for trade.[134] Mughal India had a large shipbuilding industry, which was also largely centred in the Bengal province. Economic historian Indrajit Ray estimates the shipbuilding output of Bengal during the sixteenth and seventeenth centuries at 223,250\u00a0tons annually, compared with 23,061\u00a0tons produced in nineteen colonies in North America from 1769 to 1771.[136] He also assesses ship repairing as very advanced in Bengal.[136] India's population growth accelerated under the Mughal Empire, with an unprecedented economic and demographic upsurge which boosted the Indian population by 60%[137] to 253% in 200 years during 1500\u20131700.[138] The Indian population had a faster growth during the Mughal era than at any known point in Indian history before the Mughal era.[104][137] By the time of Aurangzeb's reign, there were a total of 455,698 villages in the Mughal Empire.[139] The following table gives population estimates for the Mughal Empire, compared to the total population of South Asia including the regions of modern India, Pakistan, and Bangladesh, and compared to the world population: There was a notable presence of the Jewish diaspora in the Mughal empire. The Jewish community in the empire engaged in trading jewelry and precious stones.[141] Sarmad Kashani engaged in religious activities in the Mughal court.[142] According to Irfan Habib, cities and towns boomed under the Mughal Empire, which had a relatively high degree of urbanisation for its time, with 15% of its population living in urban centres.[143] This was higher than the percentage of the urban population in contemporary Europe at the time and higher than that of British India in the 19th century;[143] the level of urbanisation in Europe did not reach 15% until the 19th century.[144] Under Akbar's reign in 1600, the Mughal Empire's urban population was up to 17 million people, 15% of the empire's total population. This was larger than the entire urban population in Europe at the time, and even a century later in 1700, the urban population of England, Scotland and Wales did not exceed 13% of its total population,[139] while British India had an urban population that was under 13% of its total population in 1800 and 9% in 1881, a decline from the earlier Mughal era.[145] By 1700, Mughal India had an urban population of 23 million people, larger than British India's urban population of 22.3 million in 1871.[146] Those estimates were criticised by Tim Dyson, who considers them exaggerations. According to Dyson, urbanisation of the Mughal Empire was less than 9%.[147] The historian Nizamuddin Ahmad (1551\u20131621) reported that, under Akbar's reign, there were 120 large cities and 3200 townships.[143] Several cities in India had a population between a quarter-million and half-million people,[143] with larger cities including Agra (in Agra Subah) with up to 800,000 people, Lahore (in Lahore Subah) with up to 700,000 people,[148] Dhaka (in Bengal Subah) with over 1 million people,[149] and Delhi (in Delhi Subah) with over 600,000 people.[150] Cities acted as markets for the sale of goods, and provided homes for a variety of merchants, traders, shopkeepers, artisans, moneylenders, weavers, craftspeople, officials, and religious figures.[103] However, several cities were military and political centres, rather than manufacturing or commerce centres.[151] Generally, classical historiographies depicted the Mughal Empire's origin as a sedentarised agrarian society. However, modern historians such as Andr\u00e9 Wink, Jos J. L. Gommans, Anatoly Khazanov, Thomas J. Barfield, and others, argued the Mughals originated from nomadic culture.[153] Pius Malekandathil argued instead that although it was true that the Mughal had their origin as nomadic civilisation, they became more sendentarised as time passed, as exemplified by their military tradition.[154] The Mughal Empire was definitive in the early-modern and modern periods of South Asian history, with its legacy in India, Pakistan, Bangladesh and Afghanistan seen in cultural contributions such as: The procession of marriage among the royals of the Mughal Empire was recorded with many reports of extravagant gifts. One occasion was during the marriage of a son of emperor Akbar, Salim, with the daughter of a ruler of Bijapur, Raja Bhagwant Das, where the gift presented by Bhagwant Das consisted of many horses, 100 elephants, many male and female slaves of Abyssinian, Caucasian, and native Indian origins, who brought with them various gold and silver utensils as dowry.[162] The Mughals made a major contribution to the Indian subcontinent with the development of their distinctive architectural style. This style was derived from earlier Indo-Islamic architecture as well as from Iranian and Central Asian architecture (particularly Timurid architecture), while incorporating further influences from Hindu architecture.[163][164] Mughal architecture is distinguished, among other things, by bulbous domes, ogive arches, carefully-composed and polished fa\u00e7ades, and the use of hard red sandstone and marble as construction materials.[163][165] Furthermore, William Dalrymple mentioned that during the final days of the Mughal fall of Delhi in 1857, an ice house structure existed in Delhi.[166] Emperor Shah Jahan has recorded establishing an ice-house in Sirmaur, north of Delhi.[167] Many monuments were built during the Mughal era by the Muslim emperors, especially Shah Jahan, including the Taj Mahal\u2014a UNESCO World Heritage Site considered \"the jewel of Muslim art in India and one of the universally admired masterpieces of the world's heritage\",[29] attracting 7\u20138 million unique visitors a year. The palaces, tombs, gardens and forts built by the dynasty stand today in Agra, Aurangabad, Delhi, Dhaka, Fatehpur Sikri, Jaipur, Lahore, Kabul, Sheikhupura, and many other cities of India, Pakistan, Afghanistan, and Bangladesh,[168] such as: The Mughal artistic tradition, mainly expressed in painted miniatures, as well as small luxury objects, was eclectic, borrowing from Iranian, Indian, Chinese and Renaissance European stylistic and thematic elements.[169] Mughal emperors often took in Iranian bookbinders, illustrators, painters and calligraphers from the Safavid court due to the commonalities of their Timurid styles, and due to the Mughal affinity for Iranian art and calligraphy.[170] Miniatures commissioned by the Mughal emperors initially focused on large projects illustrating books with eventful historical scenes and court life, but later included more single images for albums, with portraits and animal paintings displaying a profound appreciation for the serenity and beauty of the natural world.[171] For example, Emperor Jahangir commissioned brilliant artists such as Ustad Mansur to realistically portray unusual flora and fauna throughout the empire. The literary works Akbar and Jahangir ordered to be illustrated ranged from epics like the Razmnama (a Persian translation of the Hindu epic, the Mahabharata) to historical memoirs or biographies of the dynasty such as the Baburnama and Akbarnama, and Tuzk-e-Jahangiri. Richly finished albums (muraqqa) decorated with calligraphy and artistic scenes were mounted onto pages with decorative borders and then bound with covers of stamped and gilded or painted and lacquered leather.[172] Aurangzeb (1658\u20131707) was never an enthusiastic patron of painting, largely for religious reasons, and took a turn away from the pomp and ceremonial of the court around 1668, after which he probably commissioned no more paintings.[173] Though the Mughals were of Turko-Mongol origin, their reign enacted the revival and height of the Persian language in the Indian subcontinent, and by the end of the 16th-century Turki (Chagatai) was understood by relatively few at court.[174] Accompanied by literary patronage was the institutionalisation of Persian as an official and courtly language; this led to Persian reaching nearly the status of a first language for many inhabitants of Mughal India.[175][176] Historian Muzaffar Alam argues that the Mughals used Persian purposefully as the vehicle of an overarching Indo-Persian political culture, to unite their diverse empire.[177] Persian had a profound impact on the languages of South Asia; one such language, today known as Hindustani, developed in the imperial capital of Delhi in the late Mughal era. It began to be used as a literary language in the Mughal court from the reign of Shah Jahan, who described it as the language of his dastans (prose romances) and replaced Persian as the informal language of the Muslim elite.[178][179] According to contemporary poet Mir Taqi Mir, \"Urdu was the language of Hindustan by the authority of the King.\"[180][181] Mughal India was one of the three Islamic gunpowder empires, along with the Ottoman Empire and Safavid Persia.[31][182][183] By the time he was invited by Lodi governor of Lahore, Daulat Khan, to support his rebellion against Lodi Sultan Ibrahim Khan, Babur was familiar with gunpowder firearms and field artillery, and a method for deploying them. Babur had employed Ottoman expert Ustad Ali Quli, who showed Babur the standard Ottoman formation\u2014artillery and firearm-equipped infantry protected by wagons in the centre and the mounted archers on both wings. Babur used this formation at the First Battle of Panipat in 1526, where the Afghan and Rajput forces loyal to the Delhi Sultanate, though superior in numbers but without the gunpowder weapons, were defeated. The decisive victory of the Timurid forces is one reason opponents rarely met Mughal princes in pitched battles throughout the empire's history.[184] In India, guns made of bronze were recovered from Calicut (1504) and Diu (1533).[185] Fathullah Shirazi (c.\u20091582), a Persian polymath and mechanical engineer who worked for Akbar, developed an early multi-gun shot. As opposed to the polybolos and repeating crossbows used earlier in ancient Greece and China, respectively, Shirazi's rapid-firing gun had multiple gun barrels that fired hand cannons loaded with gunpowder. It may be considered a version of a volley gun.[186] By the 17th century, Indians were manufacturing a diverse variety of firearms; large guns, in particular, became visible in Tanjore, Dacca, Bijapur and Murshidabad.[187] In the sixteenth century, Akbar was the first to initiate and use metal cylinder rockets known as bans, particularly against war elephants, during the battle of Sanbal.[188][189] In 1657, the Mughal Army used rockets during the siege of Bidar.[190] Prince Aurangzeb's forces discharged rockets and grenades while scaling the walls. Sidi Marjan was mortally wounded when a rocket struck his large gunpowder depot, and after twenty-seven days of hard fighting, Bidar was captured by the Mughals.[190] In A History of Greek Fire and Gunpowder, James Riddick Partington described Indian rockets and explosive mines:[185] The Indian war rockets ... were formidable weapons before such rockets were used in Europe. They had bam-boo rods, a rocket body lashed to the rod and iron points. They were directed at the target and fired by lighting the fuse, but the trajectory was rather erratic. The use of mines and counter-mines with explosive charges of gunpowder is mentioned for the times of Akbar and Jahangir. A new curriculum for the madrasas that stressed the importance of uloom-i-muqalat (Rational Sciences) and introduced new subjects such as geometry, medicine, philosophy, and mathematics. The new curriculum produced a series of eminent scholars, engineers and architects.[191][192] While there appears to have been little concern for theoretical astronomy, Mughal astronomers made advances in observational astronomy and produced some Zij treatises. Humayun built a personal observatory near Delhi. According to Sulaiman Nadvi, Jahangir and Shah Jahan intended to build observatories too, but were unable to do so. The astronomical instruments and observational techniques used at the Mughal observatories were mainly derived from Islamic astronomy.[193][194] In the 17th century, the Mughal Empire saw a synthesis between Islamic and Hindu astronomy, where Islamic observational instruments were combined with Hindu computational techniques.[193][194] During the decline of the Mughal Empire, the Hindu king Jai Singh II of Amber continued the work of Mughal astronomy. In the early 18th century, he built several large observatories called Yantra Mandirs, to rival Ulugh Beg's Samarkand observatory, and to improve on the earlier Hindu computations in the Siddhantas and Islamic observations in Zij-i-Sultani. The instruments he used were influenced by Islamic astronomy, while the computational techniques were derived from Hindu astronomy.[193][194] The society within the Mughal Empire operated the Karkhanas, which functioned as workshops for craftsmen. These Karkhanas were producing arms, ammunition, and also various items for the court and emperor's need such as clothes, shawls, turbans, jewelry, gold and silverware, perfumes, medicines, carpets, beddings, tents, and for the imperial stable-harnesses for the horses in irons, copper and other metals.[196][197][198] Another aspect of the remarkable invention in Mughal India is the lost-wax cast, hollow, seamless, celestial globe. It was invented in Kashmir by Ali Kashmiri ibn Luqman in 998 AH (1589\u201390 CE). Twenty other such globes were later produced in Lahore and Kashmir during the Mughal Empire. Before they were rediscovered in the 1980s, it was believed by modern metallurgists to be technically impossible to produce hollow metal globes without any seams.[199]\nA 17th-century celestial globe was also made by Diya' ad-din Muhammad in Lahore, 1668 (now in Pakistan).[200] 22 February 1555 \u2013 27 January 1556 (10 years 3 months 25 days) (49 years 9 months 0 days) (21 years 11 months 23 days) (30 years 8 months 25 days) Alamgir\u0639\u0627\u0644\u0645\u06af\u06cc\u0631 (48 years 7 months 0 days) (4 years, 253 days) (0 years, 350 days) (6 years, 48 days) (0 years, 98 days) (0 years, 105 days) (28 years, 212 days) (6 years, 37 days) (5 years, 180 days) (282 days) (27 years, 301 days) (63 days) (18 years, 339 days) (30 years, 321 days) (19 years, 360 days)",
      "ground_truth_chunk_ids": [
        "171_fixed_chunk1"
      ],
      "source_ids": [
        "S171"
      ],
      "category": "factual",
      "id": 50
    },
    {
      "question": "Compare Developmental psychology and Compound of five great dodecahedra in one sentence each: what does each describe or study?",
      "ground_truth": "Developmental psychology: Developmental psychology is the scientific study of how and why humans grow, change, and adapt across the course of their lives. Originally concerned with infants and children, the field has expanded to include adolescence, adult development, aging, and the entire lifespan.[1] Developmental psychologists aim to explain how thinking, feeling, and behaviors change throughout life. This field examines change[2] across three major dimensions, which are physical development, cognitive development, and social emotional development.[3][4] Within these three dimensions are a broad range of topics including motor skills, executive functions, moral understanding, language acquisition, social change, personality, emotional development, self-concept, and identity formation. Developmental psychology explores the influence of both nature and nurture on human development, as well as the processes of change that occur across different contexts over time. Many researchers are interested in the interactions among personal characteristics, the individual's behavior, and environmental factors, including the social context and the built environment. Ongoing debates in regards to developmental psychology include biological essentialism vs. neuroplasticity, and stages of development vs. dynamic systems of development. While research in developmental psychology has certain limitations, ongoing studies aim to understand how life stage transitions and biological factors influence human behavior and development.[5] Developmental psychology involves a range of fields,[2] such as educational psychology, child psychopathology, forensic developmental psychology, child development, cognitive psychology, ecological psychology, and cultural psychology. Influential developmental psychologists from the 20th century include Urie Bronfenbrenner, Erik Erikson, Sigmund Freud, Anna Freud, Jean Piaget, Barbara Rogoff, Esther Thelen, and Lev Vygotsky.[6] Jean-Jacques Rousseau and John B. Watson are typically cited as providing the foundation for modern developmental psychology.[7] In the mid-18th century, Jean Jacques Rousseau described three stages of development: infants (infancy), puer (childhood) and adolescence in Emile: Or, On Education. Rousseau's ideas were adopted and supported by educators at the time. Developmental psychology Compound of five great dodecahedra: This uniform polyhedron compound is a composition of 5 great dodecahedra, in the same arrangement as in the compound of 5 icosahedra. It is one of only five polyhedral compounds (along with the compound of six tetrahedra, the compound of two great dodecahedra, the compound of two small stellated dodecahedra, and the compound of five small stellated dodecahedra) which is vertex-transitive and face-transitive but not edge-transitive. This polyhedron-related article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Developmental psychology: Developmental psychology is the scientific study of how and why humans grow, change, and adapt across the course of their lives. Originally concerned with infants and children, the field has expanded to include adolescence, adult development, aging, and the entire lifespan.[1] Developmental psychologists aim to explain how thinking, feeling, and behaviors change throughout life. This field examines change[2] across three major dimensions, which are physical development, cognitive development, and social emotional development.[3][4] Within these three dimensions are a broad range of topics including motor skills, executive functions, moral understanding, language acquisition, social change, personality, emotional development, self-concept, and identity formation. Developmental psychology explores the influence of both nature and nurture on human development, as well as the processes of change that occur across different contexts over time. Many researchers are interested in the interactions among personal characteristics, the individual's behavior, and environmental factors, including the social context and the built environment. Ongoing debates in regards to developmental psychology include biological essentialism vs. neuroplasticity, and stages of development vs. dynamic systems of development. While research in developmental psychology has certain limitations, ongoing studies aim to understand how life stage transitions and biological factors influence human behavior and development.[5] Developmental psychology involves a range of fields,[2] such as educational psychology, child psychopathology, forensic developmental psychology, child development, cognitive psychology, ecological psychology, and cultural psychology. Influential developmental psychologists from the 20th century include Urie Bronfenbrenner, Erik Erikson, Sigmund Freud, Anna Freud, Jean Piaget, Barbara Rogoff, Esther Thelen, and Lev Vygotsky.[6] Jean-Jacques Rousseau and John B. Watson are typically cited as providing the foundation for modern developmental psychology.[7] In the mid-18th century, Jean Jacques Rousseau described three stages of development: infants (infancy), puer (childhood) and adolescence in Emile: Or, On Education. Rousseau's ideas were adopted and supported by educators at the time. Developmental psychology generally focuses on how and why certain changes (cognitive, social, intellectual, personality) occur over time in the course of a human life. Many theorists have made a profound contribution to this area of psychology. One of them is the psychologist Erik Erikson,[8] who created a model of eight phases of psychosocial development.[8] According to his theory, people go through different phases in their lives, each of which has its own developmental crisis that shapes a person's personality and behavior.[9] In the late 19th century, psychologists familiar with the evolutionary theory of Darwin began seeking an evolutionary description of psychological development;[7] prominent here was the pioneering psychologist G. Stanley Hall,[7] who attempted to correlate ages of childhood with previous ages of humanity. James Mark Baldwin, who wrote essays on topics that included Imitation: A Chapter in the Natural History of Consciousness and Mental Development in the Child and the Race: Methods and Processes, was significantly involved in the theory of developmental psychology.[7] Sigmund Freud, whose concepts were developmental, significantly affected public perceptions.[7] Sigmund Freud developed a theory that suggested that humans behave as they do because they are constantly seeking pleasure. This process of seeking pleasure changes through stages because people evolve. Each period of seeking pleasure that a person experiences is represented by a stage of psychosexual development. These stages symbolize the process of arriving at becoming a maturing adult.[10] The first is the oral stage, which begins at birth and ends around a year and a half of age. During the oral stage, the child finds pleasure in behaviors like sucking or other behaviors with the mouth. The second is the anal stage, from about a year or a year and a half to three years of age. During the anal stage, the child defecates from the anus and is often fascinated with its defecation. This period of development often occurs during the time when the child is being toilet-trained. The child becomes interested in feces and urine. Children begin to see themselves as independent from their parents. They begin to desire assertiveness and autonomy. The third is the phallic stage, which occurs from three to five years of age (most of a person's personality forms by this age). During the phallic stage, the child becomes aware of its sexual organs. Pleasure comes from finding acceptance and love from the opposite sex. The fourth is the latency stage, which occurs from age five until puberty. During the latency stage, the child's sexual interests are repressed. Stage five is the genital stage, which takes place from puberty until adulthood. During the genital stage, puberty begins to occur.[11] Children have now matured, and begin to think about other people instead of just themselves. Pleasure comes from feelings of affection from other people. Freud believed there is tension between the conscious and unconscious because the conscious tries to hold back what the unconscious tries to express. To explain this, he developed three personality structures: id, ego, and superego. The id, the most primitive of the three, functions according to the pleasure principle: seek pleasure and avoid pain.[12] The superego plays the critical and moralizing role, while the ego is the organized, realistic part that mediates between the desires of the id and the superego.[13] Jean Piaget, a Swiss theorist, posited that children learn by actively constructing knowledge through their interactions with their physical and social environments.[14] He suggested that the adult's role in helping the child learn was to provide appropriate materials. In his interview techniques with children that formed an empirical basis for his theories, he used something similar to Socratic questioning to get children to reveal their thinking. He argued that a principal source of development was through the child's inevitable generation of contradictions through their interactions with their physical and social worlds. The child's resolution of these contradictions led to more integrated and advanced forms of interaction, a developmental process that he called \"equilibration.\" Piaget argued that intellectual development takes place through a series of stages generated through the equilibration process. Each stage consists of steps the child must master before moving to the next step. He believed that these stages are not separate from one another, but rather that each stage builds on the previous one in a continuous learning process. He proposed four stages: sensorimotor, pre-operational, concrete operational, and formal operational. Though he did not believe these stages occurred at any given age, many studies have determined when these cognitive abilities should take place.[15] Piaget claimed that logic and morality develop through constructive stages.[16] Expanding on Piaget's work, Lawrence Kohlberg determined that the process of moral development was principally concerned with justice, and that it continued throughout the individual's lifetime.[17] He suggested three levels of moral reasoning: pre-conventional moral reasoning, conventional moral reasoning, and post-conventional moral reasoning. The pre-conventional moral reasoning is typical of children and is characterized by reasoning that is based on rewards and punishments associated with different courses of action. Conventional moral reasoning occurs during late childhood and early adolescence and is characterized by reasoning based on the rules and conventions of society. Lastly, post-conventional moral reasoning is a stage during which the individual sees society's rules and conventions as relative and subjective, rather than as authoritative.[18] Kohlberg used the Heinz Dilemma to apply to his stages of moral development. The Heinz Dilemma involves Heinz's wife dying from cancer and Heinz having the dilemma to save his wife by stealing a drug. Preconventional morality, conventional morality, and post-conventional morality applies to Heinz's situation.[19] German-American psychologist Erik Erikson and his collaborator and wife, Joan Erikson, posits eight stages of individual human development influenced by biological, psychological, and social factors throughout the lifespan.[8] At each stage the person must resolve a challenge, or an existential dilemma. Successful resolution of the dilemma results in the person ingraining a positive virtue, but failure to resolve the fundamental challenge of that stage reinforces negative perceptions of the person or the world around them and the person's personal development is unable to progress.[8] The first stage, \"Trust vs. Mistrust\", takes place in infancy. The positive virtue for the first stage is hope, in the infant learning whom to trust and having hope for a supportive group of people to be there for him/her. The second stage is \"Autonomy vs. Shame and Doubt\" with the positive virtue being will. This takes place in early childhood when the child learns to become more independent by discovering what they are capable of whereas if the child is overly controlled, feelings of inadequacy are reinforced, which can lead to low self-esteem and doubt. The third stage is \"Initiative vs. Guilt\". The virtue of being gained is a sense of purpose. This takes place primarily via play. This is the stage where the child will be curious and have many interactions with other kids. They will ask many questions as their curiosity grows. If too much guilt is present, the child may have a slower and harder time interacting with their world and other children in it. The fourth stage is \"Industry (competence) vs. Inferiority\". The virtue for this stage is competency and is the result of the child's early experiences in school. This stage is when the child will try to win the approval of others and understand the value of their accomplishments. The fifth stage is \"Identity vs. Role Confusion\". The virtue gained is fidelity and it takes place in adolescence. This is when the child ideally starts to identify their place in society, particularly in terms of their gender role. The sixth stage is \"Intimacy vs. Isolation\", which happens in young adults and the virtue gained is love. This is when the person starts to share his/her life with someone else intimately and emotionally. Not doing so can reinforce feelings of isolation. The seventh stage is \"Generativity vs. Stagnation\". This happens in adulthood and the virtue gained is care. A person becomes stable and starts to give back by raising a family and becoming involved in the community. The eighth stage is \"Ego Integrity vs. Despair\". When one grows old, they look back on their life and contemplate their successes and failures. If they resolve this positively, the virtue of wisdom is gained. This is also the stage when one can gain a sense of closure and accept death without regret or fear.[20] Michael Commons enhanced and simplified B\u00e4rbel Inhelder and Piaget's developmental theory and offers a standard method of examining the universal pattern of development. The Model of Hierarchical Complexity (MHC) is not based on the assessment of domain-specific information, It divides the Order of Hierarchical Complexity of tasks to be addressed from the Stage performance on those tasks. A stage is the order hierarchical complexity of the tasks the participant's successfully addresses. He expanded Piaget's original eight stage (counting the half stages) to seventeen stages. The stages are: The order of hierarchical complexity of tasks predicts how difficult the performance is with an R ranging from 0.9 to 0.98. In the MHC, there are three main axioms for an order to meet in order for the higher order task to coordinate the next lower order task. Axioms are rules that are followed to determine how the MHC orders actions to form a hierarchy. These axioms are: a) defined in terms of tasks at the next lower order of hierarchical complexity task action; b) defined as the higher order task action that organizes two or more less complex actions; that is, the more complex action specifies the way in which the less complex actions combine; c) defined as the lower order task actions have to be carried out non-arbitrarily.Commons, Michael L.; Gane-McCalla, Rebecca; Barker, Christopher D.; Li, Ellen Y. (2014). \"The model of hierarchical complexity as a measurement system\". Behavioral Development Bulletin. 19 (3). American Psychological Association: 9\u201368. doi:10.1037/h0100589. Ecological systems theory, originally formulated by Urie Bronfenbrenner, specifies four types of nested environmental systems, with bi-directional influences within and between the systems. The four systems are microsystem, mesosystem, exosystem, and macrosystem. Each system contains roles, norms and rules that can powerfully shape development. The microsystem is the direct environment in our lives such as our home and school. Mesosystem is how relationships connect to the microsystem. Exosystem is a larger social system where the child plays no role. Macrosystem refers to the cultural values, customs and laws of society.[21] The microsystem is the immediate environment surrounding and influencing the individual (example: school or the home setting). The mesosystem is the combination of two microsystems and how they influence each other (example: sibling relationships at home vs. peer relationships at school). The exosystem is the interaction among two or more settings that are indirectly linked (example: a father's job requiring more overtime ends up influencing his daughter's performance in school because he can no longer help with her homework). The macrosystem is broader taking into account social economic status, culture, beliefs, customs and morals (example: a child from a wealthier family sees a peer from a less wealthy family as inferior for that reason). Lastly, the chronosystem refers to the chronological nature of life events and how they interact and change the individual and their circumstances through transition (example: a mother losing her own mother to illness and no longer having that support in her life).[15] Since its publication in 1979, Bronfenbrenner's major statement of this theory, The Ecology of Human Development,[22] has had widespread influence on the way psychologists and others approach the study of human beings and their environments. As a result of this conceptualization of development, these environments\u2014from the family to economic and political structures\u2014have come to be viewed as part of the life course from childhood through to adulthood.[23] Lev Vygotsky was a Russian theorist from the Soviet era, who posited that children learn through hands-on experience and social interactions with members of their culture.[24] Vygotsky believed that a child's development should be examined during problem-solving activities.[25] Unlike Piaget, he claimed that timely and sensitive intervention by adults when a child is on the edge of learning a new task (called the \"zone of proximal development\") could help children learn new tasks. Zone of proximal development is a tool used to explain the learning of children and collaborating problem solving activities with an adult or peer.[25] This adult role is often referred to as the skilled \"master\", whereas the child is considered the learning apprentice through an educational process often termed \"cognitive apprenticeship\" Martin Hill stated that \"The world of reality does not apply to the mind of a child.\" This technique is called \"scaffolding\", because it builds upon knowledge children already have with new knowledge that adults can help the child learn.[26] Vygotsky was strongly focused on the role of culture in determining the child's pattern of development, arguing that development moves from the social level to the individual level.[26] In other words, Vygotsky claimed that psychology should focus on the progress of human consciousness through the relationship of an individual and their environment.[27] He felt that if scholars continued to disregard this connection, then this disregard would inhibit the full comprehension of the human consciousness.[27] Constructivism is a paradigm in psychology that characterizes learning as a process of actively constructing knowledge. Individuals create meaning for themselves or make sense of new information by selecting, organizing, and integrating information with other knowledge, often in the context of social interactions. Constructivism can occur in two ways: individual and social. Individual constructivism is when a person constructs knowledge through cognitive processes of their own experiences rather than by memorizing facts provided by others. Social constructivism is when individuals construct knowledge through an interaction between the knowledge they bring to a situation and social or cultural exchanges within that content.[15] A foundational concept of constructivism is that the purpose of cognition is to organize one's experiential world, instead of the ontological world around them.[28] Jean Piaget, a Swiss developmental psychologist, proposed that learning is an active process because children learn through experience and make mistakes and solve problems. Piaget proposed that learning should be whole by helping students understand that meaning is constructed.[29] Evolutionary developmental psychology is a research paradigm that applies the basic principles of Darwinian evolution, particularly natural selection, to understand the development of human behavior and cognition. It involves the study of both the genetic and environmental mechanisms that underlie the development of social and cognitive competencies, as well as the epigenetic (gene-environment interactions) processes that adapt these competencies to local conditions.[30] EDP considers both the reliably developing, species-typical features of ontogeny (developmental adaptations), as well as individual differences in behavior, from an evolutionary perspective. While evolutionary views tend to regard most individual differences as the result of either random genetic noise (evolutionary byproducts)[31] and/or idiosyncrasies (for example, peer groups, education, neighborhoods, and chance encounters)[32] rather than products of natural selection, EDP asserts that natural selection can favor the emergence of individual differences via \"adaptive developmental plasticity\".[30][33] From this perspective, human development follows alternative life-history strategies in response to environmental variability, rather than following one species-typical pattern of development.[30] EDP is closely linked to the theoretical framework of evolutionary psychology (EP), but is also distinct from EP in several domains, including research emphasis (EDP focuses on adaptations of ontogeny, as opposed to adaptations of adulthood) and consideration of proximate ontogenetic and environmental factors (i.e., how development happens) in addition to more ultimate factors (i.e., why development happens), which are the focus of mainstream evolutionary psychology.[34] Attachment theory, originally developed by John Bowlby, focuses on the importance of open, intimate, emotionally meaningful relationships.[35] Attachment is described as a biological system or powerful survival impulse that evolved to ensure the survival of the infant. A threatened or stressed child will move toward caregivers who create a sense of physical, emotional, and psychological safety for the individual. Attachment feeds on body contact and familiarity. Psychologist Harry Harlow's research with infant rhesus monkeys in the mid-20th century provided pivotal experimental support for attachment theory. His studies found that infant monkeys consistently preferred cloth surrogate mothers that provided comfort over wire ones that offered only food. These results demonstrated that emotional security and physical comfort are more critical to attachment than nourishment alone. Harlow's findings reinforced Bowlby's view that early caregiving relationships are biologically essential for healthy emotional development and social bonding later in life.[36] Later Mary Ainsworth developed the Strange Situation protocol and the concept of the secure base. This tool has been found to help understand attachment, such as the Strange Situation Test and the Adult Attachment Interview. Both of which help determine factors to certain attachment styles. The Strange Situation Test helps find \"disturbances in attachment\" and whether certain attributes are found to contribute to a certain attachment issue.[37] The Adult Attachment Interview is a tool that is similar to the Strange Situation Test but instead focuses attachment issues found in adults.[37] Both tests have helped many researchers gain more information on the risks and how to identify them.[37] Theorists have proposed four types of attachment styles:[38] secure, anxious-avoidant, anxious-resistant,[18] and disorganized.[38] Secure attachment is a healthy attachment between the infant and the caregiver. It is characterized by trust. Anxious-avoidant is an insecure attachment between an infant and a caregiver. This is characterized by the infant's indifference toward the caregiver. Anxious-resistant is an insecure attachment between the infant and the caregiver characterized by distress from the infant when separated and anger when reunited.[18] Disorganized is an attachment style without a consistent pattern of responses upon return of the parent.[38] It is possible to prevent a child's innate propensity to develop bonds. Some infants are kept in isolation or subjected to severe neglect or abuse, or they are raised without the stimulation and care of a regular caregiver. This deprivation may cause short-term consequences such as separation, rage, despair, and a brief lag in cerebral growth. Increased aggression, clinging behavior, alienation, psychosomatic illnesses, and an elevated risk of adult depression are among the long-term consequences.[39][page\u00a0needed][40][page\u00a0needed]\\ According to attachment theory, which is a psychological concept, people's capacity to develop healthy social and emotional ties later in life is greatly impacted by their early relationships with their primary caregivers, especially during infancy. This suggests that humans have an inbuilt need to develop strong bonds with caregivers in order to survive and be healthy. Childhood attachment styles can have an impact on how people behave in adult social situations, including romantic partnerships.[41] A significant concern of developmental psychology is the relationship between innateness and environmental influences on development. This is often referred to as \"nature and nurture\" or nativism versus empiricism. A nativist account of development would argue that the processes in question are innate, that is, they are specified by the organism's genes.[42] What makes a person who they are? Is it their environment or their genetics? This is the debate of nature vs nurture.[43] According to an empiricist viewpoint, those processes are learned through interaction with the environment. Today most developmental psychologists take a more holistic approach, emphasizing the interaction between genetic and environmental influences. One of the ways this relationship has been explored in recent years is through the emerging field of evolutionary developmental psychology. The dispute over innateness has been well represented in the field of language acquisition studies. A major question in this area is whether or not certain properties of human language are specified genetically or can be acquired through learning. The empiricist position on the issue of language acquisition suggests that the language input provides the necessary information required for learning the structure of language and that infants acquire language through a process of statistical learning. From this perspective, language can be acquired via general learning methods that also apply to other aspects of development, such as perceptual learning.[44] The nativist position argues that the input from language is too impoverished for infants and children to acquire the structure of language. Linguist Noam Chomsky asserts that, evidenced by the lack of sufficient information in the language input, there is a universal grammar that applies to all human languages and is pre-specified. This has led to the idea that there is a special cognitive module suited for learning language, often called the language acquisition device. Chomsky's critique of the behaviorist model of language acquisition is regarded by many as a key turning point in the decline in the prominence of the theory of behaviorism generally.[45] But Skinner's conception of \"Verbal Behavior\" has not died, perhaps in part because it has generated successful practical applications.[45] Maybe there could be \"strong interactions of both nature and nurture\".[46] Many researchers now emphasize that development results from a continuous, dynamic interaction between genetic predispositions and environmental influences. Rather than acting independently, nature and nurture are seen as intertwined forces, where genetic factors can shape sensitivity to environmental inputs, and environmental conditions can influence how genes are expressed across development.[47] One of the major discussions in developmental psychology includes whether development is discontinuous or continuous. Continuous development is quantifiable and quantitative, whereas discontinuous development is qualitative. Quantitative estimations of development can be measuring the stature of a child, and measuring their memory or consideration span. \"Particularly dramatic examples of qualitative changes are metamorphoses, such as the emergence of a caterpillar into a butterfly.\"[48] Those psychologists who bolster the continuous view of improvement propose that improvement includes slow and progressing changes all through the life span, with behavior within the prior stages of advancement giving the premise of abilities and capacities required for the other stages. \"To many, the concept of continuous, quantifiable measurement seems to be the essence of science\".[48] However, not all psychologists concur that advancement could be a continuous process. A few see advancement as a discontinuous process. They accept advancement includes unmistakable and partitioned stages with diverse sorts of behavior happening in each organization. This proposes that the development of certain capacities in each arrange, such as particular feelings or ways of considering, has a definite beginning and ending point. Nevertheless, there is no exact moment when a capacity suddenly appears or disappears. Although some sorts of considering, feeling or carrying on could seem to seem abruptly, it is more than likely that this has been developing gradually for some time.[49] Stage theories of development rest on the suspicion that development may be a discontinuous process including particular stages which are characterized by subjective contrasts in behavior. They moreover assume that the structure of the stages is not variable concurring to each person, in any case, the time of each arrangement may shift separately. Stage theories can be differentiated with ceaseless hypotheses, which set that development is an incremental process.[50] This issue involves the degree to which one becomes older renditions of their early experience or whether they develop into something different from who they were at an earlier point in development.[51] It considers the extent to which early experiences (especially infancy) or later experiences are the key determinants of a person's development. Stability is defined as the consistent ordering of individual differences with respect to some attribute.[52] Change is altering someone/something. Most human development lifespan developmentalists recognize that extreme positions are unwise. Therefore, the key to a comprehensive understanding of development at any stage requires the interaction of different factors and not only one.[53] Theory of mind is the ability to attribute mental states to ourselves and others.[54] It is a complex but vital process in which children begin to understand the emotions, motives, and feelings of not only themselves but also others. Theory of mind allows individuals to understand that others have unique beliefs and desires different from their own. This ability enables successful social interactions by recognizing and interpreting the mental states of others. If a child does not fully develop theory of mind within this crucial 5-year period, they can suffer from communication barriers that follow them into adolescence and adulthood.[55] Exposure to more people and the availability of stimuli that encourages social-cognitive growth is a factor that relies heavily on family.[56] Developmental psychology is concerned not only with describing the characteristics of psychological change over time but also seeks to explain the principles and internal workings underlying these changes. Psychologists have attempted to better understand these factors by using models. A model must simply account for the means by which a process takes place. This is sometimes done in reference to changes in the brain that may correspond to changes in behavior over the course of the development. Mathematical modeling is useful in developmental psychology for implementing theory in a precise and easy-to-study manner, allowing generation, explanation, integration, and prediction of diverse phenomena. Several modeling techniques are applied to development: symbolic, connectionist (neural network), or dynamical systems models. Dynamic systems models illustrate how many different features of a complex system may interact to yield emergent behaviors and abilities. Nonlinear dynamics has been applied to human systems specifically to address issues that require attention to temporality such as life transitions, human development, and behavioral or emotional change over time. Nonlinear dynamic systems is currently being explored as a way to explain discrete phenomena of human development such as affect,[57] second language acquisition,[58] and locomotion.[59] One critical aspect of developmental psychology is the study of neural development, which investigates how the brain changes and develops during different stages of life. Neural development focuses on how the brain changes and develops during different stages of life. Studies have shown that the human brain undergoes rapid changes during prenatal and early postnatal periods. These changes include the formation of neurons, the development of neural networks, and the establishment of synaptic connections.[60] The formation of neurons and the establishment of basic neural circuits in the developing brain are crucial for laying the foundation of the brain's structure and function, and disruptions during this period can have long-term effects on cognitive and emotional development.[61] Experiences and environmental factors play a crucial role in shaping neural development. Early sensory experiences, such as exposure to language and visual stimuli, can influence the development of neural pathways related to perception and language processing.[62] Genetic factors play a huge roll in neural development. Genetic factors can influence the timing and pattern of neural development, as well as the susceptibility to certain developmental disorders, such as autism spectrum disorder and attention-deficit/hyperactivity disorder.[63] Research finds that the adolescent brain undergoes significant changes in neural connectivity and plasticity. During this period, there is a pruning process where certain neural connections are strengthened while others are eliminated, resulting in more efficient neural networks and increased cognitive abilities, such as decision-making and impulse control.[64] The study of neural development provides crucial insights into the complex interplay between genetics, environment, and experiences in shaping the developing brain. By understanding the neural processes underlying developmental changes, researchers gain a better understanding of cognitive, emotional, and social development in humans. Cognitive development is primarily concerned with how infants and children acquire, develop, and use internal mental capabilities such as: problem-solving, memory, and language. Major topics in cognitive development are the study of language acquisition and the development of perceptual and motor skills. Piaget was one of the influential early psychologists to study the development of cognitive abilities. His theory suggests that development proceeds through a set of stages from infancy to adulthood and that there is an end point or goal. Other accounts, such as that of Lev Vygotsky, have suggested that development does not progress through stages, but rather that the developmental process that begins at birth and continues until death is too complex for such structure and finality. Rather, from this viewpoint, developmental processes proceed more continuously. Thus, development should be analyzed, instead of treated as a product to obtain. K. Warner Schaie has expanded the study of cognitive development into adulthood. Rather than being stable from adolescence, Schaie sees adults as progressing in the application of their cognitive abilities.[65] Modern cognitive development has integrated the considerations of cognitive psychology and the psychology of individual differences into the interpretation and modeling of development.[66] Specifically, the neo-Piagetian theories of cognitive development showed that the successive levels or stages of cognitive development are associated with increasing processing efficiency and working memory capacity. These increases explain differences between stages, progression to higher stages, and individual differences of children who are the same-age and of the same grade-level. However, other theories have moved away from Piagetian stage theories, and are influenced by accounts of domain-specific information processing, which posit that development is guided by innate evolutionarily-specified and content-specific information processing mechanisms. Developmental psychologists who are interested in social development examine how individuals develop social and emotional competencies. For example, they study how children form friendships, how they understand and deal with emotions, and how identity develops. Research in this area may involve study of the relationship between cognition or cognitive development and social behavior. Emotional regulation or ER refers to an individual's ability to modulate emotional responses across a variety of contexts. In young children, this modulation is in part controlled externally, by parents and other authority figures. As children develop, they take on more and more responsibility for their internal state. Studies have shown that the development of ER is affected by the emotional regulation children observe in parents and caretakers, the emotional climate in the home, and the reaction of parents and caretakers to the child's emotions.[67] Music also has an influence on stimulating and enhancing the senses of a child through self-expression.[68] A child's social and emotional development can be disrupted by motor coordination problems, evidenced by the environmental stress hypothesis. The environmental hypothesis explains how children with coordination problems and developmental coordination disorder are exposed to several psychosocial consequences which act as secondary stressors, leading to an increase in internalizing symptoms such as depression and anxiety.[69] Motor coordination problems affect fine and gross motor movement as well as perceptual-motor skills. Secondary stressors commonly identified include the tendency for children with poor motor skills to be less likely to participate in organized play with other children and more likely to feel socially isolated.[69] Social and emotional development focuses on five keys areas: Self-Awareness, Self Management, Social Awareness, Relationship Skills and Responsible Decision Making.[70] Physical development concerns the physical maturation of an individual's body until it reaches the adult stature. Although physical growth is a highly regular process, all children differ tremendously in the timing of their growth spurts.[71] Studies are being done to analyze how the differences in these timings affect and are related to other variables of developmental psychology such as information processing speed. Traditional measures of physical maturity using x-rays are less in practice nowadays, compared to simple measurements of body parts such as height, weight, head circumference, and arm span.[71] A few other studies and practices with physical developmental psychology are the phonological abilities of mature 5- to 11-year-olds, and the controversial hypotheses of left-handers being maturationally delayed compared to right-handers. A study by Eaton, Chipperfield, Ritchot, and Kostiuk in 1996 found in three different samples that there was no difference between right- and left-handers.[71] Researchers interested in memory development look at the way our memory develops from childhood and onward. According to fuzzy-trace theory, a theory of cognition originally proposed by Valerie F. Reyna and Charles Brainerd, people have two separate memory processes: verbatim and gist. These two traces begin to develop at different times as well as at a different pace. Children as young as four years old have verbatim memory, memory for surface information, which increases up to early adulthood, at which point it begins to decline. On the other hand, our capacity for gist memory, memory for semantic information, increases up to early adulthood, at which point it is consistent through old age. Furthermore, one's reliance on gist memory traces increases as one ages.[72] Neuroscientific research has contributed to understanding the biological mechanisms behind memory development. A study using diffusion MRI in children aged four to twelve found that greater maturity in white matter tracts, specifically the uncinate fasciculus and dorsal cingulum bundle, was associated with stronger episodic memory recall. These findings suggest that the structural development of white matter pathways plays a significant role in memory function during childhood.[73] Developmental psychology employs many of the research methods used in other areas of psychology. However, infants and children cannot be tested in the same ways as adults, so different methods are often used to study their development. Developmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation.[74] These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study.[75] Every developmental investigation can be characterized in terms of whether its underlying strategy involves the experimental, correlational, or case study approach.[76][77] The experimental method involves \"actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed;[77] the experimental design points to cause-and-effect relationships.[78] This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable.[77] The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables.[78] On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability.[78] The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention.[77][78] The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment;[78] however, the limitation is that it does not permit determination of cause-and-effect relationships among variables.[78] The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores.[78] Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.[77] Most developmental studies, regardless of whether they employ the experimental, correlational, or case study method, can also be constructed using research designs.[75] Research designs are logical frameworks used to make key comparisons within research studies such as: In a longitudinal study, a researcher observes many individuals born at or around the same time (a cohort) and carries out new observations as members of the cohort age. This method can be used to draw conclusions about which types of development are universal (or normative) and occur in most members of a cohort. As an example a longitudinal study of early literacy development examined in detail the early literacy experiences of one child in each of 30 families.[79] Researchers may also observe ways that development varies between individuals, and hypothesize about the causes of variation in their data. Longitudinal studies often require large amounts of time and funding, making them unfeasible in some situations. Also, because members of a cohort all experience historical events unique to their generation, apparently normative developmental trends may, in fact, be universal only to their cohort.[80] In a cross-sectional study, a researcher observes differences between individuals of different ages at the same time. This generally requires fewer resources than the longitudinal method, and because the individuals come from different cohorts, shared historical events are not so much of a confounding factor. By the same token, however, cross-sectional research may not be the most effective way to study differences between participants, as these differences may result not from their different ages but from their exposure to different historical events.[81] A third study design, the sequential design, combines both methodologies. Here, a researcher observes members of different birth cohorts at the same time, and then tracks all participants over time, charting changes in the groups. While much more resource-intensive, the format aids in a clearer distinction between what changes can be attributed to an individual or historical environment from those that are truly universal.[82] Because every method has some weaknesses, developmental psychologists rarely rely on one study or even one method to reach conclusions by finding consistent evidence from as many converging sources as possible.[77] Prenatal development is of interest to psychologists investigating the context of early psychological development. The whole prenatal development involves three main stages: germinal stage, embryonic stage and fetal stage. Germinal stage begins at conception until 2 weeks; embryonic stage means the development from 2 weeks to 8 weeks; fetal stage represents 9 weeks until birth of the baby.[83] The senses develop in the womb itself: a fetus can both see and hear by the second trimester (13 to 24 weeks of age). The sense of touch develops in the embryonic stage (5 to 8 weeks).[84] Most of the brain's billions of neurons also are developed by the second trimester.[85] Babies are hence born with some odor, taste and sound preferences, largely related to the mother's environment.[86] Some primitive reflexes too arise before birth and are still present in newborns. One hypothesis is that these reflexes are vestigial and have limited use in early human life. Piaget's theory of cognitive development suggested that some early reflexes are building blocks for infant sensorimotor development. For example, the tonic neck reflex may help development by bringing objects into the infant's field of view.[87] Other reflexes, such as the walking reflex, appear to be replaced by more sophisticated voluntary control later in infancy. This may be because the infant gains too much weight after birth to be strong enough to use the reflex, or because the reflex and subsequent development are functionally different.[88] It has also been suggested that some reflexes (for example the moro and walking reflexes) are predominantly adaptations to life in the womb with little connection to early infant development.[87] Primitive reflexes reappear in adults under certain conditions, such as neurological conditions like dementia or traumatic lesions. Ultrasounds have shown that infants are capable of a range of movements in the womb, many of which appear to be more than simple reflexes.[88] By the time they are born, infants can recognize and have a preference for their mother's voice suggesting some prenatal development of auditory perception.[88] Prenatal development and birth complications may also be connected to neurodevelopmental disorders, for example in schizophrenia. With the advent of cognitive neuroscience, embryology and the neuroscience of prenatal development is of increasing interest to developmental psychology research. Several environmental agents\u2014teratogens\u2014can cause damage during the prenatal period. These include prescription and nonprescription drugs, illegal drugs, tobacco, alcohol, environmental pollutants, infectious disease agents such as the rubella virus and the toxoplasmosis parasite, maternal malnutrition, maternal emotional stress, and Rh factor blood incompatibility between mother and child.[89] There are many statistics which prove the effects of the aforementioned substances. A leading example of this would be that at least 100,000 \"cocaine babies\" were born in the United States annually in the late 1980s. \"Cocaine babies\" are proven to have quite severe and lasting difficulties which persist throughout infancy and right throughout childhood. The drug also encourages behavioural problems in the affected children and defects of various vital organs.[90] From birth until the first year, children are referred to as infants. As they grow, children respond to their environment in unique ways.[91] Developmental psychologists vary widely in their assessment of infant psychology, and the influence the outside world has upon it. The majority of a newborn infant's time is spent sleeping.[92] At first, their sleep cycles are evenly spread throughout the day and night, but after a couple of months, infants generally become diurnal.[93] In human or rodent infants, there is always the observation of a diurnal cortisol rhythm, which is sometimes entrained with a maternal substance.[94] Nevertheless, the circadian rhythm starts to take shape, and a 24-hour rhythm is observed in just some few months after birth.[93][94] Infants can be seen to have six states, grouped into pairs: Infant perception is what a newborn can see, hear, smell, taste, and touch. These five features are considered as the \"five senses\".[97] Because of these different senses, infants respond to stimuli differently.[88] Babies are born with the ability to discriminate virtually all sounds of all human languages.[105] Infants of around six months can differentiate between phonemes in their own language, but not between similar phonemes in another language. Notably, infants are able to differentiate between various durations and sound levels and can easily differentiate all the languages they have encountered, hence easy for infants to understand a certain language compared to an adult.[106] At this stage infants also start to babble, whereby they start making vowel consonant sound as they try to understand the true meaning of language and copy whatever they are hearing in their surrounding producing their own phonemes. In various cultures, a distinct form of speech called \"babytalk\" is used when communicating with newborns and young children. This register consists of simplified terms for common topics such as family members, food, hygiene, and familiar animals. It also exhibits specific phonological patterns, such as substituting alveolar sounds with initial velar sounds, especially in languages like English. Furthermore, babytalk often involves morphological simplifications, such as regularizing verb conjugations (for instance, saying \"corned\" instead of \"cornered\" or \"goed\" instead of \"went\"). This language is typically taught to children and is perceived as their natural way of communication. Interestingly, in mythology and popular culture, certain characters, such as the \"Hausa trickster\" or the Warner Bros cartoon character \"Tweety Pie\", are portrayed as speaking in a babytalk-like manner.[107] Piaget suggested that an infant's perception and understanding of the world depended on their motor development, which was required for the infant to link visual, tactile and motor representations of objects.[108] The concept of object permanence refers to the knowledge that an object exists even when it is not directly perceived or visible; in other words, something is still there even if it is not visible. This is a crucial developmental milestone for infants, who learn that something is not necessarily lost forever just because it is hidden. When a child displays object permanence, they will look for a toy that is hidden, showing that they are aware that the item is still there even when it is covered by a blanket. Most babies start to exhibit symptoms of object permanence around the age of eight months. According to this theory, infants develop object permanence through touching and handling objects.[88] Piaget's sensorimotor stage comprised six sub-stages (see sensorimotor stages for more detail). In the early stages, development arises out of movements caused by primitive reflexes.[109] Discovery of new behaviors results from classical and operant conditioning, and the formation of habits.[109] From eight months the infant is able to uncover a hidden object but will persevere when the object is moved. Piaget concluded that infants lacked object permanence before 18 months when infants' before this age failed to look for an object where it had last been seen. Instead, infants continued to look for an object where it was first seen, committing the \"A-not-B error\". Some researchers have suggested that before the age of 8\u20139 months, infants' inability to understand object permanence extends to people, which explains why infants at this age do not cry when their mothers are gone (\"Out of sight, out of mind\"). In the 1980s and 1990s, researchers developed new methods of assessing infants' understanding of the world with far more precision and subtlety than Piaget was able to do in his time. Since then, many studies based on these methods suggest that young infants understand far more about the world than first thought. Based on recent findings, some researchers (such as Elizabeth Spelke and Renee Baillargeon) have proposed that an understanding of object permanence is not learned at all, but rather comprises part of the innate cognitive capacities of our species. According to Jean Piaget's developmental psychology, object permanence, or the awareness that objects exist even when they are no longer visible, was thought to emerge gradually between the ages of 8 and 12 months. However, experts such as Elizabeth Spelke and Renee Baillargeon have questioned this notion. They studied infants' comprehension of object permanence at a young age using novel experimental approaches such as violation-of-expectation paradigms. These findings imply that children as young as 3 to 4 months old may have an innate awareness of object permanence. Baillargeon's \"drawbridge\" experiment, for example, showed that infants were surprised when they saw occurrences that contradicted object permanence expectations. This proposition has important consequences for our understanding of infant cognition, implying that infants may be born with core cognitive abilities rather than developing them via experience and learning.[110] Other research has suggested that young infants in their first six months of life may possess an understanding of numerous aspects of the world around them, including: There are critical periods in infancy and childhood during which development of certain perceptual, sensorimotor, social and language systems depends crucially on environmental stimulation.[114] Feral children such as Genie, deprived of adequate stimulation, fail to acquire important skills and are unable to learn in later childhood. In this case, Genie is used to represent the case of a feral child because she was socially neglected and abused while she was just a young girl. She underwent abnormal child psychology which involved problems with her linguistics. This happened because she was neglected while she was very young with no one to care about her and had less human contact. The concept of critical periods is also well-established in neurophysiology, from the work of Hubel and Wiesel among others. Neurophysiology in infants generally provides correlating details that exists between neurophysiological details and clinical features and also focuses on vital information on rare and common neurological disorders that affect infants. Studies have been done to look at the differences in children who have developmental delays versus typical development. Normally when being compared to one another, mental age (MA) is not taken into consideration. There still may be differences in developmentally delayed (DD) children vs. typical development (TD) behavioral, emotional and other mental disorders. When compared to MA children there is a bigger difference between normal developmental behaviors overall. DDs can cause lower MA, so comparing DDs with TDs may not be as accurate. Pairing DDs specifically with TD children at similar MA can be more accurate. There are levels of behavioral differences that are considered as normal at certain ages. When evaluating DDs and MA in children, consider whether those with DDs have a larger amount of behavior that is not typical for their MA group. Developmental delays tend to contribute to other disorders or difficulties than their TD counterparts.[115] Infants shift between ages of one and two to a developmental stage known as toddlerhood. In this stage, an infant's transition into toddlerhood is highlighted through self-awareness, developing maturity in language use, and presence of memory and imagination. During toddlerhood, babies begin learning how to walk, talk, and make decisions for themselves. An important characteristic of this age period is the development of language, where children are learning how to communicate and express their emotions and desires through the use of vocal sounds, babbling, and eventually words.[116] Self-control also begins to develop. At this age, children take initiative to explore, experiment and learn from making mistakes. Caretakers who encourage toddlers to try new things and test their limits, help the child become autonomous, self-reliant, and confident.[117] If the caretaker is overprotective or disapproving of independent actions, the toddler may begin to doubt their abilities and feel ashamed of the desire for independence. The child's autonomic development is inhibited, leaving them less prepared to deal with the world in the future. Toddlers also begin to identify themselves in gender roles, acting according to their perception of what a man or woman should do.[118] Socially, the period of toddler-hood is commonly called the \"terrible twos\".[119] Toddlers often use their new-found language abilities to voice their desires, but are often misunderstood by parents due to their language skills just beginning to develop. A person at this stage testing their independence is another reason behind the stage's infamous label. Tantrums in a fit of frustration are also common. Erik Erikson divides childhood into four stages, each with its distinct social crisis:[120] As stated, the psychosocial crisis for Erikson is Trust versus Mistrust. Needs are the foundation for gaining or losing trust in the infant. If the needs are met, trust in the guardian and the world forms. If the needs are not met, or the infant is neglected, mistrust forms alongside feelings of anxiety and fear.[122] Autonomy versus shame follows trust in infancy. The child begins to explore their world in this stage and discovers preferences in what they like. If autonomy is allowed, the child grows in independence and their abilities. If freedom of exploration is hindered, it leads to feelings of shame and low self-esteem.[122] In the earliest years, children are \"completely dependent on the care of others\". Therefore, they develop a \"social relationship\" with their care givers and, later, with family members. During their preschool years (3\u20135), they \"enlarge their social horizons\" to include people outside the family.[123] Preoperational and then operational thinking develops, which means actions are reversible, and egocentric thought diminishes.[124] The motor skills of preschoolers increase so they can do more things for themselves. They become more independent. No longer completely dependent on the care of others, the world of this age group expands. More people have a role in shaping their individual personalities. Preschoolers explore and question their world.[125] For Jean Piaget, the child is \"a little scientist exploring and reflecting on these explorations to increase competence\" and this is done in \"a very independent way\".[126] Play is a major activity for ages 3\u20135. For Piaget, through play \"a child reaches higher levels of cognitive development.\"[127] In their expanded world, children in the 3\u20135 age group attempt to find their own way. If this is done in a socially acceptable way, the child develops the initiative. If not, the child develops guilt.[128] Children who develop \"guilt\" rather than \"initiative\" have failed Erikson's psychosocial crisis for the 3\u20135 age group. For Erik Erikson, the psychosocial crisis during middle childhood is Industry vs. Inferiority which, if successfully met, instills a sense of Competency in the child.[120] In all cultures, middle childhood is a time for developing \"skills that will be needed in their society.\"[129] School offers an arena in which children can gain a view of themselves as \"industrious (and worthy)\". They are \"graded for their school work and often for their industry\". They can also develop industry outside of school in sports, games, and doing volunteer work.[130] Children who achieve \"success in school or games might develop a feeling of competence.\" The \"peril during this period is that feelings of inadequacy and inferiority will develop.[129] Parents and teachers can \"undermine\" a child's development by failing to recognize accomplishments or being overly critical of a child's efforts.[130]\nChildren who are \"encouraged and praised\" develop a belief in their competence. Lack of encouragement or ability to excel lead to \"feelings of inadequacy and inferiority\".[131] The Centers for Disease Control (CDC) divides Middle Childhood into two stages, 6\u20138 years and 9\u201311 years, and gives \"developmental milestones for each stage\".[132][133] Entering elementary school, children in this age group begin to thinks about the future and their \"place in the world\". Working with other students and wanting their friendship and acceptance become more important. This leads to \"more independence from parents and family\". As students, they develop the mental and verbal skills \"to describe experiences and talk about thoughts and feelings\". They become less self-centered and show \"more concern for others\".[132] For children ages 9\u201311 \"friendships and peer relationships\" increase in strength, complexity, and importance. This results in greater \"peer pressure\". They grow even less dependent on their families and they are challenged academically. To meet this challenge, they increase their attention span and learn to see other points of view.[133] Adolescence is the period of life between the onset of puberty and the full commitment to an adult social role, such as worker, parent, and/or citizen. It is the period known for the formation of personal and social identity (see Erik Erikson) and the discovery of moral purpose (see William Damon). Intelligence is demonstrated through the logical use of symbols related to abstract concepts and formal reasoning. A return to egocentric thought often occurs early in the period. Only 35% develop the capacity to reason formally during adolescence or adulthood. (Huitt, W. and Hummel, J. January 1998)[134] Erik Erikson labels this stage identity versus role confusion. Erikson emphasizes the importance of developing a sense of identity in adolescence because it affects the individual throughout their life. Identity is a lifelong process and is related with curiosity and active engagement. Role confusion is often considered the current state of identity of the individual. Identity exploration is the process of changing from role confusion to resolution.[135] During Erik Erikson's identity versus role uncertainty stage, which occurs in adolescence, people struggle to form a cohesive sense of self while exploring many social roles and prospective life routes. This time is characterized by deep introspection, self-examination, and the pursuit of self-understanding. Adolescents are confronted with questions regarding their identity, beliefs, and future goals. The major problem is building a strong sense of identity in the face of society standards, peer pressure, and personal preferences. Adolescents participate in identity exploration, commitment, and synthesis, actively seeking out new experiences, embracing ideals and aspirations, and merging their changing sense of self into a coherent identity. Successfully navigating this stage builds the groundwork for good psychological development in adulthood, allowing people to pursue meaningful relationships, make positive contributions to society, and handle life's adversities with perseverance and purpose.[9] It is divided into three parts, namely: The adolescent unconsciously explores questions such as \"Who am I? Who do I want to be?\" Like toddlers, adolescents must explore, test limits, become autonomous, and commit to an identity, or sense of self. Different roles, behaviors and ideologies must be tried out to select an identity. Role confusion and inability to choose vocation can result from a failure to achieve a sense of identity through, for example, friends.[136] Early adulthood generally refers to the period between ages 18 to 39,[137] and according to theorists such as Erik Erikson, is a stage where development is mainly focused on maintaining relationships.[138] Erikson shows the importance of relationships by labeling this stage intimacy vs isolation. Intimacy suggests a process of becoming part of something larger than oneself by sacrificing in romantic relationships and working for both life and career goals.[139] Other examples include creating bonds of intimacy, sustaining friendships, and starting a family. Some theorists state that development of intimacy skills rely on the resolution of previous developmental stages. A sense of identity gained in the previous stages is also necessary for intimacy to develop. If this skill is not learned the alternative is alienation, isolation, a fear of commitment, and the inability to depend on others. Isolation, on the other hand, suggests something different than most might expect. Erikson defined it as a delay of commitment in order to maintain freedom. Yet, this decision does not come without consequences. Erikson explained that choosing isolation may affect one's chances of getting married, progressing in a career, and overall development.[139] A related framework for studying this part of the lifespan is that of emerging adulthood. Scholars of emerging adulthood, such as Jeffrey Arnett, are not necessarily interested in relationship development. Instead, this concept suggests that people transition after their teenage years into a period, not characterized as relationship building and an overall sense of constancy with life, but with years of living with parents, phases of self-discovery, and experimentation.[140] Middle adulthood generally refers to the period between ages 40 to 64. During this period, middle-aged adults experience a conflict between generativity and stagnation. Generativity is the sense of contributing to society, the next generation, or their immediate community. On the other hand, stagnation results in a lack of purpose.[141] The adult's identity continues to develop in middle-adulthood. Middle-aged adults often adopt opposite gender characteristics. The adult realizes they are half-way through their life and often reevaluate vocational and social roles. Life circumstances can also cause a reexamination of identity.[142] Physically, the middle-aged experience a decline in muscular strength, reaction time, sensory keenness, and cardiac output. Also, women experience menopause at an average age of 48.8 and a sharp drop in the hormone estrogen.[143] Men experience an equivalent endocrine system event to menopause. Andropause in males is a hormone fluctuation with physical and psychological effects that can be similar to those seen in menopausal females. As men age lowered testosterone levels can contribute to mood swings and a decline in sperm count. Sexual responsiveness can also be affected, including delays in erection and longer periods of penile stimulation required to achieve ejaculation. The important influence of biological and social changes experienced by women and men in middle adulthood is reflected in the fact that depression is highest at age 48.5 around the world.[144] The World Health Organization finds \"no general agreement on the age at which a person becomes old.\" Most \"developed countries\" set the age as 65 or 70. However, in developing countries inability to make \"active contribution\" to society, not chronological age, marks the beginning of old age.[145][146] According to Erikson's stages of psychosocial development, old age is the stage in which individuals assess the quality of their lives.[147] Erikson labels this stage as integrity versus despair. For integrated persons, there is a sense of fulfillment in life. They have become self-aware and optimistic due to life's commitments and connection to others. While reflecting on life, people in this stage develop feelings of contentment with their experiences. If a person falls into despair, they are often disappointed about failures or missed chances in life. They may feel that the time left in life is an insufficient amount to turn things around.[148] Physically, older people experience a decline in muscular strength, reaction time, stamina, hearing, distance perception, and the sense of smell.[149] They also are more susceptible to diseases such as cancer and pneumonia due to a weakened immune system.[150] Programs aimed at balance, muscle strength, and mobility have been shown to reduce disability among mildly (but not more severely) disabled elderly.[151] Sexual expression depends in large part upon the emotional and physical health of the individual. Many older adults continue to be sexually active and satisfied with their sexual activity.[152] Mental disintegration may also occur, leading to dementia or ailments such as Alzheimer's disease. The average age of onset for dementia in males is 78.8 and 81.9 for women.[153] It is generally believed that crystallized intelligence increases up to old age, while fluid intelligence decreases with age.[154] Whether or not normal intelligence increases or decreases with age depends on the measure and study. Longitudinal studies show that perceptual speed, inductive reasoning, and spatial orientation decline.[155] An article on adult cognitive development reports that cross-sectional studies show that \"some abilities remained stable into early old age\".[155] Parenting variables alone have typically accounted for 20 to 50 percent of the variance in child outcomes.[156] All parents have their own parenting styles. Parenting styles, according to Kimberly Kopko, are \"based upon two aspects of parenting behavior; control and warmth. Parental control refers to the degree to which parents manage their children's behavior. Parental warmth refers to the degree to which parents are accepting and responsive to their children's behavior.\"[157] The following parenting styles have been described in the child development literature: Parenting research has traditionally focused on mothers, but recent studies highlight the important role of fathers in child development. Children as young as 15 months benefit significantly from substantial engagement with their father.[162][163] In particular, a study in the U.S. and New Zealand found the presence of the natural father was the most significant factor in reducing rates of early sexual activity and rates of teenage pregnancy in girls.[164] However, neither a mother nor a father is actually essential in successful parenting, and both single parents as well as homosexual couples can support positive child outcomes.[165] Children need at least one consistently responsible adult with whom they can form a positive emotional bond. Having multiple such figures further increases the likelihood of positive outcomes.[165] Recent research also suggests that the way parents interact with infants can influence early brain development. Parents who guide their baby's attention during play by shifting their gaze between a toy and the child tend to have infants with more complex brain activity. This attention-guiding behavior helps infants process social cues more effectively.[166] Another parental factor often debated in terms of its effects on child development is divorce. Divorce in itself is not a determining factor of negative child outcomes. In fact, the majority of children from divorcing families fall into the normal range on measures of psychological and cognitive functioning.[167] A number of mediating factors play a role in determining the effects divorce has on a child, for example, divorcing families with young children often face harsher consequences in terms of demographic, social, and economic changes than do families with older children.[167] Positive coparenting after divorce is part of a pattern associated with positive child coping, while hostile parenting behaviors lead to a destructive pattern leaving children at risk.[167] Additionally, direct parental relationship with the child also affects the development of a child after a divorce. Overall, protective factors facilitating positive child development after a divorce are maternal warmth, positive father-child relationship, and cooperation between parents.[167] A way to improve developmental psychology is a representation of cross-cultural studies. The psychology field in general assumes that \"basic\" human developments are represented in any population, specifically the Western-Educated-Industrialized-Rich and Democratic (W.E.I.R.D.) subjects that are relied on for a majority of their studies. Previous research generalizes the findings done with W.E.I.R.D. samples because many in the Psychological field assume certain aspects of development are exempted from or are not affected by life experiences. However, many of the assumptions have been proven incorrect or are not supported by empirical research. For example, according to Kohlberg, moral reasoning is dependent on cognitive abilities. While both analytical and holistic cognitive systems do have the potential to develop in any adult, the West is still on the extreme end of analytical thinking, and the non-West tend to use holistic processes. Furthermore, moral reasoning in the West only considers aspects that support autonomy and the individual, whereas non-Western adults emphasize moral behaviors supporting the community and maintaining an image of holiness or divinity. Not all aspects of human development are universal and we can learn a lot from observing different regions and subjects.[168] An example of a non-Western model for development stages is the Indian model, focusing a large amount of its psychological research on morality and interpersonal progress. The developmental stages in Indian models are founded by Hinduism, which primarily teaches stages of life in the process of someone discovering their fate or Dharma.[169] This cross-cultural model can add another perspective to psychological development in which the West behavioral sciences have not emphasized kinship, ethnicity, or religion.[168] Indian psychologists study the relevance of attentive families during the early stages of life. The early life stages conceptualize a different parenting style from the West because it does not try to rush children out of dependency. The family is meant to help the child grow into the next developmental stage at a particular age. This way, when children finally integrate into society, they are interconnected with those around them and reach renunciation when they are older. Children are raised in joint families so that in early childhood (ages 6 months to 2 years) the other family members help gradually wean the child from its mother. During ages 2 to 5, the parents do not rush toilet training. Instead of training the child to perform this behavior, the child learns to do it as they mature at their own pace. This model of early human development encourages dependency, unlike Western models that value autonomy and independence. By being attentive and not forcing the child to become independent, they are confident and have a sense of belonging by late childhood and adolescence. This stage in life (5\u201315 years) is also when children start education and increase their knowledge of Dharma.[170] It is within early and middle adulthood that we see moral development progress. Early, middle, and late adulthood are all concerned with caring for others and fulfilling Dharma. The main distinction between early adulthood to middle or late adulthood is how far their influence reaches. Early adulthood emphasizes the importance of fulfilling the immediate family needs, until later adulthood when they broaden their responsibilities to the general public. The old-age life stage development reaches renunciation or a complete understanding of Dharma.[169] The current mainstream views in the psychological field are against the Indian model for human development. The criticism against such models is that the parenting style is overly protective and encourages too much dependency. It focuses on interpersonal instead of individual goals. Also, there are some overlaps and similarities between Erikson's stages of human development and the Indian model but both of them still have major differences. The West prefers Erickson's ideas over the Indian model because they are supported by scientific studies. The life cycles based on Hinduism are not as favored, because it is not supported with research and it focuses on the ideal human development.[169] [1][2] Compound of five great dodecahedra: This uniform polyhedron compound is a composition of 5 great dodecahedra, in the same arrangement as in the compound of 5 icosahedra. It is one of only five polyhedral compounds (along with the compound of six tetrahedra, the compound of two great dodecahedra, the compound of two small stellated dodecahedra, and the compound of five small stellated dodecahedra) which is vertex-transitive and face-transitive but not edge-transitive. This polyhedron-related article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "165_fixed_chunk1",
        "184_random_chunk1"
      ],
      "source_ids": [
        "S165",
        "S384"
      ],
      "category": "comparative",
      "id": 51
    },
    {
      "question": "Compare Database and Information security in one sentence each: what does each describe or study?",
      "ground_truth": "Database: In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data became widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file. Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use Information security: Information security (infosec) is the practice of protecting information by mitigating information risks. It is part of information risk management.[1] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[2][3] Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (known as the CIA triad, unrelated to the US government organization)[4] while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[5] This is largely achieved through a structured risk management process.[6] To standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on passwords, antivirus software, firewalls, encryption software, legal liability, security awareness and training, and so forth.[7] This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred, and destroyed.[8] While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized,[9][10] with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses.[11] They are responsible for keeping all of the technology within the company secure from malicious attacks that often attempt to acquire critical private information or gain control of the internal systems.[12][13] There are many specialist roles in Information Security including securing networks and",
      "expected_answer": "Database: In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data became widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file. Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages. Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. Because of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it. Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1] Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups: Both a database and its DBMS conform to the principles of a particular database model.[5] \"Database system\" refers collectively to the database model, database management system, and database.[6] Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed] Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.[7] Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security. The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational. The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another. The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed] Object databases were developed in the 1980s to overcome the inconvenience of object\u2013relational impedance mismatch, which led to the coining of the term \"post-relational\" and also the development of hybrid object\u2013relational databases. The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key\u2013value stores and document-oriented databases. A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs. The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense.[10] As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market. The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods: Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications. IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11] Edgar F. Codd worked at IBM in San Jose, California, in an office primarily involved in the development of hard disk systems.[12] He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[13] The paper described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of \"tables\", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each \"fact\" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated. Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based. The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit. In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys. For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided. As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic. Codd's paper inspired teams at various universities to research the subject, including one at University of California, Berkeley[12] led by Eugene Wong and Michael Stonebraker, who started INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard. IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs. In 1970, the University of Michigan began development of the MICRO Information Management System[14] based on D.L. Childs' Set-Theoretic Data model.[15][16][17] The university in 1974 hosted a debate between Codd and Bachman which Bruce Lindsay of IBM later described as \"throwing lightning bolts at each other!\".[12] MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System.[18] The system remained in production until 1998. In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine. Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata). IBM formed a team led by Codd that started working on a prototype system, System R despite opposition from others at the company.[12] The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language \u2013 SQL[citation needed] \u2013 had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2). Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.[19] Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions). In Sweden, Codd's paper was also read and Mimer SQL was developed in the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise. Another data model, the entity\u2013relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity\u2013relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.[citation needed] Besides IBM and various software companies such as Sybase and Informix Corporation, most large computer hardware vendors by the 1980s had their own database systems such as DEC's VAX Rdb/VMS.[20] The decade ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\"[21] dBASE was one of the top selling software titles in the 1980s and early 1990s. By the start of the decade databases had become a billion-dollar industry in about ten years.[20] The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields.[22] The term \"object\u2013relational impedance mismatch\" described the inconvenience of translating between programmed objects and database tables. Object databases and object\u2013relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object\u2013relational mappings (ORMs) attempt to solve the same problem. Database sales grew rapidly during the dotcom bubble and, after its end, the rise of ecommerce. The popularity of open source databases such as MySQL has grown since 2000, to the extent that Ken Jacobs of Oracle said in 2005 that perhaps \"these guys are doing to us what we did to IBM\".[20] XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records. NoSQL databases are often very fast,[23][24] do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally. In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency. NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system. Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software). Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database. One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases. Connolly and Begg define database management system (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database.\"[28] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access. The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object\u2013relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems. The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:[29] It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[30] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine. Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount. The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime.[a] Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client\u2013server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.[32] A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email. External interaction with the database will be via an application program that interfaces with the DBMS.[33] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information. A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET. Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages: Database languages are specific to a particular data model. Notable examples include: A database language may also incorporate features like: Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, \"data about the data\", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[37] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database). Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database. Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases. Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy. Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated. With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[38] Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program). Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces. This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases. Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption). Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.[39] Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability. A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs. After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.). When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation. After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc. Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state. Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[40] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc. Other DBMS features might include: Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as \"DevOps for database\".[41] The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity\u2013relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes. Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data. Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design). The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency. The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS. Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself. A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format. Common logical data models for databases include: An object\u2013relational database combines the two related structures. Physical data models include: Other models include: Specialized models are optimized for particular types of data: A database management system provides three views of the database data: While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database. The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[43] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance. The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types. Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more. The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE). Information security: Information security (infosec) is the practice of protecting information by mitigating information risks. It is part of information risk management.[1] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[2][3] Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (known as the CIA triad, unrelated to the US government organization)[4] while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[5] This is largely achieved through a structured risk management process.[6] To standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on passwords, antivirus software, firewalls, encryption software, legal liability, security awareness and training, and so forth.[7] This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred, and destroyed.[8] While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized,[9][10] with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses.[11] They are responsible for keeping all of the technology within the company secure from malicious attacks that often attempt to acquire critical private information or gain control of the internal systems.[12][13] There are many specialist roles in Information Security including securing networks and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning, electronic record discovery, and digital forensics.[14] Information security standards are techniques generally outlined in published materials that attempt to protect the information of a user or organization.[15] This environment includes users themselves, networks, devices, all software, processes, information in storage or transit, applications, services, and systems that can be connected directly or indirectly to networks. The principal objective is to reduce the risks, including preventing or mitigating attacks. These published materials consist of tools, policies, security concepts, security safeguards, guidelines, risk management approaches, actions, training, best practices, assurance and technologies. Various definitions of information security are suggested below, summarized from different sources: Information security threats come in many different forms.[26] Some of the most common threats today are software attacks, theft of intellectual property, theft of identity, theft of equipment or information, sabotage, and information extortion.[27][28] Viruses,[29] worms, phishing attacks, and Trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses.[30] Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information through social engineering.[31][32] Sabotage usually consists of the destruction of an organization's website in an attempt to cause loss of confidence on the part of its customers.[33] Information extortion consists of theft of a company's property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner, as with ransomware.[34] One of the most functional precautions against these attacks is to conduct periodical user awareness.[35] Governments, military, corporations, financial institutions, hospitals, non-profit organizations, and private businesses amass a great deal of confidential information about their employees, customers, products, research, and financial status.[36] Should confidential information about a business's customers or finances or new product line fall into the hands of a competitor or hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation.[37] From a business perspective, information security must be balanced against cost; the Gordon-Loeb Model provides a mathematical economic approach for addressing this concern.[38] For the individual, information security has a significant effect on privacy, which is viewed very differently in various cultures.[39] Since the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering.[40] Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands.[41] However, for the most part protection was achieved through the application of procedural handling controls.[42][43] Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box.[44] As postal services expanded, governments created official organizations to intercept, decipher, read, and reseal letters (e.g., the U.K.'s Secret Office, founded in 1653[45]). In the mid-nineteenth century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity.[46] For example, the British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889.[47] Section 1 of the law concerned espionage and unlawful disclosures of information, while Section 2 dealt with breaches of official trust.[48] A public interest defense was soon added to defend disclosures in the interest of the state.[49] A similar law was passed in India in 1889, The Indian Official Secrets Act, which was associated with the British colonial era and used to crack down on newspapers that opposed the Raj's policies.[50] A newer version was passed in 1923 that extended to all matters of confidential or secret information for governance.[51]  By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters.[52] Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information.[53] The establishment of computer security inaugurated the history of information security. The need for such appeared during World War II.[54] The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls.[55] An arcane range of markings evolved to indicate who could handle documents (usually officers rather than enlisted troops) and where they should be stored as increasingly complex safes and storage facilities were developed.[56] The Enigma Machine, which was employed by the Germans to encrypt the data of warfare and was successfully decrypted by Alan Turing, can be regarded as a striking example of creating and using secured information.[57] Procedures evolved to ensure documents were destroyed properly, and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g., the capture of U-570[57]). Various mainframe computers were connected online during the Cold War to complete more sophisticated tasks, in a communication process easier than mailing magnetic tapes back and forth by computer centers. As such, the Advanced Research Projects Agency (ARPA), of the United States Department of Defense, started researching the feasibility of a networked system of communication to trade information within the United States Armed Forces. In 1968, the ARPANET project was formulated by Larry Roberts, which would later evolve into what is known as the internet.[58] In 1973, important elements of ARPANET security were found by internet pioneer Robert Metcalfe to have many flaws such as the: \"vulnerability of password structure and formats; lack of safety procedures for dial-up connections; and nonexistent user identification and authorizations\", aside from the lack of controls and safeguards to keep data safe from unauthorized access. Hackers had effortless access to ARPANET, as phone numbers were known by the public.[59] Due to these problems, coupled with the constant violation of computer security, as well as the exponential increase in the number of hosts and users of the system, \"network security\" was often alluded to as \"network insecurity\".[59] The end of the twentieth century and the early years of the twenty-first century saw rapid advancements in telecommunications, computing hardware and software, and data encryption.[60] The availability of smaller, more powerful, and less expensive computing equipment made electronic data processing within the reach of small business and home users.[61] The establishment of Transfer Control Protocol/Internetwork Protocol (TCP/IP) in the early 1980s enabled different types of computers to communicate.[62] These computers quickly became interconnected through the internet.[63] The rapid growth and widespread use of electronic data processing and electronic business conducted through the internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process, and transmit.[64] The academic disciplines of computer security and information assurance emerged along with numerous professional organizations, all sharing the common goals of ensuring the security and reliability of information systems.[65] The \"CIA triad\" of confidentiality, integrity, and availability is at the heart of information security.[66] The concept was introduced in the Anderson Report in 1972 and later repeated in The Protection of Information in Computer Systems. The abbreviation was coined by Steve Lipner around 1986.[67] Debate continues about whether or not this triad is sufficient to address rapidly changing technology and business requirements, with recommendations to consider expanding on the intersections between availability and confidentiality, as well as the relationship between security and privacy.[4] Other principles such as \"accountability\" have sometimes been proposed; it has been pointed out that issues such as non-repudiation do not fit well within the three core concepts.[68] In information security, confidentiality \"is the property, that information is not made available or disclosed to unauthorized individuals, entities, or processes.\"[69] While similar to \"privacy\", the two words are not interchangeable. Rather, confidentiality is a component of privacy that implements to protect our data from unauthorized viewers.[70] Examples of confidentiality of electronic data being compromised include laptop theft, password theft, or sensitive emails being sent to the incorrect individuals.[71] In IT security, data integrity means maintaining and assuring the accuracy and completeness of data over its entire lifecycle.[72] This means that data cannot be modified in an unauthorized or undetected manner.[73] This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing.[74] Information security systems typically incorporate controls to ensure their own integrity, in particular protecting the kernel or core functions against both deliberate and accidental threats.[75] Multi-purpose and multi-user computer systems aim to compartmentalize the data and processing such that no user or process can adversely impact another: the controls may not succeed however, as we see in incidents such as malware infections, hacks, data theft, fraud, and privacy breaches.[76] More broadly, integrity is an information security principle that involves human/social, process, and commercial integrity, as well as data integrity. As such it touches on aspects such as credibility, consistency, truthfulness, completeness, accuracy, timeliness, and assurance.[77] For any information system to serve its purpose, the information must be available when it is needed.[78] This means the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly.[79] High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades.[80] Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system, essentially forcing it to shut down.[81] In the realm of information security, availability can often be viewed as one of the most important parts of a successful information security program.[citation needed] Ultimately end-users need to be able to perform job functions; by ensuring availability an organization is able to perform to the standards that an organization's stakeholders expect.[82] This can involve topics such as proxy configurations, outside web access, the ability to access shared drives and the ability to send emails.[83] Executives oftentimes do not understand the technical side of information security and look at availability as an easy fix, but this often requires collaboration from many different organizational teams, such as network operations, development operations, incident response, and policy/change management.[84] A successful information security team involves many different key roles to mesh and align for the \"CIA\" triad to be provided effectively.[85] In addition to the classic CIA triad of security goals, some organisations may want to include security goals like authenticity, accountability, non-repudiation, and reliability. In law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction, nor can the other party deny having sent a transaction.[86] It is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology.[87] It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message, and nobody else could have altered it in transit (data integrity).[88] The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised.[89] The fault for these violations may or may not lie with the sender, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity. As such, the sender may repudiate the message (because authenticity and integrity are pre-requisites for non-repudiation).[90] In 1992 and revised in 2002, the OECD's Guidelines for the Security of Information Systems and Networks[91] proposed the nine generally accepted principles: awareness, responsibility, response, ethics, democracy, risk assessment, security design and implementation, security management, and reassessment.[92] Building upon those, in 2004 the NIST's Engineering Principles for Information Technology Security[68] proposed 33 principles. In 1998, Donn Parker proposed an alternative model for the classic \"CIA\" triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian Hexad are a subject of debate amongst security professionals.[93] In 2011, The Open Group published the information security management standard O-ISM3.[94] This standard proposed an operational definition of the key concepts of security, with elements called \"security objectives\", related to access control (9), availability (3), data quality (1), compliance, and technical (4). Risk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset).[95] A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm.[96] The likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact.[97] In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property).[98] The Certified Information Systems Auditor (CISA) Review Manual 2006 defines risk management as \"the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures,[99] if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization.\"[100] There are two things in this definition that may need some clarification. First, the process of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day.[101] Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.[102] Furthermore, these processes have limitations as security breaches are generally rare and emerge in a specific context which may not be easily duplicated.[103] Thus, any process and countermeasure should itself be evaluated for vulnerabilities.[104] It is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called \"residual risk\".[105] A risk assessment is carried out by a team of people who have knowledge of specific areas of the business.[106] Membership of the team may vary over time as different parts of the business are assessed.[107] The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis. Research has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human.[108] The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment: In broad terms, the risk management process consists of:[109][110] For any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business.[117] Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business.[118] The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.[119] Selecting and implementing proper security controls will initially help an organization bring down risk to acceptable levels.[120] Control selection should follow and should be based on the risk assessment.[121] Controls can vary in nature, but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. ISO/IEC 27001 has defined controls in different areas.[122] Organizations can implement additional controls according to requirement of the organization.[123] ISO/IEC 27002 offers a guideline for organizational information security standards.[124] Defense in depth is a fundamental security philosophy that relies on overlapping security systems designed to maintain protection even if individual components fail. Rather than depending on a single security measure, it combines multiple layers of security controls both in the cloud and at network endpoints. This approach includes combinations like firewalls with intrusion-detection systems, email filtering services with desktop anti-virus, and cloud-based security alongside traditional network defenses.[125]\nThe concept can be implemented through three distinct layers of administrative, logical, and physical controls,[126] or visualized as an onion model with data at the core, surrounded by people, network security, host-based security, and application security layers.[127] The strategy emphasizes that security involves not just technology, but also people and processes working together, with real-time monitoring and response being crucial components.[125] An important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information.[128] Not all information is equal and so not all information requires the same degree of protection.[129] This requires information to be assigned a security classification.[130] The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy.[131] The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.[132] Some factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete.[133] Laws and other regulatory requirements are also important considerations when classifying information.[134] The Information Systems Audit and Control Association (ISACA) and its Business Model for Information Security also serves as a tool for security professionals to examine security from a systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.[135] The type of information security classification labels selected and used will depend on the nature of the organization, with examples being:[132] All employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification.[138] The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.[139] Access to protected information must be restricted to people who are authorized to access the information.[140] The computer programs, and in many cases the computers that process the information, must also be authorized.[141] This requires that mechanisms be in place to control the access to protected information.[141] The sophistication of the access control mechanisms should be in parity with the value of the information being protected; the more sensitive or valuable the information the stronger the control mechanisms need to be.[142] The foundation on which access control mechanisms are built start with identification and authentication.[143] Access control is generally considered in three steps: identification, authentication, and authorization.[144][71] Identification is an assertion of who someone is or what something is. If a person makes the statement \"Hello, my name is John Doe\" they are making a claim of who they are.[145] However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe.[146] Typically the claim is in the form of a username. By entering that username you are claiming \"I am the person the username belongs to\".[147] Authentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe, a claim of identity.[148] The bank teller asks to see a photo ID, so he hands the teller his driver's license.[149] The bank teller checks the license to make sure it has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe.[150] If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly, by entering the correct password, the user is providing evidence that he/she is the person the username belongs to.[151] There are three different types of information that can be used for authentication:[152][153] Strong authentication requires providing more than one type of authentication information (two-factor authentication).[159] The username is the most common form of identification on computer systems today and the password is the most common form of authentication.[160] Usernames and passwords have served their purpose, but they are increasingly inadequate.[161] Usernames and passwords are slowly being replaced or supplemented with more sophisticated authentication mechanisms such as time-based one-time password algorithms.[162] After a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change).[163] This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures.[164] The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies.[165] Different computing systems are equipped with different kinds of access control mechanisms. Some may even offer a choice of different access control mechanisms.[166] The access control mechanism a system offers will be based upon one of three approaches to access control, or it may be derived from a combination of the three approaches.[71] The non-discretionary approach consolidates all access control under a centralized administration.[167] The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform.[168][169] The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources.[167] In the mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.[140] Examples of common access control mechanisms in use today include role-based access control, available in many advanced database management systems; simple file permissions provided in the UNIX and Windows operating systems;[170] Group Policy Objects provided in Windows network systems; and Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.[171] To be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions.[172] The U.S. Treasury's guidelines for systems processing sensitive or proprietary information, for example, states that all failed and successful authentication and access attempts must be logged, and all access to information must leave some type of audit trail.[173] Also, the need-to-know principle needs to be in effect when talking about access control. This principle gives access rights to a person to perform their job functions.[174] This principle is used in the government when dealing with difference clearances.[175] Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee the least amount of privilege to prevent employees from accessing more than what they are supposed to.[176] Need-to-know helps to enforce the confidentiality-integrity-availability triad. Need-to-know directly impacts the confidential area of the triad.[177] Information security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption.[178] Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user who possesses the cryptographic key, through the process of decryption.[179] Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.[71] Cryptography provides information security with other useful applications as well, including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications.[180] Older, less secure applications such as Telnet and File Transfer Protocol (FTP) are slowly being replaced with more secure applications such as Secure Shell (SSH) that use encrypted network communications.[181] Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU\u2011T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange.[182] Software applications such as GnuPG or PGP can be used to encrypt data files and email.[183] Cryptography can introduce security problems when it is not implemented correctly.[184] Cryptographic solutions need to be implemented using industry-accepted solutions that have undergone rigorous peer review by independent experts in cryptography.[185] The length and strength of the encryption key is also an important consideration.[186] A key that is weak or too short will produce weak encryption.[186] The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information.[187] They must be protected from unauthorized disclosure and destruction, and they must be available when needed.[citation needed] Public key infrastructure (PKI) solutions address many of the problems that surround key management.[71] U.S. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.[188] In the field of information security, Harris[189]\noffers the following definitions of due care and due diligence: \"Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees[190].\" And, [Due diligence are the] \"continual activities that make sure the protection mechanisms are continually maintained and operational.\"[191] Attention should be made to two important points in these definitions.[192][193] First, in due care, steps are taken to show; this means that the steps can be verified, measured, or even produce tangible artifacts.[194][195] Second, in due diligence, there are continual activities; this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.[196] Organizations have a responsibility with practicing duty of care when applying information security. The Duty of Care Risk Analysis Standard (DoCRA)[197] provides principles and practices for evaluating risk.[198] It considers all parties that could be affected by those risks.[199] DoCRA helps evaluate safeguards if they are appropriate in protecting others from harm while presenting a reasonable burden.[200] With increased data breach litigation, companies must balance security controls, compliance, and its mission.[201] Computer security incident management is a specialized form of incident management focused on monitoring, detecting, and responding to security events on computers and networks in a predictable way.[202] Organizations implement this through incident response plans (IRPs) that are activated when security breaches are detected.[203] These plans typically involve an incident response team (IRT) with specialized skills in areas like penetration testing, computer forensics, and network security.[204] Change management is a formal process for directing and controlling alterations to the information processing environment.[205][206] This includes alterations to desktop computers, the network, servers, and software.[207] The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the processing environment as changes are made.[208] It is not the objective of change management to prevent or hinder necessary changes from being implemented.[209][210] Any change to the information processing environment introduces an element of risk.[211] Even apparently simple changes can have unexpected effects.[212] One of management's many responsibilities is the management of risk.[213][214] Change management is a tool for managing the risks introduced by changes to the information processing environment.[215] Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.[216] Not every change needs to be managed.[217][218] Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment.[219] Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management.[220] However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity.[221] The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.[222] Change management is usually overseen by a change review board composed of representatives from key business areas,[223] security, networking, systems administrators, database administration, application developers, desktop support, and the help desk.[224] The tasks of the change review board can be facilitated with the use of automated work flow application.[225] The responsibility of the change review board is to ensure the organization's documented change management procedures are followed.[226] The change management process is as follows[227] Change management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment.[259] Good change management procedures improve the overall quality and success of changes as they are implemented.[260] This is accomplished through planning, peer review, documentation, and communication.[261] ISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps[262] (Full book summary),[263] and ITIL all provide valuable guidance on implementing an efficient and effective change management program information security.[264] Business continuity management (BCM) concerns arrangements aiming to protect an organization's critical business functions from interruption due to incidents, or at least minimize the effects.[265][266] BCM is essential to any organization to keep technology and business in line with current threats to the continuation of business as usual.[267] The BCM should be included in an organizations risk analysis plan to ensure that all of the necessary business functions have what they need to keep going in the event of any type of threat to any business function.[268] It encompasses: Whereas BCM takes a broad approach to minimizing disaster-related risks by reducing both the probability and the severity of incidents, a disaster recovery plan (DRP) focuses specifically on resuming business operations as quickly as possible after a disaster.[278] A disaster recovery plan, invoked soon after a disaster occurs, lays out the steps necessary to recover critical information and communications technology (ICT) infrastructure.[279] Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.[280] Below is a partial listing of governmental laws and regulations in various parts of the world that have, had, or will have, a significant effect on data processing and information security.[281][282] Important industry sector regulations have also been included when they have a significant impact on information security.[281] The US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.[317] Describing more than simply how security aware employees are, information security culture is the ideas, customs, and social behaviors of an organization that impact information security in both positive and negative ways.[318] Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization. The way employees think and feel about security and the actions they take can have a big impact on information security in organizations. Roer & Petric (2017) identify seven core dimensions of information security culture in organizations:[319] Andersson and Reimers (2014) found that employees often do not see themselves as part of the organization Information Security \"effort\" and often take actions that ignore organizational information security best interests.[321] Research shows information security culture needs to be improved continuously. In Information Security Culture from Analysis to Change, authors commented, \"It's a never ending process, a cycle of evaluation and change or maintenance.\" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.[322]",
      "ground_truth_chunk_ids": [
        "46_fixed_chunk1",
        "157_fixed_chunk1"
      ],
      "source_ids": [
        "S046",
        "S157"
      ],
      "category": "comparative",
      "id": 52
    },
    {
      "question": "Compare Energy and Volcano in one sentence each: what does each describe or study?",
      "ground_truth": "Energy: Energy (from Ancient Greek \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 (en\u00e9rgeia) 'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J). Forms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass. These are not mutually exclusive. All living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the Sun.[6] The total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the object's components \u2013 while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself.[7] While these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to Volcano: A volcano is commonly defined as a vent or fissure in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface.[1] On Earth, volcanoes are most often found where tectonic plates are diverging or converging, and because most of Earth's plate boundaries are underwater, most volcanoes are found underwater. For example, a mid-ocean ridge, such as the Mid-Atlantic Ridge, has volcanoes caused by divergent tectonic plates whereas the Pacific Ring of Fire has volcanoes caused by convergent tectonic plates. Volcanoes resulting from divergent tectonic activity are usually non-explosive whereas those resulting from convergent tectonic activity cause violent eruptions.[2][3] Volcanoes can also form where there is stretching and thinning of the crust's plates, such as in the East African Rift, the Wells Gray-Clearwater volcanic field, and the Rio Grande rift in North America. Volcanism away from plate boundaries most likely arises from upwelling diapirs from the core\u2013mantle boundary called mantle plumes, 3,000 kilometres (1,900 mi) deep within Earth. This results in hotspot volcanism or intraplate volcanism, in which the plume may cause thinning of the crust and result in a volcanic island chain due to the continuous movement of the tectonic plate, of which the Hawaiian hotspot is an example.[4] Volcanoes are usually not created at transform tectonic boundaries where two tectonic plates slide past one another. Volcanoes, based on their frequency of eruption or volcanism, are referred to as either active, dormant, or extinct.[5] Active volcanoes have a history of volcanism and are likely to erupt again, while extinct ones are not capable of eruption at all as they have no magma source. \"Dormant\" volcanoes have not erupted in a long time \u2013 generally accepted as since the start of the Holocene, about 12,000",
      "expected_answer": "Energy: Energy (from Ancient Greek  \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 (en\u00e9rgeia)\u00a0'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J). Forms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass. These are not mutually exclusive. All living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the Sun.[6] The total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the object's components \u2013 while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself.[7] While these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to the combined potentials within an atomic nucleus from either the nuclear force or the weak force, among other examples.[8] The word energy derives from the Ancient Greek: \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1, romanized:\u00a0energeia, lit.\u2009'activity, operation',[11] which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.[12] In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the motions of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two.[13] Writing in the early 18th century, \u00c9milie du Ch\u00e2telet proposed the concept of conservation of energy in the marginalia of her French language translation of Newton's Principia Mathematica, which represented the first formulation of a conserved measurable quantity that was distinct from momentum, and which would later be called \"energy\".[14] In 1807, Thomas Young was possibly the first to use the term \"energy\" instead of vis viva, in its modern sense.[15] Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense,[16] and in 1853, William Rankine coined the term \"potential energy\".[17] The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system.[18] It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.[19] These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics.[20] Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, Walther Nernst, and others.[21] It also led to a mathematical formulation of the concept of entropy by Clausius[22] and to the introduction of laws of radiant energy by Jo\u017eef Stefan.[23] According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time.[24] Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.[25] Albert Einstein's 1905 theory of special relativity showed that rest mass corresponds to an equivalent amount of rest energy. This means that rest mass can be converted to or from equivalent amounts of (non-material) forms of energy, for example, kinetic energy, potential energy, and electromagnetic radiant energy. When this happens, rest mass is not conserved, unlike the total mass or total energy. All forms of energy contribute to the total mass and total energy. Thus, conservation of energy (total, including material or rest energy) and conservation of mass (total, not just rest) are one (equivalent) law. In the 18th century, these had appeared as two seemingly-distinct laws.[26][27] The first evidence of quantization in atoms was the observation of spectral lines in light from the sun in the early 1800s by Joseph von Fraunhofer and William Hyde Wollaston. The notion of quantized energy levels was proposed in 1913 by Danish physicist Niels Bohr in the Bohr theory of the atom. The modern quantum mechanical theory giving an explanation of these energy levels in terms of the Schr\u00f6dinger equation was advanced by Erwin Schr\u00f6dinger and Werner Heisenberg in 1926.[28] Noether's theorem shows that the symmetry of this equation is equivalent to a conservation of probability.[29] At the quantum level, mass-energy interactions are all subject to this principle.[30] During wave function collapse, the conservation of energy does not hold at the local level, although statistically the principle holds on average for sufficiently large numbers of collapses.[31] Conservation of energy does apply during wave function collapse in H. Everett's many-worlds interpretation of quantum mechanics.[32] In dimensional analysis, the base units of energy are given by: Work = Force \u00d7 Distance = M L2 T\u22122, with the fundamental dimensions of Mass M, Length L, and time T.[5] In the International System of Units (SI), the unit of energy is the joule. It is a derived unit that is equal to the energy expended, or work done, in applying a force of one newton through a distance of one metre.[1] The SI unit of power, defined as energy per unit of time, is the watt, which is one joule per second.[3] Thus, a kilowatt-hour (kWh), which can be realized as the energy delivered by one kilowatt of power for an hour, is equal to 3.6 million joules.[33] The CGS energy unit is the erg and the imperial and US customary unit is the foot-pound.[34] Other energy units such as the electronvolt, food calorie, thermodynamic kilocalorie and BTU are used in specific areas of science and commerce.[35][2] In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept. Work, a function of energy, is force times distance.[36] This says that the work (\n\n\n\nW\n\n\n{\\displaystyle W}\n\n) is equal to the line integral of the force F along a path C; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.[37] The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems.[38] These classical equations have direct analogs in nonrelativistic quantum mechanics.[39] Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).[40] Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian;[41] for example, dissipative systems with continuous symmetries need not have a corresponding conservation law. In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular, or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is usually accompanied by a decrease, and sometimes an increase, of the total energy of the substances involved. Some energy may be transferred between the surroundings and the reactants in the form of heat or light; thus the products of a reaction have sometimes more but usually less energy than the reactants. A reaction is said to be exothermic or exergonic if the final state is lower on the energy scale than the initial state; in the less common case of endothermic reactions the situation is the reverse.[42] Chemical reactions are usually not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at a given temperature\u00a0T) is related to the activation energy\u00a0E by the Boltzmann population factor\u00a0e\u2212E/kT; that is, the probability of a molecule to have energy greater than or equal to\u00a0E at a given temperature\u00a0T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction can be provided in the form of thermal energy.[43] In biology, energy is an attribute of all biological systems, from the biosphere to the smallest living organism. It enables the growth, development, and functioning of a biological cell or organelle in an organism. All living creatures rely on an external source of energy to be able to grow and reproduce \u2013 radiant energy from the Sun in the case of green plants and chemical energy (in some form) in the case of animals. Energy provided through cellular respiration is stored in nutrients such as carbohydrates (including sugars), lipids, and proteins by cells.[44] Sunlight's radiant energy is captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into carbohydrates, lipids, proteins, and oxygen.[45] Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark in a forest fire, or it may be made available more slowly for animal or human metabolism when organic molecules are ingested and catabolism is triggered by enzyme action.[46] The basal metabolism rate measures the food energy expenditure per unit time by endothermic animals at rest.[47] In other words it is the energy required by body organs to perform normally. For humans, metabolic equivalent of task (MET) compares the energy expenditure per unit mass while performing a physical activity, relative to a baseline. By convention, this baseline is 3.5\u00a0mL of oxygen consumed per kg per minute, which is the energy consumed by a typical individual when sitting quietly.[48] In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, using as a standard an average human energy expenditure of 6,900\u00a0kJ per day and a basal metabolic rate of 80 watts.[citation needed] For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 \u00f7 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum.[49] The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.[50] The daily 1,600\u20133,000 calories (7\u201313\u00a0MJ) recommended for a human adult are taken as food molecules,[51] mostly carbohydrates and fats. Only a tiny fraction of the original chemical energy is used for work:[note 1] It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy); most machines manage higher efficiencies.[citation needed] In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism's tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\").[note 2] Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology. As an example, to take just the first step in the food chain: of the estimated 124.7\u00a0Pg/a of carbon that is fixed by photosynthesis, 64.3\u00a0Pg/a (52%) are used for the metabolism of green plants,[52] i.e. reconverted into carbon dioxide and heat. Multicellular organisms such as humans have cell forms that are classified as Eukaryote. These cells include an organelle called the mitochondria that generates chemical energy for the rest of the hosting cell. Ninety percent of the oxygen intake by humans is utilized by the mitochondria, especially for nutrient processing.[53] The molecule adenosine triphosphate (ATP) is the primary energy transporter in living cells, providing an energy source for cellular processes. It is continually being broken down and synthesized as a component of cellular respiration.[54] Two examples of nutrients consumed by animals are glucose (C6H12O6) and stearin (C57H110O6). These food molecules are oxidized to carbon dioxide and water in the mitochondria:[55]\n\n\n\n\n\n\nC\n\n6\n\n\n\n\n\n\nH\n\n12\n\n\n\n\n\n\nO\n\n6\n\n\n\n\n\n+\n6\n\n\nO\n\n2\n\n\n\n\n\n\u27f6\n6\n\n\nCO\n\n2\n\n\n\n\n\n+\n6\n\n\nH\n\n2\n\n\n\n\n\nO\n\n\n\n{\\displaystyle {\\ce {C6H12O6 + 6O2 -> 6CO2 + 6H2O}}}\n\n\n\n\n\n\n\n\nC\n\n57\n\n\n\n\n\n\nH\n\n110\n\n\n\n\n\n\nO\n\n6\n\n\n\n\n\n+\n\n(\n81\n\n\n\n1\n2\n\n\n)\n\n\nO\n\n2\n\n\n\n\n\n\u27f6\n57\n\n\nCO\n\n2\n\n\n\n\n\n+\n55\n\n\nH\n\n2\n\n\n\n\n\nO\n\n\n\n{\\displaystyle {\\ce {C57H110O6 + (81 1/2) O2 -> 57CO2 + 55H2O}}}\n\n\nand some of the energy is used to convert ADP into ATP:[56][53] The rest of the chemical energy of the nutrients are converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior,[57] while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes, and hurricanes are all a result of energy transformations in our atmosphere brought about by solar energy. Sunlight is the main input to Earth's energy budget which accounts for its temperature and climate stability, after accounting for interaction with the atmosphere.[58] Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example when) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity).[59]  An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, suddenly give up some of their thermal energy to power a few days of violent air movement.[60] In a slower process, radioactive decay of atoms in the core of the Earth releases heat, which supplies more than half of the planet's internal heat budget.[61] In the present day, this radiogenic heat production was primarily driven by the decay of Uranium-235, Potassium-40, and Thorium-232 some time in the past.[62] This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may later be transformed into active kinetic energy during landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks.[63] Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars (which created these atoms).[64] Early in a planet's history, the accretion process provides impact energy that can partially or completely melt the body. This allows a planet to become differentiated by chemical element. Chemical phase changes of minerals during formation provide additional internal heating. Over time the internal heat is brought to the surface then radiated away into space, cooling the body. Accreted radiogenic heat sources settle toward the core, providing thermal energy to the planet on a geologic time scale.[65] Ongoing sedimentation provides a persistent internal energy source for gas giant planets like Jupiter and Saturn.[66] In cosmology and astronomy the phenomena of stars, nova, supernova, quasars, and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen).[67] The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.[68] The accretion of matter onto a compact object is a very efficient means of generating energy from gravitational potential. This behavior is responsible for some of the universe's brightest persistent energy sources.[69] The Penrose process is a theoretical method by which energy could be extracted from a rotating black hole.[70] Hawking radiation is the emission of black-body radiation from a black hole, which results in a steady loss of mass and rotational energy. As the object evaporates, the temperature of this radiation is predicted to increase, speeding up the process.[71] In quantum mechanics, energy is defined in terms of the energy operator\n(Hamiltonian) as a time derivative of the wave function. The Schr\u00f6dinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schr\u00f6dinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta.[72] In the solution of the Schr\u00f6dinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by the Planck relation: \n\n\n\nE\n=\nh\n\u03bd\n\n\n{\\displaystyle E=h\\nu }\n\n, where \n\n\n\nh\n\n\n{\\displaystyle h}\n\n is the Planck constant and \n\n\n\n\u03bd\n\n\n{\\displaystyle \\nu }\n\n the frequency. In the case of an electromagnetic wave these energy states are called quanta of light or photons. For matter waves, the de Broglie relation yields \n\n\n\np\n=\nh\n\u03bd\n\n\n{\\displaystyle p=h\\nu }\n\n, where \n\n\n\np\n\n\n{\\displaystyle p}\n\n is the momentum.[73] When calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically \u2013 using Lorentz transformations instead of Newtonian mechanics \u2013 Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:[74] E\n\n0\n\n\n=\n\nm\n\n0\n\n\n\nc\n\n2\n\n\n,\n\n\n{\\displaystyle E_{0}=m_{0}c^{2},}\n\n\nwhere For example, consider electron\u2013positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process \u2013 the inverse process is called pair creation \u2013 in which the rest mass of the particles is created from a sufficiently energetic photon near a nucleus.[75] In general relativity, the stress\u2013energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.[76][page\u00a0needed] Energy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"),[77] and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws. In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy\u2013momentum 4-vector).[76][page\u00a0needed] In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of spacetime (= boosts). Energy may be transformed between different forms at various efficiencies. Devices that usefully transform between these forms are called transducers. Examples of transducers include a battery (from chemical energy to electric energy), a dam (from gravitational potential energy to the kinetic energy of water spinning the blades of a turbine, and ultimately to electric energy through an electric generator), and a heat engine (from heat to work).[78][79] Examples of energy transformation include generating electric energy from heat energy via a steam turbine,[79] or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground.[80] The Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that itself (since it still contains the same total energy even in different forms) but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.[81] There are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics.[82] However, some energy transformations can be quite efficient.[83] The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a sufficiently small scale, but certain larger transformations are highly improbable because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.[84] Energy transformations in the universe over time are characterized by various kinds of potential energy, that has been available since the Big Bang, being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available.[85] Familiar examples of such processes include nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae to \"store\" energy in the creation of heavy isotopes (such as uranium and thorium), and nuclear decay, a process in which energy is released that was originally stored in these heavy elements, before they were incorporated into the Solar System and the Earth.[86] This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic and thermal energy in a very short time.[87] Yet another example of energy transformation is that of a simple gravity pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at its maximum. At its lowest point the kinetic energy is at its maximum and is equal to the decrease in potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever. Energy is transferred from potential energy (\n\n\n\n\nE\n\np\n\n\n\n\n{\\displaystyle E_{p}}\n\n) to kinetic energy (\n\n\n\n\nE\n\nk\n\n\n\n\n{\\displaystyle E_{k}}\n\n) and then back to potential energy constantly. This is referred to as conservation of energy. In this isolated system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following: The equation can then be simplified further since \n\n\n\n\nE\n\np\n\n\n=\nm\ng\nh\n\n\n{\\displaystyle E_{p}=mgh}\n\n (mass times acceleration due to gravity times the height) and \n\n\n\n\nE\n\nk\n\n\n=\n\n\n1\n2\n\n\nm\n\nv\n\n2\n\n\n\n\n{\\textstyle E_{k}={\\frac {1}{2}}mv^{2}}\n\n (half\u00a0mass times velocity squared). Then the total amount of energy can be found by adding \n\n\n\n\nE\n\np\n\n\n+\n\nE\n\nk\n\n\n=\n\nE\n\ntotal\n\n\n\n\n{\\displaystyle E_{p}+E_{k}=E_{\\text{total}}}\n\n.[88] Within a gravitational field, both mass and energy give rise to a measureable weight when trapped in a system with zero momentum. The formula E\u00a0=\u00a0mc2, derived by Albert Einstein (1905) quantifies this mass\u2013energy equivalence between relativistic mass and energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincar\u00e9 (1900), Friedrich Hasen\u00f6hrl (1904), and others (see Mass\u2013energy equivalence#History for further information). Part of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since \n\n\n\n\nc\n\n2\n\n\n\n\n{\\displaystyle c^{2}}\n\n is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy, as can be seen in nuclear reactors and nuclear weapons.[89] For example, 1\u00a0kg of rest mass equals 9\u00d71016\u00a0joules, equivalent to 21.5 megatonnes of TNT.[90] Conversely, the mass equivalent of an everyday amount energy is minuscule. Examples of large-scale transformations between the rest energy of matter and other forms of energy are found in nuclear physics and particle physics. The complete conversion of matter, such as atoms, to non-matter, such as photons, occurs during interaction with antimatter.[91] Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another is reversible, as in the pendulum system described above.[92] At the atomic scale, thermal energy is present in the form of motion and vibrations of individual atoms and molecules. When heat is generated, radiation excites lower energy states of these atoms and their surrounding fields. This heating process acts as a reservoir for part of the applied energy, from which it cannot be converted with 100% efficiency into other forms of energy.[93] According to the second law of thermodynamics, this heat can only be completely recovered as usable energy at the price of an increase in some other kind of heat-like disorder in quantum states. As the universe evolves with time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or as other kinds of increases in disorder). This has led to the hypothesis of the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), continues to decrease.[94] The fact that energy can be neither created nor destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out as work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.[95] While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations.[96] The total energy of a system can be calculated by adding up all forms of energy in the system. Richard Feynman said during a 1961 lecture:[97] There is a fact, or if you wish, a law, governing all natural phenomena that are known to date. There is no known exception to this law \u2013 it is exact so far as we know. The law is called the conservation of energy. It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same. \u2014\u200aThe Feynman Lectures on Physics Most kinds of energy (with gravitational energy being a notable exception)[98] are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.[96][97] This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time,[99] a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle \u2013 it is impossible to define the exact amount of energy during any definite time interval (though this is practically significant only for very short time intervals). The uncertainty principle should not be confused with energy conservation \u2013 rather it provides mathematical limits to which energy can in principle be defined and measured. Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appear as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.[100] In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scale, the uncertainty in the energy is given by which is similar in form to the Heisenberg Uncertainty Principle,[101] but not really mathematically equivalent thereto, since E and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics.[102] In particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum.[102] The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions).[103]:\u200a101\u200a Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law),[103]:\u200a336\u200a for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force,[104] for the Van der Waals force,[105] and some other observable phenomena.[106] Energy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat.[note 3] Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy,[note 4] tidal interactions,[107] and the conductive transfer of thermal energy.[108] Energy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:[note 5][108] where \n\n\n\nE\n\n\n{\\displaystyle E}\n\n is the amount of energy transferred, \n\n\n\nW\n\n\n{\\displaystyle W}\n\n\u00a0 represents the work done on or by the system, and \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n represents the heat flow into or out of the system. As a simplification, the heat term, \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n, can sometimes be ignored, especially for fast processes involving gases, which are poor conductors of heat, or when the thermal efficiency of the transfer is high. For such adiabatic processes, This simplified equation is the one used to define the joule, for example. Beyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (this process is illustrated by injection of an air-fuel mixture into a car engine, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by \n\n\n\n\nE\n\nmatter\n\n\n\n\n{\\displaystyle E_{\\text{matter}}}\n\n, one may write:[109] Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.[110] The first law of thermodynamics asserts that the total energy of a system and its surroundings (but not necessarily thermodynamic free energy) is always conserved[111] and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as:[112] where the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and its change dS is positive when heat is added to the system), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system). This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and PV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form by:[108] where \n\n\n\n\u03b4\nQ\n\n\n{\\displaystyle \\delta Q}\n\n is the heat supplied to the system and \n\n\n\n\u03b4\nW\n\n\n{\\displaystyle \\delta W}\n\n is the work applied to the system. The energy of a mechanical harmonic oscillator (a mass on a spring) is alternately kinetic and potential energy. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential.[88] Over a whole cycle, or over many cycles, average energy is equally split between kinetic and potential. This is an example of the equipartition principle: the total energy of a system with many degrees of freedom is equally split among all available degrees of freedom, on average.[113] This principle is vitally important to understanding the behavior of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is part of the second law of thermodynamics. The second law of thermodynamics is simple only for systems which are near or in a physical equilibrium state. For non-equilibrium systems, the laws governing the systems' behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production.[114][115] It states that nonequilibrium systems behave in such a way as to maximize their entropy production.[116] Volcano: A volcano is commonly defined as a vent or fissure in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface.[1] On Earth, volcanoes are most often found where tectonic plates are diverging or converging, and because most of Earth's plate boundaries are underwater, most volcanoes are found underwater. For example, a mid-ocean ridge, such as the Mid-Atlantic Ridge, has volcanoes caused by divergent tectonic plates whereas the Pacific Ring of Fire has volcanoes caused by convergent tectonic plates. Volcanoes resulting from divergent tectonic activity are usually non-explosive whereas those resulting from convergent tectonic activity cause violent eruptions.[2][3] Volcanoes can also form where there is stretching and thinning of the crust's plates, such as in the East African Rift, the Wells Gray-Clearwater volcanic field, and the Rio Grande rift in North America. Volcanism away from plate boundaries most likely arises from upwelling diapirs from the core\u2013mantle boundary called mantle plumes, 3,000 kilometres (1,900\u00a0mi) deep within Earth. This results in hotspot volcanism or intraplate volcanism, in which the plume may cause thinning of the crust and result in a volcanic island chain due to the continuous movement of the tectonic plate, of which the Hawaiian hotspot is an example.[4] Volcanoes are usually not created at transform tectonic boundaries where two tectonic plates slide past one another. Volcanoes, based on their frequency of eruption or volcanism, are referred to as either active, dormant, or extinct.[5] Active volcanoes have a history of volcanism and are likely to erupt again, while extinct ones are not capable of eruption at all as they have no magma source. \"Dormant\" volcanoes have not erupted in a long time \u2013 generally accepted as since the start of the Holocene, about 12,000 years ago \u2013 but may erupt again. However, dormant volcanoes are technically considered to be seismically \"active\".[5] These categories aren't entirely uniform; they may overlap for certain examples.[2][6][7] Large eruptions can affect atmospheric temperature as ash and droplets of sulfuric acid obscure the Sun and cool Earth's troposphere. Historically, large volcanic eruptions have been followed by volcanic winters which have caused catastrophic famines.[8] Other planets besides Earth have volcanoes. For example, volcanoes are very numerous on Venus.[9] Mars has significant volcanoes.[10] In 2009, a paper was published suggesting a new definition for the word 'volcano' that includes processes such as cryovolcanism. It suggested that a volcano be defined as 'an opening on a planet or moon's surface from which magma, as defined for that body, and/or magmatic gas is erupted.'[11] This article mainly covers volcanoes on Earth. See \u00a7\u00a0Volcanoes on other celestial bodies and cryovolcano for more information. The word volcano (UK: /v\u0252l\u02c8ke\u026an\u0259\u028a/; US: /v\u0251\u02d0l\u02c8ke\u026ano\u028a/) originates from the early 17th century, derived from the Italian name Vulcano, a volcanic island in the Aeolian Islands of Italy, which in turn comes from the Latin name Volc\u0101nus or Vulc\u0101nus, referring to Vulcan, the god of fire in Roman mythology.[12][13] The set of processes and phenomena involved in volcanic activity is called volcanism [early 19th century: from volcano + -ism]. The study of volcanism and volcanoes is called volcanology [mid-19th century: from volcano + -logy], sometimes spelled vulcanology.[12] According to the theory of plate tectonics, Earth's lithosphere, its rigid outer shell, is broken into sixteen larger and several smaller plates. These move continuously at a slow pace, due to convection in the underlying ductile mantle, and most volcanic activity on Earth takes place along plate boundaries, where plates are converging (and lithosphere is being destroyed) or are diverging (and new lithosphere is being created).[14] During the development of geological theory, certain concepts that allowed the grouping of volcanoes in time, place, structure and composition have developed that ultimately have had to be explained in the theory of plate tectonics. For example, some volcanoes are polygenetic with more than one period of activity during their history; other volcanoes that become extinct after erupting exactly once are monogenetic (meaning \"one life\") and such volcanoes are often grouped together in a geographical region.[15] At the mid-ocean ridges, two tectonic plates diverge from one another as hot mantle rock creeps upwards beneath the thinned oceanic crust. The decrease of pressure in the rising mantle rock leads to adiabatic expansion and the partial melting of the rock, causing volcanism and creating new oceanic crust. Most divergent plate boundaries are at the bottom of the oceans, and so most volcanic activity on Earth is submarine, forming new seafloor. Black smokers (also known as deep sea vents) are evidence of this kind of volcanic activity. Where the mid-oceanic ridge is above sea level, volcanic islands are formed, such as Iceland.[16][3] Subduction zones are places where two plates, usually an oceanic plate and a continental plate, collide. The oceanic plate subducts (dives beneath the continental plate), forming a deep ocean trench just offshore. In a process called flux melting, water released from the subducting plate lowers the melting temperature of the overlying mantle wedge, thus creating magma. This magma tends to be extremely viscous because of its high silica content, so it often does not reach the surface but cools and solidifies at depth. When it does reach the surface, however, a volcano is formed. Thus subduction zones are bordered by chains of volcanoes called volcanic arcs. Typical examples are the volcanoes in the Pacific Ring of Fire, such as the Cascade Volcanoes or the Japanese Archipelago, or the eastern islands of Indonesia.[17][2] Hotspots are volcanic areas thought to be formed by mantle plumes, which are hypothesized to be columns of hot material rising from the core-mantle boundary. As with mid-ocean ridges, the rising mantle rock experiences decompression melting which generates large volumes of magma. Because tectonic plates move across mantle plumes, each volcano becomes inactive as it drifts off the plume, and new volcanoes are created where the plate advances over the plume. The Hawaiian Islands are thought to have been formed in such a manner, as has the Snake River Plain, with the Yellowstone Caldera being part of the North American plate currently above the Yellowstone hotspot.[18][4] However, the mantle plume hypothesis has been questioned.[19] Sustained upwelling of hot mantle rock can develop under the interior of a continent and lead to rifting. Early stages of rifting are characterized by flood basalts and may progress to the point where a tectonic plate is completely split.[20][21] A divergent plate boundary then develops between the two halves of the split plate. However, rifting often fails to completely split the continental lithosphere (such as in an aulacogen), and failed rifts are characterized by volcanoes that erupt unusual alkali lava or carbonatites. Examples include the volcanoes of the East African Rift.[22] A volcano needs a reservoir of molten magma (e.g. a magma chamber), a conduit to allow magma to rise through the crust, and a vent to allow the magma to escape above the surface as lava. The erupted volcanic material (lava and tephra) that is deposited around the vent is known as a volcanic edifice, typically a volcanic cone or mountain.[2][23] The most common perception of a volcano is of a conical mountain, spewing lava and poisonous gases from a crater at its summit; however, this describes just one of the many types of volcano. The features of volcanoes are varied. The structure and behaviour of volcanoes depend on several factors. Some volcanoes have rugged peaks formed by lava domes rather than a summit crater while others have landscape features such as massive plateaus. Vents that issue volcanic material (including lava and ash) and gases (mainly steam and magmatic gases) can develop anywhere on the landform and may give rise to smaller cones such as Pu\u02bbu \u02bb\u014c\u02bb\u014d on a flank of K\u012blauea in Hawaii. Volcanic craters are not always at the top of a mountain or hill and may be filled with lakes such as with Lake Taup\u014d in New Zealand. Some volcanoes can be low-relief landform features, with the potential to be hard to recognize as such and be obscured by geological processes.[2][24][25] Other types of volcano include mud volcanoes, which are structures often not associated with known magmatic activity; and cryovolcanoes (or ice volcanoes), particularly on some moons of Jupiter, Saturn, and Neptune. Active mud volcanoes tend to involve temperatures much lower than those of igneous volcanoes except when the mud volcano is actually a vent of an igneous volcano. Volcanic fissure vents are generally found at diverging plate boundaries, they are flat, linear fractures through which basaltic lava emerges. These kinds of volcanoes are non-explosive and the basaltic lava tends to have a low viscosity and solidifies slowly leading to a gentle sloping basaltic lava plateau. They often relate or constitute shield volcanoes[2][26] Shield volcanoes, so named for their broad, shield-like profiles, are formed by the eruption of low-viscosity basaltic or andesitic lava that can flow a great distance from a vent. They generally do not explode catastrophically but are characterized by relatively gentle effusive eruptions.[2] Since low-viscosity magma is typically low in silica, shield volcanoes are more common in oceanic than continental settings. The Hawaiian volcanic chain is a series of shield cones, and they are common in Iceland, as well.[26] Olympus Mons, an extinct martian shield volcano is the largest known volcano in the Solar System.[27] Lava domes, also called dome volcanoes, have steep convex sides built by slow eruptions of highly viscous lava, for example, rhyolite.[2] They are sometimes formed within the crater of a previous volcanic eruption, as in the case of Mount St. Helens, but can also form independently, as in the case of Lassen Peak. Like stratovolcanoes, they can produce violent, explosive eruptions, but the lava generally does not flow far from the originating vent. Cryptodomes are formed when viscous lava is forced upward causing the surface to bulge. The 1980 eruption of Mount St. Helens was an example; lava beneath the surface of the mountain created an upward bulge, which later collapsed down the north side of the mountain. Cinder cones result from eruptions of mostly small pieces of scoria and pyroclastics (both resemble cinders, hence the name of this volcano type) that build up around the vent. These can be relatively short-lived eruptions that produce a cone-shaped hill perhaps 30 to 400 metres (100 to 1,300\u00a0ft) high. Most cinder cones erupt only once and some may be found in monogenetic volcanic fields that may include other features that form when magma comes into contact with water such as maar explosion craters and tuff rings.[28] Cinder cones may form as flank vents on larger volcanoes, or occur on their own. Par\u00edcutin in Mexico and Sunset Crater in Arizona are examples of cinder cones. In New Mexico, Caja del Rio is a volcanic field of over 60 cinder cones. Based on satellite images, it has been suggested that cinder cones might occur on other terrestrial bodies in the Solar system too; on the surface of Mars and the Moon.[29][30][31][32] Stratovolcanoes are tall conical mountains composed of lava flows and tephra in alternate layers, the strata that gives rise to the name. They are also known as composite volcanoes because they are created from multiple structures during different kinds of eruptions; the main conduit bringing magma to the surface branches into multiple secondary conduits and occasional laccoliths or sills, the branching conduits may form parasitic cones on the flanks of the main cone.[2] Classic examples include Mount Fuji in Japan, Mayon Volcano in the Philippines, and Mount Vesuvius and Stromboli in Italy. Ash produced by the explosive eruption of stratovolcanoes has historically posed the greatest volcanic hazard to civilizations. The lavas of stratovolcanoes are higher in silica, and therefore much more viscous, than lavas from shield volcanoes. High-silica lavas also tend to contain more dissolved gas. The combination is deadly, promoting explosive eruptions that produce great quantities of ash, as well as pyroclastic surges like the one that destroyed the city of Saint-Pierre in Martinique in 1902. They are also steeper than shield volcanoes, with slopes of 30\u201335\u00b0 compared to slopes of generally 5\u201310\u00b0, and their loose tephra are material for dangerous lahars.[33] Large pieces of tephra are called volcanic bombs. Big bombs can measure more than 1.2 metres (4\u00a0ft) across and weigh several tons.[34] A supervolcano is defined as a volcano that has experienced one or more eruptions that produced over 1,000 cubic kilometres (240\u00a0cu\u00a0mi) of volcanic deposits in a single explosive event.[35] Such eruptions occur when a very large magma chamber full of gas-rich, silicic magma is emptied in a catastrophic caldera-forming eruption. Ash flow tuffs emplaced by such eruptions are the only volcanic product with volumes rivalling those of flood basalts.[36] Supervolcano eruptions, while the most dangerous type, are very rare; four are known from the last million years, and about 60 historical VEI 8 eruptions have been identified in the geologic record over millions of years. A supervolcano can produce devastation on a continental scale, and severely cool global temperatures for many years after the eruption due to the huge volumes of sulfur and ash released into the atmosphere. Because of the enormous area they cover, and subsequent concealment under vegetation and glacial deposits, supervolcanoes can be difficult to identify in the geologic record without careful geological mapping.[37] Known examples include Yellowstone Caldera in Yellowstone National Park and Valles Caldera in New Mexico (both western United States); Lake Taup\u014d in New Zealand; Lake Toba in Sumatra, Indonesia; and Ngorongoro Crater in Tanzania. Volcanoes that, though large, are not large enough to be called supervolcanoes, may also form calderas (collapsed crater) in the same way. There may be active or dormant cones inside of the caldera or even a lake, such lakes are called Volcanogenic lakes, or simply, volcanic lakes.[38][2] Submarine volcanoes are common features of the ocean floor. Volcanic activity during the Holocene Epoch has been documented at only 119 submarine volcanoes, but there may be more than one million geologically young submarine volcanoes on the ocean floor.[39][40] In shallow water, active volcanoes disclose their presence by blasting steam and rocky debris high above the ocean's surface. In the deep ocean basins, the tremendous weight of the water prevents the explosive release of steam and gases; however, submarine eruptions can be detected by hydrophones and by the discoloration of water because of volcanic gases. Pillow lava is a common eruptive product of submarine volcanoes and is characterized by thick sequences of discontinuous pillow-shaped masses which form underwater. Even large submarine eruptions may not disturb the ocean surface, due to the rapid cooling effect and increased buoyancy in water (as compared to air), which often causes volcanic vents to form steep pillars on the ocean floor. Hydrothermal vents are common near these volcanoes, and some support peculiar ecosystems based on chemotrophs feeding on dissolved minerals. Over time, the formations created by submarine volcanoes may become so large that they break the ocean surface as new islands or floating pumice rafts. In May and June 2018, a multitude of seismic signals were detected by earthquake monitoring agencies all over the world. They took the form of unusual humming sounds, and some of the signals detected in November of that year had a duration of up to 20 minutes. An oceanographic research campaign in May 2019 showed that the previously mysterious humming noises were caused by the formation of a submarine volcano off the coast of Mayotte.[41] Subglacial volcanoes develop underneath ice caps. They are made up of lava plateaus capping extensive pillow lavas and palagonite. These volcanoes are also called table mountains, tuyas,[42] or (in Iceland) mobergs.[43] Very good examples of this type of volcano can be seen in Iceland and in British Columbia. The origin of the term comes from Tuya Butte, which is one of the several tuyas in the area of the Tuya River and Tuya Range in northern British Columbia. Tuya Butte was the first such landform analysed and so its name has entered the geological literature for this kind of volcanic formation.[44] The Tuya Mountains Provincial Park was recently established to protect this unusual landscape, which lies north of Tuya Lake and south of the Jennings River near the boundary with the Yukon Territory. Hydrothermal features, for example geysers, fumaroles, mud pools, mud volcanoes, hot springs and acidic hot springs involve water as well as geothermal or magmatic activity. Such features are common around volcanoes and are often indicative of volcanism.[2][45] Mud volcanoes or mud domes are conical structures created by eruption of liquids and gases, particularly mud (slurries), water and gases, although several activities may contribute. The largest mud volcanoes are 10 kilometres (6.2\u00a0mi) in diameter and reach 700 metres (2,300\u00a0ft) high.[46][47] Mud volcanoes can be seen off the shore of Indonesia, on the island of Baratang, in Balochistan and in central Asia. Fumaroles are vents on the surface from which hot steam and volcanic gases erupt due to the presence of superheated groundwater, these may indicate volcanic activity. Fumaroles erupting sulfurous gases are also often called solfataras.[48][2] Geysers are springs which will occasionally erupt and discharge hot water and steam. Geysers may indicate ongoing magmatism, water underground is heated by hot rocks and steam pressure builds up before being released along with a jet of hot water. Almost half of all active geysers are present in Yellowstone National Park, US.[2][49] The material that is expelled in a volcanic eruption can be classified into three types: The concentrations of different volcanic gases can vary considerably from one volcano to the next. Water vapour is typically the most abundant volcanic gas, followed by carbon dioxide[53] and sulfur dioxide. Other principal volcanic gases include hydrogen sulfide, hydrogen chloride, and hydrogen fluoride. A large number of minor and trace gases are also found in volcanic emissions, for example hydrogen, carbon monoxide, halocarbons, organic compounds, and volatile metal chlorides. The form and style of an eruption of a volcano is largely determined by the composition of the lava it erupts. The viscosity (how fluid the lava is) and the amount of dissolved gas are the most important characteristics of magma, and both are largely determined by the amount of silica in the magma. Magma rich in silica is much more viscous than silica-poor magma, and silica-rich magma also tends to contain more dissolved gases. Lava can be broadly classified into four different compositions:[54] Mafic lava flows show two varieties of surface texture: \u02bbA\u02bba (pronounced [\u02c8\u0294a\u0294a]) and p\u0101hoehoe ([pa\u02d0\u02c8ho.e\u02c8ho.e]), both Hawaiian words. \u02bbA\u02bba is characterized by a rough, clinkery surface and is the typical texture of cooler basalt lava flows. P\u0101hoehoe is characterized by its smooth and often ropey or wrinkly surface and is generally formed from more fluid lava flows. P\u0101hoehoe flows are sometimes observed to transition to \u02bba\u02bba flows as they move away from the vent, but never the reverse.[68] More silicic lava flows take the form of block lava, where the flow is covered with angular, vesicle-poor blocks. Rhyolitic flows typically consist largely of obsidian.[69] Tephra is made when magma inside the volcano is blown apart by the rapid expansion of hot volcanic gases. Magma commonly explodes as the gas dissolved in it comes out of solution as the pressure decreases when it flows to the surface. These violent explosions produce particles of material that can then fly from the volcano. Solid particles smaller than 2\u00a0mm in diameter (sand-sized or smaller) are called volcanic ash.[51][52] Tephra and other volcaniclastics (shattered volcanic material) make up more of the volume of many volcanoes than do lava flows. Volcaniclastics may have contributed as much as a third of all sedimentation in the geologic record. The production of large volumes of tephra is characteristic of explosive volcanism.[70] Through natural processes, mainly erosion, so much of the solidified erupted material that makes up the mantle of a volcano may be stripped away that its inner anatomy becomes apparent. Using the metaphor of biological anatomy, such a process is called \"dissection\".[71] When the volcano is extinct, a plug forms on its vent, over time due to erosion, the volcanic cone slowly erodes away leaving the resistant lava plug intact.[2] Cinder Hill, a feature of Mount Bird on Ross Island, Antarctica, is a prominent example of a dissected volcano. Volcanoes that were, on a geological timescale, recently active, such as for example Mount Kaimon in southern Ky\u016bsh\u016b, Japan, tend to be undissected. Devils Tower in Wyoming is a famous example of exposed volcanic plug. As of December\u00a02022[update], the Smithsonian Institution's Global Volcanism Program database of volcanic eruptions in the Holocene Epoch (the last 11,700 years) lists 9,901 confirmed eruptions from 859 volcanoes. The database also lists 1,113 uncertain eruptions and 168 discredited eruptions for the same time interval.[72][73] Eruption styles are broadly divided into magmatic, phreatomagmatic (hydrovolcanic), and phreatic eruptions.[74] The intensity of explosive volcanism is expressed using the volcanic explosivity index (VEI), which ranges from 0 for Hawaiian-type eruptions to 8 for supervolcanic eruptions:[75][76] Volcanoes vary greatly in their level of activity, with individual volcanic systems having an eruption recurrence ranging from several times a year to once in tens of thousands of years.[77] Volcanoes are informally described as erupting, active, dormant, or extinct, but the definitions of these terms are not entirely uniform among volcanologists. The level of activity of most volcanoes falls upon a graduated spectrum, with much overlap between categories, and does not always fit neatly into only one of these three separate categories.[6] The USGS defines a volcano as \"erupting\" whenever the ejection of magma from any point on the volcano is visible, including visible magma still contained within the walls of the summit crater. While there is no international consensus among volcanologists on how to define an active volcano, the USGS defines a volcano as active whenever subterranean indicators, such as earthquake swarms, ground inflation, or unusually high levels of carbon dioxide or sulfur dioxide are present.[78][79] The USGS defines a dormant volcano as any volcano that is not showing any signs of unrest such as earthquake swarms, ground swelling, or excessive noxious gas emissions, but which shows signs that it could yet become active again.[79] Many dormant volcanoes have not erupted for thousands of years, but have still shown signs that they may be likely to erupt again in the future.[80][81] Technically, any volcano that is dormant is also considered to be geologically \"active\".[5] In an article justifying the re-classification of Alaska's Mount Edgecumbe volcano from \"dormant\" to \"active\", volcanologists at the Alaska Volcano Observatory pointed out that the term \"dormant\" in reference to volcanoes has been deprecated over the past few decades and that \"[t]he term \"dormant volcano\" is so little used and undefined in modern volcanology that the Encyclopedia of Volcanoes (2000) does not contain it in the glossaries or index\",[82] however the USGS still widely employs the term. Previously a volcano was often considered to be extinct if there were no written records of its activity. Such a generalization is inconsistent with observation and deeper study, as has occurred recently with the unexpected eruption of the Chait\u00e9n volcano in 2008.[83] Modern volcanic activity monitoring techniques, and improvements in the modelling of the factors that produce eruptions, have helped the understanding of why volcanoes may remain dormant for a long time, and then become unexpectedly active again. The potential for eruptions, and their style, depend mainly upon the state of the magma storage system under the volcano, the eruption trigger mechanism and its timescale.[84]:\u200a95\u200a For example, the Yellowstone volcano has a repose/recharge period of around 700,000 years, and Toba of around 380,000 years.[85] Vesuvius was described by Roman writers as having been covered with gardens and vineyards before its unexpected eruption of 79 CE, which destroyed the towns of Herculaneum and Pompeii. Accordingly, it can sometimes be difficult to distinguish between an extinct volcano and a dormant (inactive) one. Long volcano dormancy is known to decrease awareness.[84]:\u200a96\u200a Pinatubo was an inconspicuous volcano, unknown to most people in the surrounding areas, and initially not seismically monitored before its unanticipated and catastrophic eruption of 1991. Two other examples of volcanoes that were once thought to be extinct, before springing back into eruptive activity were the long-dormant Soufri\u00e8re Hills volcano on the island of Montserrat, thought to be extinct until activity resumed in 1995 (turning its capital Plymouth into a ghost town) and Fourpeaked Mountain in Alaska, which, before its September 2006 eruption, had not erupted since before 8000 BCE. Another example is the Taftan volcano in southwestern Iran. This volcano was long thought by volcanologists to be extinct, with its last eruption having occurred an estimated 710,000 years ago. However, beginning around June 2023, the volcano began experiencing uplifting near its summit, suggesting that the volcano was dormant.[86] Extinct volcanoes are those that scientists consider unlikely to erupt again because the volcano no longer has a magma supply. Examples of extinct volcanoes are many volcanoes on the Hawaiian\u2013Emperor seamount chain in the Pacific Ocean (although some volcanoes at the eastern end of the chain are active), Hohentwiel in Germany, Shiprock in New Mexico, U.S., Capulin in New Mexico, U.S., Zuidwal volcano in the Netherlands, and many volcanoes in Italy such as Monte Vulture. Edinburgh Castle in Scotland is located atop an extinct volcano, which forms Castle Rock. Whether a volcano is truly extinct is often difficult to determine. Since \"supervolcano\" calderas can have eruptive lifespans sometimes measured in millions of years, a caldera that has not produced an eruption in tens of thousands of years may be considered dormant instead of extinct. An individual volcano in a monogenetic volcanic field can be extinct, but that does not mean a completely new volcano might not erupt close by with little or no warning, as its field may have an active magma supply. The three common popular classifications of volcanoes can be subjective and some volcanoes thought to have been extinct have erupted again. To help prevent people from falsely believing they are not at risk when living on or near a volcano, countries have adopted new classifications to describe the various levels and stages of volcanic activity.[87] Some alert systems use different numbers or colours to designate the different stages. Other systems use colours and words. Some systems use a combination of both. The Decade Volcanoes are 16 volcanoes identified by the International Association of Volcanology and Chemistry of the Earth's Interior (IAVCEI) as being worthy of particular study in light of their history of large, destructive eruptions and proximity to populated areas. They are named Decade Volcanoes because the project was initiated as part of the United Nations-sponsored International Decade for Natural Disaster Reduction (the 1990s). The 16 current Decade Volcanoes are: The Deep Earth Carbon Degassing Project, an initiative of the Deep Carbon Observatory, monitors nine volcanoes, two of which are Decade volcanoes. The focus of the Deep Earth Carbon Degassing Project is to use Multi-Component Gas Analyzer System instruments to measure CO2/SO2 ratios in real-time and in high-resolution to allow detection of the pre-eruptive degassing of rising magmas, improving prediction of volcanic activity.[88] Volcanic eruptions pose a significant threat to human civilization. However, volcanic activity has also provided humans with important resources. There are many different types of volcanic eruptions and associated activity: phreatic eruptions (steam-generated eruptions), explosive eruptions of high-silica lava (e.g., rhyolite), effusive eruptions of low-silica lava (e.g., basalt), sector collapses, pyroclastic flows, lahars (debris flows) and volcanic gas emissions. These can pose a hazard to humans. Earthquakes, hot springs, fumaroles, mud pots and geysers often accompany volcanic activity. Volcanic gases can reach the stratosphere, where they form sulfuric acid aerosols that can reflect solar radiation and lower surface temperatures significantly.[89] Sulfur dioxide from the eruption of Huaynaputina may have caused the Russian famine of 1601\u20131603.[90] Chemical reactions of sulfate aerosols in the stratosphere can also damage the ozone layer, and acids such as hydrogen chloride (HCl) and hydrogen fluoride (HF) can fall to the ground as acid rain. Excessive fluoride salts from eruptions have poisoned livestock in Iceland on multiple occasions.[91]:\u200a39\u201358\u200a Explosive volcanic eruptions release the greenhouse gas carbon dioxide and thus provide a deep source of carbon for biogeochemical cycles.[92] Ash thrown into the air by eruptions can present a hazard to aircraft, especially jet aircraft where the particles can be melted by the high operating temperature; the melted particles then adhere to the turbine blades and alter their shape, disrupting the operation of the turbine. This can cause major disruptions to air travel. A volcanic winter is thought to have taken place around 70,000 years ago after the supereruption of Lake Toba on Sumatra island in Indonesia.[93] This may have created a population bottleneck that affected the genetic inheritance of all humans today.[94] Volcanic eruptions may have contributed to major extinction events, such as the End-Ordovician, Permian-Triassic, and Late Devonian mass extinctions.[95] The 1815 eruption of Mount Tambora created global climate anomalies that became known as the \"Year Without a Summer\" because of the effect on North American and European weather.[96] The freezing winter of 1740\u201341, which led to widespread famine in northern Europe, may also owe its origins to a volcanic eruption.[97] Although volcanic eruptions pose considerable hazards to humans, past volcanic activity has created important economic resources. Tuff formed from volcanic ash is a relatively soft rock, and it has been used for construction since ancient times.[98][99] The Romans often used tuff, which is abundant in Italy, for construction.[100] The Rapa Nui people used tuff to make most of the moai statues in Easter Island.[101] Volcanic ash and weathered basalt produce some of the most fertile soil in the world, rich in nutrients such as iron, magnesium, potassium, calcium, and phosphorus.[102] Volcanic activity is responsible for emplacing valuable mineral resources, such as metal ores.[102] It is accompanied by high rates of heat flow from Earth's interior. These can be tapped as geothermal power.[102] Tourism associated with volcanoes is also a worldwide industry.[103] Many volcanoes near human settlements are heavily monitored with the aim of providing adequate advance warnings of imminent eruptions to nearby populations. Also, a better modern-day understanding of volcanology has led to some better informed governmental and public responses to unanticipated volcanic activities. While the science of volcanology may not yet be capable of predicting the exact times and dates of eruptions far into the future, on suitably monitored volcanoes the monitoring of ongoing volcanic indicators is often capable of predicting imminent eruptions with advance warnings minimally of hours, and usually of days prior to any eruptions.[104] The diversity of volcanoes and their complexities mean that eruption forecasts for the foreseeable future will be based on probability, and the application of risk management. Even then, some eruptions will have no useful warning. An example of this occurred in March 2017, when a tourist group was witnessing a presumed to be predictable Mount Etna eruption and the flowing lava came in contact with a snow accumulation causing a situational phreatic explosion causing injury to ten persons.[103] Other types of significant eruptions are known to give useful warnings of only hours at the most by seismic monitoring.[83] The recent demonstration of a magma chamber with repose times of tens of thousands of years, with potential for rapid recharge so potentially decreasing warning times, under the youngest volcano in central Europe,[84] does not tell us if more careful monitoring will be useful. Scientists are known to perceive risk, with its social elements, differently from local populations and those that undertake social risk assessments on their behalf, so that both disruptive false alarms and retrospective blame, when disasters occur, will continue to happen.[105]:\u200a1\u20133 Thus in many cases, while volcanic eruptions may still cause major property destruction, the periodic large-scale loss of human life that was once associated with many volcanic eruptions, has recently been significantly reduced in areas where volcanoes are adequately monitored. This life-saving ability is derived via such volcanic-activity monitoring programs, through the greater abilities of local officials to facilitate timely evacuations based upon the greater modern-day knowledge of volcanism that is now available, and upon improved communications technologies such as cell phones. Such operations tend to provide enough time for humans to escape at least with their lives before a pending eruption. One example of such a recent successful volcanic evacuation was the Mount Pinatubo evacuation of 1991. This evacuation is believed to have saved 20,000 lives.[106] In the case of Mount Etna, a 2021 review found 77 deaths due to eruptions since 1536 but none since 1987.[103] Citizens who may be concerned about their own exposure to risk from nearby volcanic activity should familiarize themselves with the types of, and quality of, volcano monitoring and public notification procedures being employed by governmental authorities in their areas.[107] Earth's Moon has no large volcanoes and no current volcanic activity, although recent evidence suggests it may still possess a partially molten core.[108] However, the Moon does have many volcanic features such as maria[109] (the darker patches seen on the Moon), rilles[110] and domes.[111] The planet Venus has a surface that is 90% basalt, indicating that volcanism played a major role in shaping its surface. The planet may have had a major global resurfacing event about 500 million years ago,[112] from what scientists can tell from the density of impact craters on the surface. Lava flows are widespread and forms of volcanism not present on Earth occur as well. Changes in the planet's atmosphere and observations of lightning have been attributed to ongoing volcanic eruptions, although there is no confirmation of whether or not Venus is still volcanically active. However, radar sounding by the Magellan probe revealed evidence for comparatively recent volcanic activity at Venus's highest volcano Maat Mons, in the form of ash flows near the summit and on the northern flank.[113] However, the interpretation of the flows as ash flows has been questioned.[114] There are several extinct volcanoes on Mars, four of which are vast shield volcanoes far bigger than any on Earth. They include Arsia Mons, Ascraeus Mons, Hecates Tholus, Olympus Mons, and Pavonis Mons. These volcanoes have been extinct for many millions of years,[115] but the European Mars Express spacecraft has found evidence that volcanic activity may have occurred on Mars in the recent past as well.[115] Jupiter's moon Io is the most volcanically active object in the Solar System because of tidal interaction with Jupiter. It is covered with volcanoes that erupt sulfur, sulfur dioxide and silicate rock, and as a result, Io is constantly being resurfaced. Its lavas are the hottest known anywhere in the Solar System, with temperatures exceeding 1,800 K (1,500\u00a0\u00b0C). In February 2001, the largest recorded volcanic eruptions in the Solar System occurred on Io.[116] Europa, the smallest of Jupiter's Galilean moons, also appears to have an active volcanic system, except that its volcanic activity is entirely in the form of water, which freezes into ice on the frigid surface. This process is known as cryovolcanism, and is apparently most common on the moons of the outer planets of the Solar System.[117] In 1989, the Voyager 2 spacecraft observed cryovolcanoes (ice volcanoes) on Triton, a moon of Neptune, and in 2005 the Cassini\u2013Huygens probe photographed fountains of frozen particles erupting from Enceladus, a moon of Saturn.[118][119] The ejecta may be composed of water, liquid nitrogen, ammonia, dust, or methane compounds. Cassini\u2013Huygens also found evidence of a methane-spewing cryovolcano on the Saturnian moon Titan, which is believed to be a significant source of the methane found in its atmosphere.[120] It is theorized that cryovolcanism may also be present on the Kuiper Belt Object Quaoar. A 2010 study of the exoplanet COROT-7b, which was detected by transit in 2009, suggested that tidal heating from the host star very close to the planet and neighbouring planets could generate intense volcanic activity similar to that found on Io.[121] Volcanoes are not distributed evenly over the Earth's surface but active ones with significant impact were encountered early in human history, evidenced by footprints of hominina found in East African volcanic ash dated at 3.66 million years old.[122]:\u200a104\u200a The association of volcanoes with fire and disaster is found in many oral traditions and had religious and thus social significance before the first written record of concepts related to volcanoes. Examples are: (1) the stories in the Athabascan subcultures about humans living inside mountains and a woman who uses fire to escape from a mountain,[123]:\u200a135\u200a (2) Pele's migration through the Hawarian island chain, ability to destroy forests and manifestations of the god's temper,[124] and (3) the association in Javanese folklore of a king resident in Mount Merapi volcano and a queen resident at a beach 50\u00a0km (31\u00a0mi) away on what is now known to be an earthquake fault that interacts with that volcano.[125] Many ancient accounts ascribe volcanic eruptions to supernatural causes, such as the actions of gods or demigods. The earliest known such example is a neolithic goddess at \u00c7atalh\u00f6y\u00fck.[126]:\u200a203\u200a The Ancient Greek god Hephaistos and the concepts of the underworld are aligned to volcanoes in that Greek culture.[103] However, others proposed more natural (but still incorrect) causes of volcanic activity. In the fifth century BC, Anaxagoras proposed eruptions were caused by a great wind.[127] By 65\u00a0CE, Seneca the Younger proposed combustion as the cause,[127] an idea also adopted by the Jesuit Athanasius Kircher (1602\u20131680), who witnessed eruptions of Mount Etna and Stromboli, then visited the crater of Vesuvius and published his view of an Earth in Mundus Subterraneus with a central fire connected to numerous others depicting volcanoes as a type of safety valve.[128] Edward Jorden, in his work on mineral waters, challenged this view; in 1632 he proposed sulfur \"fermentation\" as a heat source within Earth,[127] Astronomer Johannes Kepler (1571\u20131630) believed volcanoes were ducts for Earth's tears.[129][better\u00a0source\u00a0needed] In 1650, Ren\u00e9 Descartes proposed the core of Earth was incandescent and, by 1785, the works of Decartes and others were synthesized into geology by James Hutton in his writings about igneous intrusions of magma.[127] Lazzaro Spallanzani had demonstrated by 1794 that steam explosions could cause explosive eruptions and many geologists held this as the universal cause of explosive eruptions up to the 1886 eruption of Mount Tarawera which allowed in one event differentiation of the concurrent phreatomagmatic and hydrothermal eruptions from dry explosive eruption, of, as it turned out, a basalt dyke.[130]:\u200a16\u201318\u200a[131]:\u200a4\u200a Alfred Lacroix built upon his other knowledge with his studies on the 1902 eruption of Mount Pel\u00e9e,[127] and by 1928 Arthur Holmes work had brought together the concepts of radioactive generation of heat, Earth's mantle structure, partial decompression melting of magma, and magma convection.[127] This eventually led to the acceptance of plate tectonics.[132]",
      "ground_truth_chunk_ids": [
        "54_fixed_chunk1",
        "73_fixed_chunk1"
      ],
      "source_ids": [
        "S054",
        "S073"
      ],
      "category": "comparative",
      "id": 53
    },
    {
      "question": "Compare List of storms named Brenda and The Konstantinos Staikos' book collection in one sentence each: what does each describe or study?",
      "ground_truth": "List of storms named Brenda: The name Brenda has been used for nine tropical cyclones worldwide, including five in the Atlantic Ocean. In the Atlantic: In the Western Pacific Ocean: In the South-West Indian: In the Australian region: The Konstantinos Staikos' book collection: The book collection of Konstantinos Staikos is now part of the Alexander S. Onassis Public Benefit Foundation Library[1][2] It is centered on the intellectual, printing and publishing activity of the Greeks from the Fall of Constantinople in 1453 to the late 19th century. The aim of its creation was to collect and present relevant material from that time period. The genesis of the book collection dates from the 1970s. The bibliophilic interests of Konstantinos Staikos changed radically. In those years also, the Hellenic Bibliophile Society was established [3] under the Honorary Presidency of Constantinos Tsatsos. The exhibitions of books of the Society (1975) with travellers' accounts: 'Travellers in Greece from the fifteenth century to 1821', or with printed material regarding the chronicle of Greek typography: 'Outset of Greek typography' (1976) radically altered Konstantinos Staikos interests as collector and from then on he consciously turned to the study and research of the pioneers of Greek printing and the relations they cultivated with the world of books in Venice and elsewhere. His acquaintance with Georgios Ladas, who was profoundly conscious of the role played by printed books during the Ottoman domination and who collected and documented the bibliographic identity of an enormous number of books that came into his hands, empowered Konstantinos Staikos' intention to explore the chronicle of Greek typography in greater depth. The initial approach was to record printers' marks and emblems characterizing printed Greek books, resulting in the planning of the Charta of Greek Printing. At the same time the collection began to take shape, with the purchase of books entirely compatible with the terms regulating the Hellenic Bibliography as recorded by \u00c9. Legrand, printed material, that is to say, testifying to the pains and labours of the printing workshops. From 1986 the most representative body of the Konstantinos",
      "expected_answer": "List of storms named Brenda: The name Brenda has been used for nine tropical cyclones worldwide, including five in the Atlantic Ocean. In the Atlantic: In the Western Pacific Ocean: In the South-West Indian: In the Australian region: The Konstantinos Staikos' book collection: The book collection of Konstantinos Staikos is now part of the Alexander S. Onassis Public Benefit Foundation Library[1][2] It is centered on the intellectual, printing and publishing activity of the Greeks from the Fall of Constantinople in 1453 to the late 19th century. The aim of its creation was to collect and present relevant material from that time period. The genesis of the book collection dates from the 1970s. The bibliophilic interests of Konstantinos Staikos changed radically. In those years also, the Hellenic Bibliophile Society was established [3] under the Honorary Presidency of Constantinos Tsatsos. The exhibitions of books of the Society (1975) with travellers' accounts: 'Travellers in Greece from the fifteenth century to 1821', or with printed material regarding the chronicle of Greek typography: 'Outset of Greek typography' (1976) radically altered Konstantinos Staikos interests as collector and from then on he consciously turned to the study and research of the pioneers of Greek printing and the relations they cultivated with the world of books in Venice and elsewhere. His acquaintance with Georgios Ladas, who was profoundly conscious of the role played by printed books during the Ottoman domination and who collected and documented the bibliographic identity of an enormous number of books that came into his hands, empowered Konstantinos Staikos' intention to explore the chronicle of Greek typography in greater depth. The initial approach was to record printers' marks and emblems characterizing printed Greek books, resulting in the planning of the Charta of Greek Printing. At the same time the collection began to take shape, with the purchase of books entirely compatible with the terms regulating the Hellenic Bibliography as recorded by \u00c9. Legrand, printed material, that is to say, testifying to the pains and labours of the printing workshops. From 1986 the most representative body of the Konstantinos Staikos collection, covering the works and the days of Greek scholars and printers active in the period of the Italian Renaissance (late fourteenth \u2013 mid-sixteenth centuries) became the object of exhibitions for the promotion of their work. First editions by Manuel Chrysoloras, George of Trebizond, Cardinal Bessarion, Theodoros Gazis, Zacharias Kallierges, Nikolaos Vlastos and numerous others were presented successively in Florence (1986); the Benaki Museum (1987); Geneva University (1988); Strasburg (1989) and elsewhere. These exhibitions were accompanied by detailed bilingual catalogues, compiled in collaboration with M.I. Manoussakas, with introductory notes and extensive commentaries for each book. The ultimate goal of these exhibitions was the promotion of the inestimable and decisive contribution of the Greek scholars of the period to the diffusion of Greek letters and to demonstrate: the relations they cultivated with the supreme Humanists of Italy, many of whom had been their pupils. Examples from the collection were exhibited at the Hellenic Institute of Byzantine and Post-Byzantine Studies in Venice in 1993, with landmark editions by Aldus Manutius, the products of literary editors by renowned Greek scholars such as Marcus Musurus and Ioannes Gregoropoulos. In Austria, at Vienna's Imperial Library nearly all the Greek books published/printed there (1749\u20131800) were exhibited, which were the most significant examples of the Neohellenic Enlightenment. In celebration of the Five Hundred Years since the establishment of the first Greek printing press (Venice 1499), the Greek Parliament Foundation assigned to Triantafyllos Sklavenitis and Konstantinos Staikos the organization of an exhibition of the most important material of the whole period: a considerable number of incunables and printed material deriving for the greater part from his library. In 2010 the Collection was acquired by the Onassis Foundation in order to be preserved as perpetual property of the Greek Nation.",
      "ground_truth_chunk_ids": [
        "116_random_chunk1",
        "10_random_chunk1"
      ],
      "source_ids": [
        "S316",
        "S210"
      ],
      "category": "comparative",
      "id": 54
    },
    {
      "question": "Compare Mineral and Funeral in one sentence each: what does each describe or study?",
      "ground_truth": "Mineral: In geology and mineralogy, a mineral or mineral species is, broadly speaking, a solid substance with a fairly well-defined chemical composition and a specific crystal structure that occurs naturally in pure form.[1][2] The geological definition of mineral normally excludes compounds that occur only in living organisms. However, some minerals are often biogenic (such as calcite) or chemically organic compounds (such as mellite). Moreover, living organisms often synthesize inorganic minerals (such as hydroxylapatite) that also occur in rocks. The concept of mineral is distinct from rock, which is any bulk solid geologic material that is relatively homogeneous at a large enough scale. A rock may consist of one type of mineral or may be an aggregate of two or more different types of minerals, spacially segregated into distinct phases.[3] Some natural solid substances without a definite crystalline structure, such as opal or obsidian, are more properly called mineraloids.[4] If a chemical compound occurs naturally with different crystal structures, each structure is considered a different mineral species. Thus, for example, quartz and stishovite are two different minerals consisting of the same compound, silicon dioxide. The International Mineralogical Association (IMA) is the generally recognized standard body for the definition and nomenclature of mineral species. As of May 2025[update], the IMA recognizes 6,145 official mineral species.[5] The chemical composition of a named mineral species may vary somewhat because the inclusion of small amounts of impurities. Specific varieties of a species sometimes have conventional or official names of their own.[6] For example, amethyst is a purple variety of the mineral species quartz. Some mineral species can have variable proportions of two or more chemical elements that occupy equivalent positions in the mineral's structure; for example, the formula of mackinawite is given as (Fe,Ni)9S8, meaning FexNi9-xS8, where x is a variable number between 0 and 9. Funeral: A funeral is a ceremony connected with the final disposition of a corpse, such as a burial, entombment or cremation with the attendant observances.[1] Funerary customs comprise the complex of beliefs and practices used by a culture to remember and respect the dead, from interment, to various monuments, prayers, and rituals undertaken in their honour. Customs vary between cultures and religious groups. Funerals have both normative and legal components. Common secular motivations for funerals include mourning the deceased, celebrating their life, and offering support and sympathy to the bereaved; additionally, funerals may have religious aspects that are intended to help the soul of the deceased reach the afterlife, resurrection or reincarnation. The funeral usually includes a ritual through which the corpse receives a final disposition.[2] Depending on culture and religion, these can involve either the destruction of the body (for example, by cremation, sky burial, decomposition, disintegration or dissolution) or its preservation (for example, by mummification). Differing beliefs about cleanliness and the relationship between body and soul are reflected in funerary practices. A memorial service (service of remembrance or celebration of life) is a funerary ceremony that is performed without the remains of the deceased person.[3] In both a closed casket funeral[4] and a memorial service, photos of the deceased representing stages of life would be displayed on an altar. Relatives or friends would give out eulogies in both services as well.[5] The word funeral comes from the Latin funus, which had a variety of meanings, including the corpse and the funerary rites themselves. Funerary art is art produced in connection with burials, including many kinds of tombs, and objects specially made for burial like flowers with a corpse. Funeral rites pre-date modern Homo sapiens and dated to at least 300,000 years ago.[6] For example, in the Shanidar Cave in",
      "expected_answer": "Mineral: In geology and mineralogy, a mineral or mineral species is, broadly speaking, a solid substance with a fairly well-defined chemical composition and a specific crystal structure that occurs naturally in pure form.[1][2] The geological definition of mineral normally excludes compounds that occur only in living organisms. However, some minerals are often biogenic (such as calcite) or chemically organic compounds (such as mellite). Moreover, living organisms often synthesize inorganic minerals (such as hydroxylapatite) that also occur in rocks. The concept of mineral is distinct from rock, which is any bulk solid geologic material that is relatively homogeneous at a large enough scale. A rock may consist of one type of mineral or may be an aggregate of two or more different types of minerals, spacially segregated into distinct phases.[3] Some natural solid substances without a definite crystalline structure, such as opal or obsidian, are more properly called mineraloids.[4] If a chemical compound occurs naturally with different crystal structures, each structure is considered a different mineral species. Thus, for example, quartz and stishovite are two different minerals consisting of the same compound, silicon dioxide. The International Mineralogical Association (IMA) is the generally recognized standard body for the definition and nomenclature of mineral species. As of May\u00a02025[update], the IMA recognizes 6,145 official mineral species.[5] The chemical composition of a named mineral species may vary somewhat because the inclusion of small amounts of impurities.  Specific varieties of a species sometimes have conventional or official names of their own.[6] For example, amethyst is a purple variety of the mineral species quartz.  Some mineral species can have variable proportions of two or more chemical elements that occupy equivalent positions in the mineral's structure; for example, the formula of mackinawite is given as (Fe,Ni)9S8, meaning FexNi9-xS8, where x is a variable number between 0 and 9.  Sometimes a mineral with variable composition is split into separate species, more or less arbitrarily, forming a mineral group; that is the case of the silicates CaxMgyFe2-x-ySiO4, the olivine group. Besides the essential chemical composition and crystal structure, the description of a mineral species usually includes its common physical properties such as  habit, hardness, lustre, diaphaneity, colour, streak, tenacity, cleavage, fracture, system, zoning, parting, specific gravity, magnetism, fluorescence, radioactivity, as well as its taste or smell and its reaction to acid.[7][8] Minerals are classified by key chemical constituents; the two dominant systems are the Dana classification and the Strunz classification. Silicate minerals comprise approximately 90% of the Earth's crust.[9][10] Other important mineral groups include the native elements (made up of a single pure element) and compounds (combinations of multiple elements) namely sulfides (e.g. Galena PbS), oxides (e.g. quartz SiO2), halides (e.g. rock salt NaCl), carbonates (e.g. calcite CaCO3), sulfates (e.g. gypsum CaSO4\u00b72H2O), silicates (e.g. orthoclase KAlSi3O8), molybdates (e.g. wulfenite PbMoO4) and phosphates (e.g. pyromorphite Pb5(PO4)3Cl).[7] The International Mineralogical Association has established the following requirements for a substance to be considered a distinct mineral:[11][12] The details of these rules are somewhat controversial.[15] For instance, there have been several recent proposals to classify amorphous substances as minerals, but they have not been accepted by the IMA. The IMA is also reluctant to accept minerals that occur naturally only in the form of nanoparticles a few hundred atoms across, but has not defined a minimum crystal size.[11] Some authors require the material to be a stable or metastable solid at room temperature (25\u00a0\u00b0C).[15]  However, the IMA only requires that the substance be stable enough for its structure and composition to be well-determined. For example, it recognizes meridianiite (a naturally occurring hydrate of magnesium sulfate) as a mineral, even though it is formed and stable only below 2\u00a0\u00b0C. As of May\u00a02025[update], 6,145 mineral species are approved by the IMA.[5] They are most commonly named after a person, followed by discovery location; names based on chemical composition or physical properties are the two other major groups of mineral name etymologies.[18][19]  Most names end in \"-ite\"; the exceptions are usually names that were well-established before the organization of mineralogy as a discipline, for example galena and diamond. A topic of contention among geologists and mineralogists has been the IMA's decision to exclude biogenic crystalline substances. For example, Lowenstam (1981) stated that \"organisms are capable of forming a diverse array of minerals, some of which cannot be formed inorganically in the biosphere.\"[20] Skinner (2005) views all solids as potential minerals and includes biominerals in the mineral kingdom, which are those that are created by the metabolic activities of organisms. Skinner expanded the previous definition of a mineral to classify \"element or compound, amorphous or crystalline, formed through biogeochemical  processes,\" as a mineral.[21] Recent advances in high-resolution genetics and X-ray absorption spectroscopy are providing revelations on the biogeochemical relations between microorganisms and minerals that may shed new light on this question.[12][21] For example, the IMA-commissioned \"Working Group on Environmental Mineralogy and Geochemistry \" deals with minerals in the hydrosphere, atmosphere, and biosphere.[22] The group's scope includes mineral-forming microorganisms, which exist on nearly every rock, soil, and particle surface spanning the globe to depths of at least 1600 metres below the sea floor and 70 kilometres into the stratosphere (possibly entering the mesosphere).[23][24][25] Biogeochemical cycles have contributed to the formation of minerals for billions of years. Microorganisms can precipitate metals from solution, contributing to the formation of ore deposits. They can also catalyze the dissolution of minerals.[26][27][28] Prior to the International Mineralogical Association's listing, over 60 biominerals had been discovered, named, and published.[29] These minerals (a sub-set tabulated in Lowenstam (1981)[20]) are considered minerals proper according to Skinner's (2005) definition.[21] These biominerals are not listed in the International Mineral Association official list of mineral names;[30]  however, many of these biomineral representatives are distributed amongst the 78 mineral classes listed in the Dana classification scheme.[21] Skinner's (2005) definition of a mineral takes this matter into account by stating that a mineral can be crystalline or amorphous.[21] Although biominerals are not the most common form of minerals,[31] they help to define the limits of what constitutes a mineral proper. Nickel's (1995) formal definition explicitly mentioned crystallinity as a key to defining a substance as a mineral. A 2011 article defined icosahedrite, an aluminium-iron-copper alloy, as a mineral; named for its unique natural icosahedral symmetry, it is a quasicrystal. Unlike a true crystal, quasicrystals are ordered but not periodic.[32][33] A mineral assemblage is defined by Mindat.org as \"Any set of minerals in a rock, whether in [chemical] equilibrium or not\",[34] while Encyclopaedia Britannica says \"The term assemblage is frequently applied to all minerals included in a rock but more appropriately should be used for those minerals that are in equilibrium (and are known more specifically as the equilibrium assemblage)\".[35] The term is often prefixed by other terms that describe its formation.[34] A rock is an aggregate of one or more minerals[36] or mineraloids. Some rocks, such as limestone or quartzite, are composed primarily of one mineral\u00a0\u2013 calcite or aragonite in the case of limestone, and quartz in the latter case.[37][38] Other rocks can be defined by relative abundances of key (essential) minerals; a granite is defined by proportions of quartz, alkali feldspar, and plagioclase feldspar.[39] The other minerals in the rock are termed accessory minerals, and do not greatly affect the bulk composition of the rock. Rocks can also be composed entirely of non-mineral material; coal is a sedimentary rock composed primarily of organically derived carbon.[36][40] In rocks, some mineral species and groups are much more abundant than others; these are termed the rock-forming minerals. The major examples of these are quartz, the feldspars, the micas, the amphiboles, the pyroxenes, the olivines, and calcite; except for the last one, all of these minerals are silicates.[41] Overall, around 150 minerals are considered particularly important, whether in terms of their abundance or aesthetic value in terms of collecting.[42] Commercially valuable minerals and rocks, other than gemstones, metal ores, or mineral fuels, are referred to as industrial minerals.[43] For example, muscovite, a white mica, can be used for windows (sometimes referred to as isinglass), as a filler, or as an insulator.[44] Ores are minerals that have a high concentration of a certain element, typically a metal. Examples are cinnabar (HgS), an ore of mercury; sphalerite (ZnS), an ore of zinc; cassiterite (SnO2), an ore of tin; and colemanite, an ore of boron. Gems are minerals with an ornamental value, and are distinguished from non-gems by their beauty, durability, and usually, rarity. There are about 20 mineral species that qualify as gem minerals, which constitute about 35 of the most common gemstones. Gem minerals are often present in several varieties, and so one mineral can account for several different gemstones; for example, ruby and sapphire are both corundum, Al2O3.[45] The first known use of the word \"mineral\" in the English language (Middle English) was the 15th century.  The word came from Medieval Latin: minerale, from minera, mine, ore.[46] The word \"species\" comes from the Latin species, \"a particular sort, kind, or type with distinct look, or appearance\".[47] The abundance and diversity of minerals is controlled directly by their chemistry, in turn dependent on elemental abundances in the Earth. The majority of minerals observed are derived from the Earth's crust. Eight elements account for most of the key components of minerals, due to their abundance in the crust. These eight elements, summing to over 98% of the crust by weight, are, in order of decreasing abundance: oxygen, silicon, aluminium, iron, magnesium, calcium, sodium and potassium. Oxygen and silicon are by far the two most important\u00a0\u2013 oxygen composes 47% of the crust by weight, and silicon accounts for 28%.[48] The minerals that form are those that are most stable at the temperature and pressure of formation, within the limits imposed by the bulk chemistry of the parent body.[49] For example, in most igneous rocks, the aluminium and alkali metals (sodium and potassium) that are present are  primarily found in combination with oxygen, silicon, and calcium as feldspar minerals. However, if the rock is unusually rich in alkali metals, there will not be enough aluminium to combine with all the sodium as feldspar, and the excess sodium will form sodic amphiboles such as riebeckite. If the aluminium abundance is unusually high, the excess aluminium will form muscovite or other aluminium-rich minerals.[50] If silicon is deficient, part of the feldspar will be replaced by feldspathoid minerals.[51] Precise predictions of which minerals will be present in a rock of a particular composition formed at a particular temperature and pressure requires complex thermodynamic calculations. However, approximate estimates may be made using relatively simple rules of thumb, such as the CIPW norm, which gives reasonable estimates for volcanic rock formed from dry magma.[52] The chemical composition may vary between end member species of a solid solution series. For example, the plagioclase feldspars comprise a continuous series from sodium-rich end member albite (NaAlSi3O8) to calcium-rich anorthite (CaAl2Si2O8) with four recognized intermediate varieties between them (given in order from sodium- to calcium-rich): oligoclase, andesine, labradorite, and bytownite.[53] Other examples of series include the olivine series of magnesium-rich forsterite and iron-rich fayalite, and the wolframite series of manganese-rich h\u00fcbnerite and iron-rich ferberite.[54] Chemical substitution and coordination polyhedra explain this common feature of minerals. In nature, minerals are not pure substances, and are contaminated by whatever other elements are present in the given chemical system. As a result, it is possible for one element to be substituted for another.[55] Chemical substitution will occur between ions of a similar size and charge; for example, K+ will not substitute for Si4+ because of chemical and structural incompatibilities caused by a big difference in size and charge. A common example of chemical substitution is that of Si4+ by Al3+, which are close in charge, size, and abundance in the crust. In the example of plagioclase, there are three cases of substitution. Feldspars are all framework silicates, which have a silicon-oxygen ratio of 2:1, and the space for other elements is given by the substitution of Si4+ by Al3+ to give a base unit of [AlSi3O8]\u2212; without the substitution, the formula would be charge-balanced as SiO2, giving quartz.[56] The significance of this structural property will be explained further by coordination polyhedra. The second substitution occurs between Na+ and Ca2+; however, the difference in charge has to accounted for by making a second substitution of Si4+ by Al3+.[57] Coordination polyhedra are geometric representations of how a cation is surrounded by an anion. In mineralogy, coordination polyhedra are usually considered in terms of oxygen, due its abundance in the crust. The base unit of silicate minerals is the silica tetrahedron\u00a0\u2013 one Si4+ surrounded by four O2\u2212. An alternate way of describing the coordination of the silicate is by a number: in the case of the silica tetrahedron, the silicon is said to have a coordination number of 4. Various cations have a specific range of possible coordination numbers; for silicon, it is almost always 4, except for very high-pressure minerals where the compound is compressed such that silicon is in six-fold (octahedral) coordination with oxygen. Bigger cations have a bigger coordination numbers because of the increase in relative size as compared to oxygen (the last orbital subshell of heavier atoms is different too). Changes in coordination numbers leads to physical and mineralogical differences; for example, at high pressure, such as in the mantle, many minerals, especially silicates such as olivine and garnet, will change to a perovskite structure, where silicon is in octahedral coordination. Other examples are the aluminosilicates kyanite, andalusite, and sillimanite (polymorphs, since they share the formula Al2SiO5), which differ by the coordination number of the Al3+; these minerals transition from one another as a response to changes in pressure and temperature.[48] In the case of silicate materials, the substitution of Si4+ by Al3+ allows for a variety of minerals because of the need to balance charges.[58] Because the eight most common elements make up over 98% of the Earth's crust, the small quantities of the other elements that are typically present are substituted into the common rock-forming minerals. The distinctive minerals of most elements are quite rare, being found only where these elements have been concentrated by geological processes, such as hydrothermal circulation, to the point where they can no longer be accommodated in common minerals.[59] Changes in temperature and pressure and composition alter the mineralogy of a rock sample. Changes in composition can be caused by processes such as weathering or metasomatism (hydrothermal alteration). Changes in temperature and pressure occur when the host rock undergoes tectonic or magmatic movement into differing physical regimes. Changes in thermodynamic conditions make it favourable for mineral assemblages to react with each other to produce new minerals; as such, it is possible for two rocks to have an identical or a very similar bulk rock chemistry without having a similar mineralogy. This process of mineralogical alteration is related to the rock cycle. An example of a series of mineral reactions is illustrated as follows.[60] Orthoclase feldspar (KAlSi3O8) is a mineral commonly found in granite, a plutonic igneous rock. When exposed to weathering, it reacts to form kaolinite (Al2Si2O5(OH)4, a sedimentary mineral, and silicic acid): Under low-grade metamorphic conditions, kaolinite reacts with quartz to form pyrophyllite (Al2Si4O10(OH)2): As metamorphic grade increases, the pyrophyllite reacts to form kyanite and quartz: Alternatively, a mineral may change its crystal structure as a consequence of changes in temperature and pressure without reacting. For example, quartz will change into a variety of its SiO2 polymorphs, such as tridymite and cristobalite at high temperatures, and coesite at high pressures.[61] Classifying minerals ranges from simple to difficult. A mineral can be identified by several physical properties, some of them being sufficient for full identification without equivocation. In other cases, minerals can only be classified by more complex optical, chemical or X-ray diffraction analysis; these methods, however, can be costly and time-consuming.  Physical properties applied for classification include crystal structure and habit, hardness, lustre, diaphaneity, colour, streak, cleavage and fracture, and specific gravity. Other less general tests include fluorescence, phosphorescence, magnetism, radioactivity, tenacity (response to mechanical induced changes of shape or form), piezoelectricity and reactivity to dilute acids.[62] Crystal structure results from the orderly geometric spatial arrangement of atoms in the internal structure of a mineral. This crystal structure is based on regular internal atomic or ionic arrangement that is often expressed in the geometric form that the crystal takes. Even when the mineral grains are too small to see or are irregularly shaped, the underlying crystal structure is always periodic and can be determined by X-ray diffraction.[15] Minerals are typically described by their symmetry content. Crystals are restricted to 32 point groups, which differ by their symmetry. These groups are classified in turn into more broad categories, the most encompassing of these being the six crystal families.[63] These families can be described by the relative lengths of the three crystallographic axes, and the angles between them; these relationships correspond to the symmetry operations that define the narrower point groups. They are summarized below; a, b, and c represent the axes, and \u03b1, \u03b2, \u03b3 represent the angle opposite the respective crystallographic axis (e.g. \u03b1 is the angle opposite the a-axis, viz. the angle between the b and c axes):[63] The hexagonal crystal family is also split into two crystal systems\u00a0\u2013 the trigonal, which has a three-fold axis of symmetry, and the hexagonal, which has a six-fold axis of symmetry. Chemistry and crystal structure together define a mineral. With a restriction to 32 point groups, minerals of different chemistry may have identical crystal structure. For example, halite (NaCl), galena (PbS), and periclase (MgO) all belong to the hexaoctahedral point group (isometric family), as they have a similar stoichiometry between their different constituent elements. In contrast, polymorphs are groupings of minerals that share a chemical formula but have a different structure. For example, pyrite and marcasite, both iron sulfides, have the formula FeS2; however, the former is isometric while the latter is orthorhombic. This polymorphism extends to other sulfides with the generic AX2 formula; these two groups are collectively known as the pyrite and marcasite groups.[64] Polymorphism can extend beyond pure symmetry content. The aluminosilicates are a group of three minerals\u00a0\u2013 kyanite, andalusite, and sillimanite\u00a0\u2013 which share the chemical formula Al2SiO5. Kyanite is triclinic, while andalusite and sillimanite are both orthorhombic and belong to the dipyramidal point group. These differences arise corresponding to how aluminium is coordinated within the crystal structure. In all minerals, one aluminium ion is always in six-fold coordination with oxygen. Silicon, as a general rule, is in four-fold coordination in all minerals; an exception is a case like stishovite (SiO2, an ultra-high pressure quartz polymorph with rutile structure).[65] In kyanite, the second aluminium is in six-fold coordination; its chemical formula can be expressed as Al[6]Al[6]SiO5, to reflect its crystal structure. Andalusite has the second aluminium in five-fold coordination (Al[6]Al[5]SiO5) and sillimanite has it in four-fold coordination (Al[6]Al[4]SiO5).[66] Differences in crystal structure and chemistry greatly influence other physical properties of the mineral. The carbon allotropes diamond and graphite have vastly different properties; diamond is the hardest natural substance, has an adamantine lustre, and belongs to the isometric crystal family, whereas graphite is very soft, has a greasy lustre, and crystallises in the hexagonal family. This difference is accounted for by differences in bonding. In diamond, the carbons are in sp3 hybrid orbitals, which means they form a framework where each carbon is covalently bonded to four neighbours in a tetrahedral fashion; on the other hand, graphite is composed of sheets of carbons in sp2 hybrid orbitals, where each carbon is bonded covalently to only three others. These sheets are held together by much weaker van der Waals forces, and this discrepancy translates to large macroscopic differences.[67] Twinning is the intergrowth of two or more crystals of a single mineral species. The geometry of the twinning is controlled by the mineral's symmetry. As a result, there are several types of twins, including contact twins, reticulated twins, geniculated twins, penetration twins, cyclic twins, and polysynthetic twins. Contact, or simple twins, consist of two crystals joined at a plane; this type of twinning is common in spinel. Reticulated twins, common in rutile, are interlocking crystals resembling netting. Geniculated twins have a bend in the middle that is caused by start of the twin. Penetration twins consist of two single crystals that have grown into each other; examples of this twinning include cross-shaped staurolite twins and Carlsbad twinning in orthoclase. Cyclic twins are caused by repeated twinning around a rotation axis. This type of twinning occurs around three, four, five, six, or eight-fold axes, and the corresponding patterns are called threelings, fourlings, fivelings, sixlings, and eightlings. Sixlings are common in aragonite. Polysynthetic twins are similar to cyclic twins through the presence of repetitive twinning; however, instead of occurring around a rotational axis, polysynthetic twinning occurs along parallel planes, usually on a microscopic scale.[68][69] Crystal habit refers to the overall shape of the aggregate crystal of any mineral. Several terms are used to describe this property. Common habits include acicular, which describes needle-like crystals as in natrolite; dendritic (tree-pattern) is common in native copper or native gold with a groundmass (matrix); equant, which is typical of garnet; prismatic (elongated in one direction) as seen in kunzite or stibnite; botryoidal (like a bunch of grapes) seen in chalcedony; fibrous, which has fibre-like crystals as seen in wollastonite; tabular, which differs from bladed habit in that the former is platy whereas the latter has a defined elongation as seen in muscovite; and massive, which has no definite shape as seen in carnallite.[7] Related to crystal form, the quality of crystal faces is diagnostic of some minerals, especially with a petrographic microscope. Euhedral crystals have a defined external shape, while anhedral crystals do not; those intermediate forms are termed subhedral.[70][71] The hardness of a mineral defines how much it can resist scratching or indentation. This physical property is controlled by the chemical composition and crystalline structure of a mineral. The most commonly used scale of measurement is the ordinal Mohs hardness scale, which measures resistance to scratching. Defined by ten indicators, a mineral with a higher index scratches those below it. The scale ranges from talc, a phyllosilicate, to diamond, a carbon polymorph that is the hardest natural material. The scale is provided below:[72][7] A mineral's hardness is a function of its structure. Hardness is not necessarily constant for all crystallographic directions; crystallographic weakness renders some directions softer than others.[72] An example of this hardness variability exists in kyanite, which has a Mohs hardness of 51\u20442 parallel to [001] but 7 parallel to [100].[73] Other scales include these;[74] Lustre indicates how light reflects from the mineral's surface, with regard to its quality and intensity. There are numerous qualitative terms used to describe this property, which are split into metallic and non-metallic categories. Metallic and sub-metallic minerals have high reflectivity like metal; examples of minerals with this lustre are galena and pyrite. Non-metallic lustres include: adamantine, such as in diamond; vitreous, which is a glassy lustre very common in silicate minerals; pearly, such as in talc and apophyllite; resinous, such as members of the garnet group; silky which is common in fibrous minerals such as asbestiform chrysotile.[76] The diaphaneity of a mineral describes the ability of light to pass through it. Transparent minerals do not diminish the intensity of light passing through them. An example of a transparent mineral is muscovite (potassium mica); some varieties are sufficiently clear to have been used for windows. Translucent minerals allow some light to pass, but less than those that are transparent. Jadeite and nephrite (mineral forms of jade are examples of minerals with this property). Minerals that do not allow light to pass are called opaque.[77][78] The diaphaneity of a mineral depends on the thickness of the sample. When a mineral is sufficiently thin (e.g., in a thin section for petrography), it may become transparent even if that property is not seen in a hand sample. In contrast, some minerals, such as hematite or pyrite, are opaque even in thin-section.[78] Colour is the most obvious property of a mineral, but it is often non-diagnostic.[79] It is caused by electromagnetic radiation interacting with electrons (except in the case of incandescence, which does not apply to minerals).[80] Two broad classes of elements (idiochromatic and allochromatic) are defined with regard to their contribution to a mineral's colour: Idiochromatic elements are essential to a mineral's composition; their contribution to a mineral's colour is diagnostic.[77][81] Examples of such minerals are malachite (green) and azurite (blue). In contrast, allochromatic elements in minerals are present in trace amounts as impurities. An example of such a mineral would be the ruby and sapphire varieties of the mineral corundum.[81]\nThe colours of pseudochromatic minerals are the result of interference of light waves. Examples include labradorite and bornite. In addition to simple body colour, minerals can have various other distinctive optical properties, such as play of colours, asterism, chatoyancy, iridescence, tarnish, and pleochroism. Several of these properties involve variability in colour. Play of colour, such as in opal, results in the sample reflecting different colours as it is turned, while pleochroism describes the change in colour as light passes through a mineral in a different orientation. Iridescence is a variety of the play of colours where light scatters off a coating on the surface of crystal, cleavage planes, or off layers having minor gradations in chemistry.[82] In contrast, the play of colours in opal is caused by light refracting from ordered microscopic silica spheres within its physical structure.[83] Chatoyancy (\"cat's eye\") is the wavy banding of colour that is observed as the sample is rotated; asterism, a variety of chatoyancy, gives the appearance of a star on the mineral grain. The latter property is particularly common in gem-quality corundum.[82][83] The streak of a mineral refers to the colour of a mineral in powdered form, which may or may not be identical to its body colour.[81] The most common way of testing this property is done with a streak plate, which is made out of porcelain and coloured either white or black. The streak of a mineral is independent of trace elements[77] or any weathering surface.[81] A common example of this property is illustrated with hematite, which is coloured black, silver or red in hand sample, but has a cherry-red[77] to reddish-brown streak;[81][7] or with chalcopyrite, which is brassy golden in colour and leaves a black streak.[7] Streak is more often distinctive for metallic minerals, in contrast to non-metallic minerals whose body colour is created by allochromatic elements.[77] Streak testing is constrained by the hardness of the mineral, as those harder than 7 powder the streak plate instead.[81] By definition, minerals have a characteristic atomic arrangement. Weakness in this crystalline structure causes planes of weakness, and the breakage of a mineral along such planes is termed cleavage. The quality of cleavage can be described based on how cleanly and easily the mineral breaks; common descriptors, in order of decreasing quality, are \"perfect\", \"good\", \"distinct\", and \"poor\". In particularly transparent minerals, or in thin-section, cleavage can be seen as a series of parallel lines marking the planar surfaces when viewed from the side. Cleavage is not a universal property among minerals; for example, quartz, consisting of extensively interconnected silica tetrahedra, does not have a crystallographic weakness which would allow it to cleave. In contrast, micas, which have perfect basal cleavage, consist of sheets of silica tetrahedra which are very weakly held together.[84][85] As cleavage is a function of crystallography, there are a variety of cleavage types. Cleavage occurs typically in either one, two, three, four, or six directions. Basal cleavage in one direction is a distinctive property of the micas. Two-directional cleavage is described as prismatic, and occurs in minerals such as the amphiboles and pyroxenes. Minerals such as galena or halite have cubic (or isometric) cleavage in three directions, at 90\u00b0; when three directions of cleavage are present, but not at 90\u00b0, such as in calcite or rhodochrosite, it is termed rhombohedral cleavage. Octahedral cleavage (four directions) is present in fluorite and diamond, and sphalerite has six-directional dodecahedral cleavage.[84][85] Minerals with many cleavages might not break equally well in all of the directions; for example, calcite has good cleavage in three directions, but gypsum has perfect cleavage in one direction, and poor cleavage in two other directions. Angles between cleavage planes vary between minerals. For example, as the amphiboles are double-chain silicates and the pyroxenes are single-chain silicates, the angle between their cleavage planes is different. The pyroxenes cleave in two directions at approximately 90\u00b0, whereas the amphiboles distinctively cleave in two directions separated by approximately 120\u00b0 and 60\u00b0. The cleavage angles can be measured with a contact goniometer, which is similar to a protractor.[84][85] Parting, sometimes called \"false cleavage\", is similar in appearance to cleavage but is instead produced by structural defects in the mineral, as opposed to systematic weakness. Parting varies from crystal to crystal of a mineral, whereas all crystals of a given mineral will cleave if the atomic structure allows for that property. In general, parting is caused by some stress applied to a crystal. The sources of the stresses include deformation (e.g. an increase in pressure), exsolution, or twinning. Minerals that often display parting include the pyroxenes, hematite, magnetite, and corundum.[84][86] When a mineral is broken in a direction that does not correspond to a plane of cleavage, it is termed to have been fractured. There are several types of uneven fracture. The classic example is conchoidal fracture, like that of quartz; rounded surfaces are created, which are marked by smooth curved lines. This type of fracture occurs only in very homogeneous minerals. Other types of fracture are fibrous, splintery, and hackly. The latter describes a break along a rough, jagged surface; an example of this property is found in native copper.[87] Tenacity is related to both cleavage and fracture. Whereas fracture and cleavage describes the surfaces that are created when a mineral is broken, tenacity describes how resistant a mineral is to such breaking. Minerals can be described as brittle, ductile, malleable, sectile, flexible, or elastic.[88] Specific gravity numerically describes the density of a mineral. The dimensions of density are mass divided by volume with units: kg/m3 or g/cm3. Specific gravity is defined as the density of the mineral divided by the density of water at 4\u00a0\u00b0C and thus is a dimensionless quantity, identical in all unit systems.[89] It can be measured as the quotient of the mass of the sample and difference between the weight of the sample in air and its corresponding weight in water. Among most minerals, this property is not diagnostic. Rock forming minerals\u00a0\u2013 typically silicates or occasionally carbonates\u00a0\u2013 have a specific gravity of 2.5\u20133.5.[90] High specific gravity is a diagnostic property of a mineral. A variation in chemistry (and consequently, mineral class) correlates to a change in specific gravity. Among more common minerals, oxides and sulfides tend to have a higher specific gravity as they include elements with higher atomic mass. A generalization is that minerals with metallic or adamantine lustre tend to have higher specific gravities than those having a non-metallic to dull lustre. For example, hematite, Fe2O3, has a specific gravity of 5.26[91] while galena, PbS, has a specific gravity of 7.2\u20137.6,[92] which is a result of their high iron and lead content, respectively. A very high specific gravity is characteristic of native metals; for example, kamacite, an iron-nickel alloy common in iron meteorites has a specific gravity of 7.9,[93] and gold has an observed specific gravity between 15 and 19.3.[90][94] Other properties can be used to diagnose minerals. These are less general, and apply to specific minerals. Dropping dilute acid (often 10% HCl) onto a mineral aids in distinguishing carbonates from other mineral classes. The acid reacts with the carbonate ([CO3]2\u2212) group, which causes the affected area to effervesce, giving off carbon dioxide gas. This test can be further expanded to test the mineral in its original crystal form or powdered form. An example of this test is done when distinguishing calcite from dolomite, especially within the rocks (limestone and dolomite respectively). Calcite immediately effervesces in acid, whereas acid must be applied to powdered dolomite (often to a scratched surface in a rock), for it to effervesce.[95] Zeolite minerals will not effervesce in acid; instead, they become frosted after 5\u201310 minutes, and if left in acid for a day, they dissolve or become a silica gel.[96] Magnetism is a very conspicuous property of a few minerals. Among common minerals, magnetite exhibits this property strongly, and magnetism is also present, albeit not as strongly, in pyrrhotite and ilmenite.[95] Some minerals exhibit electrical properties \u2013 for example, quartz is piezoelectric \u2013 but electrical properties are rarely used as diagnostic criteria for minerals because of incomplete data and natural variation.[97] Minerals can also be tested for taste or smell. Halite, NaCl, is table salt; its potassium-bearing counterpart, sylvite, has a pronounced bitter taste. Sulfides have a characteristic smell, especially as samples are fractured, reacting, or powdered.[95] Radioactivity is a rare property found in minerals containing radioactive elements. The radioactive elements could be a defining constituent, such as uranium in uraninite, autunite, and carnotite, or present as trace impurities, as in zircon. The decay of a radioactive element damages the mineral crystal structure rendering it locally amorphous (metamict state); the optical result, termed a radioactive halo or pleochroic halo, is observable with various techniques, such as thin-section petrography.[95] In 315 BCE, Theophrastus presented his classification of minerals in his treatise On Stones. His classification was influenced by the ideas of his teachers Plato  and Aristotle. Theophrastus classified minerals as stones, earths or metals.[98] Georgius Agricola's classification of minerals in his book De Natura Fossilium, published in 1546, divided minerals into three types of substance: simple (stones, earths, metals, and congealed juices), compound (intimately mixed) and composite (separable).[98] An early classification of minerals was given by Carl Linnaeus in his seminal 1735 book Systema Naturae. He divided the natural world into three kingdoms\u00a0\u2013 plants, animals, and minerals\u00a0\u2013 and classified each with the same hierarchy.[99] In descending order, these were Phylum, Class, Order, Family, Tribe, Genus, and Species. However, while his system was justified by Charles Darwin's theory of species formation and has been largely adopted and expanded by biologists in the following centuries (who still use his Greek- and Latin-based binomial naming scheme), it had little success among mineralogists (although each distinct mineral is still formally referred to as a mineral species). Minerals are classified by variety, species, series and group, in order of increasing generality. The basic level of definition is that of mineral species, each of which is distinguished from the others by unique chemical and physical properties. For example, quartz is defined by its formula, SiO2, and a specific crystalline structure that distinguishes it from other minerals with the same chemical formula (termed polymorphs). When there exists a range of composition between two minerals species, a mineral series is defined. For example, the biotite series is represented by variable amounts of the endmembers phlogopite, siderophyllite, annite, and eastonite. In contrast, a mineral group is a grouping of mineral species with some common chemical properties that share a crystal structure. The pyroxene group has a common  formula of XY(Si,Al)2O6, where X and Y are both cations, with X typically bigger than Y; the pyroxenes are single-chain silicates that crystallize in either the orthorhombic or monoclinic crystal systems. Finally, a mineral variety is a specific type of mineral species that differs by some physical characteristic, such as colour or crystal habit. An example is amethyst, which is a purple variety of quartz.[18] Two common classifications, Dana and Strunz, are used for minerals; both rely on composition, specifically with regard to important chemical groups, and structure. James Dwight Dana, a leading geologist of his time, first published his System of Mineralogy in 1837; as of 1997[update], it is in its eighth edition. The Dana classification assigns a four-part number to a mineral species. Its class number is based on important compositional groups; the type gives the ratio of cations to anions in the mineral, and the last two numbers group minerals by structural similarity within a given type or class. The less commonly used Strunz classification, named for German mineralogist Karl Hugo Strunz, is based on the Dana system, but combines both chemical and structural criteria, the latter with regard to distribution of chemical bonds.[100] As the composition of the Earth's crust is dominated by silicon and oxygen, silicates are by far the most important class of minerals in terms of rock formation and diversity. However, non-silicate minerals are of great economic importance, especially as ores.[101][102] Non-silicate minerals are subdivided into several other classes by their dominant chemistry, which includes native elements, sulfides, halides, oxides and hydroxides, carbonates and nitrates, borates, sulfates, phosphates, and organic compounds. Most non-silicate mineral species are rare (constituting in total 8% of the Earth's crust), although some are relatively common, such as calcite, pyrite, magnetite, and hematite. There are two major structural styles observed in non-silicates: close-packing and silicate-like linked tetrahedra. Close-packed structures are a way to densely pack atoms while minimizing interstitial space. Hexagonal close-packing involves stacking layers where every other layer is the same (\"ababab\"), whereas cubic close-packing involves stacking groups of three layers (\"abcabcabc\"). Analogues to linked silica tetrahedra include SO4\u22124 (sulfate), PO4\u22124 (phosphate), AsO4\u22124 (arsenate), and VO4\u22124 (vanadate) structures. The non-silicates have great economic importance, as they concentrate elements more than the silicate minerals do.[103] The largest grouping of minerals by far are the silicates; most rocks are composed of greater than 95% silicate minerals, and over 90% of the Earth's crust is composed of these minerals.[104] The two main constituents of silicates are silicon and oxygen, which are the two most abundant elements in the Earth's crust. Other common elements in silicate minerals correspond to other common elements in the Earth's crust, such as aluminium, magnesium, iron, calcium, sodium, and potassium.[105] Some important rock-forming silicates include the feldspars, quartz, olivines, pyroxenes, amphiboles, garnets, and micas. The base unit of a silicate mineral is the [SiO4]4\u2212 tetrahedron. In the vast majority of cases, silicon is in four-fold or tetrahedral coordination with oxygen. In very high-pressure situations, silicon will be in six-fold or octahedral coordination, such as in the perovskite structure or the quartz polymorph stishovite (SiO2). In the latter case, the mineral no longer has a silicate structure, but that of rutile (TiO2), and its associated group, which are simple oxides. These silica tetrahedra are then polymerized to some degree to create various structures, such as one-dimensional chains, two-dimensional sheets, and three-dimensional frameworks. The basic silicate mineral where no polymerization of the tetrahedra has occurred requires other elements to balance out the base 4- charge. In other silicate structures, different combinations of elements are required to balance out the resultant negative charge. It is common for the Si4+ to be substituted by  Al3+ because of similarity in ionic radius and charge; in those cases, the [AlO4]5\u2212 tetrahedra form the same structures as do the unsubstituted tetrahedra, but their charge-balancing requirements are different.[106] The degree of polymerization can be described by both the structure formed and how many tetrahedral corners (or coordinating oxygens) are shared (for aluminium and silicon in tetrahedral sites):[107][108] The silicate subclasses are described below in order of decreasing polymerization. Tectosilicates, also known as framework silicates, have the highest degree of polymerization. With all corners of a tetrahedra shared, the silicon:oxygen ratio becomes 1:2. Examples are quartz, the feldspars, feldspathoids, and the zeolites. Framework silicates tend to be particularly chemically stable as a result of strong covalent bonds.[109] Forming 12% of the Earth's crust, quartz (SiO2) is the most abundant mineral species. It is characterized by its high chemical and physical resistivity. Quartz has several polymorphs, including tridymite and cristobalite at high temperatures, high-pressure coesite, and ultra-high pressure stishovite. The latter mineral can only be formed on Earth by meteorite impacts, and its structure has been compressed so much that it has changed from a silicate structure to that of rutile (TiO2). The silica polymorph that is most stable at the Earth's surface is \u03b1-quartz. Its counterpart, \u03b2-quartz, is present only at high temperatures and pressures (changes to \u03b1-quartz below 573\u00a0\u00b0C at 1 bar). These two polymorphs differ by a \"kinking\" of bonds; this change in structure gives \u03b2-quartz greater symmetry than \u03b1-quartz, and they are thus also called high quartz (\u03b2) and low quartz (\u03b1).[104][110] Feldspars are the most abundant group in the Earth's crust, at about 50%. In the feldspars, Al3+ substitutes for Si4+, which creates a charge imbalance that must be accounted for by the addition of cations. The base structure becomes either [AlSi3O8]\u2212 or [Al2Si2O8]2\u2212  There are 22 mineral species of feldspars, subdivided into two major subgroups \u2013 alkali and plagioclase \u2013 and two less common groups \u2013 celsian and banalsite. The alkali feldspars are most commonly in a series between potassium-rich orthoclase and sodium-rich albite; in the case of plagioclase, the most common series ranges from albite to calcium-rich anorthite. Crystal twinning is common in feldspars, especially polysynthetic twins in plagioclase and Carlsbad twins in alkali feldspars. If the latter subgroup cools slowly from a melt, it forms exsolution lamellae because the two components \u2013 orthoclase and albite \u2013 are unstable in solid solution. Exsolution can be on a scale from microscopic to readily observable in hand-sample; perthitic texture forms when Na-rich feldspar exsolve in a K-rich host. The opposite texture (antiperthitic), where K-rich feldspar exsolves in a Na-rich host, is very rare.[111] Feldspathoids are structurally similar to feldspar, but differ in that they form in Si-deficient conditions, which allows for further substitution by Al3+. As a result, feldspathoids are almost never found in association with quartz. A common example of a feldspathoid is nepheline ((Na, K)AlSiO4); compared to alkali feldspar, nepheline has an Al2O3:SiO2 ratio of 1:2, as opposed to 1:6 in alkali feldspar.[112] Zeolites often have distinctive crystal habits, occurring in needles, plates, or blocky masses. They form in the presence of water at low temperatures and pressures, and have channels and voids in their structure. Zeolites have several industrial applications, especially in waste water treatment.[113] Phyllosilicates consist of sheets of polymerized tetrahedra. They are bound at three oxygen sites, which gives a characteristic silicon:oxygen ratio of 2:5. Important examples include the mica, chlorite, and the kaolinite-serpentine groups. In addition to the tetrahedra, phyllosilicates have a sheet of octahedra (elements in six-fold coordination by oxygen) that balance out the basic tetrahedra, which have a negative charge (e.g. [Si4O10]4\u2212) These tetrahedra (T) and octahedra (O) sheets are stacked in a variety of combinations to create phyllosilicate layers. Within an octahedral sheet, there are three octahedral sites in a unit structure; however, not all of the sites may be occupied. In that case, the mineral is termed dioctahedral, whereas in other case it is termed trioctahedral.[114] The layers are weakly bound by van der Waals forces, hydrogen bonds, or sparse ionic bonds, which causes a crystallographic weakness, in turn leading to a prominent basal cleavage among the phyllosilicates.[115] The kaolinite-serpentine group consists of T-O stacks (the 1:1 clay minerals); their hardness ranges from 2 to 4, as the sheets are held by hydrogen bonds. The 2:1 clay minerals (pyrophyllite-talc) consist of T-O-T stacks, but they are softer (hardness from 1 to 2), as they are instead held together by van der Waals forces. These two groups of minerals are subgrouped by octahedral occupation; specifically, kaolinite and pyrophyllite are dioctahedral whereas serpentine and talc trioctahedral.[116] Micas are also T-O-T-stacked phyllosilicates, but differ from the other T-O-T and T-O-stacked subclass members in that they incorporate aluminium into the tetrahedral sheets (clay minerals have Al3+ in octahedral sites). Common examples of micas are muscovite, and the biotite series. Mica T-O-T layers are bonded together by metal ions, giving them a greater hardness than other phyllosilicate minerals, though they retain perfect basal cleavage.[117] The chlorite group is related to mica group, but a brucite-like (Mg(OH)2) layer between the T-O-T stacks.[118] Because of their chemical structure, phyllosilicates typically have flexible, elastic, transparent layers that are electrical insulators and can be split into very thin flakes. Micas can be used in electronics as insulators, in construction, as optical filler, or even cosmetics. Chrysotile, a species of serpentine, is the most common mineral species in industrial asbestos, as it is less dangerous in terms of health than the amphibole asbestos.[119] Inosilicates consist of tetrahedra repeatedly bonded in chains. These chains can be single, where a tetrahedron is bound to two others to form a continuous chain; alternatively, two chains can be merged to create double-chain silicates. Single-chain silicates have a silicon:oxygen ratio of 1:3 (e.g. [Si2O6]4\u2212), whereas the double-chain variety has a ratio of 4:11, e.g. [Si8O22]12\u2212. Inosilicates contain two important rock-forming mineral groups; single-chain silicates are most commonly pyroxenes, while double-chain silicates are often amphiboles.[120] Higher-order chains exist (e.g. three-member, four-member, five-member chains, etc.) but they are rare.[121] The pyroxene group consists of 21 mineral species.[122] Pyroxenes have a general structure formula of XY(Si2O6), where X is an octahedral site, while Y can vary in coordination number from six to eight. Most varieties of pyroxene consist of permutations of Ca2+, Fe2+ and Mg2+ to balance the negative charge on the backbone. Pyroxenes are common in the Earth's crust (about 10%) and are a key constituent of mafic igneous rocks.[123] Amphiboles have great variability in chemistry, described variously as a \"mineralogical garbage can\" or a \"mineralogical shark swimming a sea of elements\". The backbone of the amphiboles is the [Si8O22]12\u2212; it is balanced by cations in three possible positions, although the third position is not always used, and one element can occupy both remaining ones. Finally, the amphiboles are usually hydrated, that is, they have a hydroxyl group ([OH]\u2212), although it can be replaced by a fluoride, a chloride, or an oxide ion.[124] Because of the variable chemistry, there are over 80 species of amphibole, although variations, as in the pyroxenes, most commonly involve mixtures of Ca2+, Fe2+ and Mg2+.[122] Several amphibole mineral species can have an asbestiform crystal habit. These asbestos minerals form long, thin, flexible, and strong fibres, which are electrical insulators, chemically inert and heat-resistant; as such, they have several applications, especially in construction materials. However, asbestos are known carcinogens, and cause various other illnesses, such as asbestosis; amphibole asbestos (anthophyllite, tremolite, actinolite, grunerite, and riebeckite) are considered more dangerous than chrysotile serpentine asbestos.[125] Cyclosilicates, or ring silicates, have a ratio of silicon to oxygen of 1:3. Six-member rings are most common, with a base structure of [Si6O18]12\u2212; examples include the tourmaline group and beryl. Other ring structures exist, with 3, 4, 8, 9, 12 having been described.[126]  Cyclosilicates tend to be strong, with elongated, striated crystals.[127] Tourmalines have a very complex chemistry that can be described by a general formula XY3Z6(BO3)3T6O18V3W. The T6O18 is the basic ring structure, where T is usually Si4+, but substitutable by Al3+ or B3+. Tourmalines can be subgrouped by the occupancy of the X site, and from there further subdivided by the chemistry of the W site. The Y and Z sites can accommodate a variety of cations, especially various transition metals; this variability in structural transition metal content gives the tourmaline group greater variability in colour. Other cyclosilicates include beryl, Al2Be3Si6O18, whose varieties include the gemstones emerald (green) and aquamarine (bluish). Cordierite is structurally similar to beryl, and is a common metamorphic mineral.[128] Sorosilicates, also termed disilicates, have tetrahedron-tetrahedron bonding at one oxygen, which results in a 2:7 ratio of silicon to oxygen. The resultant common structural element is the [Si2O7]6\u2212 group. The most common disilicates by far are members of the epidote group. Epidotes are found in variety of geologic settings, ranging from mid-ocean ridge to granites to metapelites. Epidotes are built around the structure [(SiO4)(Si2O7)]10\u2212 structure; for example, the mineral species epidote has calcium, aluminium, and ferric iron to charge balance: Ca2Al2(Fe3+, Al)(SiO4)(Si2O7)O(OH). The presence of iron as Fe3+ and Fe2+ helps buffer oxygen fugacity, which in turn is a significant factor in petrogenesis.[129] Other examples of sorosilicates include lawsonite, a  metamorphic mineral forming in the blueschist facies (subduction zone setting with low temperature and high pressure), vesuvianite, which takes up a significant amount of calcium in its chemical structure.[129][130] Orthosilicates consist of isolated tetrahedra that are charge-balanced by other cations.[131] Also termed nesosilicates, this type of silicate has a silicon:oxygen ratio of 1:4 (e.g. SiO4). Typical orthosilicates tend to form blocky equant crystals, and are fairly hard.[132] Several rock-forming minerals are part of this subclass, such as the aluminosilicates, the olivine group, and the garnet group. The aluminosilicates \u2013bkyanite, andalusite, and sillimanite, all Al2SiO5 \u2013 are structurally composed of one [SiO4]4\u2212 tetrahedron, and one Al3+ in octahedral coordination. The remaining Al3+ can be in six-fold coordination (kyanite), five-fold (andalusite) or four-fold (sillimanite); which mineral forms in a given environment is depend on pressure and temperature conditions. In the olivine structure, the main olivine series of (Mg, Fe)2SiO4 consist of magnesium-rich forsterite and iron-rich fayalite. Both iron and magnesium are in octahedral by oxygen. Other mineral species having this structure exist, such as tephroite, Mn2SiO4.[133] The garnet group has a general formula of X3Y2(SiO4)3, where X is a large eight-fold coordinated cation, and Y is a smaller six-fold coordinated cation. There are six ideal endmembers of garnet, split into two group. The pyralspite garnets have Al3+ in the Y position: pyrope (Mg3Al2(SiO4)3), almandine (Fe3Al2(SiO4)3), and spessartine (Mn3Al2(SiO4)3). The ugrandite garnets have Ca2+ in the X position: uvarovite (Ca3Cr2(SiO4)3), grossular (Ca3Al2(SiO4)3) and andradite (Ca3Fe2(SiO4)3). While there are two subgroups of garnet, solid solutions exist between all six end-members.[131] Other orthosilicates include zircon, staurolite, and topaz. Zircon (ZrSiO4) is useful in geochronology as U6+ can substitute for Zr4+; furthermore, because of its very resistant structure, it is difficult to reset it as a chronometer. Staurolite is a common metamorphic intermediate-grade index mineral. It has a particularly complicated crystal structure that was only fully described in 1986. Topaz (Al2SiO4(F, OH)2, often found in granitic pegmatites associated with tourmaline, is a common gemstone mineral.[134] Native elements are those that are not chemically bonded to other elements. This mineral group includes native metals, semi-metals, and non-metals, and various alloys and solid solutions. The metals are held together by metallic bonding, which confers distinctive physical properties such as their shiny metallic lustre, ductility and malleability, and electrical conductivity. Native elements are subdivided into groups by their structure or chemical attributes. The gold group, with a cubic close-packed structure, includes metals such as gold, silver, and copper. The platinum group is similar in structure to the gold group. The iron-nickel group is characterized by several iron-nickel alloy species. Two examples are kamacite and taenite, which are found in iron meteorites; these species differ by the amount of Ni in the alloy; kamacite has less than 5\u20137% nickel and is a variety of native iron, whereas the nickel content of taenite ranges from 7\u201337%. Arsenic group minerals consist of semi-metals, which have only some metallic traits; for example, they lack the malleability of metals. Native carbon occurs in two allotropes, graphite and diamond; the latter forms at very high pressure in the mantle, which gives it a much stronger structure than graphite.[135] The sulfide minerals are chemical compounds of one or more metals or semimetals with a chalcogen or pnictogen, of which sulfur is most common. Tellurium, arsenic, or selenium can substitute for the sulfur. Sulfides tend to be soft, brittle minerals with a high specific gravity. Many powdered sulfides, such as pyrite, have a sulfurous smell when powdered. Sulfides are susceptible to weathering, and many readily dissolve in water; these dissolved minerals can be later redeposited, which creates enriched secondary ore deposits.[136] Sulfides are classified by the ratio of the metal or semimetal to the sulfur, such as M:S equal to 2:1, or 1:1.[137] Many sulfide minerals are economically important as metal ores; examples include sphalerite (ZnS), an ore of zinc, galena (PbS), an ore of lead, cinnabar (HgS), an ore of mercury, and molybdenite (MoS2, an ore of molybdenum.[138] Pyrite (FeS2), is the most commonly occurring sulfide, and can be found in most geological environments. It is not, however, an ore of iron, but can be instead oxidized to produce sulfuric acid.[139] Related to the sulfides are the rare sulfosalts, in which a metallic element is bonded to sulfur and a semimetal such as antimony, arsenic, or bismuth. Like the sulfides, sulfosalts are typically soft, heavy, and brittle minerals.[140] Oxide minerals are divided into three categories: simple oxides, hydroxides, and multiple oxides. Simple oxides are characterized by O2\u2212 as the main anion and primarily ionic bonding. They can be further subdivided by the ratio of oxygen to the cations. The periclase group consists of minerals with a 1:1 ratio. Oxides with a 2:1 ratio include cuprite (Cu2O) and water ice. Corundum group minerals have a 2:3 ratio, and includes minerals such as corundum (Al2O3), and hematite (Fe2O3). Rutile group minerals have a ratio of 1:2; the eponymous species, rutile (TiO2) is the chief ore of titanium; other examples include cassiterite (SnO2; ore of tin), and pyrolusite (MnO2; ore of manganese).[141][142]  In hydroxides, the dominant anion is the hydroxyl ion, OH\u2212. Bauxites are the chief aluminium ore, and are a heterogeneous mixture of the hydroxide minerals diaspore, gibbsite, and bohmite; they form in areas with a very high rate of chemical weathering (mainly tropical conditions).[143]  Finally, multiple oxides are compounds of two metals with oxygen. A major group within this class are the spinels, with a general formula of X2+Y3+2O4. Examples of species include spinel (MgAl2O4), chromite (FeCr2O4), and magnetite (Fe3O4). The latter is readily distinguishable by its strong magnetism, which occurs as it has iron in two oxidation states (Fe2+Fe3+2O4), which makes it a multiple oxide instead of a single oxide.[144] The halide minerals are compounds in which a halogen (fluorine, chlorine, iodine, or bromine) is the main anion. These minerals tend to be soft, weak, brittle, and water-soluble. Common examples of halides include halite (NaCl, table salt), sylvite (KCl), and fluorite (CaF2). Halite and sylvite commonly form as evaporites, and can be dominant minerals in chemical sedimentary rocks. Cryolite, Na3AlF6, is a key mineral in the extraction of aluminium from bauxites; however, as the only significant occurrence at Ivittuut, Greenland, in a granitic pegmatite, was depleted, synthetic cryolite can be made from fluorite.[145] The carbonate minerals are those in which the main anionic group is carbonate, [CO3]2\u2212. Carbonates tend to be brittle, many have rhombohedral cleavage, and all react with acid.[146] Due to the last characteristic, field geologists often carry dilute hydrochloric acid to distinguish carbonates from non-carbonates. The reaction of acid with carbonates, most commonly found as the polymorph calcite and aragonite (CaCO3), relates to the dissolution and precipitation of the mineral, which is a key in the formation of limestone caves, features within them such as stalactite and stalagmites, and karst landforms. Carbonates are most often formed as biogenic or chemical sediments in marine environments. The carbonate group is structurally a triangle, where a central C4+ cation is surrounded by three O2\u2212 anions; different groups of minerals form from different arrangements of these triangles.[147] The most common carbonate mineral is calcite, which is the primary constituent of sedimentary limestone and metamorphic marble. Calcite, CaCO3, can have a significant percentage of magnesium substituting for calcium. Under high-Mg conditions, its polymorph aragonite will form instead; the marine geochemistry in this regard can be described as an aragonite or calcite sea, depending on which mineral preferentially forms. Dolomite is a double carbonate, with the formula CaMg(CO3)2. Secondary dolomitization of limestone is common, in which calcite or aragonite are converted to dolomite; this reaction increases pore space (the unit cell volume of dolomite is 88% that of calcite), which can create a reservoir for oil and gas. These two mineral species are members of eponymous mineral groups: the calcite group includes carbonates with the general formula XCO3, and the dolomite group constitutes minerals with the general formula XY(CO3)2.[148] The sulfate minerals all contain the sulfate anion, [SO4]2\u2212. They tend to be transparent to translucent, soft, and many are fragile.[149] Sulfate minerals commonly form as evaporites, where they precipitate out of evaporating saline waters. Sulfates can also be found in hydrothermal vein systems associated with sulfides,[150] or as oxidation products of sulfides.[151] Sulfates can be subdivided into anhydrous and hydrous minerals. The most common hydrous sulfate by far is gypsum, CaSO4\u22c52H2O. It forms as an evaporite, and is associated with other evaporites such as calcite and halite; if it incorporates sand grains as it crystallizes, gypsum can form desert roses. Gypsum has very low thermal conductivity and maintains a low temperature when heated as it loses that heat by dehydrating; as such, gypsum is used as an insulator in materials such as plaster and drywall. The anhydrous equivalent of gypsum is anhydrite; it can form directly from seawater in highly arid conditions. The barite group has the general formula XSO4, where the X is a large 12-coordinated cation. Examples include barite (BaSO4), celestine (SrSO4), and anglesite (PbSO4); anhydrite is not part of the barite group, as the smaller Ca2+ is only in eight-fold coordination.[152] The phosphate minerals are characterized by the tetrahedral [PO4]3\u2212 unit, although the structure can be generalized, and phosphorus is replaced by antimony, arsenic, or vanadium. The most common phosphate is the apatite group; common species within this group are fluorapatite (Ca5(PO4)3F), chlorapatite (Ca5(PO4)3Cl) and hydroxylapatite (Ca5(PO4)3(OH)). Minerals in this group are the main crystalline constituents of teeth and bones in vertebrates. The relatively abundant monazite group has a general structure of ATO4, where T is phosphorus or arsenic, and A is often a rare-earth element (REE). Monazite is important in two ways: first, as a REE \"sink\", it can sufficiently concentrate these elements to become an ore; secondly, monazite group elements can incorporate relatively large amounts of uranium and thorium, which can be used in monazite geochronology to date the rock based on the decay of the U and Th to lead.[153] The Strunz classification includes a class for organic minerals. These rare compounds contain organic carbon, but can be formed by a geologic process. For example, whewellite, CaC2O4\u22c5H2O is an oxalate that can be deposited in hydrothermal ore veins. While hydrated calcium oxalate can be found in coal seams and other sedimentary deposits involving organic matter, the hydrothermal occurrence is not considered to be related to biological activity.[102] Mineral classification schemes and their definitions are evolving to match recent advances in mineral science. Recent changes have included the addition of an organic class, in both the new Dana and the Strunz classification schemes.[154][155] The organic class includes a very rare group of minerals with hydrocarbons. The IMA Commission on New Minerals and Mineral Names adopted in 2009 a hierarchical scheme for the naming and classification of mineral groups and group names and established seven commissions and four working groups to review and classify minerals into an official listing of their published names.[156][157]  According to these new rules, \"mineral species can be grouped in a number of different ways, on the basis of chemistry, crystal structure, occurrence, association, genetic history, or resource, for example, depending on the purpose to be served by the classification.\"[156] It has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on Mars.  Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.[158] In January 2014, NASA reported that studies by the Curiosity and Opportunity rovers on Mars would search for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable.[159][160][161][162] The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars became a primary NASA objective.[159][160] Funeral: A funeral is a ceremony connected with the final disposition of a corpse, such as a burial, entombment or cremation with the attendant observances.[1] Funerary customs comprise the complex of beliefs and practices used by a culture to remember and respect the dead, from interment, to various monuments, prayers, and rituals undertaken in their honour. Customs vary between cultures and religious groups.  Funerals have both normative and legal components. Common secular motivations for funerals include mourning the deceased, celebrating their life, and offering support and sympathy to the bereaved; additionally, funerals may have religious aspects that are intended to help the soul of the deceased reach the afterlife, resurrection or reincarnation. The funeral usually includes a ritual through which the corpse receives a final disposition.[2] Depending on culture and religion, these can involve either the destruction of the body (for example, by cremation, sky burial, decomposition, disintegration or dissolution) or its preservation (for example, by mummification). Differing beliefs about cleanliness and the relationship between body and soul are reflected in funerary practices. A memorial service (service of remembrance or celebration of life) is a funerary ceremony that is performed without the remains of the deceased person.[3] In both a closed casket funeral[4] and a memorial service, photos of the deceased representing stages of life would be displayed on an altar. Relatives or friends would give out eulogies in both services as well.[5] The word funeral comes from the Latin funus, which had a variety of meanings, including the corpse and the funerary rites themselves. Funerary art is art produced in connection with burials, including many kinds of tombs, and objects specially made for burial like flowers with a corpse. Funeral rites pre-date modern Homo sapiens and dated to at least 300,000 years ago.[6] For example, in the Shanidar Cave in Iraq, in Pontnewydd Cave in Wales and at other sites across Europe and the Near East,[6] Archaeologists have discovered Neanderthal skeletons with a characteristic layer of flower pollen. This deliberate burial and reverence given to the dead has been interpreted as suggesting that Neanderthals had religious beliefs,[6] although the evidence is not unequivocal \u2013 while the dead were apparently buried deliberately, burrowing rodents could have introduced the flowers.[7] Substantial cross-cultural and historical research document funeral customs as a highly predictable, stable force in communities.[8][9] Funeral customs tend to be characterized by five \"anchors\": significant symbols, gathered community, ritual action, cultural heritage, and transition of the dead body (corpse).[2] The most common venues for funeral services would be in a place of worship (synagogue or church) or a funeral home. However, a cemetery's chapel features a reflecting serene intimacy as well as a respectful environment for clergy, mourning families and friends. Graveside services are a less common option for these rituals. A mausoleum's chapel mostly intends to be for entombment after the funeral itself. These two funerary chapels both generously accommodate open or closed-casket services prior to a traditional burial within the cemetery. If a funeral is subsequently followed by cremation, the service would be in a crematorium. In the Bah\u00e1\u02bc\u00ed Faith, burial law prescribes both the location of burial and burial practices and precludes cremation of the dead. It is forbidden to carry the body for more than one hour's journey from the place of death. Before interment the body should be wrapped in a shroud of silk or cotton, and a ring should be placed on its finger bearing the inscription \"I came forth from God, and return unto Him, detached from all save Him, holding fast to His Name, the Merciful, the Compassionate\". The coffin should be of crystal, stone or hard fine wood. Also, before interment, a specific Prayer for the Dead[10] is ordained. The body should be placed with the feet facing the Qiblih. The formal prayer and the ring are meant to be used for those who have reached 15 years of age. Since there are no Bah\u00e1'\u00ed clergy, services are usually conducted under the guidance, or with the assistance of, a Local Spiritual Assembly.[11][12][13] A Buddhist funeral marks the transition from one life to the next for the deceased. It also reminds the living of their own mortality. Cremation is the preferred choice,[14] although burial is also allowed. Buddhists in Tibet perform sky burials where the body is exposed to be eaten by vultures. The body is dissected with a blade on the mountain top before the exposure. Crying and wailing is discouraged and the rogyapas (body breakers who perform the ritual) laugh as if they are doing farm work. Tibetan Buddhists believe that a lighthearted atmosphere during the funeral helps the soul of the dead to get a better afterlife. After the vultures consume all the flesh the rogpyas smash the bones into pieces and mix them with tsampa to feed to the vultures.[15] Congregations of varied denominations perform different funeral ceremonies, but most involve offering prayers, scripture reading from the Bible, a sermon, homily, or eulogy, and music.[2][16] One issue of concern as the 21st century began was with the use of secular music at Christian funerals, a custom generally forbidden by the Catholic Church.[17] Christian burials have traditionally occurred on consecrated ground such as in churchyards. There are many funeral norms in Christianity.[18] Burial, rather than a destructive process such as cremation, was the traditional practice amongst Christians, because of the belief in the resurrection of the body. Cremations later came into widespread use, although some denominations forbid them. The US Conference of Catholic Bishops said \"The Church earnestly recommends that the pious custom of burying the bodies of the deceased be observed; nevertheless, the Church does not prohibit cremation unless it was chosen for reasons contrary to Christian doctrine\" (canon 1176.3).[19][20] Antyesti, literally 'last rites' or 'last sacrifice', refers to the rite-of-passage rituals associated with a funeral in Hinduism.[21] It is sometimes referred to as Antima Samskaram, Antya-kriya, Anvarohanyya, or Vahni Sanskara. A dead adult Hindu is cremated, while a dead child is typically buried.[22][23] The rite of passage is said to be performed in harmony with the sacred premise that the microcosm of all living beings is a reflection of a macrocosm of the universe.[24] The soul (Atman, Brahman) is believed to be the immortal essence that is released at the Antyeshti ritual, but both the body and the universe are vehicles and transitory in various schools of Hinduism. They consist of five elements: air, water, fire, earth and space.[24] The last rite of passage returns the body to the five elements and origins.[22][24] The roots of this belief are found in the Vedas, for example in the hymns of Rigveda in section 10.16, as follows: Burn him not up, nor quite consume him, Agni: let not his body or his skin be scattered,\nO all possessing Fire, when thou hast matured him, then send him on his way unto the Fathers.\nWhen thou hast made him ready, all possessing Fire, then do thou give him over to the Fathers,\nWhen he attains unto the life that waits him, he shall become subject to the will of gods.\nThe Sun receive thine eye, the Wind thy Prana (life-principle, breathe); go, as thy merit is, to earth or heaven.\nGo, if it be thy lot, unto the waters; go, make thine home in plants with all thy members. \u2014\u200aRigveda 10.16[25] The final rites of a burial, in case of untimely death of a child, is rooted in Rigveda's section 10.18, where the hymns mourn the death of the child, praying to deity Mrityu to \"neither harm our girls nor our boys\", and pleads the earth to cover, protect the deceased child as a soft wool.[26][27] Among Hindus, the dead body is usually cremated within a day of death. In Hindu tradition, the body is usually kept at home with the family until its time for cremation. A typical Hindu funeral includes three main stages: a gathering or wake in the home, the cremation itself\u2014referred to as mukhagni\u2014and a follow-up ritual called the shraddha ceremony.[28] The body is washed, wrapped in white cloth for a man or a widow, red for a married woman,[23] the two toes tied together with a string, a Tilak (red mark) placed on the forehead.[22] The dead adult's body is carried to the cremation ground near a river or water, by family and friends, and placed on a pyre with feet facing south.[23] The eldest son, or a male mourner, or a priest then bathes before leading the cremation ceremonial function.[22][29] He circumambulates the dry wood pyre with the body, says a eulogy or recites a hymn in some cases, places sesame seed in the dead person's mouth, sprinkles the body and the pyre with ghee (clarified butter), then draws three lines signifying Yama (deity of the dead), Kala (time, deity of cremation) and the dead.[22] The pyre is then set ablaze, while the mourners mourn. The ash from the cremation is consecrated to the nearest river or sea.[29] After the cremation, a period of mourning is observed for 10 to 12 days after which the immediate male relatives or the sons of the deceased shave their head, trim their nails, recites prayers with the help of priest or Brahmin and invite all relatives, kins, friends and neighbours to eat a simple meal together in remembrance of the deceased.\nDuring the mourning period, sleeping arrangements in the home change too. Mattresses are taken off the beds and placed on the floor, and for twelve days, everyone in the household sleeps on the floor as part of the funeral customs.[30] This day, in some communities, also marks a day when the poor and needy are offered food in memory of the dead.[31] In most Hindu communities the last day of the mourning is called as Terahveen (the thirteenth day), and on this day items of basic needs along with some favourite items of the deceased are donated to the priests. Also on the same day the eldest son of the family is ceremonially crowned (called Pagdi Rasm) for he is now the head of the family. A feast is also organised for Brahmins, family members, and friends.[32] The belief that bodies are infested by Nasu upon death greatly influenced Zoroastrian burial ceremonies and funeral rites. Burial and cremation of corpses was prohibited, as such acts would defile the sacred creations of earth and fire respectively.[33] Burial of corpses was so looked down upon that the exhumation of \"buried corpses was regarded as meritorious.\" For these reasons, \"Towers of Silence\" were developed\u2014open air, amphitheater like structures in which corpses were placed so carrion-eating birds could feed on them. Sagd\u012bd, meaning 'seen by a dog,' is a ritual that must be performed as promptly after death as possible. The dog is able to calculate the degree of evil within the corpse, and entraps the contamination so it may not spread further, expelling Nasu from the body.[34] Nasu remains within the corpse until it has been seen by a dog, or until it has been consumed by a dog or a carrion-eating bird.[35] According to chapter 31 of the Denkard, the reasoning for the required consumption of corpses is that the evil influences of Nasu are contained within the corpse until, upon being digested, the body is changed from the form of nasa into nourishment for animals. The corpse is thereby delivered over to the animals, changing from the state of corrupted nasa to that of hixr, which is \"dry dead matter,\" considered to be less polluting. A path through which a funeral procession has traveled must not be passed again, as Nasu haunts the area thereafter, until the proper rites of banishment are performed.[36] Nasu is expelled from the area only after \"a yellow dog with four eyes, or a white dog with yellow ears\" is walked through the path three times.[37] If the dog goes unwillingly down the path, it must be walked back and forth up to nine times to ensure that Nasu has been driven off.[38] Zoroastrian ritual exposure of the dead is first known of from the writings of the mid-5th century BCE Herodotus, who observed the custom amongst Iranian expatriates in Asia Minor. In Herodotus' account (Histories i.140), the rites are said to have been \"secret\", but were first performed after the body had been dragged around by a bird or dog. The corpse was then embalmed with wax and laid in a trench. While the discovery of ossuaries in both eastern and western Iran dating to the 5th and 4th centuries BCE indicates that bones were isolated, that this separation occurred through ritual exposure cannot be assumed: burial mounds,[39] where the bodies were wrapped in wax, have also been discovered. The tombs of the Achaemenid emperors at Naqsh-e Rustam and Pasargadae likewise suggest non-exposure, at least until the bones could be collected. According to legend (incorporated by Ferdowsi into his Shahnameh), Zoroaster is himself interred in a tomb at Balkh (in present-day Afghanistan). Writing on the culture of the Persians, Herodotus reports on the Persian burial customs performed by the Magi, which are kept secret. However, he writes that he knows they expose the body of male dead to dogs and birds of prey, then they cover the corpse in wax, and then it is buried.[40] The Achaemenid custom is recorded for the dead in the regions of Bactria, Sogdia, and Hyrcania, but not in Western Iran. The Byzantine historian Agathias has described the burial of the Sasanian general Mihr-Mihroe: \"the attendants of Mermeroes took up his body and removed it to a place outside the city and laid it there as it was, alone and uncovered according to their traditional custom, as refuse for dogs and horrible carrion\". Towers are a much later invention and are first documented in the early 9th century CE. The ritual customs surrounding that practice appear to date to the Sassanid era (3rd\u20137th century CE). They are known in detail from the supplement to the Sh\u0101yest n\u0113 Sh\u0101yest, the two Revayats collections, and the two Saddars. Funerals in Islam (called Janazah in Arabic) follow fairly specific rites. In all cases, however, sharia (Islamic religious law) calls for burial of the body, preceded by a simple ritual involving bathing and shrouding the body, followed by salat (prayer). Burial rituals should normally take place as soon as possible and include: The mourning period is 40 days long.[44] In Judaism, funerals follow fairly specific rites, though they are subject to variation in custom. Halakha calls for preparatory rituals involving bathing and shrouding the body accompanied by prayers and readings from the Hebrew Bible, and then a funeral service marked by eulogies and brief prayers, and then the lowering of the body into the grave and the filling of the grave. Traditional law and practice forbid cremation of the body; the Reform Jewish movement generally discourages cremation but does not outright forbid it.[45][46] Burial rites should normally take place as soon as possible and include: In Sikhism death is considered a natural process, an event that has absolute certainty and only happens as a direct result of God's Will or Hukam.[48] In Sikhism, birth and death are closely associated, as they are part of the cycle of human life of \"coming and going\" (Punjabi: \u0a06\u0a35\u0a23\u0a41 \u0a1c\u0a3e\u0a23\u0a3e, romanized:\u00a0Aana Jaana) which is seen as a transient stage towards Liberation (\u0a2e\u0a4b\u0a16\u0a41 \u0a26\u0a41\u0a06\u0a30\u0a41, Mokh Du-aar), understood as completely in unity with God. Sikhs believe in reincarnation. Death is only the progression of the soul on its journey from God, through the created universe and back to God again. In life a Sikh is expected to constantly remember death so that they may be sufficiently prayerful, detached and righteous to break the cycle of birth and death and return to God. The public display of grief by wailing or crying out loud at the funeral (called Antam Sanskar) is discouraged and should be kept to a minimum. Cremation is the preferred method of disposal, burial and burial at sea are also allowed if by necessity or by the will of the person. Markers such as gravestones, monuments, etc. are not allowed, because the body is considered to be just the shell and the person's soul is their real self.[49] On the day of the cremation, the body is washed and dressed and then taken to the Gurdwara or home where hymns (Shabadads) from Sri Guru Granth Sahib Ji, the Sikh Scriptures are recited by the congregation. Kirtan may also be performed by Ragis while the relatives of the deceased recite \"Waheguru\" sitting near the coffin. This service normally takes from 30 to 60 minutes. At the conclusion of the service, an Ardas is said before the coffin is taken to the cremation site. At the point of cremation, a few more Shabadads may be sung and final speeches are made about the deceased person. The eldest son or a close relative generally lights the fire. This service usually lasts about 30 to 60 minutes. The ashes are later collected and disposed of by immersing them in a river, preferably one of the five rivers in the state of Punjab, India. The ceremony in which the Sidharan Paath is begun after the cremation ceremony, may be held when convenient, wherever the Sri Guru Granth Sahib Ji is present. Hymns are sung from Sri Guru Granth Sahib Ji; the first five and final verses of \"Anand Sahib,\" the \"Song of Bliss,\" are recited or sung. The first five verses of Sikhism's morning prayer, \"Japji Sahib\", are read aloud to begin the Sidharan paath. A hukam, or random verse, is then read from Sri Guru Granth Sahib Ji. Ardas, a prayer, is offered, and Prashad, a sacred sweet, is distributed. Langar, a meal, is then served to guests. While the Sidharan paath is being read, the family may also sing hymns daily. Reading may take as long as needed to complete the paath. This ceremony is followed by Sahaj Paath Bhog, Kirtan Sohila, night time prayer is recited for one week, and finally Ardas called the \"Antim Ardas\" (\"Final Prayer\") is offered the last week.[50] It was custom for an officiant to walk in front of the coffin with a horse's skull; this tradition was still observed by Welsh peasants up until the 19th century.[51] The Greek word for funeral \u2013 k\u0113de\u00eda (\u03ba\u03b7\u03b4\u03b5\u03af\u03b1) \u2013 derives from the verb k\u0113domai (\u03ba\u03ae\u03b4\u03bf\u03bc\u03b1\u03b9), that means attend to, take care of someone. Derivative words are also k\u0113dem\u00f3n (\u03ba\u03b7\u03b4\u03b5\u03bc\u03ce\u03bd, \"guardian\") and k\u0113demon\u00eda (\u03ba\u03b7\u03b4\u03b5\u03bc\u03bf\u03bd\u03af\u03b1, \"guardianship\"). From the Cycladic civilization in 3000 BCE until the Hypo-Mycenaean era in 1200\u20131100 BCE the main practice of burial is interment. The cremation of the dead that appears around the 11th century BCE constitutes a new practice of burial and is probably an influence from the East. Until the Christian era, when interment becomes again the only burial practice, both cremation and interment had been practiced depending on the area.[52] The ancient Greek funeral since the Homeric era included the pr\u00f3thesis (\u03c0\u03c1\u03cc\u03b8\u03b5\u03c3\u03b9\u03c2), the ekphor\u00e1 (\u1f10\u03ba\u03c6\u03bf\u03c1\u03ac), the burial and the per\u00eddeipnon (\u03c0\u03b5\u03c1\u03af\u03b4\u03b5\u03b9\u03c0\u03bd\u03bf\u03bd). In most cases, this process is followed faithfully in Greece until today.[53] Pr\u00f3thesis is the deposition of the body of the deceased on the funeral bed and the threnody of his relatives. Today the body is placed in the casket, that is always open in Greek funerals. This part takes place in the house where the deceased had lived. An important part of the Greek tradition is the epicedium, the mournful songs that are sung by the family of the deceased along with professional mourners (who are extinct in the modern era). The deceased was watched over by his beloved the entire night before the burial, an obligatory ritual in popular thought, which is maintained still. Ekphor\u00e1 is the process of transport of the mortal remains of the deceased from his residence to the church, nowadays, and afterward to the place of burial. The procession in the ancient times, according to the law, should have passed silently through the streets of the city. Usually certain favourite objects of the deceased were placed in the coffin in order to \"go along with him\". In certain regions, coins to pay Charon, who ferries the dead to the underworld, are also placed inside the casket. A last kiss is given to the beloved dead by the family before the coffin is closed. The Roman orator Cicero[54] describes the habit of planting flowers around the tomb as an effort to guarantee the repose of the deceased and the purification of the ground, a custom that is maintained until today. After the ceremony, the mourners return to the house of the deceased for the per\u00eddeipnon, the dinner after the burial. According to archaeological findings \u2013 traces of ash, bones of animals, shards of crockery, dishes and basins \u2013 the dinner during the classical era was also organized at the burial spot. Taking into consideration the written sources, however, the dinner could also be served in the houses.[55] The Necrodeipnon (\u039d\u03b5\u03ba\u03c1\u03cc\u03b4\u03b5\u03b9\u03c0\u03bd\u03bf\u03bd) was the funeral banquet which was given at the house of the nearest relative.[56][57] Two days after the burial, a ceremony called \"the thirds\" was held. Eight days after the burial the relatives and the friends of the deceased assembled at the burial spot, where \"the ninths\" would take place, a custom still kept. In addition to this, in the modern era, memorial services take place 40 days, 3 months, 6 months, 9 months, 1 year after the death and from then on every year on the anniversary of the death. The relatives of the deceased, for an unspecified length of time that depends on them, are in mourning, during which women wear black clothes and men a black armband.[clarification needed] Nekysia (\u039d\u03b5\u03ba\u03cd\u03c3\u03b9\u03b1), meaning the day of the dead, and Genesia (\u0393\u03b5\u03bd\u03ad\u03c3\u03b9\u03b1), meaning the day of the forefathers (ancestors), were yearly feasts in honour of the dead.[58][59] Nemesia (\u039d\u03b5\u03bc\u03ad\u03c3\u03b9\u03b1) or Nemeseia (N\u03b5\u03bc\u03ad\u03c3\u03b5\u03b9\u03b1) was also a yearly feast in honour of the dead, most probably intended for averting the anger of the dead.[60][61] In ancient Rome, the eldest surviving male of the household, the pater familias, was summoned to the death-bed, where he attempted to catch and inhale the last breath of the decedent. Funerals of the socially prominent usually were undertaken by professional undertakers called libitinarii. No direct description has been passed down of Roman funeral rites. These rites usually included a public procession to the tomb or pyre where the body was to be cremated. The surviving relations bore masks bearing the images of the family's deceased ancestors. The right to carry the masks in public eventually was restricted to families prominent enough to have held curule magistracies. Mimes, dancers, and musicians hired by the undertakers, and professional female mourners, took part in these processions. Less well-to-do Romans could join benevolent funerary societies (collegia funeraticia) that undertook these rites on their behalf. Nine days after the disposal of the body, by burial or cremation, a feast was given (cena novendialis) and a libation poured over the grave or the ashes. Since most Romans were cremated, the ashes typically were collected in an urn and placed in a niche in a collective tomb called a columbarium (literally, \"dovecote\").[62] During this nine-day period, the house was considered to be tainted, funesta, and was hung with Taxus baccata or Mediterranean Cypress branches to warn passersby. At the end of the period, the house was swept out to symbolically purge it of the taint of death. Several Roman holidays commemorated a family's dead ancestors, including the Parentalia, held February 13 through 21, to honor the family's ancestors; and the Feast of the Lemures, held on May 9, 11, and 13, in which ghosts (larvae) were feared to be active, and the pater familias sought to appease them with offerings of beans. The Romans prohibited cremation or inhumation within the sacred boundary of the city (pomerium), for both religious and civil reasons, so that the priests might not be contaminated by touching a dead body, and that houses would not be endangered by funeral fires. Restrictions on the length, ostentation, expense of, and behaviour during funerals and mourning gradually were enacted by a variety of lawmakers. Often the pomp and length of rites could be politically or socially motivated to advertise or aggrandise a particular kin group in Roman society. This was seen as deleterious to society and conditions for grieving were set. For instance, under some laws, women were prohibited from loud wailing or lacerating their faces and limits were introduced for expenditure on tombs and burial clothes. The Romans commonly built tombs for themselves during their lifetime. Hence these words frequently occur in ancient inscriptions, V.F. Vivus Facit, V.S.P. Vivus Sibi Posuit. The tombs of the rich usually were constructed of marble, the ground enclosed with walls, and planted around with trees. But common sepulchres usually were built below ground, and called hypogea. There were niches cut out of the walls, in which the urns were placed; these, from their resemblance to the niche of a pigeon-house, were called columbaria. Within the United States and Canada, in most cultural groups and regions, the funeral rituals can be divided into three parts: visitation, funeral, and the burial service. A home funeral (services prepared and conducted by the family, with little or no involvement from professionals) is legal in nearly every part of North America, but in the 21st century, they are uncommon in the US.[63] At the visitation (also called a \"viewing\", \"wake\" or \"calling hours\"), in Christian or secular Western custom, the body of the deceased person (or decedent) is placed on display in the casket (also called a coffin, however almost all body containers are caskets). The viewing often takes place on one or two evenings before the funeral. In the past, it was common practice to place the casket in the decedent's home or that of a relative for viewing. This practice continues in many areas of Ireland and Scotland. The body is traditionally dressed in the decedent's best clothes. In recent times there has been more variation in what the decedent is dressed in \u2013 some people choose to be dressed in clothing more reflective of how they dressed in life. The body will often be adorned with common jewelry, such as watches, necklaces, brooches, etc. The jewelry may be taken off and given to the family of the deceased prior to burial or be buried with the deceased. Jewelry has to be removed before cremation in order to prevent damage to the crematory. The body may or may not be embalmed, depending upon such factors as the amount of time since the death has occurred, religious practices, or requirements of the place of burial. The most commonly prescribed aspects of this gathering are that the attendees sign a book kept by the deceased's survivors to record who attended. In addition, a family may choose to display photographs taken of the deceased person during his/her life (often, formal portraits with other family members and candid pictures to show \"happy times\"), prized possessions and other items representing his/her hobbies and/or accomplishments. A more recent trend[when?] is to create a DVD with pictures and video of the deceased, accompanied by music, and play this DVD continuously during the visitation. The viewing is either \"open casket\", in which the embalmed body of the deceased has been clothed and treated with cosmetics for display; or \"closed casket\", in which the coffin is closed. The coffin may be closed if the body was too badly damaged because of an accident or fire or other trauma, deformed from illness, if someone in the group is emotionally unable to cope with viewing the corpse, or if the deceased did not wish to be viewed. In cases such as these, a picture of the deceased, usually a formal photo, is placed atop the casket. However, this step is foreign to Judaism; Jewish funerals are held soon after death (preferably within a day or two, unless more time is needed for relatives to come), and the corpse is never displayed. Torah law forbids embalming.[64] Traditionally flowers (and music) are not sent to a grieving Jewish family as it is a reminder of the life that is now lost. The Jewish shiva tradition discourages family members from cooking, so food is brought by friends and neighbors.[44] (See also Jewish bereavement.) The decedent's closest friends and relatives who are unable to attend frequently send flowers to the viewing, with the exception of a Jewish funeral,[65] where flowers would not be appropriate (donations are often given to a charity instead). Obituaries sometimes contain a request that attendees do not send flowers (e.g. \"In lieu of flowers\"). The use of these phrases has been on the rise for the past century. In the US in 1927, only 6% of the obituaries included the directive, with only 2% of those mentioned charitable contributions instead. By the middle of the century, they had grown to 15%, with over 54% of those noting a charitable contribution as the preferred method of expressing sympathy.[66] The deceased is usually transported from the funeral home to a church in a hearse, a specialized vehicle designed to carry casketed remains. The deceased is often transported in a procession (also called a funeral cort\u00e8ge), with the hearse, funeral service vehicles, and private automobiles traveling in a procession to the church or other location where the services will be held. In a number of jurisdictions, special laws cover funeral processions \u2013 such as requiring most other vehicles to give right-of-way to a funeral procession. Funeral service vehicles may be equipped with light bars and special flashers to increase their visibility on the roads. They may also all have their headlights on, to identify which vehicles are part of the cortege, although the practice also has roots in ancient Roman customs.[67] After the funeral service, if the deceased is to be buried the funeral procession will proceed to a cemetery if not already there. If the deceased is to be cremated, the funeral procession may then proceed to the crematorium. Funeral customs vary from country to country. In the United States, any type of noise other than quiet whispering or mourning is considered disrespectful. A burial tends to cost more than a cremation.[68] At a religious burial service, conducted at the side of the grave, tomb, mausoleum or cremation, the body of the decedent is buried or cremated at the conclusion. Sometimes, the burial service will immediately follow the funeral, in which case a funeral procession travels from the site of the funeral to the burial site. In some other cases, the burial service is the funeral, in which case the procession might travel from the cemetery office to the grave site. Other times, the burial service takes place at a later time, when the final resting place is ready, if the death occurred in the middle of winter. If the decedent served in a branch of the Armed forces, military rites are often accorded at the burial service.[69] In many religious traditions, pallbearers, usually males who are relatives or friends of the decedent, will carry the casket from the chapel (of a funeral home or church) to the hearse, and from the hearse to the site of the burial service.[70] Most religions expect coffins to be kept closed during the burial ceremony. In Eastern Orthodox funerals, the coffins are reopened just before burial to allow mourners to look at the deceased one last time and give their final farewells. Greek funerals are an exception as the coffin is open during the whole procedure unless the state of the body does not allow it. Morticians may ensure that all jewelry, including wristwatch, that were displayed at the wake are in the casket before it is buried or entombed. Custom requires that everything goes into the ground; however this is not true for Jewish services. Jewish tradition stipulates that nothing of value is buried with the deceased. In the case of cremation such items are usually removed before the body goes into the furnace. Pacemakers are removed prior to cremation \u2013 if left in they could explode. Funerals for indigenous people, like many other cultures, are a method to remember, commemorate and respect the dead through their own cultural practices and traditions. In the past, there has been scrutiny when the topic of indigenous funeral sites was approached. Thus the federal government deemed it necessary to include a series of acts that would protect and accurately affiliate some of these burials with their correct native individuals or groups. This was enacted through the Native American Graves Protection and Repatriation Act. Furthermore, in 2001 California created the California Native American Graves Protection and Repatriation Act that would \"require all state agencies and museums that receive state funding and that have possession or control over collections of humans remains or cultural items to provide a process for identification and repatriates of these items to appropriate tribes.\" In 2020, it was amended to include tribes that were beyond State and Federal knowledge. In the Ipai, Tipai, Paipai, and Kiliwa regions funeral practices are similar in their social and power dynamics. The way that these funeral sites were created was based on previous habitation. Meaning, these were sites were their peoples may have died or if they had been a temporary home for some of these groups.[71] Additionally, these individual burials were characterized by grave markers and/or grave offerings. The markers included inverted metates, fractured pieces of metates as well as cairns. As for offerings, food, shell and stone beads were often found in burial mounds along with portions human remains. The state of the human remains found at the site can vary, data suggests[71] that cremations are recent in prehistory compared to just burials. Ranging from the middle Holocene era to the Late Prehistoric Period. Additionally, the position these people were placed in plays a role in how the afterlife was viewed. With recent ethnographic evidence coming from the Yuman people, it is believed that the spirits of the dead could potentially harm the living. so, they would often layer the markers or offerings above the body so that they would be unable to \"leave\" their graves and enact harm. In the Los Angeles Basin, researchers discovered communal mourning features at West Bluffs and Landing Hill. These communal mourning rituals were estimated to have taken place during the Intermediate Period (3,000-1,000 B.P.). Archaeologists have found fragmented pieces of a large schist pestle which was deliberately broken in a methodical way. Other fragmented vessels show signs of uneven burning on the interior surface presumed to have been caused by burning combustible material. In the West Bluffs and Landing Hill assemblages there are many instances of artifacts that were dyed in red ochre pigment after being broken. The tradition of intentionally breaking objects has been a custom in the region for thousands of  years for the purpose of releasing the spirit within the object, reducing harm to the community, or as an expression of grief. Pigmentation of grave goods also has many interpretations, the Chumash associate the color red with both earth and fire. While some researchers consider the usage of the red pigment as an important transitional moment in the adult life cycle.[72] A memorial service[73] or memorial gathering is one given for the deceased, often without the body present. The service takes place after cremation or burial at sea, after an entombment in a mausoleum's crypt, after donation of the body to an academic or research institution, after a traditional burial in a cemetery plot (remains either in a coffin or an urn) or after the ashes have been scattered someplace. It is also significant when the person is missing and presumed dead, or known to be deceased though the body is not recoverable. These services often take place at a funeral home;[74] however, they can be held in a home, cemetery chapel, university, town hall, country club, restaurant, beach, community center, workplace, place of worship, hospital chapel, health club, performing arts center, wedding chapel, national park, townhouse, civic center, hotel, museum, sports field, pub, urban park or other location of some significance.[75][76] A memorial service may include speeches (eulogies), prayers, poems, or songs (most particularly hymns) to commemorate the deceased. Pictures of the deceased and flowers with sometimes an urn are usually placed where the coffin would normally be placed. After the sudden deaths of important public officials, public memorial services have been held by communities, including those without any specific connection to the deceased. For examples, community memorial services were held after the assassinations of US presidents James A. Garfield and William McKinley. In Finland, religious funerals (hautajaiset) are quite ascetic and typically follow Lutheran traditions.[77] The local priest or minister says prayers and blesses the deceased in their house. The mourners (saattov\u00e4ki) traditionally bring food to the mourners' house. Common current practice has the deceased placed into the coffin in the place where they died. The undertaker will pick up the coffin and place it in the hearse and drive it to the funeral home, while the closest relatives or friends of the deceased will follow the hearse in a funeral procession in their own cars. The coffin will be held at the funeral home until the day of the funeral. The funeral services may be divided into two parts. First is the church service (siunaustilaisuus) in a cemetery chapel or local church, then the burial.[78] The majority of Italians are Roman Catholic and follow Catholic funeral traditions. Historically, mourners would walk in a funeral procession to the gravesite; today vehicles are used. Greek funerals are generally held in churches, including a Trisagion service. There is usually a 40-day mourning period, and the end of which, a memorial service is held. Every year following, a similar service takes place, to mark the anniversary of the death.[79][80] In Poland, in urban areas, there are usually two, or just one \"stop\". The body, brought by a hearse from the mortuary, may be taken to a church or to a cemetery chapel. There is then a funeral mass or service at the cemetery chapel. Following the mass or Service the casket is carried in procession (usually on foot) by hearse to the grave. Once at the grave-site, the priest will commence the graveside committal service and the casket is lowered. The mass or service usually takes place at the cemetery. In some traditional rural areas, the wake (czuwanie) takes place in the house of the deceased or their relatives. The body lies in state for three days in the house. The funeral usually takes place on the third day. Family, neighbors and friends gather and pray during the day and night on those three days and nights. There are usually three stages in the funeral ceremony (ceremonia pogrzebowa, pogrzeb): the wake (czuwanie), then the body is carried by procession (usually on foot) or people drive in their own cars to the church or cemetery chapel for mass, and another procession by foot to the gravesite. After the funeral, families gather for a post-funeral get-together (stypa). It can be at the family home, or at a function hall. In Poland cremation is less popular because the Catholic Church in Poland prefers traditional burials (though cremation is allowed). Cremation is more popular among non-religious people and Protestants in Poland. An old funeral rite from the Scottish Highlands involved burying the deceased with a wooden plate resting on his chest. On the plate were placed a small amount of earth and salt, to represent the future of the deceased. The earth hinted that the body would decay and become one with the earth, while the salt represented the soul, which does not decay. This rite was known as \"earth laid upon a corpse\". This practice was also carried out in Ireland, as well as in parts of England, particularly in Leicestershire, although in England the salt was intended to prevent air from distending the corpse.[81] In Spain, a burial or cremation may occur very soon after a death. Most Spaniards are Roman Catholics and follow Catholic funeral traditions. First, family and friends sit with the deceased during the wake until the burial. Wakes are a social event and a time to laugh and honor the dead. Following the wake comes the funeral mass (Tanatorio) at the church or cemetery chapel. Following the mass is the burial. The coffin is then moved from the church to the local cemetery, often with a procession of locals walking behind the hearse. The first Swedish evangelical order of burial was given in Olaus Petri's handbook of 1529. From the medieval order, it had only kept burial and cremation.[82] The funeral where the priest blessed the recently deceased, which after the Reformation came to be called a reading, was forbidden in the church order of 1686, but was taken over by lay people instead. It was then followed by the wake, which was banned by the church law in 1686, when it was often considered degenerate to do dancing and games where beer and brandy were served.[83] It came however, to live on in the custom of \"singing out corpses\". In older times, the grave was often shoveled closed during the hymn singing. During the 17th century, homilies became common, they were later replaced by grift speeches, which, however, never became mandatory. In 1686, it was decided that those who had lived a Christian life should be honestly and properly buried in a grave. It also determined that the burial would be performed by a priest in the Church of Sweden (later some religious communities were given the right to bury their dead themselves). Burial could only take place at a burial site intended for the purpose. Loss of honorable burial became a punishment. A distinction was made between silent burial (for some serious criminals) and quiet burial without singing and bell ringing and with abbreviated ritual (for some criminals, unbaptized children and for those who committed suicide). Church burial was compulsory for members of the Church of Sweden until 1926, when the possibility was opened for civil burial.[82] In the UK, funerals are commonly held at a church, crematorium or cemetery chapel.[84] Historically, it was customary to bury the dead, but since the 1960s, cremation has been more common.[85] While there is no visitation ceremony like in North America, relatives may view the body beforehand at the funeral home. A room for viewing is usually called a chapel of rest.[86] Funerals typically last about half an hour.[87] They are sometimes split into two ceremonies: a main funeral and a shorter committal ceremony. In the latter, the coffin is either handed over to a crematorium[87] or buried in a cemetery.[88] This allows the funeral to be held at a place without cremation or burial facilities. Alternatively, the entire funeral may be held in the chapel of the crematorium or cemetery. It is not customary to view a cremation; instead, the coffin may be removed from the chapel or hidden with curtains towards the end of the funeral.[87] After the funeral, it is common for the mourners to gather for refreshments. This is sometimes called a wake, though this is different from how the term is used in other countries, where a wake is a ceremony before the funeral.[84] Traditionally, a good funeral (as they were called) had one draw the curtains for a period of time; at the wake, when new visitors arrived, they would enter from the front door and leave through the back door. The women stayed at home whilst the men attended the funeral, the village priest would then visit the family at their home to talk about the deceased and to console them.[89] The first child of William Price, a Welsh Neo-Druidic priest, died in 1884. Believing that it was wrong to bury a corpse, and thereby pollute the earth, Price decided to cremate his son's body, a practice which had been common in Celtic societies.\nThe police arrested him for the illegal disposal of a corpse.[90] Price successfully argued in court that while the law did not state that cremation was legal, it also did not state that it was illegal. The case set a precedent that, together with the activities of the newly founded Cremation Society of Great Britain, led to the Cremation Act 1902.[91] The Act imposed procedural requirements before a cremation could occur and restricted the practice to authorised places.[92] A growing number of families choose to hold a life celebration or celebration of life[93][94] event for the deceased. Like memorial services, this ceremony is held after burial, entombment or cremation of the deceased. An urn can be on display with flowers and photos on the altar after cremation like in a memorial service. Unlike funerals, the focus of the ceremony is on the life that was lived.[95] Such ceremonies may be held outside the funeral home or place of worship; country clubs, cemetery chapels, restaurants, beaches, performing arts centers, wedding chapels, urban parks, sports fields, hotels, civic centers, museums, hospital chapels, community centers, town halls, pubs and sporting facilities are popular choices based on the specific interests of the deceased. Celebrations of life focus on including the person's best qualities, interests, achievements and impact, rather than mourning a death.[93] Some events are portrayed as joyous parties, instead of a traditional somber funeral. Taking on happy and hopeful tones, celebrations of life discourage wearing black and focus on the deceased's individuality.[93] An extreme example might have \"a fully stocked open bar, catered food, and even favors.\"[94] Notable recent celebrations of life ceremonies include those for Ren\u00e9 Ang\u00e9lil[96] and Maya Angelou.[97] In Australia, funerary customs continue to evolve in response to cultural diversity and environmental awareness; see Funeral rituals and trends in Australia for details of current practices. Originating in New Orleans, Louisiana, U.S., alongside the emergence of jazz music in late 19th and early 20th centuries, the jazz funeral is a traditionally African-American burial ceremony and celebration of life unique to New Orleans that involves a parading funeral procession accompanied by a brass band playing somber hymns followed by upbeat jazz music. Traditional jazz funerals begin with a processional led by the funeral director, family, friends, and the brass band, i.e., the \"main line\", who march from the funeral service to the burial site while the band plays slow dirges and Christian hymns. After the body is buried, or \"cut loose\", the band begins to play up-tempo, joyful jazz numbers, as the main line parades through the streets and crowds of \"second liners\" join in and begin dancing and marching along, transforming the funeral into a street festival.[98] The terms \"green burial\" and \"natural burial\", used interchangeably, apply to ceremonies that aim to return the body with the earth with little to no use of artificial, non-biodegradable materials. As a concept, the idea of uniting an individual with the natural world after they die appears as old as human death itself, being widespread before the rise of the funeral industry. Holding environmentally-friendly ceremonies as a modern concept first attracted widespread attention in the 1990s. In terms of North America, the opening of the first explicitly \"green\" burial cemetery in the U.S. took place in the state of South Carolina. However, the Green Burial Council, which came into being in 2005, has based its operations out of California. The institution works to officially certify burial practices for funeral homes and cemeteries, making sure that appropriate materials are used.[99] Religiously, some adherents of the Roman Catholic Church often have particular interest in \"green\" funerals given the faith's preference to full burial of the body as well as the theological commitments to care for the environment stated in Catholic social teaching.[99] Those with concerns about the effects on the environment of traditional burial or cremation may be placed into a natural bio-degradable green burial shroud. That, in turn, sometimes gets placed into a simple coffin made of cardboard or other easily biodegradable material. Furthermore, individuals may choose their final resting place to be in a specially designed park or woodland, sometimes known as an \"ecocemetery\", and may have a tree or other item of greenery planted over their grave both as a contribution to the environment and a symbol of remembrance. Humanists UK organises a network of humanist funeral celebrants or officiants across England and Wales, Northern Ireland, and the Channel Islands[100] and a similar network is organised by the Humanist Society Scotland. Humanist officiants are trained and experienced in devising and conducting suitable ceremonies for non-religious individuals.[101] Humanist funerals recognise no \"afterlife\", but celebrate the life of the person who has died.[100] In the twenty-first century, humanist funerals were held for well-known people including Claire Rayner,[102] Keith Floyd,[103][104] Linda Smith,[105] and Ronnie Barker.[106] In areas outside of the United Kingdom, Ireland has featured an increasing number of non-religious funeral arrangements according to publications such as Dublin Live. This has occurred in parallel with a trend of increasing numbers of people carefully scripting their own funerals before they die, writing the details of their own ceremonies. The Irish Association of Funeral Directors has reported that funerals without a religious focus occur mainly in more urbanized areas in contrast to rural territories.[107] Notably, humanist funerals have started to become more prominent in other nations such as the Republic of Malta, in which civil rights activist and humanist Ramon Casha had a large scale event at the Radisson Blu Golden Sands resort devoted to laying him to rest. Although such non-religious ceremonies are \"a rare scene in Maltese society\" due to the large role of the Roman Catholic Church within that country's culture, according to Lovin Malta, \"more and more Maltese people want to know about alternative forms of burial... without any religion being involved\".[108][109] Actual events during non-religious funerals vary, but they frequently reflect upon the interests and personality of the deceased. For example, the humanist ceremony for the aforementioned Keith Floyd, a restaurateur and television personality, included a reading of Rudyard Kipling's poetic work \"If\u2014\" and a performance by musician Bill Padley.[103] Organizations such as the Irish Institute of Celebrants have stated that more and more regular individuals request training for administering funeral ceremonies, instead of leaving things to other individuals.[107] More recently, some commercial organisations offer civil funerals that can integrate traditionally religious content.[110] Funerals specifically for fallen members of fire or police services are common in United States and Canada. These funerals involve honour guards from police forces and/or fire services from across the country and sometimes from overseas.[111] A parade of officers often precedes or follows the hearse carrying the fallen comrade.[111] A traditional fire department funeral consists of two raised aerial ladders.[112] The firefighters travel under the aerials on their ride, on the fire apparatus, to the cemetery. Once there, the grave service includes the playing of bagpipes. The pipes have come to be a distinguishing feature of a fallen hero's funeral. Also a \"Last Alarm Bell\" is rung. A portable fire department bell is tolled at the conclusion of the ceremony. A Masonic funeral is held at the request of a departed Mason or family member. The service may be held in any of the usual places or a Lodge room with committal at graveside, or the complete service can be performed at any of the aforementioned places without a separate committal. Freemasonry does not require a Masonic funeral. There is no single convention for a Masonic funeral service. Some Grand Lodges have a prescribed service (as it is a worldwide organisation). Some of the customs include the presiding officer wearing a hat while doing his part in the service, the Lodge members placing sprigs of evergreen on the casket, and a small white leather apron may being placed in or on the casket. The hat may be worn because it is Masonic custom (in some places in the world) for the presiding officer to have his head covered while officiating. To Masons, the sprig of evergreen is a symbol of immortality. A Mason wears a white leather apron, called a \"lambskin\", on becoming a Mason, and he may continue to wear it even in death.[113][114] In most East Asian, South Asian and many Southeast Asian cultures, the wearing of white is symbolic of death. In these societies, white or off-white robes are traditionally worn to symbolize that someone has died and can be seen worn among relatives of the deceased during a funeral ceremony. In Chinese culture, red is strictly forbidden as it is a traditionally symbolic color of happiness. Exceptions are sometimes made if the deceased has reached an advanced age such as 85, in which case the funeral is considered a celebration, where wearing white with some red is acceptable. Contemporary Western influence however has meant that dark-colored or black attire is now often also acceptable for mourners to wear (particularly for those outside the family). In such cases, mourners wearing dark colors at times may also wear a white or off-white armband or white robe. Contemporary South Korean funerals typically mix western culture with traditional Korean culture, largely depending on socio-economic status, region, and religion. In almost all cases, all related males in the family wear woven armbands representing seniority and lineage in relation to the deceased, and must grieve next to the deceased for a period of three days before burying the body. During this period of time, it is customary for the males in the family to personally greet all who come to show respect. While burials have been preferred historically, recent trends show a dramatic increase in cremations due to shortages of proper burial sites and difficulties in maintaining a traditional grave. The ashes of the cremated corpse are commonly stored in columbaria. Most Japanese funerals are conducted with Buddhist and/or Shinto rites.[115] Many ritually bestow a new name on the deceased; funerary names typically use obsolete or archaic kanji and words, to avoid the likelihood of the name being used in ordinary speech or writing. The new names are typically chosen by a Buddhist priest, after consulting the family of the deceased. Religious thought among the Japanese people is generally a blend of Shint\u014d and Buddhist beliefs. In modern practice, specific rites concerning an individual's passage through life are generally ascribed to one of these two faiths. Funerals and follow-up memorial services fall under the purview of Buddhist ritual, and 90% Japanese funerals are conducted in a Buddhist manner[?]. Aside from the religious aspect, a Japanese funeral usually includes a wake, the cremation of the deceased, and inclusion within the family grave. Follow-up services are then performed by a Buddhist priest on specific anniversaries after death. According to an estimate in 2005, 99% of all deceased Japanese are cremated.[116] In most cases the cremated remains are placed in an urn and then deposited in a family grave. In recent years however, alternative methods of disposal have become more popular, including scattering of the ashes, burial in outer space, and conversion of the cremated remains into a diamond that can be set in jewelry. Funeral practices and burial customs in the Philippines encompass a wide range of personal, cultural, and traditional beliefs and practices which Filipinos observe in relation to death, bereavement, and the proper honoring, interment, and remembrance of the dead. These practices have been vastly shaped by the variety of religions and cultures that entered the Philippines throughout its complex history. Most if not all present-day Filipinos, like their ancestors, believe in some form of an afterlife and give considerable attention to honouring the dead.[117] Except amongst Filipino Muslims (who are obliged to bury a corpse less than 24 hours after death), a wake is generally held from three days to a week.[118] Wakes in rural areas are usually held in the home, while in urban settings the dead is typically displayed in a funeral home. Friends and neighbors bring food to the family, such as pancit noodles and bibingka cake; any leftovers are never taken home by guests, because of a superstition against it.[44] Apart from spreading the news about someone's death verbally,[118] obituaries are also published in newspapers. Although the majority of the Filipino people are Christians,[119] they have retained some traditional indigenous beliefs concerning death.[120][121] In Korea, funerals are typically held for three days and different things are done in each day. The first day: on the day a person dies, the body is moved to a funeral hall. They prepare clothes for the body and put them into a chapel of rest. Then food is prepared for the deceased. It is made up of three bowls of rice and three kinds of Korean side dishes. Also, there has to be three coins and three straw shoes. This can be cancelled if the family of the dead person have a particular religion.[122] On the second day the funeral director washes the body and shrouding is done. Then, a family member of the dead person puts uncooked rice in the mouth of the body. This step does not have to be done if the family has a certain religion. After putting the rice in the mouth, the body is moved into a coffin. Family members, including close relatives, of the dead person will wear mourning clothing. Typically, mourning for a woman includes Korean traditional clothes, Hanbok, and mourning for man includes a suit. The color has to be black. The ritual ceremony begins when they are done with changing clothes and preparing foods for the dead person. The ritual ceremony is different depending on their religion. After the ritual ceremony family members will start to greet guests.[123] On the third day, the family decides whether to bury the body in the ground or cremate the body. In the case of burial, three family members sprinkle dirt on the coffin three times. In the case of cremation, there is no specific ritual; the only requirement is a jar to store burned bones and a place to keep the jar. Other than these facts, in Korea, people who come to the funeral bring condolence money. Also, a food called Yukgaejang is served to guests, oftentimes with the Korean distilled drink called soju.[124] In Mongolia, like many other cultures, funeral practices are considered extremely important.[citation needed], possessing significant elements of both native Mongolian rituals and Buddhist tradition.[125] For Mongolians who are very strict about tradition, families choose from three different ways of burial: open-air burial which is most common, cremation, and embalming. Many factors go into deciding which funeral practice to do. These consisted of the family's social standing, the cause of death, and the place of death. Embalming was mainly chosen by members of the Lamaistic Church; by choosing this practice, they are usually buried in a sitting position. This would show that they would always be in the position of prayer. Also, more important people such as nobles would be buried with weapons, horses and food in their coffins to help them prepare for the next world.[126] The coffin is designed and built by three to four relatives, mainly men. The builders bring planks to the hut where the dead is located and put together the box and the lid. The same people who build the coffin also decorate the funeral. Most of this work is done after dusk. With specific instruction, they work on decorations inside the youngest daughter's house. The reason for this is so the deceased is not disturbed at night.[127] In Vietnam, Buddhism is the most commonly practiced religion, however, most burial methods do not coincide with the Buddhist belief of cremation.[128] The body of the deceased is moved to a loved one's house and placed in an expensive coffin. The body usually stays there for about three days, allowing time for people to visit and place gifts in the mouth.[128] This stems from the Vietnamese belief that the dead should be surrounded by their family. This belief goes so far as to include superstition as well. If somebody is dying in Vietnamese culture, they are rushed home from the hospital so they can die there, because if they die away from home it is believed to be bad luck to take a corpse home.[129] Many services are also held in the Vietnamese burial practices. One is held before moving the coffin from the home and the other is held at the burial site.[130] After the burial of the loved one, incense is burned at the gravesite and respect is paid to all the nearby graves. Following this, the family and friends return to the home and enjoy a feast to celebrate the life of the recently departed.[130] Even after the deceased has been buried, the respect and honor continues. For the first 49 days after the burying, the family holds a memorial service every 7 days, where the family and friends come back together to celebrate the life of their loved one. After this, they meet again on the 100th day after the death, then 265 days after the death, and finally they meet on the anniversary of the death of their loved one, a whole year later, to continue to celebrate the glorious life of their recently departed.[131] The Vietnamese funeral, or \u0111\u00e1m gi\u1ed7, is a less somber occasion than most traditional Western funerals. The \u0111\u00e1m gi\u1ed7 is a celebration of the deceased's life and is centered around the deceased's family.[132] Family members might wear a traditional garment called a mourning headband to signify their relationship with the deceased. Typical mourning headbands are thin strips of fabric that are wrapped around the wearer's head. Traditionally, the deceased's closest family members, such as children, siblings, spouses, and parents will wear white mourning headbands. More distant family members' headband colors may vary. In some cultures, the deceased's nieces, nephews, or grandchildren may be required to wear white headbands with red dots. Other societies may encourage grandchildren to wear white headbands with blue dots. Fourth generation grandchildren often wear yellow mourning headbands. The use of mourning headbands emphasizes the importance of personal and familial roles in Vietnamese society. It also allows funeral attendants to carefully choose their interactions and offer condolences to those closest to the deceased.[133] Traditionally, attendants of a Vietnamese funeral service are encouraged to wear the color white. In many East Asian cultures, white is viewed as a sign of loss and mourning. In Vietnam, members of the Caodaist faith believe that white represents purity and the ability to communicate beyond spiritual worlds.[134] African funerals are usually open to many visitors. The custom of burying the dead in the floor of dwelling-houses has been to some degree prevalent on the Gold Coast of Africa. The ceremony depends on the traditions of the ethnicity the deceased belonged to. The funeral may last for as much as a week. Another custom, a kind of memorial, frequently takes place seven years after the person's death. These funerals and especially the memorials may be extremely expensive for the family in question. Cattle, sheep, goats, and poultry, may be offered and then consumed. The Ashanti and Akan ethnic groups in Ghana typically wear red and black during funerals. For special family members, there is typically a funeral celebration with singing and dancing to honor the life of the deceased. Afterwards, the Akan hold a sombre funeral procession and burial with intense displays of sorrow. Other funerals in Ghana are held with the deceased put in elaborate Fantasy coffins colored and shaped after a certain object, such as a fish, crab, boat, and even airplanes.[131] The Kane Kwei Carpentry Workshop in Teshie, named after Seth Kane Kwei who invented this new style of coffin, has become an international reference for this form of art. Evidence of Africa's earliest funeral was found in Kenya in 2021. A 78,000 year old Middle Stone Age grave of a three-year-old child was discovered in Panga ya Saidi cave complex, Kenya. Researchers said the child's head appeared to have been laid on a pillow. The body had been laid in a fetal position.[135][136] In Kenya funerals are an expensive undertaking. Keeping bodies in morgues to allow for fund raising is a common occurrence more so in urban areas. Some families opt to bury their dead in the countryside homes instead of urban cemeteries, thus spending more money on transporting the dead. The first emperor of the Qin dynasty, Qin Shi Huang's mausoleum is located in the Lintong District of Xi'an, Shaanxi Province. Qin Shi Huang's tomb is one of the World Heritage sites in China. Its remarkable feature and size have been known as one of the most important historical sites in China.[137] Qin Shi Huang is the first emperor who united China for the first time. The mausoleum was built in 247 BCE after he became the emperor of the Qin dynasty. Ancient Chinese mausoleums have unique characteristics compared to other cultures[citation?]. Ancient Chinese thought that the soul remains even after death, (immortal soul) regarded funeral practices as an important tradition.[138] From their long history, the construction of mausoleums has developed over time, creating monumental and massive ancient emperor's tomb. Archeologists have found more than 8,000 life-sized figures resembling an army surrounding the emperor's tomb.[139] The primary purpose of the placement of Terracotta Army is to protect the emperor's tomb. The figures were composed of clay and fragments of pottery. The Terracotta Army represents soldiers, horses, government officials, and even musicians. The arrangement and the weapons they are carrying accurately represent the real formations and weapons of the time. Furthermore, facial features aren't identical, each sculpture bearing a unique look. The Imperial Tombs of the Ming and Qing Dynasties are included as World Heritage Sites. The three Imperial Tombs of the Qin dynasty were added in 2000 and 2003.[140] The three tombs were all built in the 17th century. The tombs have been constructed to memorialize the emperors of the Qing dynasty and their ancestors. In tradition, Chinese have followed Feng Shui to build and decorate the interior. All of the tombs are strictly made following the superstition of Feng Shui. The Imperial Tombs of the Ming and Qing Dynasties clearly show the cultural and architectural tradition that has existed in the area for more than 500 years[citation?]. In Chinese culture, the tombs were considered as a portal between the world of the living and the dead[citation?]. Chinese believed that the portal would divide the soul into two parts. The half of the soul would go to heaven, and the other half would remain within the physical body.[141] From about 1600 to 1914 Europe had two professions that have almost entirely disappeared. The mute appears in art quite frequently, but in literature is probably best known from Dickens's Oliver Twist (1837\u20131839). Oliver is working for Mr Sowerberry when characterised thus: \"There's an expression of melancholy in his face, my dear... which is very interesting. He would make a delightful mute, my love.\" And in Martin Chuzzlewit (1842\u20131844), Moult, the undertaker, states: \"This promises to be one of the most impressive funerals,...no limitation of expense...I have orders to put on my whole establishment of mutes, and mutes come very dear, Mr Pecksniff\". The main function of a funeral mute was to stand around at funerals with a sad, pathetic face. A symbolic protector of the deceased, the mute would usually stand near the door of the home or church. In Victorian times, mutes would wear somber clothing including black cloaks, top hats with trailing hatbands, and gloves.[142] The professional mourner, generally a woman, would shriek and wail (often while clawing her face and tearing at her clothing), to encourage others to weep. Records document forms of professional mourning from Ancient Greece.[143][144] The 2003 award-winning Philippine comedy Crying Ladies revolves around the lives of three women who are part-time professional mourners for the Chinese-Filipino community in Manila's Chinatown. According to the film, the Chinese use professional mourners to help expedite the entry of a deceased loved one's soul into heaven by giving the impression that he or she was a good and loving person, well-loved by many. High-ranking national figures such as heads of state, prominent politicians, military figures, national heroes and eminent cultural figures may be offered state funerals. Common methods of disposal are: Some people choose to make their funeral arrangements in advance so that at the time of their death, their wishes are known to their family. However, the extent to which decisions regarding the disposition of a decedent's remains (including funeral arrangements) can be controlled by the decedent while still alive vary from one jurisdiction to another. In the United States, there are states which allow one to make these decisions for oneself if desired, for example by appointing an agent to carry out one's wishes; in other states, the law allows the decedent's next-of-kin to make the final decisions about the funeral without taking the wishes of the decedent into account.[150] The decedent may, in most U.S. jurisdictions, provide instructions as to the funeral by means of a last will and testament. These instructions can be given some legal effect if bequests are made contingent on the heirs carrying them out, with alternative gifts if they are not followed. This requires the will to become available in time; aspects of the disposition of the remains of US President Franklin Delano Roosevelt ran contrary to a number of his stated wishes, which were found in a safe that was not opened until after the funeral. Some people donate their bodies to a medical school for use in research or education. Medical students frequently study anatomy from donated cadavers; they are also useful in forensic research.[151] Some medical conditions, such as amputations or various surgeries can make the cadaver unsuitable for these purposes; in other cases the bodies of people who had certain medical conditions are useful for research into those conditions. Many medical schools rely on the donation of cadavers for the teaching of anatomy.[152] It is also possible to arrange for donate organs and tissue after death for treating the sick, or even whole cadavers for forensic research at body farms.",
      "ground_truth_chunk_ids": [
        "75_fixed_chunk1",
        "134_random_chunk1"
      ],
      "source_ids": [
        "S075",
        "S334"
      ],
      "category": "comparative",
      "id": 55
    },
    {
      "question": "Compare Dartmouth Jack-O-Lantern and Chelsea F.C.\u2013Liverpool F.C. rivalry in one sentence each: what does each describe or study?",
      "ground_truth": "Dartmouth Jack-O-Lantern: The Dartmouth Jack-O-Lantern (also known as the Jacko)[1] is a college humor magazine, founded at Dartmouth College in 1908. One of the magazine's oldest traditions is \"Stockman's Dogs\". In the October 1934 issue, F.C. Stockman (class of 1935) drew a single-panel cartoon of two dogs talking to each other. That same cartoon has appeared in virtually every issue published since, always with a different caption.[2] The magazine is alluded to in the opening lines of F. Scott Fitzgerald's short story \"The Lost Decade\", which was first published in Esquire in 1939.[3] Jack-O-Lantern writers Nic Duquette and Chris Plehal invented the unofficial Dartmouth mascot Keggy the Keg in the fall of 2003.[4] From 1972 to 1974 the Editor in chief was playwright Robert DeKanter '74. Among the first Dartmouth women on the staff was Barbara Donnelly, '77, later a writer for the Wall Street Journal. DeKanter was succeeded by the team Brad Brinegar and Maxwell Anderson, both '77. One evening in July, 1975, cartoonists Brian \"Hojo\" Hansen '76 and Mike Mosher '77 slipped in and painted a cubist rendition of bibulous alumni in translucent acrylic washes upon the wall. When this was eradicated the following week, Hansen and Mosher replaced it with a Renaissance-style \"pittura infamante\" (topic of an art history lecture in Carpenter Hall) called Allegory of the Evisceration of Humor, depicting Brinegar and Anderson abusing a Jack-O-Lantern figure. \"This was the perfect crime\" enthused Hansen, \"for to paint it over would prove our point: that they have no sense of humor.\" From 1976 to 1978 the Editor was N. Brooks Clark, who published a Jack-O-Lantern calendar during his tenure. Clark wrote a parody of the controversial college-issued sex guide, which he called Thrilling Contraception Comics and Stories, illustrated by Mosher and featuring a wisecracking spermatozoic guide, Snappy Sammy Sperm. Chelsea F.C.\u2013Liverpool F.C. rivalry: The Chelsea F.C.\u2013Liverpool F.C. rivalry is an inter-city rivalry between English professional football clubs Chelsea and Liverpool. Chelsea play their home games at Stamford Bridge, while Liverpool play their home games at Anfield. Though both clubs have frequently competed in the same division for over a century, the modern rivalry between Chelsea and Liverpool began in the early 2000s, when the two clubs clashed repeatedly in cup competitions, particularly in the FA Cup, the League Cup, and the UEFA Champions League. The clubs have competed in seven major cup finals: the 2005 League Cup final, which Chelsea won 3\u20132 after extra time, the 2006 Community Shield, which Liverpool won 2\u20131, the 2012 FA Cup final, which Chelsea won 2\u20131, the 2019 UEFA Super Cup, which Liverpool won 5\u20134 on penalties, the 2022 EFL Cup and FA Cup finals, both of which saw Liverpool win on penalties after two goalless affairs, and the 2024 EFL Cup final, which Liverpool won 1\u20130 after extra time. The two clubs also met in five consecutive Champions League campaigns; in the group stage of the 2005\u201306 season, where both legs finished as goalless draws, in the quarter-finals of the 2008\u201309 season, which Chelsea won 7\u20135 on aggregate, and in the semi-finals of the 2004\u201305, 2006\u201307 and 2007\u201308 seasons, with Liverpool winning the former two and Chelsea winning the latter one.[1][2] Overall, Liverpool have won more of the meetings, defeating Chelsea 87 times to their 67 wins, and a further 46 games ended in draws, as of their latest clash in October 2025. Chelsea's record win over the Reds was a 6\u20131 thrashing at Stamford Bridge in August 1937, whereas Liverpool's biggest win was a 6\u20130 home win in April 1935. In 1904, Gus Mears acquired the Stamford Bridge athletics stadium in Fulham with the aim",
      "expected_answer": "Dartmouth Jack-O-Lantern: The Dartmouth Jack-O-Lantern (also known as the Jacko)[1] is a college humor magazine, founded at Dartmouth College in 1908. One of the magazine's oldest traditions is \"Stockman's Dogs\". In the October 1934 issue, F.C. Stockman (class of 1935) drew a single-panel cartoon of two dogs talking to each other. That same cartoon has appeared in virtually every issue published since, always with a different caption.[2] The magazine is alluded to in the opening lines of F. Scott Fitzgerald's short story \"The Lost Decade\", which was first published in Esquire in 1939.[3] Jack-O-Lantern writers Nic Duquette and Chris Plehal invented the unofficial Dartmouth mascot Keggy the Keg in the fall of 2003.[4] From 1972 to 1974 the Editor in chief was playwright Robert DeKanter '74.  Among the first Dartmouth women on the staff was Barbara Donnelly, '77, later a writer for the Wall Street Journal. DeKanter was succeeded by the team Brad Brinegar and Maxwell Anderson, both '77.  One evening in July, 1975, cartoonists Brian \"Hojo\" Hansen '76 and Mike Mosher '77 slipped in and painted a cubist rendition of bibulous alumni in translucent acrylic washes upon the wall.  When this was eradicated the following week, Hansen and Mosher replaced it with a Renaissance-style \"pittura infamante\" (topic of an art history lecture in Carpenter Hall) called Allegory of the Evisceration of Humor, depicting Brinegar and Anderson abusing a Jack-O-Lantern figure.  \"This was the perfect crime\" enthused Hansen, \"for to paint it over would prove our point: that they have no sense of humor.\" From 1976 to 1978 the Editor was N. Brooks Clark, who published a Jack-O-Lantern calendar during his tenure.  Clark wrote a parody of the controversial college-issued sex guide, which he called Thrilling Contraception Comics and Stories, illustrated by Mosher and featuring a wisecracking spermatozoic guide, Snappy Sammy Sperm.  It was reprinted in the 1982 Holt paperback collection of 1970s college humor,[5] whose lead editor Joey Green was the founding editor of the Cornell Lunatic. A 2006 video prank by the Jack-O-Lantern on a Dartmouth College tour group entitled \"Drinkin' Time\" was featured in an article by the Chronicle of Higher Education,[6] posted by AOL on the Online Video Blog,[7] and was mentioned by The Volokh Conspiracy.[8] As of November\u00a02013[update], the video has garnered over 585,000 views on YouTube.[9] The Jacko publishes print issues approximately four times a year, as well as regularly updated online content and occasional video productions. The magazine devotes one publication cycle each year to a parody of the campus newspaper, The Dartmouth.[1] Some notable writers, artists, comedians and politicians began their careers at the Jacko, including:[10] Chelsea F.C.\u2013Liverpool F.C. rivalry: The Chelsea F.C.\u2013Liverpool F.C. rivalry is an inter-city rivalry between English professional football clubs Chelsea and Liverpool. Chelsea play their home games at Stamford Bridge, while Liverpool play their home games at Anfield. Though both clubs have frequently competed in the same division for over a century, the modern rivalry between Chelsea and Liverpool began in the early 2000s, when the two clubs clashed repeatedly in cup competitions, particularly in the FA Cup, the League Cup, and the UEFA Champions League. The clubs have competed in seven major cup finals: the 2005 League Cup final, which Chelsea won 3\u20132 after extra time, the 2006 Community Shield, which Liverpool won 2\u20131, the 2012 FA Cup final, which Chelsea won 2\u20131, the 2019 UEFA Super Cup, which Liverpool won 5\u20134 on penalties, the 2022 EFL Cup and FA Cup finals, both of which saw Liverpool win on penalties after two goalless affairs, and the 2024 EFL Cup final, which Liverpool won 1\u20130 after extra time. The two clubs also met in five consecutive Champions League campaigns; in the group stage of the 2005\u201306 season, where both legs finished as goalless draws, in the quarter-finals of the 2008\u201309 season, which Chelsea won 7\u20135 on aggregate, and in the semi-finals of the 2004\u201305, 2006\u201307 and 2007\u201308 seasons, with Liverpool winning the former two and Chelsea winning the latter one.[1][2] Overall, Liverpool have won more of the meetings, defeating Chelsea 87 times to their 67 wins, and a further 46 games ended in draws, as of their latest clash in October 2025. Chelsea's record win over the Reds was a 6\u20131 thrashing at Stamford Bridge in August 1937, whereas Liverpool's biggest win was a 6\u20130 home win in April 1935. In 1904, Gus Mears acquired the Stamford Bridge athletics stadium in Fulham with the aim of turning it into a football ground. An offer to lease it to nearby Fulham F.C. was turned down, so Mears opted to found his own club to use the stadium. As there was already a team named Fulham in the borough, the name of the adjacent borough of Chelsea was chosen for the new club; names like Kensington FC, Stamford Bridge FC and London FC were also considered.[3] Chelsea Football Club was founded on 10 March 1905 at The Rising Sun pub (now The Butcher's Hook),[4][5] opposite the present-day main entrance to the ground on Fulham Road, and were elected to the Football League shortly afterwards. Chelsea won promotion to the First Division in their second season, and yo-yoed between the First and Second Divisions in its early years. The team reached the 1915 FA Cup final, where they lost to Sheffield United at Old Trafford, and finished third in the First Division in 1920, the club's best league campaign to that point.[6] Chelsea had a reputation for signing star players[7] and attracted large crowds. The club had the highest average attendance in English football in ten separate seasons[8] including 1907\u201308,[9] 1909\u201310,[10] 1911\u201312,[11] 1912\u201313,[12] 1913\u201314[13] and 1919\u201320.[14][15] They were FA Cup semi-finalists in 1920 and 1932, and remained in the First Division throughout the 1930s, but success eluded the club in the inter-war years. Liverpool Football Club was founded following a dispute between the Everton committee and John Houlding, club president and owner of the land at Anfield. After eight years at the stadium, Everton relocated to Goodison Park in 1892 and Houlding founded Liverpool F.C. to play at Anfield.[16] Originally named \"Everton F.C. and Athletic Grounds Ltd\" (Everton Athletic for short), the club became Liverpool F.C. in March 1892 and gained official recognition three months later, after The Football Association refused to recognise the club as Everton.[17] Liverpool played their first match on 1 September 1892, a pre-season friendly match against Rotherham Town, which they won 7\u20131. The team Liverpool fielded against Rotherham was composed entirely of Scottish players\u2014the players who came from Scotland to play in England in those days were known as the Scotch Professors. Manager John McKenna had recruited the players after a scouting trip to Scotland\u2014so they became known as the \"team of Macs\".[18] The team won the Lancashire League in its debut season and joined the Football League Second Division at the start of the 1893\u201394 season. After the club was promoted to the First Division in 1896, Tom Watson was appointed manager. He led Liverpool to its first league title in 1901, before winning it again in 1906.[19] Chelsea and Liverpool were not traditional rivals, meeting first for the first time on 25 December 1907 at Anfield in the Football League First Division, which ended in a 4\u20131 win for Chelsea. However, for the next 96 years, Chelsea would only manage one single league title, which came in 1955, whereas Liverpool (who were already two-time champions) would go on to win the First Division title sixteen more times, cementing the Reds' status as one of the biggest clubs in England and in Europe, along with major rivals Manchester United, whereas Chelsea were considered to be a mid-table club, and their rivalry with Liverpool was non-existent during the years leading up to the 21st century. We were the new kids on the block who had a few quid and signed a load of players. Jos\u00e9 Mourinho puffed his chest out and then we kept playing each other. It was a clash of two ideals. \u2014\u200aFrank Lampard on Chelsea's sudden rise to success[20] The seeds of the Chelsea vs. Liverpool rivalry were beginning to be sowed in May 2003. The first major meeting that would spark this feud was on the final day of the 2002\u201303 Premier League season, where fourth-placed Chelsea were to play fifth-placed Liverpool at Stamford Bridge in a clash for UEFA Champions League football. Both teams were level on 64 points, with the Blues having a +8 superior goal difference. The three teams that were above them, Manchester United in 1st, Arsenal in 2nd and Newcastle United in 3rd had already accumulated enough points to qualify for next season's Champions League, and sixth-placed Blackburn Rovers were unable to qualify, meaning Liverpool had to defeat Chelsea otherwise they would miss out on Champions League football next season. A goal from Sami Hyypi\u00e4 in the 11th minute put the Reds 1\u20130 up, but Chelsea equalised just two minutes later through Marcel Desailly. Fourteen minutes later, Chelsea found themselves ahead via a Jesper Gr\u00f8nkj\u00e6r strike. Steven Gerrard was dismissed two minutes from full-time, as Chelsea won 2\u20131 and ensured their place in the Champions League next season, with Liverpool having to settle for UEFA Cup (now Europa League) football instead. In July 2003, long-time chairman of Chelsea Ken Bates sold the club to Russian billionaire Roman Abramovich for \u00a3140,000,000. Chelsea spent \u00a3103,000,000 on transfers in the summer of 2003, which included the signings of Joe Cole from West Ham United and Hern\u00e1n Crespo from Inter Milan. Unlike the previous years, Chelsea under Abramovich had now become serious title contenders, threatening the likes of Manchester United and Arsenal, who combined had won ten of the first eleven Premier League titles. The first meeting between Chelsea and Liverpool after the Abramovich takeover was on the first matchday of the new campaign, at Anfield. Chelsea won 2\u20131, courtesy of goals from Juan Sebasti\u00e1n Ver\u00f3n and Jimmy Floyd Hasselbaink. Liverpool would get revenge in the reverse fixture at Stamford Bridge in January 2004, which saw Bruno Cheyrou condemn Chelsea to a 1\u20130 home defeat. However, despite the mass spend of Chelsea, they would still be unable to win the league, finishing as runners-up to the undefeated Arsenal. Meanwhile, Liverpool finished in fourth place, nineteen points behind Chelsea, but still qualifying for the Champions League. In the summer of 2004, Chelsea and Liverpool had respectively appointed managers Jos\u00e9 Mourinho and Rafael Ben\u00edtez, which was the beginning of a vicious rivalry between the pair. In their first season as rivals, they clashed five times, including two Premier League victories for Mourinho, both of which finished 1\u20130 to Chelsea and both of those goals being scored by Joe Cole. In the year 2005 alone, Chelsea and Liverpool met seven times. On 27 February 2005, Liverpool faced Chelsea in the final of the Football League Cup. This was Liverpool's tenth appearance in a Football League Cup final, having won seven of them (1981, 1982, 1983, 1984, 1995, 2001, 2003) and losing twice (1978, 1987). For Chelsea, this was their fourth appearance in the final, winning the cup final in 1965 and 1998, and losing in 1972. Liverpool had defeated Millwall, Middlesbrough, Tottenham Hotspur and Watford en route to the final, whereas Chelsea got past West Ham United, Newcastle United, West London rivals Fulham and Manchester United. A crowd of 78,000 at the Millennium Stadium in Cardiff saw John Arne Riise score a volley inside the first minute to put Liverpool ahead. The score remained 1-0 to the Reds for 79 minutes, until Steven Gerrard headed into his own net from a Chelsea free kick to give the Blues a lifeline. Jos\u00e9 Mourinho was also made to watch from the stands after making a gesture to the Liverpool fans. The score was 1\u20131 at full-time, taking the game to extra-time. Goals from Didier Drogba and Mateja Ke\u017eman put Chelsea 3\u20131 up. A goal from Antonio N\u00fa\u00f1ez a minute later reduced the deficit for Liverpool, but the Blues would triumph 3\u20132 and win the League Cup for the third time. Following the match, Mourinho defended the gesture that saw him dismissed, claiming that it had been intended for the media and not Liverpool fans: \"The signal of close your mouth was not for them but for the press, they speak too much and in my opinion they try to do everything to disturb Chelsea. Wait, don't speak too soon. We lost two matches and in my opinion you (the media) try to take confidence from us and put pressure on us.\" Mourinho was happy that Chelsea had won, but said the victory was not special: \"It's just one more. I had a few before this, I'm very happy to win. It's important for the fans, for the club and especially for the players.\"[21] Just two months after the League Cup final, the two clubs met yet again in the semi-finals of the Champions League. The first leg at Stamford Bridge ended in a goalless stalemate, however, in the second leg at Anfield, Liverpool controversially won 1\u20130, thanks to a goal scored by Luis Garc\u00eda, which despite Chelsea's attempts to clear the ball off the line, the goal was given. It was dubbed as a \"ghost goal\" by Jos\u00e9 Mourinho, which popularised the term for other future incidents. This would seal Liverpool's place in the Champions League final, where they would take on AC Milan, famously coming back from 3\u20130 down and winning the Champions League on penalties. Prior to the match, Chelsea were in hot pursuit of Steven Gerrard. Liverpool had rejected a \u00a332,000,000 bid from Chelsea, however, in a shocking turn of events, Gerrard had rejected a \u00a3100,000-a-week contract offer and had submitted a transfer request, just six weeks after inspiring the comeback to help Liverpool win their fifth Champions League. He eventually changed his mind, soon after signing a new four-year deal and later stating that he would rather win one Premier League title at Liverpool than multiple at Chelsea, as it would mean more to him. Chelsea's failed signing of Liverpool's elite poster boy resulted in yet more bad blood developing between the two sets of supporters. As for the Premier League season, Chelsea were runaway winners, winning their first Premier League title (second overall), finishing twelve points clear of second-placed Arsenal. They amassed a then record-setting 95 points, also winning 29 games, a record Chelsea themselves broke in 2016-17 with 30 wins[22][circular reference] (both broken by Manchester City in 2017\u201318) and conceding 15 goals, a record that still stands to this day as the best defensive record in Premier League history. Liverpool, meanwhile, finished fifth (a regression from the previous season), behind their Merseyside derby rivals Everton, who finished fourth, and 37 points behind Chelsea. However, this also meant that despite being the winners of the 2004\u201305 Champions League, they were not guaranteed a place in next season's edition, as they had finished outside of the top four of the Premier League. On 10 June 2005, UEFA decided to grant Liverpool special dispensation to defend their title, however, they would have to enter in the first qualifying round, and were denied country protection, which meant they could face any English team at any stage of the competition.[23][24][25] Liverpool would go on to defeat The New Saints, FBK Kaunas and CSKA Sofia in the Champions League first, second and third qualifying rounds, respectively, to advance to the group stage, where they were drawn in Group G, along with Chelsea, Anderlecht, and Real Betis, although both of their matches against Chelsea were 0\u20130 draws.[26] Liverpool would finish top of the group with 12 points, with Chelsea finishing second, just behind the Reds with 11. Mourinho's Chelsea would manage to get the better of Liverpool in their Premier League clashes, defeating them 4\u20131 away at Anfield in October 2005, which made them the first Premier League opposition team to score four goals at Anfield[a] and also beating them 2\u20130 at Stamford Bridge in February 2006. However, Ben\u00edtez's Liverpool were victorious in their semi-final encounter in the FA Cup, winning 2\u20131, ending Chelsea's hopes for their first ever double and progressing to the FA Cup final, where Steven Gerrard would score an equaliser in added time to help Liverpool defeat West Ham United 3\u20131 on penalties, in what became known as The Gerrard Final. After the match, Mourinho refused to shake Ben\u00edtez's hand and claimed that the best team had lost, pointing to his side's superior league position, stating: \"Did the best team win? I don't think so. In a one-off game, maybe they will surprise me and they can do it. In the Premiership, the distance between the teams is 45 points over two seasons.\" Chelsea would win the Premier League for a second consecutive season, finishing on 91 points, whereas Liverpool, who were also title contenders throughout the season as well, finished third on 82 points, a point behind second-placed Manchester United, and 9 points behind Chelsea. As Chelsea and Liverpool were the respective winners of the 2005\u201306 Premier League and the 2005\u201306 FA Cup, this meant that they would be playing each other in the 2006 FA Community Shield on 13 August 2006, at the Millennium Stadium in Cardiff, the same venue that hosted the 2005 Football League Cup final a year and a half prior, which saw Chelsea beat Liverpool 3\u20132. Chelsea were also the defending champions, having beaten their London rivals Arsenal the previous year. The Blues were making their sixth appearance in the Community Shield, having previously won in 1955 and 2000, and losing in 1970 and 1997. Liverpool, on the other hand, were appearing for the 21st time, emerging outright victorious eight times (1966, 1976, 1979, 1980, 1982, 1988, 1989, 2001), sharing the shield six times (1964, 1965, 1974, 1977, 1986, 1990) and losing it six times (1922, 1971, 1983, 1984, 1992, 2002). In the match, John Arne Riise opened the scoring for Liverpool early in the first half, only for Chelsea's recently signed forward Andriy Shevchenko to equalise shortly before half-time. Both sides had chances to win the match in the second half, but a Peter Crouch goal late in the half ensured Liverpool won the match 2\u20131, and won their 15th Community Shield. The two teams were again drawn against each other in the Champions League, squaring off in the semi-finals of the competition. Chelsea would win the first leg at Stamford Bridge 1\u20130 courtesy of a goal from Joe Cole, but Liverpool won the second leg 1\u20130 as well at Anfield, with Daniel Agger ensuring the tie finished 1\u20131 on aggregate. The team that would progress was decided in a penalty shootout. Liverpool would win the penalty shootout 4\u20131, sending them to their second Champions League final in three years, which would be a rematch of the 2005 edition, which saw AC Milan get their revenge on Liverpool and defeat them 2\u20131. I'll be honest. I couldn't stand Chelsea as a club. It surpassed Everton and Manchester United as our rivalry for a period. The following season saw Jos\u00e9 Mourinho depart Chelsea in September 2007 by mutual consent, and would replaced by Avram Grant, but they would still defeat Liverpool 2\u20130 in the quarter-finals of the League Cup, with goals from Frank Lampard and Andriy Shevchenko sending the Reds crashing out of the competition. Chelsea and Liverpool were drawn against each other yet again in the semi-finals of the Champions League. In the first leg at Anfield, a Dirk Kuyt goal two minutes before half-time put Liverpool ahead, and the scoreline would remain unchanged until the 95th minute, which saw John Arne Riise scored own goal to give Chelsea an advantage, with the match finishing 1\u20131 and the Blues heading into the second leg at Stamford Bridge with a crucial away goal. Chelsea would defeat Liverpool 3\u20132, with a brace from Didier Drogba and an emotional penalty from Frank Lampard seeing Chelsea finally get the better of Liverpool in the Champions League, and sending them to their first ever Champions League final, which they would go on to lose 6\u20135 on penalties to Manchester United. On 26 October 2008, Chelsea hosted Liverpool at Stamford Bridge in the ninth gameweek of the new Premier League campaign. At this point, Chelsea were top of the Premier League, and Liverpool were second, with both teams having 20 points and Chelsea having a +9 superior goal difference. Chelsea hadn't lost a home match in the Premier League in over four and a half years, last losing at Stamford Bridge to Arsenal in February 2004, and were looking to extend their lead at the top of the table and their home unbeaten run to 87 games. In surprising fashion, however, Liverpool would defeat Chelsea 1\u20130, with a 10th-minute strike from Xabi Alonso that deflected off Chelsea defender Jos\u00e9 Bosingwa sending the Reds to the top of the Premier League and ending the Blues' record-setting 86-game home unbeaten run, their first home league defeat in over four years, which is still the record for the most home games unbeaten in the Premier League.[29] In the reverse fixture at Anfield in February 2009, Liverpool defeated Chelsea again, this time winning 2\u20130, with both goals coming from Fernando Torres late in the game. This was the first season in Premier League history that Liverpool had completed a Premier League double over Chelsea. They would also finish second, above Chelsea who finished third, making it the first Premier League season since 2001\u201302 where Liverpool finished above Chelsea in the Premier League table. In the quarter finals of the Champions League, Liverpool and Chelsea were drawn against each other again, marking the fifth consecutive season in which they played together in the Champions League, the most in Champions League history. In the first leg at Anfield, Chelsea emphatically won 3\u20131, with two goals from defender Branislav Ivanovi\u0107 and a goal from Didier Drogba giving Chelsea an advantage in the second leg at Stamford Bridge, which saw both teams play out a thrilling 4\u20134 draw, with Chelsea winning 7\u20135 on aggregate and progressing to the semi-finals of the Champions League. From 2004 to 2009, Chelsea and Liverpool met a staggering 24 times.[30][31][32] After Rafael Ben\u00edtez departed from Liverpool in June 2010, the club struggled greatly under new manager Roy Hodgson, which saw them nine out of their first twenty matches in the Premier League and sitting 12th in the table, and one of the players who struggled was elite Spanish striker Fernando Torres. Despite Torres having a successful three and a half seasons at Liverpool, which saw him score 81 goals in nearly 150 appearances, he failed to win a single trophy at the club. Chelsea had previously expressed interest in signing Torres in 2008, but Torres responded by saying it would be \"many years\" before he left Liverpool.[33][34] On 27 January 2011, Liverpool rejected a \u00a340,000,000 bid from Chelsea for Torres, which was followed by Torres handing in a transfer request the next day, which was also rejected. Chelsea finally completed the signing of Torres on 31 January 2011, for \u00a350,000,000, a then British transfer record and making Torres the sixth most expensive player in football history at the time, with the signing enraging Liverpool fans and boiling the blood in the rivalry even further. Ironically, Torres made his Chelsea debut against Liverpool at Stamford Bridge on 6 February, where was he was greeted with flags and signs held up by Liverpool fans labelling him as a \"traitor\". Liverpool would go on to beat Chelsea 1\u20130, with a 69th-minute goal from Raul Meireles putting Fernando Torres' Chelsea debut in vain. The next season's edition of the Premier League saw both Chelsea and Liverpool underperform, with both teams finishing outside of the top four, which in normal circumstances would have saw them both absent from Europe entirely next season, with Chelsea, who finished sixth, qualifying for the Champions League as the Champions League winners, which also put their fierce London rivals Tottenham Hotspur, who finished fourth, in the nightmare scenario of finishing in the top four and still not qualifying for the Champions League, who had to settle for Europa League football instead. Meanwhile, Liverpool, who finished 8th, qualified for the third qualifying round of the Europa League as the runners-up of the FA Cup. However, Liverpool still managed to do a Premier League double over Chelsea, defeating them both home and away, which included a 4\u20131 humiliation at Anfield in May 2012. On 5 May 2012, Chelsea and Liverpool faced off in the final of the FA Cup for the very first time, at Wembley Stadium. Chelsea were looking to win their first trophy of the season, being managed by interim manager Roberto Di Matteo, who was prosperous about being appointed as Chelsea manager on a permanent basis. Meanwhile, Liverpool, who were being managed by club legend Kenny Dalglish, had already won the League Cup by beating Cardiff City on penalties in the final, also defeating Chelsea 2\u20130 in the fifth round en route to the final, and were aiming for a double. For Chelsea, this was their 11th appearance in a FA Cup final, having won on six occasions (1970, 1997, 2000, 2007, 2009, 2010) and lost on four occasions (1915, 1967, 1994, 2002). As for Liverpool, this was their 14th FA Cup final, winning the trophy seven times (1965, 1974, 1986, 1989, 1992, 2001, 2006) and being beaten six times (1914, 1950, 1971, 1977, 1988, 1996). On their way to the final, Chelsea defeated Portsmouth, West London rivals Queens Park Rangers, Birmingham City, Leicester City, and London rivals Tottenham Hotspur, whereas Liverpool defeated Oldham Athletic, rivals Manchester United, Brighton & Hove Albion, Stoke City, and Merseyside rivals Everton to earn their place in the final. In the match, Ramires put Chelsea in front in the 11th minute after he dispossessed Liverpool midfielder Jay Spearing and beat Pepe Reina in the Liverpool area. Chelsea extended their lead in the 52nd minute when striker Didier Drogba scored. Liverpool substitute Andy Carroll scored in the 64th minute to reduce the deficit to one goal. Carroll thought he had scored a second in the 81st minute, but his header was saved on the line by Chelsea goalkeeper Petr \u010cech. Carroll ran off celebrating, thinking he had equalised and the ball had crossed the line, but referee Phil Dowd did not award a goal (unlike the Luis Garc\u00eda \"ghost goal\" seven years prior), and Chelsea held on to win the match 2\u20131 and the FA Cup for the seventh time. On 21 April 2013, during Liverpool's 2\u20132 draw with Chelsea in a Premier League match at Anfield, Liverpool striker Luis Su\u00e1rez bit Chelsea defender Branislav Ivanovi\u0107. This was not the first time that something like this had happened; it was the second time that Su\u00e1rez had bitten an opponent.[35] It was not noticed by the officials, and Su\u00e1rez scored an equalizer in injury time.[36] The bite prompted UK Prime Minister David Cameron to call on the FA to take a hard line with Su\u00e1rez: the FA charged him with violent conduct and he was fined an undisclosed sum by his club.[37] Contrary to claims from Su\u00e1rez, Ivanovi\u0107 did not accept an apology.[37] Su\u00e1rez accepted the violent conduct charge but denied the FA's claim the standard punishment of three matches was clearly insufficient for his offence.[38] A three-man independent panel appointed by the FA decided on a ten-game ban for Su\u00e1rez, who did not appeal the ban; the panel criticized Su\u00e1rez for not appreciating \"the seriousness\" of the incident when he argued against a long ban. The panel also wanted to send a \"strong message that such deplorable behaviours do not have a place in football\", while noting that \"all players in the higher level of the game are seen as role models, have the duty to act professionally and responsibly, and set the highest example of good conduct to the rest of the game \u2013 especially to young players\".[39] The 2013\u201314 Premier League season saw Chelsea, Liverpool, Arsenal and Manchester City battle it out in a four-way title race, which eventually boiled down to Chelsea, Liverpool, and City. Chelsea did the Premier League double over both Liverpool and Manchester City, but inconsistent form and losses against the low-block teams saw them fail to win the Premier League. On 27 April 2014, Liverpool, now managed by Brendan Rodgers, welcomed Chelsea, now managed by a returning Jos\u00e9 Mourinho, to Anfield. At this point, the high-flying Reds were top of the Premier League on 80 points with just three games to go, five points clear of second-placed Chelsea and six of third-placed Manchester City (who had a game in hand). They had also scored nearly a century of Premier League goals, and were on course to win their first ever Premier League title, which would have happened if they were to win their last three games, which were Chelsea at home, Crystal Palace away, and Newcastle United at home. Additionally, a win against Chelsea would have seen the Blues be unable to catch Liverpool, as they would have been eight points behind them with two games left had they have won. The match saw Steven Gerrard infamously slip while receiving a pass in first half injury-time, which allowed Demba Ba to score for Chelsea and put them 1\u20130 up at Anfield. Liverpool ultimately were unable to equalise, as a goal from Willian in the dying moments of the game saw Chelsea run out 2\u20130 winners, with Liverpool only now being two points clear of their rivals from London and three points of clear of Manchester City, who had a game in hand and had +8 superior goal difference. Liverpool followed this up by throwing away a 3\u20130 lead at Crystal Palace and only managing to come out with a 3\u20133 draw, all but confirming Manchester City's Premier League victory. The next season, Chelsea hosted Liverpool at Stamford Bridge on 10 May 2015, who at this point had been top of the Premier League for every single matchday. Liverpool provided a guard of honour for Chelsea before kick off. The match finished 1\u20131, with the goals coming from John Terry and Steven Gerrard. In the match, Gerrard, who had confirmed a few months prior that he would be departing from Liverpool at the end of the season, received a standing ovation from both the Liverpool and Chelsea fans as he was being substituted off. In a post-match interview, Gerrard had mixed feelings about being clapped off the pitch by Chelsea fans, stating: \"I was more happy with the ovation from the Liverpool fans. I think Chelsea fans have showed respect for a couple of seconds for me, but they've slaughtered me all game, so I'm not going to get drawn into wishing the Chelsea fans very well. It was nice of them to turn up for once today. But yeah, you know when you get a standing ovation at a stadium, it's nice, but what's important to me is the support from the Liverpool fans and they've been with me since day one.\" Since then, the rivalry has cooled down a little bit, though fans of both clubs still hold a dislike for each other. Liverpool, then under J\u00fcrgen Klopp, would beat Chelsea in four successive major finals during this period: the 2019 UEFA Super Cup, the 2022 EFL Cup final and the 2022 FA Cup final (all on penalties), as well as finally winning their first Premier League title in the 2019\u201320 season. The two clubs met again in the final of the 2024 EFL Cup, with Liverpool winning 1\u20130 in extra-time thanks to a header from Reds captain Virgil van Dijk.[41] Chelsea also gave Liverpool a guard of honour when the latter won the 2019\u201320 and 2024\u201325 titles. Below are the players who have played for both Chelsea and Liverpool.[42][43] In 2024, former City academy player Rio Ngumoha joined Liverpool, having never played for Chelsea's first team.",
      "ground_truth_chunk_ids": [
        "204_random_chunk1",
        "216_random_chunk1"
      ],
      "source_ids": [
        "S404",
        "S416"
      ],
      "category": "comparative",
      "id": 56
    },
    {
      "question": "Compare Photography and Ferrari 166 S in one sentence each: what does each describe or study?",
      "ground_truth": "Photography: This is an accepted version of this page Photography is the art, application, and practice of creating images by recording light, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. It is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.[1] A person who operates a camera to capture or take photographs is called a photographer, while the captured image, also known as a photograph, is the result produced by the camera. Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive, depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing. Before the emergence of digital photography, photographs that utilized film had to be developed to produce negatives or projectable slides, and negatives had to be printed as positive images, usually in enlarged form. This was typically done by photographic laboratories, but many amateur photographers, students, and photographic artists did their own processing. The word \"photography\" was created from the Greek roots \u03c6\u03c9\u03c4\u03cc\u03c2 (ph\u014dt\u00f3s), genitive of \u03c6\u1ff6\u03c2 (ph\u014ds), \"light\"[2] and \u03b3\u03c1\u03b1\u03c6\u03ae (graph\u00e9) Ferrari 166 S: The Ferrari 166 S is a sports car built by Ferrari between 1948 and 1953, as a evolution of its Colombo V12-powered 125 S racer. It was adapted into a sports car for the street in the form of the 166 Inter. Only 12 Ferrari 166 S were produced, nine of them with cycle-fenders as the Spyder Corsa. It was soon followed by the updated and highly successful Ferrari 166 MM (Mille Miglia), of which 47 were made from 1948 to 1953. Its early victories in the Targa Florio and Mille Miglia and others in international competition made the manufacturer a serious competitor in the racing industry.[4] Both were later replaced by the 2.3 L 195 S. The 166 shared its Aurelio Lampredi-designed tube frame[5] and double wishbone/live axle suspension with the 125. Like the 125, the wheelbase was 2420 mm long. Nine 166 Spyder Corsas and three 166 Sports were built. The first two 166 S models were coachbuilt by Carrozzeria Allemano and the last one by Carlo Anderloni at Carrozzeria Touring. Majority of the 166 MM cars were bodied at Touring in a barchetta form. The 1.5 L Gioacchino Colombo-designed V12 engine of the 125 was changed, however, with single overhead camshafts specified and a larger 2.0 L (1995 cc/121 in\u00b3) displacement. This was achieved with both a bore and stroke increase, to 60 by 58.8 mm respectively. Output was 110 PS (81 kW) at 5,600 rpm to 130 PS (96 kW) at 6,500 rpm with three carburetors, giving top speed of 170\u2013215 km/h (106\u2013134 mph).[6][7] For the 166 MM power output rose to 140 PS (103 kW) at 6,600 rpm and top speed to 220 km/h (137 mph).[8] Motor Trend Classic named the 166 MM Barchetta as number six in their list of the ten \"Greatest Ferraris",
      "expected_answer": "Photography: Photography is the art, application, and practice of creating images by recording light, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. It is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.[1] A person who operates a camera to capture or take photographs is called a photographer, while the captured image, also known as a photograph, is the result produced by the camera. Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive, depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing. Before the emergence of digital photography, photographs that utilized film had to be developed to produce negatives or projectable slides, and negatives had to be printed as positive images, usually in enlarged form. This was typically done by photographic laboratories, but many amateur photographers, students, and photographic artists did their own processing. The word \"photography\" was created from the Greek roots \u03c6\u03c9\u03c4\u03cc\u03c2 (ph\u014dt\u00f3s), genitive of \u03c6\u1ff6\u03c2 (ph\u014ds), \"light\"[2] and \u03b3\u03c1\u03b1\u03c6\u03ae (graph\u00e9) \"representation by means of lines\" or \"drawing\",[3] together meaning \"drawing with light\".[4] Several people may have coined the same new term from these roots independently. H\u00e9rcules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, photographie, in private notes which a Brazilian historian believes were written in 1834.[5] This claim is widely reported but is not yet largely recognized internationally. The first use of the word by Florence became widely known after the research of Boris Kossoy in 1980.[6] On 25 February 1839, the German newspaper Vossische Zeitung published an article titled Photographie, discussing several priority claims, especially that of Henry Fox Talbot's, in relation to Daguerre's claim of invention.[7] The article is the earliest known occurrence of the word in public print.[8] It was signed \"J.M.\", believed to have been Berlin astronomer Johann von Maedler.[9] The astronomer John Herschel is also credited with coining the word, independent of Talbot, in 1839.[10] The inventors Nic\u00e9phore Ni\u00e9pce, Talbot, and Louis Daguerre seem not to have known or used the word \"photography\", but referred to their processes as \"Heliography\" (Ni\u00e9pce), \"Photogenic Drawing\"/\"Talbotype\"/\"Calotype\" (Talbot), and \"Daguerreotype\" (Daguerre).[9] Photography is the result of combining several technical discoveries relating to seeing an image and capturing the image. The discovery of the camera obscura (\"dark chamber\" in Latin) that provides an image of a scene dates back to ancient China. Greek mathematicians Aristotle and Euclid independently described a camera obscura in the 5th and 4th centuries BCE.[11][12] In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments.[13] The Arab physicist Ibn al-Haytham (Alhazen) (965\u20131040) also invented a camera obscura as well as the first true pinhole camera.[12][14][15] The invention of the camera has been traced back to the work of Ibn al-Haytham.[16] While the effects of a single light passing through a pinhole had been described earlier,[16] Ibn al-Haytham gave the first correct analysis of the camera obscura,[17] including the first geometrical and quantitative descriptions of the phenomenon,[18] and was the first to use a screen in a dark room so that an image from one side of a hole in the surface could be projected onto a screen on the other side.[19] He also first understood the relationship between the focal point and the pinhole,[20] and performed early experiments with afterimages, laying the foundations for the invention of photography in the 19th century.[15] Leonardo da Vinci mentions natural camerae obscurae that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. Renaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western art. It is a box with a small hole in one side, which allows specific light rays to enter, projecting an inverted image onto a viewing screen or paper. The birth of photography was then concerned with inventing means to capture and keep the image produced by the camera obscura. Albertus Magnus (1193\u20131280) discovered silver nitrate,[21] and Georg Fabricius (1516\u20131571) discovered silver chloride.[22] Daniele Barbaro described a diaphragm in 1566.[23] Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694.[24] Around 1717, Johann Heinrich Schulze used a light-sensitive slurry to capture images of cut-out letters on a bottle and on that basis many German sources and some international ones credit Schulze as the inventor of photography.[25][26]\nThe fiction book Giphantie, published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.[23] In June 1802, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance.[27] He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that \"the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver.\" The shadow images eventually darkened all over.[28] The first permanent photoetching was an image produced in 1822 by the French inventor Nic\u00e9phore Ni\u00e9pce, but it was destroyed in a later attempt to make prints from it.[29] Ni\u00e9pce was successful again in 1825. In 1826 he made the View from the Window at Le Gras, the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).[30] Because Ni\u00e9pce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy. Ni\u00e9pce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Ni\u00e9pce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements\u2014a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and \"fixed\" with hot saturated salt water\u2014were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait. In Brazil, Hercules Florence had started working out a silver-salt-based paper process in 1832, later naming it photographia, at least four years before John Herschel coined the English word photography. In 1834, having settled on silver nitrate on paper, a combination which had been the subject of experiments by Thomas Wedgwood around the year 1800, Florence's notebooks indicate that he eventually succeeded in creating light-fast, durable images.[31] Partly because he never published his invention adequately, partly because he was an obscure inventor living in a remote and undeveloped province, H\u00e9rcules Florence died, in Brazil, unrecognized internationally as one of the inventors of photography during his lifetime.[32][33][34] Meanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834[35] but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method in a paper to the Royal Society[35] and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as daguerreotypes could only be replicated by rephotographing them with a camera.[36] Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.[37][38] In March 1837,[39] Franz von Kobell, used silver chloride and a cardboard camera to make pictures in negative of the Frauenkirche and other buildings in Munich, then taking another picture of the negative to get a positive, the actual black and white reproduction of a view on the object. In 1839, Kobell, together with Carl August von Steinheil, reported on their experiments to the Bavarian Academy of Sciences. The pictures produced were round with a diameter of 4\u00a0cm, the method was later named the \"Steinheil method\". In France, Hippolyte Bayard invented his own process for producing direct positive paper prints and claimed to have invented photography earlier than Daguerre or Talbot.[40] British chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the \"blueprint\". He was the first to use the terms \"photography\", \"negative\" and \"positive\". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to \"fix\" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839. In the March 1851 issue of The Chemist, Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper. Many advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908. Glass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 21st century. Hurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised. The first flexible photographic roll film was marketed by George Eastman, founder of Kodak in 1885, but this original \"film\" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose known as nitrate film. Although cellulose acetate or \"safety film\" had been introduced by Kodak in 1908,[42] at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16\u00a0mm and 8\u00a0mm home movies, nitrate film remained standard for theatrical 35\u00a0mm motion pictures until it was finally discontinued in 1951. Films remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats.[43] Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive \"look\" of film based photographs compared to digital images is likely due to a combination of factors, including (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors),[44] (2) resolution, and (3) continuity of tone.[45] Originally, all photography was monochrome, or black-and-white. Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost, chemical stability, and its \"classic\" photographic look. The tones and contrast between light and dark areas define black-and-white photography.[46] Monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process, publicly revealed in 1847, produces brownish tones. Many photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color. Color photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not \"fix\" the photograph to prevent the color from quickly fading when exposed to white light. The first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855.[47][48] The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters.[47][48] This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s. Russian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color \"fringes\" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images. Implementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability. Autochrome, the first commercially successful color process, was introduced by the Lumi\u00e8re brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s. Kodachrome, the first modern \"integral tripack\" (or \"monopack\") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure. Agfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product. Instant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963. Color photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995\u20132005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive \"look\". In 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. The first digital camera to both record and save images in a digital format was the Fujix DS-1P created by Fujifilm in 1988.[49] In 1991, Kodak unveiled the DCS 100, the first commercially available digital single-lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born. Digital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film.[50] An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications. Digital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones. A large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; dual photography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques. The camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.[51] Photographers control the camera and lens to \"expose\" the light recording material to the required amount of light to form a \"latent image\" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal\u2013oxide\u2013semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on paper. The camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera). As soon as photographic materials became \"fast\" (sensitive) enough for taking candid or surreptitious pictures, small \"detective\" cameras were made, some actually disguised as a book or handbag or pocket watch (the Ticka camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens. The movie camera is a type of photographic camera that takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a \"frame\". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the \"frame rate\" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures to create the illusion of motion.[52] Photographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion.[53] While known colloquially as \"3-D\" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras). Dualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.[54] Ultraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions. Modified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350\u00a0nm to 1000\u00a0nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400\u00a0nm to 700\u00a0nm.[55] Replacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters). Uses of full spectrum photography are for fine art photography, geology, forensics and law enforcement. Layering is a photographic composition technique that manipulates the foreground, subject or middle-ground, and background layers in a way that they all work together to tell a story through the image.[56] Layers may be incorporated by altering the focal length, distorting the perspective by positioning the camera in a certain spot.[57] People, movement, light and a variety of objects can be used in layering.[58] Digital methods of image capture and display processing have enabled the new technology of \"light field photography\" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected after the photograph has been captured.[59] As explained by Michael Faraday in 1846, the \"light field\" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point. These additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm. Besides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures. Amateur photographers take photos for personal use, as a hobby or out of casual interest, rather than as a business or job. The quality of amateur work can be comparable to that of many professionals. Amateurs can fill a gap in subjects or topics that might not otherwise be photographed if they are not commercially useful or salable. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera.[60] Twenty-first century social media and near-ubiquitous camera phones have made photographic and video recording pervasive in everyday life. In the mid-2010s smartphone cameras added numerous automatic assistance features like color management, autofocus face detection and image stabilization that significantly decreased skill and effort needed to take high quality images.[61] Commercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. The commercial photographic world could include: During the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.\nAt first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else. The aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images \"written with light\"; Nic\u00e9phore Ni\u00e9pce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art. Clive Bell in his classic essay Art states that only \"significant form\" can distinguish art from what is not art. There must be some one quality without which a work of art cannot exist; possessing which, in the least degree, no work is altogether worthless. What is this quality? What quality is shared by all objects that provoke our aesthetic emotions? What quality is common to Sta. Sophia and the windows at Chartres, Mexican sculpture, a Persian bowl, Chinese carpets, Giotto's frescoes at Padua, and the masterpieces of Poussin, Piero della Francesca, and Cezanne? Only one answer seems possible\u00a0\u2013 significant form. In each, lines and colors combined in a particular way, certain forms and relations of forms, stir our aesthetic emotions.[62] On 7 February 2007, Sotheby's London sold the 2001 photograph 99 Cent II Diptychon for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.[63] Conceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract. In parallel to this development, the then largely separate interface between painting and photography was closed in the second half of the 20th century with the chemigram of Pierre Cordier and the chemogram of Josef H. Neumann.[64] In 1974 the chemograms by Josef H. Neumann concluded the separation of the painterly background and the photographic layer by showing the picture elements in a symbiosis that had never existed before, as an unmistakable unique specimen, in a simultaneous painterly and at the same time real photographic perspective, using lenses, within a photographic layer, united in colors and shapes. This Neumann chemogram from the 1970s thus differs from the beginning of the previously created cameraless chemigrams of a Pierre Cordier and the photogram Man Ray or L\u00e1szl\u00f3 Moholy-Nagy of the previous decades. These works of art were almost simultaneous with the invention of photography by various important artists who characterized Hippolyte Bayard, Thomas Wedgwood, William Henry Fox Talbot in their early stages, and later Man Ray and L\u00e1szl\u00f3 Moholy-Nagy in the twenties and by the painter in the thirties Edmund Kesting and Christian Schad by draping objects directly onto appropriately sensitized photo paper and using a light source without a camera.\n[65] Photojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining, including sports photography. The camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are usually taken from three vantage points: overview, mid-range, and close-up.[66] In 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24-hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century.[67][68] Charles Brooke a little later developed similar instruments for the Greenwich Observatory.[69] Science regularly uses image technology that has derived from the design of the pinhole camera to avoid distortions that can be caused by lenses. X-ray machines are similar in design to pinhole cameras, with high-grade filters and laser radiation.[70]\nPhotography has become universal in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.[71] The first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an \"Ion\" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.[72] Wildlife photography involves capturing images of various forms of wildlife. Unlike other forms of photography such as product or food photography, successful wildlife photography requires a photographer to choose the right place and right time when specific wildlife are present and active. It often requires great patience and considerable skill and command of the right photographic equipment.[73] There are many ongoing questions about different aspects of photography. In her On Photography (1977), Susan Sontag dismisses the objectivity of photography. This is a highly debated subject within the photographic community.[74] Sontag argues, \"To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power.\"[75] Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation. Modern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's Rear Window (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.[75] The camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate \u2013 all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.[75] Digital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make \"photomontages\", passing them as \"real\" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography. Photography is one of the new media forms that changes perception and changes the structure of society.[76] Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that \"to photograph is to turn people into objects that can be symbolically possessed\". Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.[75] One of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a \"tourist gaze\"[77] in which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a \"reverse gaze\"[78] through which indigenous photographees can position the tourist photographer as a shallow consumer of images. Photography is both restricted and protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view.[79] In the UK, the Counter-Terrorism Act (2008) has increased the power of the police to prevent people, even press photographers, from taking pictures in public places.[80] In South Africa, any person may photograph any other person, without their permission, in public spaces and the only specific restriction placed on what may not be photographed by government is related to anything classed as national security. Each country has different laws. Ferrari 166 S: The Ferrari 166 S is a sports car built by Ferrari between 1948 and 1953, as a evolution of its Colombo V12-powered 125 S racer. It was adapted into a sports car for the street in the form of the 166 Inter. Only 12 Ferrari 166 S were produced, nine of them with cycle-fenders as the Spyder Corsa. It was soon followed by the updated and highly successful Ferrari 166 MM (Mille Miglia), of which 47 were made from 1948 to 1953. Its early victories in the Targa Florio and Mille Miglia and others in international competition made the manufacturer a serious competitor in the racing industry.[4] Both were later replaced by the 2.3\u00a0L 195 S. The 166 shared its Aurelio Lampredi-designed tube frame[5] and double wishbone/live axle suspension with the 125. Like the 125, the wheelbase was 2420\u00a0mm long. Nine 166 Spyder Corsas and three 166 Sports were built. The first two 166 S models were coachbuilt by Carrozzeria Allemano and the last one by Carlo Anderloni at Carrozzeria Touring. Majority of the 166 MM cars were bodied at Touring in a barchetta form. The 1.5\u00a0L Gioacchino Colombo-designed V12 engine of the 125 was changed, however, with single overhead camshafts specified and a larger 2.0\u00a0L (1995\u00a0cc/121\u00a0in\u00b3) displacement. This was achieved with both a bore and stroke increase, to 60 by 58.8\u00a0mm respectively. Output was 110\u00a0PS (81\u00a0kW) at 5,600\u00a0rpm to 130\u00a0PS (96\u00a0kW) at 6,500\u00a0rpm with three carburetors, giving top speed of 170\u2013215\u00a0km/h (106\u2013134\u00a0mph).[6][7] For the 166 MM power output rose to 140\u00a0PS (103\u00a0kW) at 6,600\u00a0rpm and top speed to 220\u00a0km/h (137\u00a0mph).[8] Motor Trend Classic named the 166 MM Barchetta as number six in their list of the ten \"Greatest Ferraris of all time\".[9] The Ferrari 166 S won Targa Florio with Clemente Biondetti and Igor Troubetzkoy in 1948. In 1949, Biondetti also won in the 166 SC with Benedetti as co-driver. The 166 S won 1948 Mille Miglia, also driven by Biondetti, this time with Giuseppe Navone.[10] In 1949 Mille Miglia, the Ferrari 166 MM Barchettas scored 1-2 victory with Biondetti/Salani and Bonetto/Carpani respectively.[11] In 1949, the 166 MM also won the 24 Hours of Le Mans in the hands of Luigi Chinetti and Lord Selsdon, and so the \n166 was the only car ever to win all three races.[12] Another 166 won the 1949 Spa 24 Hours. A 166 chassis, this time with the bigger 195 S engine, won the Mille Miglia again in 1950 with drivers Giannino Marzotto and Marco Crosara. The oldest Ferrari car with an undisputed pedigree[citation needed] is s/n 002C, a 166 Spider Corsa which was originally a 159 and is currently owned and driven by James Glickenhaus. S/n 0052M, a 1950 166 MM Touring Barchetta was uncovered in a barn and was shown in public for the first time since 1959 in the August 2006 issue of Cavallino magazine. One 166 MM, 1949 s/n 0018M, was bodied by Zagato in 'Panoramica' style, very similar to their one-off Maserati A6 1500, also designed by Vieri Rapi. It is considered as first Ferrari coachbuilt by Zagato. A year later it was rebodied as Zagato Spyder.[13] The original car was recreated in 2007 as part of Zagato's Sanction Lost programme.[14]",
      "ground_truth_chunk_ids": [
        "93_fixed_chunk1",
        "16_random_chunk1"
      ],
      "source_ids": [
        "S093",
        "S216"
      ],
      "category": "comparative",
      "id": 57
    },
    {
      "question": "Compare Jack Keeney and Ocean in one sentence each: what does each describe or study?",
      "ground_truth": "Jack Keeney: John Christopher \"Jack\" Keeney (February 19, 1922 \u2013 November 19, 2011) was an American prosecutor who retired in 2010 as U.S. deputy United States Assistant Attorney General. At age 88, he was at the time the DOJ's oldest employee, and one of the longest-serving career employees in the history of the United States government. Upon his retirement, Keeney was the longest-serving federal prosecutor in American history.[1] Keeney spent decades in the United States Department of Justice Criminal Division, starting in 1951. On numerous occasions, Keeney served as Acting Assistant Attorney General. Keeney was born in Ashley, Pennsylvania, on February 19, 1922.[1] Keeney was a pilot in the Army Air Corps during World War II, and was held by German forces as a prisoner of war. Keeney graduated from the University of Scranton in 1947.[1] He received law degrees from Dickinson School of Law in 1949 and from George Washington University Law School in 1953.[1] In 2000, the Justice Department named one of its buildings (1301 New York Avenue, N.W., Washington, D.C.) after Keeney, an honor rarely bestowed on a living person.[1] In the month following his death, the Justice Department created the John C. Keeney Award for Exceptional Integrity and Professionalism. The John C. Keeney Award recognizes a Justice Department employee who has demonstrated outstanding professionalism and integrity over a sustained period of time or an employee who has displayed extraordinary strength of character in a unique situation, as Mr. Keeney displayed during his years of service to the federal government.[2] Keeney died on November 19, 2011, at his home in Kensington, Maryland, aged 89.[3] This American law\u2013related biographical article is a stub. You can help Wikipedia by adding missing information. Ocean: The ocean is the body of salt water that covers approximately 70.8% of Earth.[8] The ocean is conventionally divided into large bodies of water, which are also referred to as oceans (in descending order by area: the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Antarctic/Southern Ocean, and the Arctic Ocean),[9][10][11] and are themselves mostly divided into seas, gulfs and subsequent bodies of water. The ocean contains 97% of Earth's water[8] and is the primary component of Earth's hydrosphere, acting as a huge reservoir of heat for Earth's energy budget, as well as for its carbon cycle and water cycle, forming the basis for climate and weather patterns worldwide. The ocean is essential to life on Earth, harbouring most of Earth's animals and protist life,[12] originating photosynthesis and therefore Earth's atmospheric oxygen, still supplying half of it.[13] Ocean scientists split the ocean into vertical and horizontal zones based on physical and biological conditions. Horizontally the ocean covers the oceanic crust, which it shapes. Where the ocean meets dry land it covers relatively shallow continental shelfs, which are part of Earth's continental crust. Human activity is mostly coastal with high negative impacts on marine life. Vertically the pelagic zone is the open ocean's water column from the surface to the ocean floor. The water column is further divided into zones based on depth and the amount of light present. The photic zone starts at the surface and is defined to be \"the depth at which light intensity is only 1% of the surface value\"[14]: 36 (approximately 200 m in the open ocean). This is the zone where photosynthesis can occur. In this process plants and microscopic algae (free-floating phytoplankton) use light, water, carbon dioxide, and nutrients to produce organic matter. As a result, the photic zone is the most biodiverse",
      "expected_answer": "Jack Keeney: John Christopher \"Jack\" Keeney (February 19, 1922 \u2013 November 19, 2011) was an American prosecutor who retired in 2010 as U.S. deputy United States Assistant Attorney General. At age 88, he was at the time the DOJ's oldest employee, and one of the longest-serving career employees in the history of the United States government. Upon his retirement, Keeney was the longest-serving federal prosecutor in American history.[1] Keeney spent decades in the United States Department of Justice Criminal Division, starting in 1951. On numerous occasions, Keeney served as Acting Assistant Attorney General. Keeney was born in Ashley, Pennsylvania, on February 19, 1922.[1] Keeney was a pilot in the Army Air Corps during World War II, and was held by German forces as a prisoner of war. Keeney graduated from the University of Scranton in 1947.[1] He received law degrees from Dickinson School of Law in 1949 and from George Washington University Law School in 1953.[1] In 2000, the Justice Department named one of its buildings (1301 New York Avenue, N.W., Washington, D.C.) after Keeney, an honor rarely bestowed on a living person.[1] In the month following his death, the Justice Department created the John C. Keeney Award for Exceptional Integrity and Professionalism. The John C. Keeney Award recognizes a Justice Department employee who has demonstrated outstanding professionalism and integrity over a sustained period of time or an employee who has displayed extraordinary strength of character in a unique situation, as Mr. Keeney displayed during his years of service to the federal government.[2] Keeney died on November 19, 2011, at his home in Kensington, Maryland, aged 89.[3] This American law\u2013related biographical article is a stub. You can help Wikipedia by adding missing information. Ocean: The ocean is the body of salt water that covers approximately 70.8% of Earth.[8] The ocean is conventionally divided into large bodies of water, which are also referred to as oceans (in descending order by area: the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Antarctic/Southern Ocean, and the Arctic Ocean),[9][10][11]  and are themselves mostly divided into seas, gulfs and subsequent bodies of water. The ocean contains 97% of Earth's water[8] and is the primary component of Earth's hydrosphere, acting as a huge reservoir of heat for Earth's energy budget, as well as for its carbon cycle and water cycle, forming the basis for climate and weather patterns worldwide. The ocean is essential to life on Earth, harbouring most of Earth's animals and protist life,[12] originating photosynthesis and therefore Earth's atmospheric oxygen, still supplying half of it.[13] Ocean scientists split the ocean into vertical and horizontal zones based on physical and biological conditions. Horizontally the ocean covers the oceanic crust, which it shapes. Where the ocean meets dry land it covers relatively shallow continental shelfs, which are part of Earth's continental crust. Human activity is mostly coastal with high negative impacts on marine life. Vertically the pelagic zone is the open ocean's water column from the surface to the ocean floor. The water column is further divided into zones based on depth and the amount of light present. The photic zone starts at the surface and is defined to be \"the depth at which light intensity is only 1% of the surface value\"[14]:\u200a36\u200a (approximately 200\u00a0m in the open ocean). This is the zone where photosynthesis can occur. In this process plants and microscopic algae (free-floating phytoplankton) use light, water, carbon dioxide, and nutrients to produce organic matter. As a result, the photic zone is the most biodiverse and the source of the food supply which sustains most of the ocean ecosystem. Light can only penetrate a few hundred more meters; the rest of the deeper ocean is cold and dark (these zones are called mesopelagic and aphotic zones). Ocean temperatures depend on the amount of solar radiation reaching the ocean surface. In the tropics, surface temperatures can rise to over 30\u00a0\u00b0C (86\u00a0\u00b0F). Near the poles where sea ice forms, the temperature in equilibrium is about \u22122\u00a0\u00b0C (28\u00a0\u00b0F). In all parts of the ocean, deep ocean temperatures range between \u22122\u00a0\u00b0C (28\u00a0\u00b0F) and 5\u00a0\u00b0C (41\u00a0\u00b0F).[15] Constant circulation of water in the ocean creates ocean currents. Those currents are caused by forces operating on the water, such as temperature and salinity differences, atmospheric circulation (wind), and the Coriolis effect.[16] Tides create tidal currents, while wind and waves cause surface currents. The Gulf Stream, Kuroshio Current, Agulhas Current and Antarctic Circumpolar Current are all major ocean currents. Such currents transport massive amounts of water, gases, pollutants and heat to different parts of the world, and from the surface into the deep ocean.  All this has impacts on the global climate system. Ocean water contains dissolved gases, including oxygen, carbon dioxide and nitrogen. An exchange of these gases occurs at the ocean's surface. The solubility of these gases depends on the temperature and salinity of the water.[17] The carbon dioxide concentration in the atmosphere is rising due to CO2 emissions, mainly from fossil fuel combustion. As the oceans absorb CO2 from the atmosphere, a higher concentration leads to ocean acidification (a drop in pH value).[18] The ocean provides many benefits to humans such as ecosystem services, access to seafood and other marine resources, and a means of transport. The ocean is known to be the habitat of over 230,000 species, but may hold considerably more \u2013 perhaps over two million species.[19] Yet, the ocean faces many environmental threats, such as marine pollution, overfishing, and the effects of climate change. Those effects include ocean warming, ocean acidification and sea level rise. The continental shelf and coastal waters are most affected by human activity. The terms \"the ocean\" or \"the sea\" used without specification refer to the interconnected body of salt water covering the majority of Earth's surface, i.e., the world ocean.[9][10] It includes the Pacific, Atlantic, Indian, Antarctic/Southern, and Arctic oceans.[20] As a general term, \"the ocean\" and \"the sea\" are often interchangeable.[21] Strictly speaking, a \"sea\" is a body of water (generally a division of the world ocean) partly or fully enclosed by land.[22] The word \"sea\" can also be used for many specific, much smaller bodies of seawater, such as the North Sea or the Red Sea. There is no sharp distinction between seas and oceans, though generally seas are smaller, and are often partly (as marginal seas) or wholly (as inland seas) bordered by land.[23] In medieval Europe, the World Sea was the body of water that encircled the Continent (the mainland of Europe, Asia and Africa), and thus excluded the Mediterranean, Black and Caspian Seas.[citation needed] The contemporary concept of the World Ocean was coined in the early 20th century by the Russian oceanographer Yuly Shokalsky to refer to the continuous ocean that covers and encircles most of Earth.[24][25] The global, interconnected body of salt water is sometimes referred to as the World Ocean, the global ocean or the great ocean.[26][27][28] The concept of a continuous body of water with relatively unrestricted exchange between its components is critical in oceanography.[29] The word ocean comes from the figure in classical antiquity, Oceanus (/o\u028a\u02c8si\u02d0\u0259n\u0259s/; Ancient Greek: \u1f68\u03ba\u03b5\u03b1\u03bd\u03cc\u03c2 \u014ckean\u00f3s,[30] pronounced [\u0254\u02d0kean\u00f3s]), the elder of the Titans in classical Greek mythology. Oceanus was believed by the ancient Greeks and Romans to be the divine personification of an enormous river encircling the world. The concept of \u014ckean\u00f3s could have an Indo-European connection. Greek \u014ckean\u00f3s has been compared to the Vedic epithet \u0101-\u015b\u00e1y\u0101na-, predicated of the dragon V\u1e5btra-, who captured the cows/rivers. Related to this notion, the Okeanos is represented with a dragon-tail on some early Greek vases.[31] Scientists believe that a sizable quantity of water would have been in the material that formed Earth.[32] Water molecules would have escaped Earth's gravity more easily when it was less massive during its formation. This is called atmospheric escape. During planetary formation, Earth possibly had magma oceans. Subsequently, outgassing, volcanic activity and meteorite impacts, produced an early atmosphere of carbon dioxide, nitrogen and water vapor, according to current theories.\nThe gases and the atmosphere are thought to have accumulated over millions of years. After Earth's surface had significantly cooled, the water vapor over time would have condensed, forming Earth's first oceans.[33] The early oceans might have been significantly hotter than today and appeared green due to high iron content.[34] Geological evidence helps constrain the time frame for liquid water existing on Earth. A sample of pillow basalt (a type of rock formed during an underwater eruption) was recovered from the Isua Greenstone Belt and provides evidence that water existed on Earth 3.8\u00a0billion years ago.[35] In the Nuvvuagittuq Greenstone Belt, Quebec, Canada, rocks dated at 3.8\u00a0billion years old by one study[36] and 4.28\u00a0billion years old by another[37] show evidence of the presence of water at these ages.[35] If oceans existed earlier than this, any geological evidence either has yet to be discovered, or has since been destroyed by geological processes like crustal recycling.\nHowever, in August 2020, researchers reported that sufficient water to fill the oceans may have always been on the Earth since the beginning of the planet's formation.[38][39][40] In this model, atmospheric greenhouse gases kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity.[41] The origin of Earth's oceans is unknown. Oceans are thought to have formed in the Hadean eon and may have been the cause for the emergence of life. Plate tectonics, post-glacial rebound, and sea level rise continually change the coastline and structure of the world ocean. A global ocean has existed in one form or another on Earth for eons. Since its formation the ocean has taken many conditions and shapes with many past ocean divisions and potentially at times covering the whole globe.[42] During colder climatic periods, more ice caps and glaciers form, and enough of the global water supply accumulates as ice to lessen the amounts in other parts of the water cycle. The reverse is true during warm periods. During the last ice age, glaciers covered almost one-third of Earth's land mass with the result being that the oceans were about 122\u00a0m (400\u00a0ft) lower than today. During the last global \"warm spell,\" about 125,000 years ago, the seas were about 5.5\u00a0m (18\u00a0ft) higher than they are now. About three million years ago the oceans could have been up to 50\u00a0m (165\u00a0ft) higher.[43] The entire ocean, containing 97% of Earth's water, spans 70.8% of Earth's surface,[8] making it Earth's global ocean or world ocean.[24][26] This makes Earth, along with its vibrant hydrosphere a \"water world\"[44][45] or \"ocean world\",[46][47] particularly in Earth's early history when the ocean is thought to have possibly covered Earth completely.[42] The ocean's shape is irregular, unevenly dominating Earth's surface. This leads to the distinction of Earth's surface into land and water hemispheres, as well as the division of the ocean into different oceans. Seawater covers about 361,000,000\u00a0km2 (139,000,000\u00a0sq\u00a0mi) and the ocean's furthest pole of inaccessibility, known as \"Point Nemo\", in a region known as spacecraft cemetery of the South Pacific Ocean, at 48\u00b052.6\u2032S 123\u00b023.6\u2032W\ufeff / \ufeff48.8767\u00b0S 123.3933\u00b0W\ufeff / -48.8767; -123.3933\ufeff (Point Nemo). This point is roughly 2,688\u00a0km (1,670\u00a0mi) from the nearest land.[48] There are different customs to subdivide the ocean and are adjourned by smaller bodies of water such as bays, bights, gulfs, seas, and straits. For practical and historical reasons, it is customary to divide the World Ocean into a set of five major oceans. By convention these are the Pacific, Atlantic, Indian, Arctic, and Southern (Antarctic) oceans. This five-ocean model only fully crystallized in the early 21st century, when the Southern Ocean, delineated by the Antarctic Circumpolar Current, was recognized by various government and international bodies: by the U.S. Board on Geographic Names since 1999,[49] and by the International Hydrographic Organization since 2000.[50] The five principal oceans are listed below in descending order of area and volume: The ocean fills Earth's oceanic basins. Earth's oceanic basins cover different geologic provinces of Earth's oceanic crust as well as continental crust. As such it covers mainly Earth's structural basins, but also continental shelves. Beside the world ocean, the Black and Caspian seas also occupy oceanic basins; the Mediterranean has in the past been cut off into its own basin. In mid-ocean, magma is constantly being thrust through the seabed between adjoining plates to form mid-oceanic ridges and here convection currents within the mantle tend to drive the two plates apart. Parallel to these ridges and nearer the coasts, one oceanic plate may slide beneath another oceanic plate in a process known as subduction. Deep trenches are formed here and the process is accompanied by friction as the plates grind together. The movement proceeds in jerks which cause earthquakes, heat is produced and magma is forced up creating underwater mountains, some of which may form chains of volcanic islands near to deep trenches. Near some of the boundaries between the land and sea, the slightly denser oceanic plates slide beneath the continental plates and more subduction trenches are formed. As they grate together, the continental plates are deformed and buckle causing mountain building and seismic activity.[60][61] Every ocean basin has a mid-ocean ridge, which creates a long mountain range beneath the ocean. Together they form the global mid-oceanic ridge system that features the longest mountain range in the world. The longest continuous mountain range is 65,000\u00a0km (40,000\u00a0mi). This underwater mountain range is several times longer than the longest continental mountain range\u00a0\u2013 the Andes.[62] Oceanographers of the Nippon Foundation-GEBCO Seabed 2030 Project (Seabed 2030) state that as of 2024 just over 26% of the ocean floor has been mapped at a higher resolution than provided by satellites, while the ocean as a whole will never be fully explored,[63] with some estimating 5% of it having been explored.[64] The zone where land meets sea is known as the coast, and the part between the lowest spring tides and the upper limit reached by splashing waves is the shore. A beach is the accumulation of sand or shingle on the shore.[65] A headland is a point of land jutting out into the sea and a larger promontory is known as a cape. The indentation of a coastline, especially between two headlands, is a bay. A small bay with a narrow inlet is a cove and a large bay may be referred to as a gulf.[66] Coastlines are influenced by several factors, including the strength of the waves arriving on the shore, the gradient of the land margin, the composition and hardness of the coastal rock, the inclination of the off-shore slope and the changes of the level of the land due to local uplift or submergence.[65] Normally, waves roll towards the shore at the rate of six to eight per minute and these are known as constructive waves as they tend to move material up the beach and have little erosive effect. Storm waves arrive on shore in rapid succession and are known as destructive waves as the swash moves beach material seawards. Under their influence, the sand and shingle on the beach is ground together and abraded. Around high tide, the power of a storm wave impacting on the foot of a cliff has a shattering effect as air in cracks and crevices is compressed and then expands rapidly with release of pressure. At the same time, sand and pebbles have an erosive effect as they are thrown against the rocks. This tends to undercut the cliff, and normal weathering processes such as the action of frost follows, causing further destruction. Gradually, a wave-cut platform develops at the foot of the cliff and this has a protective effect, reducing further wave-erosion.[65] Material worn from the margins of the land eventually ends up in the sea. Here it is subject to attrition as currents flowing parallel to the coast scour out channels and transport sand and pebbles away from their place of origin. Sediment carried to the sea by rivers settles on the seabed causing deltas to form in estuaries. All these materials move back and forth under the influence of waves, tides and currents.[65] Dredging removes material and deepens channels but may have unexpected effects elsewhere on the coastline. Governments make efforts to prevent flooding of the land by the building of breakwaters, seawalls, dykes and levees and other sea defences. For instance, the Thames Barrier is designed to protect London from a storm surge,[67] while the failure of the dykes and levees around New Orleans during Hurricane Katrina created a humanitarian crisis in the United States. Most of the ocean is blue in color, but in some places the ocean is blue-green, green, or even yellow to brown.[68] Blue ocean color is a result of several factors. First, water preferentially absorbs red light, which means that blue light remains and is reflected back out of the water. Red light is most easily absorbed and thus does not reach great depths, usually to less than 50 meters (164\u00a0ft). Blue light, in comparison, can penetrate up to 200 meters (656\u00a0ft).[69] Second, water molecules and very tiny particles in ocean water preferentially scatter blue light more than light of other colors. Blue light scattering by water and tiny particles happens even in the very clearest ocean water,[70] and is similar to blue light scattering in the sky. The main substances that affect the color of the ocean include dissolved organic matter, living phytoplankton with chlorophyll pigments, and non-living particles like marine snow and mineral sediments.[71] Chlorophyll can be measured by satellite observations and serves as a proxy for ocean productivity (marine primary productivity) in surface waters. In long term composite satellite images, regions with high ocean productivity show up in yellow and green colors because they contain more (green) phytoplankton, whereas areas of low productivity show up in blue. Ocean water represents the largest body of water within the global water cycle (oceans contain 97% of Earth's water). Evaporation from the ocean moves water into the atmosphere to later rain back down onto land and the ocean.[72] Oceans have a significant effect on the biosphere. The ocean as a whole is thought to cover approximately 90% of the Earth's biosphere.[73] Oceanic evaporation, as a phase of the water cycle, is the source of most rainfall (about 90%),[72] causing a global cloud cover of 67% and a consistent oceanic cloud cover of 72%.[74] Ocean temperatures affect climate and wind patterns that affect life on land. One of the most dramatic forms of weather occurs over the oceans: tropical cyclones (also called \"typhoons\" and \"hurricanes\" depending upon where the system forms). As the world's ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth, forms part of the carbon cycle and water cycle, and \u2013 as a huge heat reservoir \u2013 influences climate and weather patterns. The motions of the ocean surface, known as undulations or wind waves, are the partial and alternate rising and falling of the ocean surface. The series of mechanical waves that propagate along the interface between water and air is called swell \u2013 a term used in sailing, surfing and navigation.[75] These motions profoundly affect ships on the surface of the ocean and the well-being of people on those ships who might suffer from sea sickness. Wind blowing over the surface of a body of water forms waves that are perpendicular to the direction of the wind. The friction between air and water caused by a gentle breeze on a pond causes ripples to form. A stronger gust blowing over the ocean causes larger waves as the moving air pushes against the raised ridges of water. The waves reach their maximum height when the rate at which they are travelling nearly matches the speed of the wind. In open water, when the wind blows continuously as happens in the Southern Hemisphere in the Roaring Forties, long, organized masses of water called swell roll across the ocean.[76]:\u200a83\u201384\u200a[77][78] If the wind dies down, the wave formation is reduced, but already-formed waves continue to travel in their original direction until they meet land. The size of the waves depends on the fetch, the distance that the wind has blown over the water and the strength and duration of that wind. When waves meet others coming from different directions, interference between the two can produce broken, irregular seas.[77] Constructive interference can lead to the formation of unusually high rogue waves.[79] Most waves are less than 3\u00a0m (10\u00a0ft) high[79] and it is not unusual for strong storms to double or triple that height.[80] Rogue waves, however, have been documented at heights above 25 meters (82\u00a0ft).[81][82] The top of a wave is known as the crest, the lowest point between waves is the trough and the distance between the crests is the wavelength. The wave is pushed across the surface of the ocean by the wind, but this represents a transfer of energy and not horizontal movement of water. As waves approach land and move into shallow water, they change their behavior. If approaching at an angle, waves may bend (refraction) or wrap around rocks and headlands (diffraction). When the wave reaches a point where its deepest oscillations of the water contact the ocean floor, they begin to slow down. This pulls the crests closer together and increases the waves' height, which is called wave shoaling. When the ratio of the wave's height to the water depth increases above a certain limit, it \"breaks\", toppling over in a mass of foaming water.[79] This rushes in a sheet up the beach before retreating into the ocean under the influence of gravity.[83] Earthquakes, volcanic eruptions or other major geological disturbances can set off waves that can lead to tsunamis in coastal areas which can be very dangerous.[84][85] The ocean's surface is an important reference point for oceanography and geography, particularly as mean sea level. The ocean surface has globally little, but measurable topography, depending on the ocean's volumes. The ocean surface is a crucial interface for oceanic and atmospheric processes. Allowing interchange of particles, enriching the air and water, as well as grounds by some particles becoming sediments. This interchange has fertilized life in the ocean, on land and air. All these processes and components together make up ocean surface ecosystems. Tides are the regular rise and fall in water level experienced by oceans, primarily driven by the Moon's gravitational tidal forces upon the Earth. Tidal forces affect all matter on Earth, but only fluids like the ocean demonstrate the effects on human timescales. (For example, tidal forces acting on rock may produce tidal locking between two planetary bodies.) Though primarily driven by the Moon's gravity, oceanic tides are also substantially modulated by the Sun's tidal forces, by the rotation of the Earth, and by the shape of the rocky continents blocking oceanic water flow. (Tidal forces vary more with distance than the \"base\" force of gravity: the Moon's tidal forces on Earth are more than double the Sun's,[86] despite the latter's much stronger gravitational force on Earth. Earth's tidal forces upon the Moon are 20x stronger than the Moon's tidal forces on the Earth.) The primary effect of lunar tidal forces is to bulge Earth matter towards the near and far sides of the Earth, relative to the moon. The \"perpendicular\" sides, from which the Moon appears in line with the local horizon, experience \"tidal troughs\". Since it takes nearly 25 hours for the Earth to rotate under the Moon (accounting for the Moon's 28-day orbit around Earth), tides thus cycle over a course of 12.5 hours. However, the rocky continents pose obstacles for the tidal bulges, so the timing of tidal maxima may not actually align with the Moon in most localities on Earth, as the oceans are forced to \"dodge\" the continents. Timing and magnitude of tides vary widely across the Earth as a result of the continents. Thus, knowing the Moon's position does not allow a local to predict tide timings, instead requiring precomputed tide tables which account for the continents and the Sun, among others. During each tidal cycle, at any given place the tidal waters rise to maximum height, high tide, before ebbing away again to the minimum level, low tide. As the water recedes, it gradually reveals the foreshore, also known as the intertidal zone. The difference in height between the high tide and low tide is known as the tidal range or tidal amplitude.[87][88]  When the sun and moon are aligned (full moon or new moon), the combined effect results in the higher \"spring tides\", while the sun and moon misaligning (half moons) result in lesser tidal ranges.[87] In the open ocean tidal ranges are less than 1 meter, but in coastal areas these tidal ranges increase to more than 10 meters in some areas.[89] Some of the largest tidal ranges in the world occur in the Bay of Fundy and Ungava Bay in Canada, reaching up to 16 meters.[90] Other locations with record high tidal ranges include the Bristol Channel between England and Wales, Cook Inlet in Alaska, and the R\u00edo Gallegos in Argentina.[91] Tides are not to be confused with storm surges, which can occur when high winds pile water up against the coast in a shallow area and this, coupled with a low pressure system, can raise the surface of the ocean dramatically above a typical high tide. The average depth of the oceans is about 4\u00a0km. More precisely the average depth is 3,688 meters (12,100\u00a0ft).[77] Nearly half of the world's marine waters are over 3,000 meters (9,800\u00a0ft) deep.[28] \"Deep ocean,\" which is anything below 200 meters (660\u00a0ft), covers about 66% of Earth's surface.[92] This figure does not include seas not connected to the World Ocean, such as the Caspian Sea. The deepest region of the ocean is at the Mariana Trench, located in the Pacific Ocean near the Northern Mariana Islands.[93] The maximum depth has been estimated to be 10,971 meters (35,994\u00a0ft). The British naval vessel Challenger II surveyed the trench in 1951 and named the deepest part of the trench the \"Challenger Deep\". In 1960, the Trieste successfully reached the bottom of the trench, manned by a crew of two men. Oceanographers classify the ocean into vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The ocean zones can be grouped by light penetration into (from top to bottom): the photic zone, the mesopelagic zone and the aphotic deep ocean zone: The pelagic part of the aphotic zone can be further divided into vertical regions according to depth and temperature:[95] Distinct boundaries between ocean surface waters and deep waters can be drawn based on the properties of the water. These boundaries are called thermoclines (temperature), haloclines (salinity), chemoclines (chemistry), and pycnoclines (density). If a zone undergoes dramatic changes in temperature with depth, it contains a thermocline, a distinct boundary between warmer surface water and colder deep water. In tropical regions, the thermocline is typically deeper compared to higher latitudes. Unlike polar waters, where solar energy input is limited, temperature stratification is less pronounced, and a distinct thermocline is often absent. This is due to the fact that surface waters in polar latitudes are nearly as cold as deeper waters. Below the thermocline, water everywhere in the ocean is very cold, ranging from \u22121\u00a0\u00b0C to 3\u00a0\u00b0C. Because this deep and cold layer contains the bulk of ocean water, the average temperature of the world ocean is 3.9\u00a0\u00b0C.[96] If a zone undergoes dramatic changes in salinity with depth, it contains a halocline. If a zone undergoes a strong, vertical chemistry gradient with depth, it contains a chemocline. Temperature and salinity control ocean water density. Colder and saltier water is denser, and this density plays a crucial role in regulating the global water circulation within the ocean.[95] The halocline often coincides with the thermocline, and the combination produces a pronounced pycnocline, a boundary between less dense surface water and dense deep water. The pelagic zone can be further subdivided into two sub regions based on distance from land: the neritic zone and the oceanic zone. The neritic zone covers the water directly above the continental shelves, including coastal waters. On the other hand, the oceanic zone includes all the completely open water. The littoral zone covers the region between low and high tide and represents the transitional area between marine and terrestrial conditions. It is also known as the intertidal zone because it is the area where tide level affects the conditions of the region.[95] The combined volume of water in all the oceans is roughly 1.335\u00a0billion cubic kilometers (1.335 sextillion liters, 320.3\u00a0million cubic miles).[77][97][98] It has been estimated that there are 1.386 billion cubic kilometres (333 million cubic miles) of water on Earth.[99][100][101] This includes water in gaseous, liquid and frozen forms as soil moisture, groundwater and permafrost in the Earth's crust (to a depth of 2\u00a0km); oceans and seas, lakes, rivers and streams, wetlands, glaciers, ice and snow cover on Earth's surface; vapour, droplets and crystals in the air; and part of living plants, animals and unicellular organisms of the biosphere. Saltwater accounts for 97.5% of this amount, whereas fresh water accounts for only 2.5%. Of this fresh water, 68.9% is in the form of ice and permanent snow cover in the Arctic, the Antarctic and mountain glaciers; 30.8% is in the form of fresh groundwater; and only 0.3% of the fresh water on Earth is in easily accessible lakes, reservoirs and river systems.[102] The total mass of Earth's hydrosphere is about 1.4 \u00d7 1018 tonnes, which is about 0.023% of Earth's total mass. At any given time, about 2 \u00d7 1013 tonnes of this is in the form of water vapor in the Earth's atmosphere (for practical purposes, 1 cubic metre of water weighs 1 tonne). Approximately 71% of Earth's surface, an area of some 361 million square kilometres (139.5 million square miles), is covered by ocean. The average salinity of Earth's oceans is about 35\u00a0grams of salt per kilogram of sea water (3.5%).[103] Ocean temperatures depends on the amount of solar radiation falling on its surface. In the tropics, with the Sun nearly overhead, the temperature of the surface layers can rise to over 30\u00a0\u00b0C (86\u00a0\u00b0F) while near the poles the temperature in equilibrium with the sea ice is about \u22122\u00a0\u00b0C (28\u00a0\u00b0F). There is a continuous circulation of water in the oceans. Warm surface currents cool as they move away from the tropics, and the water becomes denser and sinks. The cold water moves back towards the equator as a deep sea current, driven by changes in the temperature and density of the water, before eventually welling up again towards the surface. Deep ocean water has a temperature between \u22122\u00a0\u00b0C (28\u00a0\u00b0F) and 5\u00a0\u00b0C (41\u00a0\u00b0F) in all parts of the globe.[15] The temperature gradient over the water depth is related to the way the surface water mixes with deeper water or does not mix (a lack of mixing is called ocean stratification). This depends on the temperature: in the tropics the warm surface layer of about 100 m is quite stable and does not mix much with deeper water, while near the poles winter cooling and storms makes the surface layer denser and it mixes to great depth and then stratifies again in summer. The photic depth is typically about 100 m (but varies) and is related to this heated surface layer.[104] It is clear that the ocean is warming as a result of climate change, and this rate of warming is increasing.[105]:\u200a9\u200a The global ocean was the warmest it had ever been recorded by humans in 2022.[106] This is determined by the ocean heat content, which exceeded the previous 2021 maximum in 2022.[106] The steady rise in ocean temperatures is an unavoidable result of the Earth's energy imbalance, which is primarily caused by rising levels of greenhouse gases.[106] Between pre-industrial times and the 2011\u20132020 decade, the ocean's surface has heated between 0.68 and 1.01\u00a0\u00b0C.[107]:\u200a1214 The temperature and salinity of ocean waters vary significantly across different regions. This is due to differences in the local water balance (precipitation vs. evaporation) and the \"sea to air\" temperature gradients. These characteristics can vary widely from one ocean region to another. The table below provides an illustration of the sort of values usually encountered. Seawater with a typical salinity of 35\u2030 has a freezing point of about \u22121.8\u00a0\u00b0C (28.8\u00a0\u00b0F).[95][113] Because sea ice is less dense than water, it floats on the ocean's surface (as does fresh water ice, which has an even lower density). Sea ice covers about 7% of the Earth's surface and about 12% of the world's oceans.[114][115][116] Sea ice usually starts to freeze at the very surface, initially as a very thin ice film. As further freezing takes place, this ice film thickens and can form ice sheets. The ice formed incorporates some sea salt, but much less than the seawater it forms from. As the ice forms with low salinity this results in saltier residual seawater. This in turn increases density and promotes vertical sinking of the water.[117] An ocean current is a continuous, directed flow of seawater caused by several forces acting upon the water. These include wind, the Coriolis effect, temperature and salinity differences.[16] Ocean currents are primarily horizontal water movements that have different origins such as tides for tidal currents, or wind and waves for surface currents. Tidal currents are in phase with the tide, hence are quasiperiodic; associated with the influence of the moon and sun pull on the ocean water. Tidal currents may form various complex patterns in certain places, most notably around headlands.[118] Non-periodic or non-tidal currents are created by the action of winds and changes in density of water. In littoral zones, breaking waves are so intense and the depth measurement so low, that maritime currents reach often 1 to 2 knots.[119] The wind and waves create surface currents (designated as \"drift currents\"). These currents can decompose in one quasi-permanent current (which varies within the hourly scale) and one movement of Stokes drift under the effect of rapid waves movement (which vary on timescales of a couple of seconds). The quasi-permanent current is accelerated by the breaking of waves, and in a lesser governing effect, by the friction of the wind on the surface.[119] This acceleration of the current takes place in the direction of waves and dominant wind. Accordingly, when the ocean depth increases, the rotation of the earth changes the direction of currents in proportion with the increase of depth, while friction lowers their speed. At a certain ocean depth, the current changes direction and is seen inverted in the opposite direction with current speed becoming null: known as the Ekman spiral. The influence of these currents is mainly experienced at the mixed layer of the ocean surface, often from 400 to 800 meters of maximum depth. These currents can considerably change and are dependent on the yearly seasons. If the mixed layer is less thick (10 to 20 meters), the quasi-permanent current at the surface can adopt quite a different direction in relation to the direction of the wind. In this case, the water column becomes virtually homogeneous above the thermocline.[119] The wind blowing on the ocean surface will set the water in motion. The global pattern of winds (also called atmospheric circulation) creates a global pattern of ocean currents. These are driven not only by the wind but also by the effect of the circulation of the earth (coriolis force). These major ocean currents include the Gulf Stream, Kuroshio Current, Agulhas Current and Antarctic Circumpolar Current. The Antarctic Circumpolar Current encircles Antarctica and influences the area's climate, connecting currents in several oceans.[119] Collectively, currents move enormous amounts of water and heat around the globe influencing climate. These wind driven currents are largely confined to the top hundreds of meters of the ocean. At greater depth, the thermohaline circulation drives water motion. For example, the Atlantic meridional overturning circulation (AMOC) is driven by the cooling of surface waters in the polar latitudes in the north and south, creating dense water which sinks to the bottom of the ocean. This cold and dense water moves slowly away from the poles which is why the waters in the deepest layers of the world ocean are so cold. This deep ocean water circulation is relatively slow and water at the bottom of the ocean can be isolated from the ocean surface and atmosphere for hundreds or even a few thousand years.[119] This circulation has important impacts on the global climate system and on the uptake and redistribution of pollutants and gases such as carbon dioxide, for example by moving contaminants from the surface into the deep ocean. Ocean currents greatly affect Earth's climate by transferring heat from the tropics to the polar regions. This affects air temperature and precipitation in coastal regions and further inland. Surface heat and freshwater fluxes create global density gradients, which drive the thermohaline circulation that is a part of large-scale ocean circulation. It plays an important role in supplying heat to the polar regions, and thus in sea ice regulation.[citation needed] Oceans moderate the climate of locations where prevailing winds blow in from the ocean. At similar latitudes, a place on Earth with more influence from the ocean will have a more moderate climate than a place with more influence from land. For example, the cities San Francisco (37.8 N) and New York (40.7 N) have different climates because San Francisco has more influence from the ocean. San Francisco, on the west coast of North America, gets winds from the west over the Pacific Ocean. New York, on the east coast of North America gets winds from the west over land, so New York has colder winters and hotter, earlier summers than San Francisco. Warmer ocean currents yield warmer climates in the long term, even at high latitudes. At similar latitudes, a place influenced by warm ocean currents will have a warmer climate overall than a place influenced by cold ocean currents.[citation needed] Changes in the thermohaline circulation are thought to have significant impacts on Earth's energy budget. Because the thermohaline circulation determines the rate at which deep waters reach the surface, it may also significantly influence atmospheric carbon dioxide concentrations. Modern observations, climate simulations and paleoclimate reconstructions suggest that the Atlantic meridional overturning circulation (AMOC) has weakened since the preindustrial era. The latest climate change projections in 2021 suggest that the AMOC is likely to weaken further over the 21st century.[120]:\u200a19\u200a Such a weakening could cause large changes to global climate, with the North Atlantic particularly vulnerable.[120]:\u200a19 Salinity is a measure of the total amounts of dissolved salts in seawater. It was originally measured via measurement of the amount of chloride in seawater and hence termed chlorinity. It is now standard practice to gauge it by measuring electrical conductivity of the water sample. Salinity can be calculated using the chlorinity, which is a measure of the total mass of halogen ions (includes fluorine, chlorine, bromine, and iodine) in seawater. According to an international agreement, the following formula is used to determine salinity:[122] The average ocean water chlorinity is about 19.2\u2030 (equal to 1.92%), and thus the average salinity is around 34.7\u2030 (3.47%).[122] Salinity has a major influence on the density of seawater. A zone of rapid salinity increase with depth is called a halocline. As seawater's salt content increases, so does the temperature at which its maximum density occurs. Salinity affects both the freezing and boiling points of water, with the boiling point increasing with salinity. At atmospheric pressure,[123] normal seawater freezes at a temperature of about \u22122\u00a0\u00b0C. Salinity is higher in Earth's oceans where there is more evaporation and lower where there is more precipitation. If precipitation exceeds evaporation, as is the case in polar and some temperate regions, salinity will be lower. Salinity will be higher if evaporation exceeds precipitation, as is sometimes the case in tropical regions. For example, evaporation is greater than precipitation in the Mediterranean Sea, which has an average salinity of 38\u2030, more saline than the global average of 34.7\u2030.[124] Thus, oceanic waters in polar regions have lower salinity content than oceanic waters in tropical regions.[122] However, when sea ice forms at high latitudes, salt is excluded from the ice as it forms, which can increase the salinity in the residual seawater in polar regions such as the Arctic Ocean.[95][125] Due to the effects of climate change on oceans, observations of sea surface salinity between 1950 and 2019 indicate that regions of high salinity and evaporation have become more saline while regions of low salinity and more precipitation have become fresher.[126] It is very likely that the Pacific and Antarctic/Southern Oceans have freshened while the Atlantic has become more saline.[126] Ocean water contains large quantities of dissolved gases, including oxygen, carbon dioxide and nitrogen. These dissolve into ocean water via gas exchange at the ocean surface, with the solubility of these gases depending on the temperature and salinity of the water.[17] The four most abundant gases in earth's atmosphere and oceans are nitrogen, oxygen, argon, and carbon dioxide. In the ocean by volume, the most abundant gases dissolved in seawater are carbon dioxide (including bicarbonate and carbonate ions, 14 mL/L on average), nitrogen (9 mL/L), and oxygen (5 mL/L) at equilibrium at 24\u00a0\u00b0C (75\u00a0\u00b0F)[128][129][130] All gases are more soluble \u2013 more easily dissolved \u2013 in colder water than in warmer water. For example, when salinity and pressure are held constant, oxygen concentration in water almost doubles when the temperature drops from that of a warm summer day 30\u00a0\u00b0C (86\u00a0\u00b0F) to freezing 0\u00a0\u00b0C (32\u00a0\u00b0F). Similarly, carbon dioxide and nitrogen gases are more soluble at colder temperatures, and their solubility changes with temperature at different rates.[128][131] Photosynthesis in the surface ocean releases oxygen and consumes carbon dioxide. Phytoplankton, a type of microscopic free-floating algae, controls this process. After the plants have grown, oxygen is consumed and carbon dioxide released, as a result of bacterial decomposition of the organic matter created by photosynthesis in the ocean. The sinking and bacterial decomposition of some organic matter in deep ocean water, at depths where the waters are out of contact with the atmosphere, leads to a reduction in oxygen concentrations and increase in carbon dioxide, carbonate and bicarbonate.[104] This cycling of carbon dioxide in oceans is an important part of the global carbon cycle. The oceans represent a major carbon sink for carbon dioxide taken up from the atmosphere by photosynthesis and by dissolution (see also carbon sequestration). There is also increased attention on carbon dioxide uptake in coastal marine habitats such as mangroves and saltmarshes. This process is often referred to as \"Blue carbon\". The focus is on these ecosystems because they are strong carbon sinks as well as ecologically important habitats under threat from human activities and environmental degradation. As deep ocean water circulates throughout the globe, it contains gradually less oxygen and gradually more carbon dioxide with more time away from the air at the surface. This gradual decrease in oxygen concentration happens as sinking organic matter continuously gets decomposed during the time the water is out of contact with the atmosphere.[104] Most of the deep waters of the ocean still contain relatively high concentrations of oxygen sufficient for most animals to survive. However, some ocean areas have very low oxygen due to long periods of isolation of the water from the atmosphere. These oxygen deficient areas, called oxygen minimum zones or hypoxic waters, will generally be made worse by the effects of climate change on oceans.[133][134] The pH value at the surface of oceans (global mean surface pH) is currently approximately in the range of 8.05[135] to 8.08.[136] This makes it slightly alkaline. The pH value at the surface used to be about 8.2 during the past 300 million years.[137] However, between 1950 and 2020, the average pH of the ocean surface fell from approximately 8.15 to 8.05.[138] Carbon dioxide emissions from human activities are the primary cause of this process called ocean acidification, with atmospheric carbon dioxide (CO2) levels exceeding 410 ppm (in 2020).[139] CO2 from the atmosphere is absorbed by the oceans. This produces carbonic acid (H2CO3) which dissociates into a bicarbonate ion (HCO\u22123) and a hydrogen ion (H+). The presence of free hydrogen ions (H+) lowers the pH of the ocean. There is a natural gradient of pH in the ocean which is related to the breakdown of organic matter in deep water which slowly lowers the pH with depth: The pH value of seawater is naturally as low as 7.8 in deep ocean waters as a result of degradation of organic matter there.[140] It can be as high as 8.4 in surface waters in areas of high biological productivity.[104] The definition of global mean surface pH refers to the top layer of the water in the ocean, up to around 20 or 100 m depth. In comparison, the average depth of the ocean is about 4\u00a0km. The pH value at greater depths (more than 100 m) has not yet been affected by ocean acidification in the same way. There is a large body of deeper water where the natural gradient of pH from 8.2 to about 7.8 still exists and it will take a very long time to acidify these waters, and equally as long to recover from that acidification. But as the top layer of the ocean (the photic zone) is crucial for its marine productivity, any changes to the pH value and temperature of the top layer can have many knock-on effects, for example on marine life and ocean currents (such as effects of climate change on oceans).[104] The key issue in terms of the penetration of ocean acidification is the way the surface water mixes with deeper water or does not mix (a lack of mixing is called ocean stratification). This in turn depends on the water temperature and hence is different between the tropics and the polar regions (see ocean#Temperature).[104] The chemical properties of seawater complicate pH measurement, and several distinct pH scales exist in chemical oceanography.[141] There is no universally accepted reference pH-scale for seawater and the difference between measurements based on multiple reference scales may be up to 0.14 units.[142] Alkalinity is the balance of base (proton acceptors) and acids (proton donors) in seawater, or indeed any natural waters. The alkalinity acts as a chemical buffer, regulating the pH of seawater. While there are many ions in seawater that can contribute to the alkalinity, many of these are at very low concentrations. This means that the carbonate, bicarbonate and borate ions are the only significant contributors to seawater alkalinity in the open ocean with well oxygenated waters. The first two of these ions contribute more than 95% of this alkalinity.[104] The chemical equation for alkalinity in seawater is: The growth of phytoplankton in surface ocean waters leads to the conversion of some bicarbonate and carbonate ions into organic matter. Some of this organic matter sinks into the deep ocean where it is broken down back into carbonate and bicarbonate. This process is related to ocean productivity or marine primary production. Thus alkalinity tends to increase with depth and also along the global thermohaline circulation from the Atlantic to the Pacific and Indian Ocean, although these increases are small. The concentrations vary overall by only a few percent.[104][140] The absorption of CO2 from the atmosphere does not affect the ocean's alkalinity.[143]:\u200a2252\u200a It does lead to a reduction in pH value though (termed ocean acidification).[139] The ocean waters contain many chemical elements as dissolved ions. Elements dissolved in ocean waters have a wide range of concentrations. Some elements have very high concentrations of several grams per liter, such as sodium and chloride, together making up the majority of ocean salts. Other elements, such as iron, are present at tiny concentrations of just a few nanograms (10\u22129 grams) per liter.[122] The concentration of any element depends on its rate of supply to the ocean and its rate of removal. Elements enter the ocean from rivers, the atmosphere and hydrothermal vents. Elements are removed from ocean water by sinking and becoming buried in sediments or evaporating to the atmosphere in the case of water and some gases.  By estimating the residence time of an element, oceanographers examine the balance of input and removal. Residence time is the average time the element would spend dissolved in the ocean before it is removed. Heavily abundant elements in ocean water such as sodium, have high input rates. This reflects high abundance in rocks and rapid rock weathering, paired with very slow removal from the ocean due to sodium ions being comparatively unreactive and highly soluble. In contrast, other elements such as iron and aluminium are abundant in rocks but very insoluble, meaning that inputs to the ocean are low and removal is rapid. These cycles represent part of the major global cycle of elements that has gone on since the Earth first formed. The residence times of the very abundant elements in the ocean are estimated to be millions of years, while for highly reactive and insoluble elements, residence times are only hundreds of years.[122] A few elements such as nitrogen, phosphorus, iron, and potassium essential for life, are major components of biological material, and are commonly known as \"nutrients\". Nitrate and phosphate have ocean residence times of 10,000[146] and 69,000[147] years, respectively, while potassium is a much more abundant ion in the ocean with a residence time of 12 million[148] years. The biological cycling of these elements means that this represents a continuous removal process from the ocean's water column as degrading organic material sinks to the ocean floor as sediment. Phosphate from intensive agriculture and untreated sewage is transported via runoff to rivers and coastal zones to the ocean where it is metabolized. Eventually, it sinks to the ocean floor and is no longer available to humans as a commercial resource.[149] Production of rock phosphate, an essential ingredient in inorganic fertilizer,[150] is a slow geological process that occurs in some of the world's ocean sediments, rendering mineable sedimentary apatite (phosphate) a non-renewable resource (see peak phosphorus). This continual net deposition loss of non-renewable phosphate from human activities, may become a resource issue for fertilizer production and food security in future.[151][152] Life within the ocean evolved 3\u00a0billion years prior to life on land. Both the depth and the distance from shore strongly influence the biodiversity of the plants and animals present in each region.[154] The diversity of life in the ocean is immense, including: Marine life, which is also known as sea life or ocean life, refers to all the marine organisms that live in salt water habitats, or ecological communities that encompass all aquatic animals, plants, algae, fungi, protists, single-celled microorganisms and associated viruses living in the saline water of marine habitats, either the sea water of marginal seas and oceans, or the brackish water of coastal wetlands, lagoons, estuaries and inland seas. As of 2023[update], more than 242,000 marine species have been documented, and perhaps two million marine species are yet to be documented. On average, researches describe about 2,300 new marine species each year.[156][157] The study of marine life spans into multiple fields, which is primarily marine biology, as well as biological oceanography. A marine habitat is a habitat that supports marine life. Marine life depends in some way on the saltwater that is in the sea (the term marine comes from the Latin mare, meaning sea or ocean). A habitat is an ecological or environmental area inhabited by one or more living  species.[158] The marine environment supports many kinds of these habitats. Marine ecosystems are the largest of Earth's aquatic ecosystems and exist in waters that have a high salt content. These systems contrast with freshwater ecosystems, which have a lower salt content. Marine waters cover more than 70% of the surface of the Earth and account for more than 97% of Earth's water supply[159][160] and 90% of habitable space on Earth.[161] Seawater has an average salinity of 35 parts per thousand of water. Actual salinity varies among different marine ecosystems.[162] Marine ecosystems can be divided into many zones depending upon water depth and shoreline features. The oceanic zone is the vast open part of the ocean where animals such as      whales, sharks, and tuna live. The benthic zone consists of substrates below water where many invertebrates live. The intertidal zone is the area between high and low tides. Other near-shore (neritic) zones can include mudflats, seagrass meadows, mangroves, rocky intertidal systems, salt marshes, coral reefs, kelp forests and lagoons. In the deep water, hydrothermal vents may occur where chemosynthetic sulfur bacteria form the base of the food web. The ocean has been linked to human activity throughout history. These activities serve a wide variety of purposes, including navigation and exploration, naval warfare, travel, shipping and trade, food production (e.g. fishing, whaling, seaweed farming, aquaculture), leisure (cruising, sailing, recreational boat fishing, scuba diving), power generation (see marine energy and offshore wind power), extractive industries (offshore drilling and deep sea mining), freshwater production via desalination. Many of the world's goods are moved by ship between the world's seaports.[163] Large quantities of goods are transported across the ocean, especially across the Atlantic and around the Pacific Rim.[164] Many types of cargo including manufactured goods, are typically transported in standard sized, lockable containers that are loaded on purpose-built container ships at dedicated terminals.[165] Containerization greatly boosted the efficiency and reduced the cost of shipping products by sea. This was a major factor in the rise of globalization and exponential increases in international trade in the mid-to-late 20th century.[166] Oceans are also the major supply source for the fishing industry. Some of the major harvests are shrimp, fish, crabs, and lobster.[73] The biggest global commercial fishery is for anchovies, Alaska pollock and tuna.[167]:\u200a6\u200a A report by FAO in 2020 stated that \"in 2017, 34 percent of the fish stocks of the world's marine fisheries were classified as overfished\".[167]:\u200a54\u200a Fish and other fishery products from both wild fisheries and aquaculture are among the most widely consumed sources of protein and other essential nutrients. Data in 2017 showed that \"fish consumption accounted for 17 percent of the global population's intake of animal proteins\".[167] To fulfill this need, coastal countries have exploited marine resources in their exclusive economic zone. Fishing vessels are increasingly venturing out to exploit stocks in international waters.[168] The ocean has a vast amount of energy carried by ocean waves, tides, salinity differences, and ocean temperature differences which can be harnessed to generate electricity.[169] Forms of sustainable marine energy include tidal power, ocean thermal energy and wave power.[169][170] Offshore wind power is captured by wind turbines placed out on the ocean; it has the advantage that wind speeds are higher than on land, though wind farms are more costly to construct offshore.[171] There are large deposits of petroleum, as oil and natural gas, in rocks beneath the ocean floor. Offshore platforms and drilling rigs extract the oil or gas and store it for transport to land.[172] \"Freedom of the seas\" is a principle in international law dating from the seventeenth century. It stresses freedom to navigate the oceans and disapproves of war fought in international waters. Today, this concept is enshrined in the United Nations Convention on the Law of the Sea (UNCLOS).[173] The International Maritime Organization (IMO), which was ratified in 1958, is mainly responsible for maritime safety, liability and compensation, and has held some conventions on marine pollution related to shipping incidents. Ocean governance is the conduct of the policy, actions and affairs regarding the world's oceans.[174] Human activities affect marine life and marine habitats through many negative influences, such as marine pollution (including marine debris and microplastics), overfishing, ocean acidification, and other effects of climate change on oceans. There are many effects of climate change on oceans. One of the most important is an increase in ocean temperatures. More frequent marine heatwaves are linked to this. The rising temperature contributes to a rise in sea levels due to the expansion of water as it warms and the melting of ice sheets on land. Other effects on oceans include sea ice decline, reducing pH values and oxygen levels, as well as increased ocean stratification. All this can lead to changes of ocean currents, for example a weakening of the Atlantic meridional overturning circulation (AMOC).[105] The main cause of these changes are the emissions of greenhouse gases from human activities, mainly burning of fossil fuels and deforestation. Carbon dioxide and methane are examples of greenhouse gases. The additional greenhouse effect leads to ocean warming because the ocean takes up most of the additional heat in the climate system.[176] The ocean also absorbs some of the extra carbon dioxide that is in the atmosphere. This causes the pH value of the seawater to drop.[177] Scientists estimate that the ocean absorbs about 25% of all human-caused CO2 emissions.[177] The various layers of the oceans have different temperatures. For example, the water is colder towards the bottom of the ocean. This temperature stratification will increase as the ocean surface warms due to rising air temperatures.[178]:\u200a471\u200a Connected to this is a decline in mixing of the ocean layers, so that warm water stabilises near the surface. A reduction of cold, deep water circulation follows. The reduced vertical mixing makes it harder for the ocean to absorb heat. So a larger share of future warming goes into the atmosphere and land. One result is an increase in the amount of energy available for tropical cyclones and other storms. Another result is a decrease in nutrients for fish in the upper ocean layers. These changes also reduce the ocean's capacity to store carbon.[179] At the same time, contrasts in salinity are increasing. Salty areas are becoming saltier and fresher areas less salty.[180] Warmer water cannot contain the same amount of oxygen as cold water. As a result, oxygen from the oceans moves to the atmosphere. Increased thermal stratification may reduce the supply of oxygen from surface waters to deeper waters. This lowers the water's oxygen content even more.[181] The ocean has already lost oxygen throughout its water column. Oxygen minimum zones are increasing in size worldwide.[178]:\u200a471 These changes harm marine ecosystems, and this can lead to biodiversity loss or changes in species distribution.[105] This in turn can affect fishing and coastal tourism. For example, rising water temperatures are harming tropical coral reefs. The direct effect is coral bleaching on these reefs, because they are sensitive to even minor temperature changes. So a small increase in water temperature could have a significant impact in these environments. Another example is loss of sea ice habitats due to warming. This will have severe impacts on polar bears and other animals that rely on it. The effects of climate change on oceans put additional pressures on ocean ecosystems which are already under pressure by other impacts from human activities.[105] Marine pollution occurs when substances used or spread by humans, such as industrial, agricultural, and residential waste; particles; noise; excess carbon dioxide; or invasive organisms enter the ocean and cause harmful effects there. The majority of this waste (80%) comes from land-based activity, although marine transportation significantly contributes as well.[182] It is a combination of chemicals and trash, most of which comes from land sources and is washed or blown into the ocean. This pollution results in damage to the environment, to the health of all organisms, and to economic structures worldwide.[183] Since most inputs come from land, via rivers, sewage, or the atmosphere, it means that continental shelves are more vulnerable to pollution. Air pollution is also a contributing factor, as it carries iron, carbonic acid, nitrogen, silicon, sulfur, pesticides, and dust particles into the ocean.[184] The pollution often comes from nonpoint sources such as agricultural runoff, wind-blown debris, and dust. These nonpoint sources are largely due to runoff that enters the ocean through rivers, but wind-blown debris and dust can also play a role, as these pollutants can settle into waterways and oceans.[185] Pathways of pollution include direct discharge, land runoff, ship pollution, bilge pollution, dredging (which can create dredge plumes), atmospheric pollution and, potentially, deep sea mining. Different types of marine pollution can be grouped as pollution from marine debris, plastic pollution, including microplastics, ocean acidification, nutrient pollution, toxins, and underwater noise. Plastic pollution in the ocean is a type of marine pollution by plastics, ranging in size from large original material such as bottles and bags, down to microplastics formed from the fragmentation of plastic materials. Marine debris is mainly discarded human rubbish which floats on, or is suspended in the ocean. Plastic pollution is harmful to marine life. Another concern is the runoff of nutrients (nitrogen and phosphorus) from intensive agriculture, and the disposal of untreated or partially treated sewage to rivers and subsequently oceans. These nitrogen and phosphorus nutrients (which are also contained in fertilizers) stimulate phytoplankton and macroalgal growth, which can lead to harmful algal blooms (eutrophication) which can be harmful to humans as well as marine creatures. Excessive algal growth can also smother sensitive coral reefs and lead to loss of biodiversity and coral health. A second major concern is that the degradation of algal blooms can lead to consumption of oxygen in coastal waters, a situation that may worsen with climate change as warming reduces vertical mixing of the water column.[186] Many potentially toxic chemicals adhere to tiny particles which are then taken up by plankton and benthic animals, most of which are either deposit feeders or filter feeders. In this way, the toxins are concentrated upward within ocean food chains. When pesticides are incorporated into the marine ecosystem, they quickly become absorbed into marine food webs. Once in the food webs, these pesticides can cause mutations, as well as diseases, which can be harmful to humans as well as the entire food web. Toxic metals can also be introduced into marine food webs. These can cause a change to tissue matter, biochemistry, behavior, reproduction, and suppress growth in marine life. Also, many animal feeds have a high fish meal or fish hydrolysate content. In this way, marine toxins can be transferred to land animals, and appear later in meat and dairy products. Overfishing is the removal of aquatic animals\u2014primarily fish\u2014from a body of water at a rate greater than that the species can replenish its population naturally (i.e. the overexploitation of the fishery's existing fish stocks), resulting in the species becoming increasingly underpopulated in that area. Excessive fishing practices can occur in water bodies of any sizes, from ponds, wetlands, rivers, lakes to seas and oceans, and can result in resource depletion, reduced biological growth rates and low biomass levels. Sustained overfishing, especially industrial-scale commercial fishing, can lead to critical depensation, where the fish population is no longer able to sustain itself, resulting in extirpation or even extinction of species. Some forms of overfishing, such as the overfishing of sharks, has led to the upset of entire marine ecosystems.[187] Types of overfishing include growth overfishing, recruitment overfishing, and ecosystem overfishing. Overfishing not only causes negative impacts on biodiversity and ecosystem functioning, but also reduces fish production, which subsequently leads to negative social and economic consequences.[188][page\u00a0needed] Ocean protection serves to safeguard the ecosystems in the oceans upon which humans depend.[189][190] Protecting these ecosystems from threats is a major component of environmental protection. One of protective measures is the creation and enforcement of marine protected areas (MPAs). Marine protection may need to be considered within a national, regional and international context.[191] Other measures include supply chain transparency requirement policies, policies to prevent marine pollution, ecosystem-assistance (e.g. for coral reefs) and support for sustainable seafood (e.g. sustainable fishing practices and types of aquaculture). There is also the protection of marine resources and components whose extraction or disturbance would cause substantial harm, engagement of broader publics and impacted communities,[192] and the development of ocean clean-up projects (removal of marine plastic pollution). Examples of the latter include Clean Oceans International and The Ocean Cleanup. In 2021, 43 expert scientists published the first scientific framework version that \u2013 via integration, review, clarifications and standardization \u2013 enables the evaluation of levels of protection of marine protected areas and can serve as a guide for any subsequent efforts to improve, plan and monitor marine protection quality and extents. Examples are the efforts towards the 30%-protection-goal of the \"Global Deal For Nature\"[193] and the UN's Sustainable Development Goal 14 (\"life below water\").[194][195] In March 2023 a High Seas Treaty was signed. It is legally binding. The main achievement is the new possibility to create marine protected areas in international waters. By doing so the agreement now makes it possible to protect 30% of the oceans by 2030 (part of the 30 by 30 target).[196][197] The treaty has articles regarding the principle \"polluter-pays\", and different impacts of human activities including areas beyond the national jurisdiction of the countries making those activities. The agreement was adopted by the 193 United Nations Member States.[198]",
      "ground_truth_chunk_ids": [
        "280_random_chunk1",
        "60_fixed_chunk1"
      ],
      "source_ids": [
        "S480",
        "S060"
      ],
      "category": "comparative",
      "id": 58
    },
    {
      "question": "Compare Grete Jenny and Neuroplasticity in one sentence each: what does each describe or study?",
      "ground_truth": "Grete Jenny: Grete Jenny (27 February 1930 \u2013 15 November 2015)[1] was an Austrian sprinter. She competed in the women's 4 \u00d7 100 metres relay at the 1948 Summer Olympics.[2] This biographical article relating to Austrian athletics is a stub. You can help Wikipedia by adding missing information. Neuroplasticity: Neuroplasticity, also known as neural plasticity or just plasticity, is the medium of neural networks in the brain to change through growth and reorganization. Neuroplasticity refers to the brain's ability to reorganize and rewire its neural connections, enabling it to adapt and function in ways that differ from its prior state. This process can occur in response to learning new skills, experiencing environmental changes, recovering from injuries, or adapting to sensory or cognitive deficits. Such adaptability highlights the dynamic and ever-evolving nature of the brain, even into adulthood.[1] These changes range from individual neuron pathways making new connections, to systematic adjustments like cortical remapping or neural oscillation. Other forms of neuroplasticity include homologous area adaptation, cross modal reassignment, map expansion, and compensatory masquerade.[2] Examples of neuroplasticity include circuit and network changes that result from learning a new ability, information acquisition,[3] environmental influences,[4] pregnancy,[5] caloric intake,[6] practice/training,[7] and psychological stress.[8] Neuroplasticity was once thought by neuroscientists to manifest only during childhood,[9][10] but research in the later half of the 20th century showed that many aspects of the brain exhibit plasticity through adulthood.[11] The developing brain exhibits a higher degree of plasticity than the adult brain.[12] Activity-dependent plasticity can have significant implications for healthy development, learning, memory, and recovery from brain damage.[13][14][15] The term plasticity was first applied to behavior in 1890 by William James in The Principles of Psychology where the term was used to describe \"a structure weak enough to yield to an influence, but strong enough not to yield all at once\".[16][17] The first person to use the term neural plasticity appears to have been the Polish neuroscientist Jerzy Konorski.[11][18] One of the first experiments providing evidence for neuroplasticity was conducted in 1793, by Italian anatomist Michele Vincenzo Malacarne, who described experiments in which he paired animals, trained one",
      "expected_answer": "Grete Jenny: Grete Jenny (27 February 1930 \u2013 15 November 2015)[1] was an Austrian sprinter. She competed in the women's 4 \u00d7 100 metres relay at the 1948 Summer Olympics.[2] This biographical article relating to Austrian athletics is a stub. You can help Wikipedia by adding missing information. Neuroplasticity: Neuroplasticity, also known as neural plasticity or just plasticity, is the medium of neural networks in the brain to change through growth and reorganization. Neuroplasticity refers to the brain's ability to reorganize and rewire its neural connections, enabling it to adapt and function in ways that differ from its prior state. This process can occur in response to learning new skills, experiencing environmental changes, recovering from injuries, or adapting to sensory or cognitive deficits. Such adaptability highlights the dynamic and ever-evolving nature of the brain, even into adulthood.[1] These changes range from individual neuron pathways making new connections, to systematic adjustments like cortical remapping or neural oscillation. Other forms of neuroplasticity include homologous area adaptation, cross modal reassignment, map expansion, and compensatory masquerade.[2] Examples of neuroplasticity include circuit and network changes that result from learning a new ability, information acquisition,[3] environmental influences,[4] pregnancy,[5] caloric intake,[6] practice/training,[7] and psychological stress.[8] Neuroplasticity was once thought by neuroscientists to manifest only during childhood,[9][10] but research in the later half of the 20th century showed that many aspects of the brain exhibit plasticity through adulthood.[11] The developing brain exhibits a higher degree of plasticity than the adult brain.[12] Activity-dependent plasticity can have significant implications for healthy development, learning, memory, and recovery from brain damage.[13][14][15] The term plasticity was first applied to behavior in 1890 by William James in The Principles of Psychology where the term was used to describe \"a structure weak enough to yield to an influence, but strong enough not to yield all at once\".[16][17] The first person to use the term neural plasticity appears to have been the Polish neuroscientist Jerzy Konorski.[11][18] One of the first experiments providing evidence for neuroplasticity was conducted in 1793, by Italian anatomist Michele Vincenzo Malacarne, who described experiments in which he paired animals, trained one of the pair extensively for years, and then dissected both. Malacarne discovered that the cerebellums of the trained animals were substantially larger than the cerebellum of the untrained animals. However, while these findings were significant, they were eventually forgotten.[19] In 1890, the idea that the brain and its function are not fixed throughout adulthood was proposed by William James in The Principles of Psychology, though the idea was largely neglected.[17] Up until the 1970s, neuroscientists believed that the brain's structure and function was essentially fixed throughout adulthood.[20] While the brain was commonly understood as a nonrenewable organ in the early 1900s, the pioneering neuroscientist Santiago Ram\u00f3n y Cajal used the term neuronal plasticity to describe nonpathological changes in the structure of adult brains. Based on his renowned neuron doctrine, Cajal first described the neuron as the fundamental unit of the nervous system that later served as an essential foundation to develop the concept of neural plasticity.[21] Many neuroscientists used the term plasticity to explain the regenerative capacity of the peripheral nervous system only. Cajal, however, used the term plasticity to reference his findings of degeneration and regeneration in the adult brain (a part of the central nervous system). This was controversial, with some like Walther Spielmeyer and Max Bielschowsky arguing that the CNS cannot produce new cells.[22][23] The term has since been broadly applied: Given the central importance of neuroplasticity, an outsider would be forgiven for assuming that it was well defined and that a basic and universal framework served to direct current and future hypotheses and experimentation. Sadly, however, this is not the case. While many neuroscientists use the word neuroplasticity as an umbrella term it means different things to different researchers in different subfields ... In brief, a mutually agreed-upon framework does not appear to exist.[24] In 1923, Karl Lashley conducted experiments on rhesus monkeys that demonstrated changes in neuronal pathways, which he concluded were evidence of plasticity. Despite this, and other research that suggested plasticity, neuroscientists did not widely accept the idea of neuroplasticity. Inspired by work from Nicolas Rashevsky,[25] in 1943, McCulloch and Pitts proposed the artificial neuron, with a learning rule, whereby new synapses are produced when neurons fire simultaneously.[26] This is then extensively discussed in The organization of behavior (Hebb, 1949) and is now known as Hebbian learning. In 1945, Justo Gonzalo concluded from his research on brain dynamics, that, contrary to the activity of the projection areas, the \"central\" cortical mass (more or less equidistant from the visual, tactile and auditive projection areas), would be a \"maneuvering mass\", rather unspecific or multisensory, with capacity to increase neural excitability and re-organize the activity by means of plasticity properties.[27] He gives as a first example of adaptation, to see upright with reversing glasses in the Stratton experiment,[28] and specially, several first-hand brain injuries cases in which he observed dynamic and adaptive properties in their disorders, in particular in the inverted perception disorder [e.g., see pp 260\u201362 Vol. I (1945), p 696 Vol. II (1950)].[27] He stated that a sensory signal in a projection area would be only an inverted and constricted outline that would be magnified due to the increase in recruited cerebral mass, and re-inverted due to some effect of brain plasticity, in more central areas, following a spiral growth.[29] Marian Diamond of the University of California, Berkeley, produced the first scientific evidence of anatomical brain plasticity, publishing her research in 1964.[30][31] Other significant evidence was produced in the 1960s and after, notably from scientists including Paul Bach-y-Rita, Michael Merzenich along with Jon Kaas, as well as several others.[20][32] An attempt to describe the mechanisms of neuroplasticity, an early version of the computational theory of mind derived from Hebb's work, was put forward by Peter Putnam and Robert W. Fuller in that time.[33][34] In the 1960s, Paul Bach-y-Rita invented a device that was tested on a small number of people, and involved a person sitting in a chair, embedded in which were nubs that were made to vibrate in ways that translated images received in a camera, allowing a form of vision via sensory substitution.[35][36] Studies in people recovering from stroke also provided support for neuroplasticity, as regions of the brain that remained healthy could sometimes take over, at least in part, functions that had been destroyed; Shepherd Ivory Franz did work in this area.[37][38] Eleanor Maguire documented changes in hippocampal structure associated with acquiring the knowledge of London's layout in local taxi drivers.[39][40][41] A redistribution of grey matter was indicated in London Taxi Drivers compared to controls. This work on hippocampal plasticity not only interested scientists, but also engaged the public and media worldwide. Michael Merzenich is a neuroscientist who has been one of the pioneers of neuroplasticity for over three decades. He has made some of \"the most ambitious claims for the field \u2013 that brain exercises may be as useful as drugs to treat diseases as severe as schizophrenia \u2013 that plasticity exists from cradle to the grave, and that radical improvements in cognitive functioning \u2013 how we learn, think, perceive, and remember are possible even in the elderly.\"[35] Merzenich's work was affected by a crucial discovery made by David Hubel and Torsten Wiesel in their work with kittens. The experiment involved sewing one eye shut and recording the cortical brain maps. Hubel and Wiesel saw that the portion of the kitten's brain associated with the shut eye was not idle, as expected. Instead, it processed visual information from the open eye. It was \"\u2026as though the brain didn't want to waste any 'cortical real estate' and had found a way to rewire itself.\"[35] This implied neuroplasticity during the critical period. However, Merzenich argued that neuroplasticity could occur beyond the critical period. His first encounter with adult plasticity came when he was engaged in a postdoctoral study with Clinton Woosley. The experiment was based on observation of what occurred in the brain when one peripheral nerve was cut and subsequently regenerated. The two scientists micromapped the hand maps of monkey brains before and after cutting a peripheral nerve and sewing the ends together. Afterwards, the hand map in the brain that they expected to be jumbled was nearly normal. This was a substantial breakthrough. Merzenich asserted that, \"If the brain map could normalize its structure in response to abnormal input, the prevailing view that we are born with a hardwired system had to be wrong. The brain had to be plastic.\"[35] Merzenich received the 2016 Kavli Prize in Neuroscience \"for the discovery of mechanisms that allow experience and neural activity to remodel brain function.\"[42] There are different ideas and theories on what biological processes allow for neuroplasticity to occur. The core of this phenomenon is based upon synapses and how connections between them change based on neuron functioning. It is widely agreed upon that neuroplasticity takes on many forms, as it is a result of a variety of pathways. These pathways, mainly signaling cascades, allow for gene expression alterations that lead to neuronal changes, and thus neuroplasticity. There are a number of other factors that are thought to play a role in the biological processes underlying the changing of neural networks in the brain. Some of these factors include synapse regulation via phosphorylation, the role of inflammation and inflammatory cytokines, proteins such as Bcl-2 proteins and neutrophorins, energy production via mitochondria,[43] and acetylcholine.[44] JT Wall and J Xu have traced the mechanisms underlying neuroplasticity. Re-organization is not cortically emergent, but occurs at every level in the processing hierarchy; this produces the map changes observed in the cerebral cortex.[45] Christopher Shaw and Jill McEachern (eds) in \"Toward a theory of Neuroplasticity\", state that there is no all-inclusive theory that overarches different frameworks and systems in the study of neuroplasticity. However, researchers often describe neuroplasticity as \"the ability to make adaptive changes related to the structure and function of the nervous system.\"[46] Correspondingly, two types of neuroplasticity are often discussed: structural neuroplasticity and functional neuroplasticity. Structural plasticity is often understood as the brain's ability to change its neuronal connections. The changes of grey matter proportion or the synaptic strength in the brain are considered as examples of structural neuroplasticity. This type of neuroplasticity often studies the effect of various internal or external stimuli on the brain's anatomical reorganization. New neurons are constantly produced and integrated into the central nervous system based on this type of neuroplasticity.[47] Researchers nowadays use multiple cross-sectional imaging methods (i.e. magnetic resonance imaging (MRI), computerized tomography (CT)) to study the structural alterations of the human brains.[48] Structural neuroplasticity is currently investigated more within the field of neuroscience in current academia.[21] Adult neurogenesis \"has not been convincingly demonstrated in humans\".[47] Functional plasticity refers to the brain's ability to alter and adapt the functional properties of network of neurons. It can occur in four known ways namely: Homologous area adaptation is the assumption of a particular cognitive process by a homologous region in the opposite hemisphere.[49] For instance, through homologous area adaptation a cognitive task is shifted from a damaged part of the brain to its homologous area in opposite side of the brain. Homologous area adaptation is a type of functional neuroplasticity that occur usually in children rather than adults. In map expansion, cortical maps related to particular cognitive tasks expand due to frequent exposure to stimuli. Map expansion has been proven through experiments performed in relation to the study: experiment on effect of frequent stimulus on functional connectivity of the brain was observed in individuals learning spatial routes.[50] Cross-model reassignment involves reception of novel input signals to a brain region which has been stripped of its default input. Functional plasticity through compensatory masquerade occurs using different cognitive processes for an already established cognitive task when the initial process cannot be followed due to impairment. Changes in the brain associated with functional neuroplasticity can occur in response to two different types of events: In the latter case the functions from one part of the brain transfer to another part of the brain based on the demand to produce recovery of behavioral or physiological processes.[51] Regarding physiological forms of activity-dependent plasticity, those involving synapses are referred to as synaptic plasticity. The strengthening or weakening of synapses that results in an increase or decrease of firing rate of the neurons are called long-term potentiation (LTP) and long-term depression (LTD), respectively, and they are considered as examples of synaptic plasticity that are associated with memory.[52] The cerebellum is a typical structure with combinations of LTP/LTD and redundancy within the circuitry, allowing plasticity at several sites.[53] More recently it has become clearer that synaptic plasticity can be complemented by another form of activity-dependent plasticity involving the intrinsic excitability of neurons, which is referred to as intrinsic plasticity.[54][55][56] This, as opposed to homeostatic plasticity does not necessarily maintain the overall activity of a neuron within a network but contributes to encoding memories.[57] Also, many studies have indicated functional neuroplasticity in the level of brain networks, where training alters the strength of functional connections.[58][59] Although a recent study discusses that these observed changes should not directly relate to neuroplasticity, since they may root in the systematic requirement of the brain network for reorganization.[60] The adult brain is not entirely \"hard-wired\" with fixed neuronal circuits. There are many instances of cortical and subcortical rewiring of neuronal circuits in response to training as well as in response to injury. There is ample evidence[61] for the active, experience-dependent re-organization of the synaptic networks of the brain involving multiple inter-related structures including the cerebral cortex.[62] The specific details of how this process occurs at the molecular and ultrastructural levels are topics of active neuroscience research. The way experience can influence the synaptic organization of the brain is also the basis for a number of theories of brain function including the general theory of mind and neural Darwinism. The concept of neuroplasticity is also central to theories of memory and learning that are associated with experience-driven alteration of synaptic structure and function in studies of classical conditioning in invertebrate animal models such as Aplysia.[citation needed] There is evidence that neurogenesis (birth of brain cells) occurs in the adult, rodent brain\u2014and such changes can persist well into old age.[63] The evidence for neurogenesis is mainly restricted to the hippocampus and olfactory bulb, but research has revealed that other parts of the brain, including the cerebellum, may be involved as well.[64] However, the degree of rewiring induced by the integration of new neurons in the established circuits is not known, and such rewiring may well be functionally redundant.[65] Addiction is a state characterized by compulsive engagement in rewarding stimuli, despite adverse consequences. The process of developing an addiction occurs through instrumental learning, which is otherwise known as operant conditioning. Neuroscientists believe that drug addicts\u2019 behavior is a direct correlation to some physiological change in their brain, caused by using drugs. This view believes there is a bodily function in the brain causing the addiction. This is brought on by a change in the brain caused by brain damage or adaptation from chronic drug use.[66][67] In humans, addiction is diagnosed according to diagnostic models such as the Diagnostic and Statistical Manual of Mental Disorders, through observed behaviors. There has been significant advancement in understanding the structural changes that occur in parts of the brain involved in the reward pathway (mesolimbic system) that underlies addiction.[68] Most research has focused on two portions of the brain: the ventral tegmental area, (VTA) and the nucleus accumbens (NAc).[69] The VTA is the portion of the mesolimbic system responsible for spreading dopamine to the whole system. The VTA is stimulated by \u2033rewarding experiences\u2033. The release of dopamine by the VTA induces pleasure, thus reinforcing behaviors that lead to the reward.[70] Drugs of abuse increase the VTA's ability to project dopamine to the rest of the reward circuit.[71] These structural changes only last 7\u201310 days,[72] however, indicating that the VTA cannot be the only part of the brain that is affected by drug use, and changed during the development of addiction. The nucleus accumbens (NAc) plays an essential part in the formation of addiction. Almost every drug with addictive potential induces the release of dopamine into the NAc.[73] In contrast to the VTA, the NAc shows long-term structural changes. Drugs of abuse weaken the connections within the NAc after habitual use,[74] as well as after use then withdrawal.[75] A surprising consequence of neuroplasticity is that the brain activity associated with a given function can be transferred to a different location; this can result from normal experience and also occurs in the process of recovery from brain injury. Neuroplasticity is the fundamental issue that supports the scientific basis for treatment of acquired brain injury with goal-directed experiential therapeutic programs in the context of rehabilitation approaches to the functional consequences of the injury. Neuroplasticity is gaining popularity as a theory that, at least in part, explains improvements in functional outcomes with physical therapy post-stroke. Rehabilitation techniques that are supported by evidence which suggest cortical reorganization as the mechanism of change include constraint-induced movement therapy, functional electrical stimulation, treadmill training with body-weight support, and virtual reality therapy. Robot assisted therapy is an emerging technique, which is also hypothesized to work by way of neuroplasticity, though there is currently insufficient evidence to determine the exact mechanisms of change when using this method.[76] One group has developed a treatment that includes increased levels of progesterone injections in brain-injured patients. \"Administration of progesterone after traumatic brain injury[77] (TBI) and stroke reduces edema, inflammation, and neuronal cell death, and enhances spatial reference memory and sensory-motor recovery.\"[78] In a clinical trial, a group of severely injured patients had a 60% reduction in mortality after three days of progesterone injections.[79] However, a study published in the New England Journal of Medicine in 2014 detailing the results of a multi-center NIH-funded phase III clinical trial of 882 patients found that treatment of acute traumatic brain injury with the hormone progesterone provides no significant benefit to patients when compared with placebo.[80] For decades, researchers assumed that humans had to acquire binocular vision, in particular stereopsis, in early childhood or they would never gain it. In recent years, however, successful improvements in persons with amblyopia, convergence insufficiency or other stereo vision anomalies have become prime examples of neuroplasticity; binocular vision improvements and stereopsis recovery are now active areas of scientific and clinical research.[81][82][83] In the phenomenon of phantom limb sensation, a person continues to feel pain or sensation within a part of their body that has been amputated. This is strangely common, occurring in 60\u201380% of amputees.[84] An explanation for this is based on the concept of neuroplasticity, as the cortical maps of the removed limbs are believed to have become engaged with the area around them in the postcentral gyrus. This results in activity within the surrounding area of the cortex being misinterpreted by the area of the cortex formerly responsible for the amputated limb. The relationship between phantom limb sensation and neuroplasticity is a complex one. In the early 1990s V.S. Ramachandran theorized that phantom limbs were the result of cortical remapping. However, in 1995 Herta Flor and her colleagues demonstrated that cortical remapping occurs only in patients who have phantom pain.[85] Her research showed that phantom limb pain (rather than referred sensations) was the perceptual correlate of cortical reorganization.[86] This phenomenon is sometimes referred to as maladaptive plasticity. In 2009, Lorimer Moseley and Peter Brugger carried out an experiment in which they encouraged arm amputee subjects to use visual imagery to contort their phantom limbs into impossible[clarification needed] configurations. Four of the seven subjects succeeded in performing impossible movements of the phantom limb. This experiment suggests that the subjects had modified the neural representation of their phantom limbs and generated the motor commands needed to execute impossible movements in the absence of feedback from the body.[87] Individuals who have chronic pain experience prolonged pain at sites that may have been previously injured, yet are otherwise currently healthy. This phenomenon is related to neuroplasticity due to a maladaptive reorganization of the nervous system, both peripherally and centrally. During the period of tissue damage, noxious stimuli and inflammation cause an elevation of nociceptive input from the periphery to the central nervous system. Prolonged nociception from the periphery then elicits a neuroplastic response at the cortical level to change its somatotopic organization for the painful site, inducing central sensitization.[88] For instance, individuals experiencing complex regional pain syndrome demonstrate a diminished cortical somatotopic representation of the hand contralaterally as well as a decreased spacing between the hand and the mouth.[89] Additionally, chronic pain has been reported to significantly reduce the volume of grey matter in the brain globally, and more specifically at the prefrontal cortex and right thalamus.[90] However, following treatment, these abnormalities in cortical reorganization and grey matter volume are resolved, as well as their symptoms. Similar results have been reported for phantom limb pain,[91] chronic low back pain[92] and carpal tunnel syndrome.[93] A number of studies have linked meditation practice to differences in cortical thickness or density of gray matter.[94][95][96][97] One of the most well-known studies to demonstrate this was led by Sara Lazar, from Harvard University, in 2000.[98] Richard Davidson, a neuroscientist at the University of Wisconsin, has led experiments in collaboration with the Dalai Lama on effects of meditation on the brain. His results suggest that meditation may lead to change in the physical structure of brain regions associated with attention, anxiety, depression, fear, anger, and compassion as well as the ability of the body to heal itself.[99][100] There is substantial evidence that artistic engagement in a therapeutic environment can create changes in neural network connections as well as increase cognitive flexibility.[101][102] In one 2013 study, researchers found evidence that long-term, habitual artistic training (e.g. musical instrument practice, purposeful painting, etc.) can \"macroscopically imprint a neural network system of spontaneous activity in which the related brain regions become functionally and topologically modularized in both domain-general and domain-specific manners\".[103] In simple terms, brains repeatedly exposed to artistic training over long periods develop adaptations to make such activity both easier and more likely to spontaneously occur. Some researchers and academics have suggested that artistic engagement has substantially altered the human brain throughout our evolutionary history. D.W Zaidel, adjunct professor of behavioral neuroscience and contributor at VAGA, has written that \"evolutionary theory links the symbolic nature of art to critical pivotal brain changes in Homo sapiens supporting increased development of language and hierarchical social grouping\".[104] There is evidence that engaging in music-supported therapy can improve neuroplasticity in patients who are recovering from brain injuries. Music-supported therapy can be used for patients that are undergoing stroke rehabilitation where a one month study of stroke patients participating in music-supported therapy showed a significant improvement in motor control in their affected hand.[105] Another finding was the examination of grey matter volume of adults developing brain atrophy and cognitive decline where playing a musical instrument, such as the piano, or listening to music can increase grey matter volume in areas such as the caudate nucleus, Rolandic operculum, and cerebellum.[106] Evidence also suggests that music-supported therapy can improve cognitive performance, well-being, and social behavior in patients who are recovering from damage to the orbitofrontal cortex (OFC) and recovering from mild traumatic brain injury. Neuroimaging post music-supported therapy revealed functional changes in OFC networks, with improvements observed in both task-based and resting-state fMRI analyses.[107] Beyond clinical rehabilitation, music has been shown to induce neuroplastic changes in healthy individuals through long-term training and repeated exposure.[108] Studies comparing musicians and non-musicians have demonstrated structural and functional brain differences associated with musical practice, particularly when training begins early in life.[98] Musicians often exhibit increased gray and white matter volume in motor, auditory, and cerebellar regions, reflecting adaptations related to fine motor control, auditory processing, and timing.[108] Evidence of cortical remapping has also been observed, such as enlarged cortical representations of the fingers most frequently used during instrument performance.[108] Music training strongly affects the auditory system, with musicians showing enhanced activation and structural differences in primary and secondary auditory cortices involved in processing pitch, rhythm, and melody.[108] Functional changes have been observed not only at the cortical level but also in subcortical structures, including the brainstem, where musicians demonstrate faster and stronger neural responses to sound.[108] Across the lifespan, sustained musical engagement has been associated with reduced age-related decline in certain brain regions and a lower risk of cognitive impairment, suggesting that music-related neuroplasticity may contribute to long-term brain health.[108] Aerobic exercise increases the production of neurotrophic factors (compounds that promote growth or survival of neurons), such as brain-derived neurotrophic factor (BDNF), insulin-like growth factor 1 (IGF-1), and vascular endothelial growth factor (VEGF).[109][110][111] Exercise-induced effects on the hippocampus are associated with measurable improvements in spatial memory.[112][113][114][115] Consistent aerobic exercise over a period of several months induces marked clinically significant improvements in executive function (i.e., the \"cognitive control\" of behavior) and increased gray matter volume in multiple brain regions, particularly those that give rise to cognitive control.[111][112][116][117] The brain structures that show the greatest improvements in gray matter volume in response to aerobic exercise are the prefrontal cortex and hippocampus;[111][112][113] moderate improvements are seen in the anterior cingulate cortex, parietal cortex, cerebellum, caudate nucleus, and nucleus accumbens.[111][112][113] Higher physical fitness scores (measured by VO2 max) are associated with better executive function, faster processing speed, and greater volume of the hippocampus, caudate nucleus, and nucleus accumbens.[112] Due to hearing loss, the auditory cortex and other association areas of the brain in deaf and/or hard of hearing people undergo compensatory plasticity.[118][119][120] The auditory cortex usually reserved for processing auditory information in hearing people now is redirected to serve other functions, especially for vision and somatosensation. Deaf individuals have enhanced peripheral visual attention,[121] better motion change but not color change detection ability in visual tasks,[119][120][122] more effective visual search,[123] and faster response time for visual targets[124][125] compared to hearing individuals. Altered visual processing in deaf people is often found to be associated with the repurposing of other brain areas including primary auditory cortex, posterior parietal association cortex (PPAC), and anterior cingulate cortex (ACC).[126] A review by Bavelier et al. (2006) summarizes many aspects on the topic of visual ability comparison between deaf and hearing individuals.[127] Brain areas that serve a function in auditory processing repurpose to process somatosensory information in congenitally deaf people. They have higher sensitivity in detecting frequency change in vibration above threshold[128] and higher and more widespread activation in auditory cortex under somatosensory stimulation.[129][118] However, speeded response for somatosensory stimuli is not found in deaf adults.[124] Neuroplasticity is involved in the development of sensory function. The brain is born immature and then adapts to sensory inputs after birth. In the auditory system, congenital hearing loss, a rather frequent inborn condition affecting 1 of 1000 newborns, has been shown to affect auditory development, and implantation of a sensory prostheses activating the auditory system has prevented the deficits and induced functional maturation of the auditory system.[130] Due to a sensitive period for plasticity, there is also a sensitive period for such intervention within the first 2\u20134 years of life. Consequently, in prelingually deaf children, early cochlear implantation, as a rule, allows the children to learn the mother language and acquire acoustic communication.[131] Due to vision loss, the visual cortex in blind people may undergo cross-modal plasticity, and therefore other senses may have enhanced abilities. Or the opposite could occur, with the lack of visual input weakening the development of other sensory systems. One study suggests that the right posterior middle temporal gyrus and superior occipital gyrus reveal more activation in the blind than in the sighted people during a sound-moving detection task.[132] Several studies support the latter idea and found weakened ability in audio distance evaluation, proprioceptive reproduction, threshold for visual bisection, and judging minimum audible angle.[133][134] Human echolocation is a learned ability for humans to sense their environment from echoes. This ability is used by some blind people to navigate their environment and sense their surroundings in detail. Studies in 2010[135] and 2011[136] using functional magnetic resonance imaging techniques have shown that parts of the brain associated with visual processing are adapted for the new skill of echolocation. Studies with blind patients, for example, suggest that the click-echoes heard by these patients were processed by brain regions devoted to vision rather than audition.[136] Reviews of MRI and electroencephalography (EEG) studies on individuals with ADHD suggest that the long-term treatment of ADHD with stimulants, such as amphetamine or methylphenidate, decreases abnormalities in brain structure and function found in subjects with ADHD, and improves function in several parts of the brain, such as the right caudate nucleus of the basal ganglia,[137][138][139] left ventrolateral prefrontal cortex (VLPFC), and superior temporal gyrus.[140] In addition to pharmacological treatment, non-pharmacological interventions that leverage neuroplasticity have been proposed as potential approaches for managing ADHD symptoms. Cognitive training and other behavioral therapies aim to improve attention, self-regulation, and impulse control by promoting functional and structural changes in neural circuits associated with executive function.[141] Computerized cognitive training programs have been shown to target underdeveloped neural networks in individuals with ADHD, leading to improvements in attention and working memory through repeated stimulation of specific brain regions.[141] These interventions may produce longer-term neuroplastic changes that overlap with brain areas affected by stimulant medications, suggesting that neuroplasticity-based therapies could complement or, in some cases, reduce reliance on pharmacological treatment.[141] Neuroplasticity is most active in childhood as a part of normal human development, and can also be seen as an especially important mechanism for children in terms of risk and resiliency.[142] Trauma is considered a great risk as it negatively affects many areas of the brain and puts a strain on the sympathetic nervous system from constant activation. Trauma thus alters the brain's connections such that children who have experienced trauma may be hyper vigilant or overly aroused.[143] However, a child's brain can cope with these adverse effects through the actions of neuroplasticity.[144] Neuroplasticity is shown in four different categories in children and covering a wide variety of neuronal functioning. These four types include impaired, excessive, adaptive, and plasticity.[145] There are many examples of neuroplasticity in human development. For example, Justine Ker and Stephen Nelson looked at the effects of musical training on neuroplasticity, and found that musical training can contribute to experience dependent structural plasticity. This is when changes in the brain occur based on experiences that are unique to an individual. Examples of this are learning multiple languages, playing a sport, doing theatre, etc. A study done by Hyde in 2009, showed that changes in the brain of children could be seen in as little as 15 months of musical training.[146] Ker and Nelson suggest this degree of plasticity in the brains of children can \"help provide a form of intervention for children... with developmental disorders and neurological diseases.\"[147] In a single lifespan, individuals of an animal species may encounter various changes in brain morphology. Many of these differences are caused by the release of hormones in the brain; others are the product of evolutionary factors or developmental stages.[148][149][150][151] Some changes occur seasonally in species to enhance or generate response behaviors. Changing brain behavior and morphology to suit other seasonal behaviors is relatively common in animals.[152] These changes can improve the chances of mating during breeding season.[148][149][150][152][153][154] Examples of seasonal brain morphology change can be found within many classes and species. Within the class Aves, black-capped chickadees experience an increase in the volume of their hippocampus and strength of neural connections to the hippocampus during fall months.[155][156] These morphological changes within the hippocampus which are related to spatial memory are not limited to birds, as they can also be observed in rodents and amphibians.[152] In songbirds, many song control nuclei in the brain increase in size during mating season.[152] Among birds, changes in brain morphology to influence song patterns, frequency, and volume are common.[157] Gonadotropin-releasing hormone (GnRH) immunoreactivity, or the reception of the hormone, is lowered in European starlings exposed to longer periods of light during the day.[148][149] The California sea hare, a gastropod, has more successful inhibition of egg-laying hormones outside of mating season due to increased effectiveness of inhibitors in the brain.[150] Changes to the inhibitory nature of regions of the brain can also be found in humans and other mammals.[151] In the amphibian Bufo japonicus, part of the amygdala is larger before breeding and during hibernation than it is after breeding.[153] Seasonal brain variation occurs within many mammals. Part of the hypothalamus of the common ewe is more receptive to GnRH during breeding season than at other times of the year.[154] Humans experience a change in the \"size of the hypothalamic suprachiasmatic nucleus and vasopressin-immunoreactive neurons within it\"[151] during the fall, when these parts are larger. In the spring, both reduce in size.[158] A group of scientists found that if a small stroke (an infarction) is induced by obstruction of blood flow to a portion of a monkey's motor cortex, the part of the body that responds by movement moves when areas adjacent to the damaged brain area are stimulated. In one study, intracortical microstimulation (ICMS) mapping techniques were used in nine normal monkeys. Some underwent ischemic-infarction procedures and the others, ICMS procedures. The monkeys with ischemic infarctions retained more finger flexion during food retrieval and after several months this deficit returned to preoperative levels.[159] With respect to the distal forelimb representation, \"postinfarction mapping procedures revealed that movement representations underwent reorganization throughout the adjacent, undamaged cortex.\"[159] Understanding of interaction between the damaged and undamaged areas provides a basis for better treatment plans in stroke patients. Current research includes the tracking of changes that occur in the motor areas of the cerebral cortex as a result of a stroke. Thus, events that occur in the reorganization process of the brain can be ascertained. The treatment plans that may enhance recovery from strokes, such as physiotherapy, pharmacotherapy, and electrical-stimulation therapy, are also being studied. Jon Kaas, a professor at Vanderbilt University, has been able to show \"how somatosensory area 3b and ventroposterior (VP) nucleus of the thalamus are affected by longstanding unilateral dorsal-column lesions at cervical levels in macaque monkeys.\"[160] Adult brains have the ability to change as a result of injury but the extent of the reorganization depends on the extent of the injury. His recent research focuses on the somatosensory system, which involves a sense of the body and its movements using many senses. Usually, damage of the somatosensory cortex results in impairment of the body perception. Kaas' research project is focused on how these systems (somatosensory, cognitive, motor systems) respond with plastic changes resulting from injury.[160] One recent study of neuroplasticity involves work done by a team of doctors and researchers at Emory University, specifically Donald Stein[161] and David Wright. This is the first treatment in 40 years that has significant results in treating traumatic brain injuries while also incurring no known side effects and being cheap to administer.[79] Stein noticed that female mice seemed to recover from brain injuries better than male mice, and that at certain points in the estrus cycle, females recovered even better. This difference may be attributed to different levels of progesterone, with higher levels of progesterone leading to the faster recovery from brain injury in mice. However, clinical trials showed progesterone offers no significant benefit for traumatic brain injury in human patients.[162] Transcriptional profiling of the frontal cortex of persons ranging from 26 to 106 years of age defined a set of genes with reduced expression after age 40, and especially after age 70.[163] Genes that play central roles in synaptic plasticity were the most significantly affected by age, generally showing reduced expression over time. There was also a marked increase in cortical DNA damage, likely oxidative DNA damage, in gene promoters with aging.[163] Reactive oxygen species appear to have a significant role in the regulation of synaptic plasticity and cognitive function.[164] However age-related increases in reactive oxygen species may also lead to impairments in these functions. There is a beneficial effect of multilingualism on people's behavior and cognition. Numerous studies have shown that people who study more than one language have better cognitive functions and flexibilities than people who only speak one language. Bilinguals are found to have longer attention spans, stronger organization and analyzation skills, and a better theory of mind than monolinguals. Researchers have found that the effect of multilingualism on better cognition is due to neuroplasticity.[citation needed] In one prominent study, neurolinguists used a voxel-based morphometry (VBM) method to visualize the structural plasticity of brains in healthy monolinguals and bilinguals. They first investigated the differences in density of grey and white matter between two groups and found the relationship between brain structure and age of language acquisition. The results showed that grey-matter density in the inferior parietal cortex for multilinguals were significantly greater than monolinguals. The researchers also found that early bilinguals had a greater density of grey matter relative to late bilinguals in the same region. The inferior parietal cortex is a brain region highly associated with the language learning, which corresponds to the VBM result of the study.[165] Recent studies have also found that learning multiple languages not only re-structures the brain but also boosts brain's capacity for plasticity. A recent study found that multilingualism not only affects the grey matter but also white matter of the brain. White matter is made up of myelinated axons that is greatly associated with learning and communication. Neurolinguists used a diffusion tensor imaging (DTI) scanning method to determine the white matter intensity between monolinguals and bilinguals. Increased myelinations in white matter tracts were found in bilingual individuals who actively used both languages in everyday life. The demand of handling more than one language requires more efficient connectivity within the brain, which resulted in greater white matter density for multilinguals.[166] While it is still debated whether these changes in brain are result of genetic disposition or environmental demands, many evidences suggest that environmental, social experience in early multilinguals affect the structural and functional reorganization in the brain.[167][168] Historically, the monoamine imbalance hypothesis of depression played a dominant role in psychiatry and drug development.[169] However, while traditional antidepressants cause a quick increase in noradrenaline, serotonin, or dopamine, there is a significant delay in their clinical effect and often an inadequate treatment response.[170] As neuroscientists pursued this avenue of research, clinical and preclinical data across multiple modalities began to converge on pathways involved in neuroplasticity.[171] They found a strong inverse relationship between the number of synapses and severity of depression symptoms[172] and discovered that in addition to their neurotransmitter effect, traditional antidepressants improved neuroplasticity but over a significantly protracted time course of weeks or months.[173] The search for faster acting antidepressants found success in the pursuit of ketamine, a well-known anesthetic agent, that was found to have potent anti-depressant effects after a single infusion due to its capacity to rapidly increase the number of dendritic spines and to restore aspects of functional connectivity.[174] Additional neuroplasticity promoting compounds with therapeutic effects that were both rapid and enduring have been identified through classes of compounds including serotonergic psychedelics, cholinergic scopolamine, and other novel compounds. To differentiate between traditional antidepressants focused on monoamine modulation and this new category of fast acting antidepressants that achieve therapeutic effects through neuroplasticity, the term psychoplastogen was introduced.[175] Nicotine affects the brain by binding to nicotinic acetylcholine receptors, the same receptors acetylcholine binds to, which has been linked with Neuroplasticity.[176] Nicotine use may lower the rate of neuroplasticity in the brain by damaging the nicotinic-acetylcholine receptors needed to reuptake the acetylcholine necessary for neuroplasticity.[177]",
      "ground_truth_chunk_ids": [
        "169_random_chunk1",
        "164_fixed_chunk1"
      ],
      "source_ids": [
        "S369",
        "S164"
      ],
      "category": "comparative",
      "id": 59
    },
    {
      "question": "Compare Robots (2005 film) and Richelieu, Kentucky in one sentence each: what does each describe or study?",
      "ground_truth": "Robots (2005 film): Robots is a 2005 American animated science fiction comedy film directed by Chris Wedge and written by David Lindsay-Abaire, Lowell Ganz and Babaloo Mandel. Produced by 20th Century Fox Animation and Blue Sky Studios, it stars the voices of Ewan McGregor, Halle Berry, Greg Kinnear, Mel Brooks, Amanda Bynes, Drew Carey and Robin Williams. The story follows an ambitious inventor robot named Rodney Copperbottom, who seeks to work for his idol Bigweld's company in Robot City, but discovers a plot by its new leader Phineas T. Ratchet and his mother to forcibly upgrade the city's populace and eradicate struggling robots, known as \"outmodes\". Development of the film began in 2000, following a failed attempt by Wedge and children's author William Joyce to adapt Joyce's 1993 children's book Santa Calls. They instead decided to create an original story based on robots. The project was approved by executive producer Chris Meledandri in 2001 and production began the next year. Robots premiered at the Mann Village Theatre in Westwood, Los Angeles, on March 6, 2005, and was released in the United States on March 11 by 20th Century Fox. The film was praised by critics for its humor, animation, and performances, while its story and characters were deemed somewhat formulaic.[4] The film was commercially successful, grossing $262.5 million worldwide against a $75\u201380 million budget. A sequel was discussed but never produced due to the studio shifting focus to its flagship franchise, Ice Age.[5] In a world of robots, Rodney Copperbottom, son of Herb and Lydia Copperbottom, is an aspiring young inventor from the city of Rivet Town. He idolizes Bigweld, a famous inventor and philanthropist whose company, Bigweld Industries, employs other inventors and provides poor robots with spare parts. Rodney develops a small, flying robot named Wonderbot to assist his father, who works Richelieu, Kentucky: Richelieu is an unincorporated community in Logan County and Butler County, Kentucky, United States.[1] Richelieu is located near Logan County's northeastern boundary with Butler County along Kentucky Route 1038. It is also located near the tripoint where Logan and Butler County boundaries meet with those of Warren County.[2] This Logan County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information. This Butler County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Robots (2005 film): Robots is a 2005 American animated science fiction comedy film directed by Chris Wedge and written by David Lindsay-Abaire, Lowell Ganz and Babaloo Mandel. Produced by 20th Century Fox Animation and Blue Sky Studios, it stars the voices of Ewan McGregor, Halle Berry, Greg Kinnear, Mel Brooks, Amanda Bynes, Drew Carey and Robin Williams. The story follows an ambitious inventor robot named Rodney Copperbottom, who seeks to work for his idol Bigweld's company in Robot City, but discovers a plot by its new leader Phineas T. Ratchet and his mother to forcibly upgrade the city's populace and eradicate struggling robots, known as \"outmodes\". Development of the film began in 2000, following a failed attempt by Wedge and children's author William Joyce to adapt Joyce's 1993 children's book Santa Calls. They instead decided to create an original story based on robots. The project was approved by executive producer Chris Meledandri in 2001 and production began the next year. Robots premiered at the Mann Village Theatre in Westwood, Los Angeles, on March 6, 2005, and was released in the United States on March 11 by 20th Century Fox. The film was praised by critics for its humor, animation, and performances, while its story and characters were deemed somewhat formulaic.[4] The film was commercially successful, grossing $262.5 million worldwide against a $75\u201380 million budget. A sequel was discussed but never produced due to the studio shifting focus to its flagship franchise, Ice Age.[5] In a world of robots, Rodney Copperbottom, son of Herb and Lydia Copperbottom, is an aspiring young inventor from the city of Rivet Town. He idolizes Bigweld, a famous inventor and philanthropist whose company, Bigweld Industries, employs other inventors and provides poor robots with spare parts. Rodney develops a small, flying robot named Wonderbot to assist his father, who works as a dishwasher in a restaurant. When Herb's boss, Mr. Gunk, confronts them, however, Wonderbot malfunctions and wreaks havoc in the kitchen. To help Herb pay for the damages, Rodney travels to Robot City, hoping to present Wonderbot to Bigweld Industries. However, upon his arrival, Rodney is ejected from Bigweld Industries by its new CEO, Phineas T. Ratchet. In Bigweld's absence, Ratchet has discontinued production of spare parts and inventions for the poor outmoded robots, prioritizing expensive \"upgrades\". Meanwhile, Ratchet's mother Madame Gasket runs the Chop Shop, a facility that recycles scrap metal, including that of deceased or outmoded robots, into ingots for upgrades. Rodney befriends Fender Pinwheeler, a ne'er-do-well who introduces him to a group of outmoded robots known as the \"Rusties\". Rodney and his new friends help to fix outmodes throughout the neighborhood, but they are eventually unable to cope with the demand due to the spare part shortage. Hoping to enlist Bigweld's help, Rodney and Fender infiltrate the Bigweld Ball, but Ratchet announces that Bigweld will not attend. An enraged Rodney confronts Ratchet, who orders his security team to eliminate him. Cappy, an executive opposed to Ratchet, rescues Rodney and Fender. While Fender is captured by the Chop Shop, he discovers their plan to scrap all outmoded robots with new machines designed to destroy them. Rodney and Cappy fly to Bigweld's mansion, where he lives as a recluse and reveals that Ratchet's greed led to his resignation and refuses to help them. A distraught Rodney calls his parents, but Herb inspires him to fight for his dreams. Fender escapes the Chop Shop and exposes Ratchet's plot. Rodney rallies the Rusties, and Bigweld, reinvigorated by Rodney's spirit, joins them to stop Ratchet. Rodney and his friends return to Bigweld Industries where Ratchet attempts to dispose of Bigweld, who ends up being rolled into the Chop Shop. Rodney upgrades the Rusties and leads them in a battle against Ratchet, Gasket, and their army. Gasket is eventually flung into the incinerator and destroyed, and Ratchet is stripped of his upgrades and left chained with his father. Retaking control of Bigweld Industries, Bigweld holds a public ceremony in Rivet Town, where he nominates Rodney as his new second-in-command and eventual successor. Rodney provides Herb with new replacement parts and a flugelhorn-like instrument to fulfill his life-long dream of being a musician. Herb leads the townspeople in a rendition of \"Get Up Offa That Thing\". Initially, Chris Wedge and William Joyce wanted to make a film adaptation of Joyce's 1993 book Santa Calls. After a failed animation test in 2000 (which resulted in 20th Century Fox Animation declining to make the film), Wedge and Joyce decided to instead develop an original story about a world of robots. In 2001, the duo pitched the concept to 20th Century Fox Animation president Chris Meledandri, as a visual idea. Although not initially impressed, Meledandri agreed to greenlight the film and served as its executive producer.[11] The film began production in 2002, shortly after Ice Age was released. Wedge reunited with the crew from his first film, including Carlos Saldanha as the co-director. In June 2003, the film was announced by Fox at the American Museum of Natural History's IMAX theater. This announcement confirmed the entire cast and slated the film for its 2005 release.[12] Robots was originally scheduled for a 2004 release,[13] but the release date was changed to 2005. The film had its world premiere on March 6, 2005, in Westwood, Los Angeles,[6][7] and it was released theatrically on March 11, 2005. The film was the first to feature the new trailer for Star Wars: Episode III \u2013 Revenge of the Sith;[14] it was reported that Star Wars fans went to see the movie just to see the trailer and hear the voice of Ewan McGregor, who also played Obi-Wan Kenobi in the Star Wars prequel trilogy, as Rodney Copperbottom. The film also featured the exclusive trailer for Blue Sky's next film Ice Age: The Meltdown, then called Ice Age 2.[15] Robots was digitally re-mastered into IMAX format (IMAX DMR) and released in select IMAX theaters around the world. It was the first 20th Century Fox film that was released on the same day on IMAX and conventional 35mm screens. It was also the first IMAX DMR film released in the spring season, and the second IMAX DMR film distributed by Fox.[16] The film was released on DVD and VHS in both fullscreen and widescreen on September 27, 2005.[17] The DVD release was accompanied by an original short animated film based on Robots, titled Aunt Fanny's Tour of Booty.[18][19] The short is a prequel to the film, as it takes place during Fender's arrival in Robot City. In the short, Aunt Fanny gives a tour of the Robot City Train Station to a motley collection of robots, including Fender Pinwheeler, Zinc, Tammy, Hacky and an Old Lady-Bot.[18][19] The film was released in high definition on Blu-ray on March 22, 2011.[20] The Asian Blu-ray release of Robots includes Aunt Fanny's Tour of Booty, but it is not included on either the US nor European Blu-ray releases (possibly due to a request from the Office of Film and Literature Classification (OFLC) to remove the short from the Australian DVD release, for they gave the short a PG rating).[citation needed] The film was released on March 11, 2005, in the United States and Canada and grossed $36 million in 3,776 theaters in its opening weekend, ranking #1 at the box office.[21] It grossed a total of $260.7 million worldwide: $128.2 million in the United States and Canada, and $132.5 million in other territories.[2] On Rotten Tomatoes, the film has an approval rating of 64% based on 184 reviews, with an average rating of 6.6/10. The site's consensus reads: \"Robots delights on a visual level, but the story feels like it came off an assembly line.\"[22] Metacritic gives the film a weighted average score of 64 out of 100 based on 33 reviews, indicating \"generally favorable reviews\".[23] Audiences surveyed by CinemaScore gave the film an \"A\" on a scale of A+ to F.[24] Roger Ebert of the Chicago Sun-Times gave the film three and a half stars out of four, stating that \"this is a movie that is a joy to behold entirely apart from what it is about. It looks happy, and, more to the point, it looks harmonious.\"[25] Caroline Westbrook of Empire magazine gave the film a three stars out of five, and said, \"Kids will love it and their adult companions will be warmly entertained\u2014but it's far from a computer-animated classic.\"[26] Rob Mackie of The Guardian gave the film three stars out of five, saying that it \"skillfully combines adult and kids' comedy. But for all the imaginative splendours and a sharp script, Robots is never quite as distinctive as its predecessor, Ice Age.\"[27] Common Sense Media gave the film four stars out of five, calling it an \"endearing 'follow your dreams' story with plenty of laughs\".[28] Robots won an ASCAP award in the category of top box-office films. The movie received two Annie Award nominations (Outstanding Character Design in a Feature Production and Outstanding Production Design in an Animated Feature Production; both for William Joyce and Steve Martino for the latter) and two Kids' Choice Award nominations (Favorite Animated Movie and Favorite Voice From an Animated Movie for Robin Williams's performance as Fender). Robots was also nominated for a Teen Choice Award (Choice Movie: Animated/Computer Generated) and a Visual Effects Society Award.[citation needed] The film is recognized by American Film Institute in these lists: Robots: Original Motion Picture Score was composed by John Powell, conducted by Pete Anthony, performed by the Hollywood Studio Symphony and released on March 15, 2005, by Var\u00e8se Sarabande and Fox Music.[30][31] A video game based on the film was released on February 24, 2005, for the Game Boy Advance, GameCube, Nintendo DS, PlayStation 2, Xbox and Windows. It was developed by Eurocom for home consoles and Windows, and by Griptonite Games for the Game Boy Advance and Nintendo DS. It was published by Vivendi Universal Games. The game received mixed to average reviews from critics.[32][33][34] Following the release of Robots, both Wedge and Joyce have expressed interest in making a sequel.[35][36] In light of the Release the Snyder Cut movement and the closure of Blue Sky Studios, a movement to release a director's cut of Robots gained traction in 2022.[37] A proposed director's cut was first mentioned on the film's original DVD audio commentary with Wedge and Joyce, in which Wedge said that there would be alternate takes in certain scenes, and that Cappy would have been more fleshed out.[38] Richelieu, Kentucky: Richelieu is an unincorporated community in Logan County and Butler County, Kentucky, United States.[1] Richelieu is located near Logan County's northeastern boundary with Butler County along Kentucky Route 1038. It is also located near the tripoint where Logan and Butler County boundaries meet with those of Warren County.[2] This Logan County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information. This Butler County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "11_random_chunk1",
        "52_random_chunk1"
      ],
      "source_ids": [
        "S211",
        "S252"
      ],
      "category": "comparative",
      "id": 60
    },
    {
      "question": "Compare Barbara Walters and List of Cantharis species in one sentence each: what does each describe or study?",
      "ground_truth": "Barbara Walters: Barbara Jill Walters (September 25, 1929 \u2013 December 30, 2022) was an American broadcast journalist and television personality.[1][2] Known for her interviewing ability and popularity with viewers, she appeared as a host of numerous television programs, including Today, the ABC Evening News, 20/20, and The View. Walters was a working journalist from 1951 until her retirement in 2014.[3][4][5] Walters was inducted into the Television Hall of Fame in 1989, received a Lifetime Achievement Award from the NATAS in 2000 and a star on the Hollywood Walk of Fame in 2007. Walters began her career at WNBT-TV (NBC's flagship station in New York) in 1953 as writer-producer of a news-and-information program aimed at the juvenile audience, Ask the Camera, hosted by Sandy Becker. She joined the staff of the network's Today show in the early 1960s as a writer and segment producer of women's-interest stories. Her popularity with viewers led to her receiving more airtime, and in 1974 she became co-host of the program, the first woman to hold such a position on an American news program.[6][7][8] During 1976, she continued to be a pioneer for women in broadcasting while becoming the first American female co-anchor of a network evening news program, alongside Harry Reasoner on the ABC Evening News. Walters was a correspondent, producer and co-host on the ABC news magazine 20/20 from 1979 to 2004. She became known for an annual special aired on ABC, Barbara Walters' 10 Most Fascinating People. During her career, Walters interviewed every sitting U.S. president and first lady from Richard and Pat Nixon to Barack and Michelle Obama.[9][10] She also interviewed both Donald Trump and Joe Biden, although not when either was president. She also gained acclaim and notoriety for interviewing subjects such as Fidel Castro, Anwar Sadat, Menachem Begin, Katharine Hepburn, Sean Connery, List of Cantharis species: This is a list of 95 species in Cantharis, a genus of soldier beetles in the family Cantharidae.[1][2][3] Data sources: i = ITIS,[1] c = Catalogue of Life,[4] g = GBIF,[2] b = Bugguide.net,[3] f = Fauna Europaea[5]",
      "expected_answer": "Barbara Walters: Barbara Jill Walters (September 25, 1929\u00a0\u2013 December 30, 2022) was an American broadcast journalist and television personality.[1][2] Known for her interviewing ability and popularity with viewers, she appeared as a host of numerous television programs, including Today, the ABC Evening News, 20/20, and The View. Walters was a working journalist from 1951 until her retirement in 2014.[3][4][5] Walters was inducted into the Television Hall of Fame in 1989, received a Lifetime Achievement Award from the NATAS in 2000 and a star on the Hollywood Walk of Fame in 2007. Walters began her career at WNBT-TV (NBC's flagship station in New York) in 1953 as writer-producer of a news-and-information program aimed at the juvenile audience, Ask the Camera, hosted by Sandy Becker. She joined the staff of the network's Today show in the early 1960s as a writer and segment producer of women's-interest stories. Her popularity with viewers led to her receiving more airtime, and in 1974 she became co-host of the program, the first woman to hold such a position on an American news program.[6][7][8] During 1976, she continued to be a pioneer for women in broadcasting while becoming the first American female co-anchor of a network evening news program, alongside Harry Reasoner on the ABC Evening News. Walters was a correspondent, producer and co-host on the ABC news magazine 20/20 from 1979 to 2004. She became known for an annual special aired on ABC, Barbara Walters' 10 Most Fascinating People. During her career, Walters interviewed every sitting U.S. president and first lady from Richard and Pat Nixon to Barack and Michelle Obama.[9][10] She also interviewed both Donald Trump and Joe Biden, although not when either was president. She also gained acclaim and notoriety for interviewing subjects such as Fidel Castro, Anwar Sadat, Menachem Begin, Katharine Hepburn, Sean Connery, Monica Lewinsky, Hugo Ch\u00e1vez, Vladimir Putin,[11] Shah Mohammad Reza Pahlavi, Jiang Zemin, Saddam Hussein, and Bashar al-Assad.[12] Walters created, produced, and co-hosted the ABC daytime talk show The View; she appeared on the program from 1997 until 2014.[13] Later she continued to host several special reports for 20/20 as well as documentary series for Investigation Discovery. Her final on-air appearance for ABC News was in 2015.[14][15][16][17][18] Walters last publicly appeared in 2016. Barbara Jill Walters was born on September 25, 1929,[19][a] in Boston, a daughter of Dena (n\u00e9e Seletsky) and Lou Walters (born Louis Abraham Warmwater);[21][22] her parents were children of Russian Jewish immigrants.[23][24] Her paternal grandfather, Abraham Isaac Waremwasser, was born in the Polish city of \u0141\u00f3d\u017a and emigrated to England where he changed his surname to Warmwater.[25] Walters' father was born in London in 1898 and moved to New York City with his father and two brothers on August 28, 1909. His mother and four sisters arrived there the following year.[26] During Walters' childhood, her father managed the Latin Quarter nightclub in Boston, which was owned in partnership with E.\u00a0M. Loew. In 1942, her father opened the club's famous New York location. He also worked as a Broadway producer and produced the Ziegfeld Follies of 1943;[27][28] he was also the entertainment director for the Tropicana Resort and Casino in Las Vegas. He imported the Folies Berg\u00e8re stage show from Paris to the resort's main showroom.[29] Walters' older brother, Burton, was 14 months old when he died of pneumonia.[30][31] Her elder sister, Jacqueline, was born with mental disabilities and died of ovarian cancer in 1985.[32] According to Walters, her father made and lost several fortunes throughout his life in show business. He was a booking agent, and (unlike her uncles in the shoe and dress businesses) his job was not very stable. During the good times, she recalled her father taking her to the rehearsals of the nightclub shows he directed and produced. The actresses and dancers would make a huge fuss over her and twirl her around until she was dizzy, after which she said her father would take her out to get hot dogs.[33] Walters said that being surrounded by celebrities when she was young kept her from being \"in awe\" of them.[34] When she was a young woman, her father lost his night clubs and the family's penthouse on Central Park West. As Walters recalled, \"He had a breakdown. He went down to live in our house in Florida, and then the government took the house, and they took the car, and they took the furniture. [...] My mother should have married the way her friends did, to a man who was a doctor or who was in the dress business.\"[35] During her childhood in Miami Beach, she briefly lived with the mobster Bill Dwyer.[36] Walters attended Lawrence School, a public school in Brookline, Massachusetts; she left halfway through fifth grade when her father moved the family to Miami Beach in 1939.[37] She continued attending public school in Miami Beach.[38] After her father moved the family to New York City, she spent eighth grade at the private Ethical Culture Fieldston School,[39] after which the family moved back to Miami Beach.[40] She returned to New York City after tenth grade and attended Birch Wathen School, another private school.[41][42][43] In 1951, she earned a Bachelor of Arts in English from Sarah Lawrence College in Yonkers, New York.[44] Walters was employed for about a year at a small advertising agency in New York City and began working at the NBC network's flagship station WNBT-TV (now WNBC), doing publicity and writing press releases. In 1953 she produced a 15-minute children's program, Ask the Camera, which was directed by Roone Arledge. She also started producing for TV host Igor Cassini (Cholly Knickerbocker), but left the network after Cassini pressured her to marry him and started a fistfight with the man she was interested in. She went to WPIX to produce the Eloise McElhone Show, which was canceled in 1954.[45] She became a writer on The Morning Show at CBS in 1955.[46] After a few years working at Tex McCrary Inc. as a publicist and as a writer at Redbook magazine, Walters joined NBC's The Today Show as a writer and researcher in 1961.[47] She moved up, becoming the show's regular \"Today Girl,\" handling lighter assignments and the weather. In her autobiography, she described this era before the Women's Movement as a time when it was believed that nobody would take a woman seriously reporting \"hard news.\" Previous \"Today Girls\" (whom Walters called \"tea pourers\") included Florence Henderson, Helen O'Connell, Estelle Parsons, and Lee Meriwether.[48][49] Within a year, she had become a reporter-at-large, developing, writing, and editing her own reports and interviews.[50] One very well-received film segment was \"A Day in the Life of a Nun.\" Another was about the daily life of a Playboy Bunny.[51] Beginning in 1971, Walters hosted her own local NBC affiliate show, Not for Women Only, which ran in the mornings after The Today Show.[52][53] Walters had a great relationship with host Hugh Downs for years. When Frank McGee was named host in 1971, he refused to do joint interviews with Walters unless he was given the first three questions.[54] She was not named co-host of the show until McGee's death in 1974 when NBC officially designated Walters as the program's first female co-host.[55] She became the first female co-host of an American news program.[56] Walters signed a five-year, $5-million contract with ABC, establishing her as the highest-paid news anchor, either male or female.[9] She and Harry Reasoner co-anchored the ABC Evening News from 1976 to 1978, making her the first American female network news anchor.[56] Reasoner had a difficult relationship with Walters because he disliked having a co-anchor, even though he worked with former CBS colleague Howard K. Smith nightly on ABC for several years. Walters said that the tension between the two was because Reasoner did not want to work with a co-anchor and also because he was unhappy at ABC, not because he disliked Walters personally.[57] In 1981, five years after the start of their short-lived ABC partnership and well after Reasoner returned to CBS News, Walters and her former co-anchor had a memorable (and cordial) 20/20 interview on the occasion of Reasoner's new book release.[58] In 1979, Walters reunited with former The Today Show host Downs as a correspondent on the ABC newsmagazine 20/20. She became Downs' co-host in 1984, and remained with the program until she retired as co-host in 2004.[59] Throughout her career at ABC, Walters appeared on ABC news specials as a commentator, including presidential inaugurations and the coverage of the September 11 attacks. She was also chosen to be the moderator for the third and final debate between candidates Jimmy Carter and Gerald Ford, held on the campus of the College of William and Mary at Phi Beta Kappa Memorial Hall in Williamsburg, Virginia, during the 1976 presidential election.[60] In 1984, she moderated a presidential debate which was held at the Dana Center for the Humanities at Saint Anselm College in Goffstown, New Hampshire.[61] Walters was known for \"personality journalism\"[62] and her \"scoop\" interviews.[63] In 1976, she first aired her highly rated, occasional, primetime Barbara Walters Specials interview program. Her first guests included a joint appearance by President-elect Jimmy Carter and Rosalynn Carter, and a separate interview with singer-actress Barbra Streisand.[64] In November 1977, she landed the first joint interview with Egyptian president Anwar Al Sadat and Israeli prime minister Menachem Begin, while they were working out the terms of the eventual Egypt\u2013Israel peace treaty.[65][66] According to The New York Times, when she competed with Walter Cronkite to interview both world leaders, at the end of Cronkite's interview, he is heard saying: \"Did Barbara get anything I didn't get?\"[67] Walters had sit-down interviews with world leaders, including the Shah of Iran, Mohammad Reza Pahlavi, and his wife, the Empress Farah Pahlavi;[68] Russia's Boris Yeltsin and Vladimir Putin;[69] China's Jiang Zemin; the UK's Margaret Thatcher;[70] Cuba's Fidel Castro,[71] as well as India's Indira Gandhi,[72] Czechoslovakia's V\u00e1clav Havel,[73] Libya's Muammar al-Gaddafi,[74] King Hussein of Jordan,[75] King Abdullah of Saudi Arabia,[76] Venezuelan President Hugo Ch\u00e1vez,[77] Iraq's Saddam Hussein and many others. Walters interviewed other influential people including pop icon Michael Jackson, Katharine Hepburn, Vogue editor Anna Wintour,[78] and Laurence Olivier in 1980.[79] Walters considered Robert Smithdas, a deaf-blind man who spent his life improving the lives of other individuals who are deaf-blind, as her most inspirational interviewee.[80] Walters was widely lampooned for asking actress Katharine Hepburn, \"If you were a tree, what kind would you be?\" On the last 20/20 television episode in which she appears, Walters showed a video of the Hepburn interview, showing the actress saying that she felt like a strong tree in her old age. Walters followed up with the question, \"What kind of a tree?\", and Hepburn responded \"an oak\" because they do not get Dutch elm disease.[81] According to Walters, for years Hepburn refused her requests for an interview. When Hepburn finally agreed she said she wanted to meet Walters first. Walters walked affably, while Hepburn was at the top of the stairs and said, \"You're late. Have you brought me chocolates?\"[82] Walters had not, but said she never showed up without them from then on. They had several other meetings later, mostly in Hepburn's living room where she would give Walters her opinions. These included that careers and marriage did not mix, as well as her feeling that combining children with careers was out of the question. Walters said Hepburn's opinions stuck with her so much, she could repeat them almost verbatim from that point onward.[33] Her television special about Cuban leader Fidel Castro aired on ABC-TV on June 9, 1977. Although the footage of her two days of interviewing Castro in Cuba showed his personality, in part, as freewheeling, charming, and humorous,[83] she pointedly said to him, \"You allow no dissent. Your newspapers, radio, television, motion pictures are under state control.\" To this, he replied, \"Barbara, our concept of freedom of the press is not yours. If you asked us if a newspaper could appear here against socialism, I can say honestly no, it cannot appear. It would not be allowed by the party, the government, or the people. In that sense we do not have the freedom of the press that you possess in the U.S. and we are very satisfied about that.\"[84] She concluded the broadcast saying, \"What we disagreed on most profoundly is the meaning of freedom\u2014and that is what truly separates us.\"[85] At the time, Walters did not mention that she had seen New York Yankees owner George Steinbrenner, pitcher Whitey Ford, and several coaches in Cuba who were there to assist Cuban ballplayers.[86] On March 3, 1999, her interview with Monica Lewinsky was seen by a record 74 million viewers, the highest rating ever for a news program.[87] Walters asked Lewinsky, \"What will you tell your children when you have them?\" Lewinsky replied, \"Mommy made a big mistake,\" at which point Walters brought the program to a dramatic conclusion, turning to the camera and saying, \"that is the understatement of the year.\"[88] Barbara Walters' 10 Most Fascinating People was aired annually starting in 1993.[89] In 2000, she quizzed pop star Ricky Martin about his sexuality years before he publicly came out. The singer later said that \"he felt violated\".[90] In 2010, Walters said that she regretted having pushed him on the issue.[91] Walters was a co-host of the daytime talk show The View; for 25 years she was also a co-executive producer of BarWall Productions alongside her business partner, Bill Geddie. Geddie and Walters were co-creators of the company. The View premiered on August 11, 1997.[92] In the original opening credits Walters said the show is a forum for women of \"different generations, backgrounds, and views.\"[93] \"Be careful what you wish for...\" was part of the opening credits of its second season. On The View, she won Daytime Emmy Awards for Best Talk Show in 2003 and Best Talk Show Host (with longtime host Joy Behar, moderator Whoopi Goldberg, Elisabeth Hasselbeck, and Sherri Shepherd) in 2009.[94] Walters retired from being a co-host on May 15, 2014.[95] She returned as a guest co-host on an intermittent basis in 2014 and 2015 even in retirement. After leaving her role as 20/20 co-host in 2004, Walters remained a part-time contributor of special programming and interviews for ABC News until 2016. On March 7, 2010, Walters announced that she would no longer hold Oscar interviews but would still work for ABC and on The View.[96] On March 28, 2013, numerous media outlets reported that Walters would retire in May 2014 and that she would make the announcement on the show four days later.[97][98][99][100] However, on the April 1 episode, she neither confirmed nor denied the retirement rumors; she said \"if and when I might have an announcement to make, I will do it on this program, I promise, and the paparazzi guys\u2014you will be the last to know\".[101][102] Six weeks later Walters confirmed that she would be retiring from television hosting and interviewing, as originally reported; she made the official announcement on the May 13, 2013, episode of The View. She also announced that she would continue as the show's executive producer for as long as it \"is on the air\".[103][104][105][106][107] On June 10, 2014, it was announced she would come out of retirement for a special 20/20 interview with Peter Rodger, the father of the perpetrator of the 2014 Isla Vista killings, Elliot Rodger.[15][108] In 2015, Walters hosted special 20/20 episodes featuring interviews with Mary Kay Letourneau[14] and Donald and Melania Trump.[16] In 2015, Walters hosted the documentary series American Scandals on Investigation Discovery.[17] Walters continued to host 10 Most Fascinating People on ABC in 2014[109] and 2015.[18] Her last on-air interview was with Donald Trump for ABC News in December 2015,[110] and she made her final public appearance in 2016.[111][112] On January 1, 2023, ABC ran a special called \"Our Barbara\" and a 20/20 senior producer noted, \"For a number of years we kept her office just as is (after 2016), the papers came every day. Outside of her office she still retained her office extension.\"[113] Walters was married four times to three different men. Her first husband was Robert Henry Katz, a business executive and former Navy lieutenant. They married on June 20, 1955, at the Plaza Hotel in New York City.[1][114] The marriage was reportedly annulled after eleven months,[115] in 1957.[116] Her second husband was Lee Guber, a theatrical producer and theater owner. They married on December 8, 1963, and divorced in 1976. After Walters had three miscarriages, the couple adopted a baby girl named Jacqueline Dena Guber (born in 1968 and adopted the same year; she was named for Walters' sister).[117] Walters' third husband was Merv Adelson who at the time was the CEO of Lorimar Television. They married in 1981 and divorced in 1984. They remarried in 1986 and divorced for the second time in 1992.[118] Walters dated lawyer[119][120] Roy Cohn in college; he said that he proposed marriage to Walters the night before her wedding to Lee Guber, but Walters denied this happened.[30] She explained her lifelong devotion to Cohn as gratitude for his help in her adoption of her daughter, Jacqueline.[121] In her autobiography, Walters says she also felt grateful to Cohn because of legal assistance he had provided to her father. According to Walters, her father was the subject of an arrest warrant for \"failure to appear\" after he failed to show up for a New York court date because the family was in Las Vegas; Cohn was able to have the charge dismissed.[122] Walters testified as a character witness at Cohn's 1986 disbarment trial.[123] Walters dated future U.S. Federal Reserve chairman Alan Greenspan in the 1970s[124] and was linked romantically to United States Senator John Warner in the 1990s.[125] In Walters's autobiography Audition, she wrote that she had an affair in the 1970s with Edward Brooke, then a married United States Senator from Massachusetts. It is not clear whether Walters also was married at the time. Walters said they ended the affair to protect their careers from scandal.[126] In 2007, she dated Pulitzer Prize\u2013winning gerontologist Robert Neil Butler.[127] Walters was a close friend of Tom Brokaw, Woody Allen, Joan Rivers, and Fox News head Roger Ailes.[128] In 2013, Walters said she regretted not having more children.[129][130] In May 2010, Walters said she would be having an open-heart operation to replace a faulty aortic valve. She had known that she was suffering from aortic stenosis, even though she was symptom-free. Four days after the operation, Walters' spokeswoman, Cindi Berger, said that the procedure to fix the faulty heart valve \"went well, and the doctors are very pleased with the outcome\".[131] Walters returned to The View and her Sirius XM satellite show, Here's Barbara, in September 2010.[132][133] Walters retired permanently from both shows four years later.[134] Walters died at her home in Manhattan on December 30, 2022, at age 93. She had been suffering from dementia in her later years.[135][9][136] Her last words were, \"No regrets \u2013 I had a great life.\" Those words were etched into her gravestone at Lakeside Memorial Park, a Jewish cemetery in Doral, Florida, where Walters is buried next to her sister and parents.[137][138][139] Her parents had spent their final years in Florida, residing in Miami, with her father a resident of Douglas Gardens Jewish Home for the Aged.[140][141] Walters began her career when the prevalent view among television executives (all of whom were male) was that women reporting news about war, politics and other important matters would be taken lightly by viewers.[65] Her success is credited with creating career opportunities for future female network anchors, including Jane Pauley, Katie Couric and Diane Sawyer. Walters often got her interviewees to speak about their perspectives and share anecdotes.[9][65] She was inducted into the Television Hall of Fame in 1989.[47] On June 15, 2007, Walters received a star on the Hollywood Walk of Fame.[142] She won Daytime and Prime Time Emmy Awards, a Women in Film Lucy Award,[143] and a GLAAD Excellence in Media award.[144] In 2008, Walters was honored with the Disney Legends award, given to those who made an outstanding contribution to The Walt Disney Company, which owns the network ABC. That same year, she received the Lifetime Achievement Award from the New York Women's Agenda. On September 21, 2009, Walters was honored with a Lifetime Achievement Award at the 30th Annual News and Documentary Emmy Awards at New York City's Lincoln Center.[145] Walters' status as a prominent figure in popular culture was reflected by Gilda Radner's gentle parody of her as \"Baba Wawa\" on Saturday Night Live in the late 1970s,[146] featuring Walters' distinctive speech including her rounded \"R's\". Her name appeared in the January 23, 1995, New York Times Monday Crossword Puzzle.[147] There was a social media campaign to have Barbara in Times Square to ring in 2020 by saying her iconic introduction to 20/20. As she was unable to appear CNN hired Cheri Oteri who was famous for her impression of Walters on Saturday Night Live to appear in her place.[148] Daytime Emmy Awards NAACP Image Award Women in Film Crystal + Lucy Awards Golden Plate Award of the American Academy of Achievement[164] In the late 1960s, Walters wrote a magazine article, \"How to Talk to Practically Anyone About Practically Anything\", which drew upon the kinds of things people said to her, which were often mistakes.[167] Shortly after the article appeared, she received a letter from Doubleday expressing interest in expanding it into a book. Walters felt that it would help \"tongue-tied, socially awkward people\u2014the many people who worry that they can't think of the right thing to say to start a conversation.\"[167] Walters published the book How to Talk with Practically Anybody About Practically Anything in 1970, with the assistance of ghostwriter June Callwood.[168] To Walters's great surprise, the book was a success. As of 2008, it had gone through eight printings, sold hundreds of thousands of copies worldwide, and had been translated into at least six languages.[167] Walters published her autobiography, Audition: A Memoir, in 2008.[169] List of Cantharis species: This is a list of 95 species in Cantharis, a genus of soldier beetles in the family Cantharidae.[1][2][3] Data sources: i = ITIS,[1] c = Catalogue of Life,[4] g = GBIF,[2] b = Bugguide.net,[3] f = Fauna Europaea[5]",
      "ground_truth_chunk_ids": [
        "244_random_chunk1",
        "121_random_chunk1"
      ],
      "source_ids": [
        "S444",
        "S321"
      ],
      "category": "comparative",
      "id": 61
    },
    {
      "question": "Compare The Gray Nun of Belgium and Artur Serobyan in one sentence each: what does each describe or study?",
      "ground_truth": "The Gray Nun of Belgium: The Gray Nun of Belgium was a 1915 film announced for release on the Alliance Program by Dramatic Feature Films, Frank Joslyn Baum's short-lived successor to The Oz Film Manufacturing Company. Despite the advertising in Motion Picture News announcing its release date, Katharine Rogers, in L. Frank Baum: Creator of Oz, believes that Alliance found the film inferior and refused to distribute it. The exhibition copy, which may have been a work print, may have been the only copy ever struck. Baum himself thought that exchanges and exhibitors dismissed the film \"rather arbitrarily\" based on the Oz Company name.[1] In the film, Betty Pierce played a Mother Superior who aided Allied soldiers during World War I. Cathrine Countiss played the title role. It also starred David Proctor, Mae Wells, Katherine Griffith, Raymond Russell, Robert Dunbar, Harry Clements, and James Spencer.[2] Wells and Russell were prominent actors in the Oz Company, having played roles such as Mombi and Dr. Pipt, respectively. This article about a film on World War I is a stub. You can help Wikipedia by adding missing information. This article relating to \"The Wonderful Wizard of Oz\" or one of its derivative works is a stub. You can help Wikipedia by adding missing information. Artur Serobyan: Artur Serobyan (Armenian: \u0531\u0580\u0569\u0578\u0582\u0580 \u054d\u0565\u0580\u0578\u0562\u0575\u0561\u0576, born 2 July 2003) is an Armenian professional footballer who plays as a winger or forward for Armenian Premier League club Ararat-Armenia, and the Armenia national team.[citation needed] Artur Serobyan is a graduate of the Yerevan Football Academy. At the age of 17, he already played in the senior team of Ararat-Armenia.[1] In August 2021, his loan transfer to BKMA was announced.[2] On 1 September 2023, his loan transfer to Portugal's top league club Casa Pia was announced until the end of the 2023\u201324 season.[3] On 10 January 2024, Casa Pia terminated Serobyan's loan and he returned to Ararat-Armenia.[4] On 14 January 2025, Serobyan signed for Sheriff Tiraspol on loan.[5][6] On 16 August 2025, Sheriff announced that Serobyan had left the club after his loan deal had been ended by mutual agreement.[7] Serobyan made his senior international debut for Armenia national team on 24 March 2022 in a friendly game against Montenegro, where he came in off the bench and played the last 20 minutes.[8] Sheriff Tiraspol Ararat-Armenia Individual",
      "expected_answer": "The Gray Nun of Belgium: The Gray Nun of Belgium was a 1915 film announced for release on the Alliance Program by Dramatic Feature Films, Frank Joslyn Baum's short-lived successor to The Oz Film Manufacturing Company. Despite the advertising in Motion Picture News announcing its release date, Katharine Rogers, in L. Frank Baum:  Creator of Oz, believes that Alliance found the film inferior and refused to distribute it.  The exhibition copy, which may have been a work print, may have been the only copy ever struck.  Baum himself thought that exchanges and exhibitors dismissed the film \"rather arbitrarily\" based on the Oz Company name.[1] In the film, Betty Pierce played a Mother Superior who aided Allied soldiers during World War I. Cathrine Countiss played the title role.  It also starred David Proctor, Mae Wells, Katherine Griffith, Raymond Russell, Robert Dunbar, Harry Clements, and James Spencer.[2] Wells and Russell were prominent actors in the Oz Company, having played roles such as Mombi and Dr. Pipt, respectively. This article about a film on World War I is a stub. You can help Wikipedia by adding missing information. This article relating to \"The Wonderful Wizard of Oz\" or one of its derivative works is a stub. You can help Wikipedia by adding missing information. Artur Serobyan: Artur Serobyan (Armenian: \u0531\u0580\u0569\u0578\u0582\u0580 \u054d\u0565\u0580\u0578\u0562\u0575\u0561\u0576, born 2 July 2003) is an Armenian professional footballer who plays as a winger or forward for Armenian Premier League club Ararat-Armenia, and the Armenia national team.[citation needed] Artur Serobyan is a graduate of the Yerevan Football Academy. At the age of 17, he already played in the senior team of Ararat-Armenia.[1] In August 2021, his loan transfer to BKMA was announced.[2] On 1 September 2023, his loan transfer to Portugal's top league club Casa Pia was announced until the end of the 2023\u201324 season.[3] On 10 January 2024, Casa Pia terminated Serobyan's loan and he returned to Ararat-Armenia.[4] On 14 January 2025, Serobyan signed for Sheriff Tiraspol on loan.[5][6] On 16 August 2025, Sheriff announced that Serobyan had left the club after his loan deal had been ended by mutual agreement.[7] Serobyan made his senior international debut for Armenia national team on 24 March 2022 in a friendly game against Montenegro, where he came in off the bench and played the last 20 minutes.[8] Sheriff Tiraspol Ararat-Armenia Individual",
      "ground_truth_chunk_ids": [
        "90_random_chunk1",
        "190_random_chunk1"
      ],
      "source_ids": [
        "S290",
        "S390"
      ],
      "category": "comparative",
      "id": 62
    },
    {
      "question": "Compare Climate change and Parabothria in one sentence each: what does each describe or study?",
      "ground_truth": "Climate change: Present-day climate change includes both global warming\u2014the ongoing increase in global average temperature\u2014and its wider effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The modern-day rise in global temperatures is driven by human activities, especially fossil fuel (coal, oil and natural gas) burning since the Industrial Revolution.[3][4] Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases.[5] These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Earth's atmosphere now has roughly 50% more carbon dioxide, the main gas driving global warming, than it did at the end of the pre-industrial era, reaching levels not seen for millions of years.[6] Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common.[7] Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline.[8] Higher temperatures are also causing more intense storms, droughts, and other weather extremes.[9] Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct.[10] Even if efforts to minimize future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise.[11] Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss.[12] Human migration and conflict can also be a result.[13] The World Health Organization calls climate change one of the biggest threats to global health in the 21st century.[14] Societies and ecosystems will experience more severe risks without action to limit warming.[15] Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some Parabothria: Parabothria is a genus of parasitic flies in the family Tachinidae. There is one described species in Parabothria, P. punoensis.[1][2] This article related to members of the fly family Tachinidae is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Climate change: Present-day climate change includes both global warming\u2014the ongoing increase in global average temperature\u2014and its wider effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The modern-day rise in global temperatures is driven by human activities, especially fossil fuel (coal, oil and natural gas) burning since the Industrial Revolution.[3][4] Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases.[5] These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Earth's atmosphere now has roughly 50% more carbon dioxide, the main gas driving global warming, than it did at the end of the pre-industrial era, reaching levels not seen for millions of years.[6] Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common.[7] Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline.[8] Higher temperatures are also causing more intense storms, droughts, and other weather extremes.[9] Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct.[10] Even if efforts to minimize future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise.[11] Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss.[12] Human migration and conflict can also be a result.[13] The World Health Organization calls climate change one of the biggest threats to global health in the 21st century.[14] Societies and ecosystems will experience more severe risks without action to limit warming.[15] Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some limits to adaptation have already been reached.[16] Poorer communities are responsible for a small share of global emissions, yet have the least ability to adapt and are most vulnerable to climate change.[17][18] Many climate change impacts have been observed in the first decades of the 21st century, with 2024 the warmest on record at +1.60\u00a0\u00b0C (2.88\u00a0\u00b0F) since regular tracking began in 1850.[20][21] Additional warming will increase these impacts and can trigger tipping points, such as melting all of the Greenland ice sheet.[22] Under the 2015 Paris Agreement, nations collectively agreed to keep warming \"well under 2\u00a0\u00b0C\". However, with pledges made under the Agreement, global warming would still reach about 2.8\u00a0\u00b0C (5.0\u00a0\u00b0F) by the end of the century.[23] There is widespread support for climate action worldwide,[24][25] and most countries aim to stop emitting carbon dioxide.[26] Fossil fuels can be phased out by stopping subsidising them, conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind, solar, hydro, and nuclear power.[27] Cleanly generated electricity can replace fossil fuels for powering transportation, heating buildings, and running industrial processes.[28] Carbon can also be removed from the atmosphere, for instance by increasing forest cover and farming with methods that store carbon in soil.[29][30][31] Before the 1980s, it was unclear whether the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution. Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time.[32] In the 1980s, the terms global warming and climate change became more common, often being used interchangeably.[33][34][35] Scientifically, global warming refers only to increased global average surface temperature, while climate change describes both global warming and its effects on Earth's climate system, such as precipitation changes.[32] Climate change can also be used more broadly to include changes to the climate that have happened throughout Earth's history as result of natural processes.[36] The term anthropogenic climate change is sometimes used to describe climate change resulting from human activities.[37] Global warming\u2014used as early as 1975[38]\u2014became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate.[39] Since the 2000s, usage of climate change has increased.[40] Various scientists, politicians and media may use the terms climate crisis or climate emergency to talk about climate change, and may use the term global heating instead of global warming.[41][42] Over the last few million years the climate cycled through ice ages. One of the hotter periods was the Last Interglacial, around 125,000 years ago, where temperatures were between 0.5\u00a0\u00b0C and 1.5\u00a0\u00b0C warmer than before the start of global warming.[45] This period saw sea levels 5 to 10 metres higher than today. The most recent glacial maximum 20,000 years ago was some 5\u20137\u00a0\u00b0C colder. This period has sea levels that were over 125 metres (410\u00a0ft) lower than today.[46] Temperatures stabilized in the current interglacial period beginning 11,700 years ago.[47] This period also saw the start of agriculture.[48] Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age, did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions.[49][50] Climate information for that period comes from climate proxies, such as trees and ice cores.[51][52] Around 1850 thermometer records began to provide global coverage.[55]\nBetween the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling from sulfur dioxide emissions. Sulfur dioxide causes acid rain, but it also produces sulfate aerosols in the atmosphere, which reflect sunlight and cause global dimming. After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature.[56][57][58] Ongoing changes in climate have had no precedent for several thousand years.[59] Multiple datasets all show worldwide increases in surface temperature,[60] at a rate of around 0.2\u00a0\u00b0C per decade.[61] The 2014\u20132023 decade warmed to an average 1.19\u00a0\u00b0C [1.06\u20131.30\u00a0\u00b0C] compared to the pre-industrial baseline (1850\u20131900).[62] Not every single year was warmer than the last: internal climate variability processes can make any year 0.2\u00a0\u00b0C warmer or colder than the average.[63] From 1998 to 2013, negative phases of two such processes, Pacific Decadal Oscillation (PDO)[64] and Atlantic Multidecadal Oscillation (AMO)[65] caused a short slower period of warming called the \"global warming hiatus\".[66] After the \"hiatus\", the opposite occurred, with 2024 well above the recent average at more than +1.5\u00a0\u00b0C.[67] This is why the temperature change is defined in terms of a 20-year average, which reduces the noise of hot and cold years and decadal climate patterns, and detects the long-term signal.[68]:\u200a5\u200a[69] A wide range of other observations reinforce the evidence of warming.[70][71] The upper atmosphere is cooling, because greenhouse gases are trapping heat near the Earth's surface, and so less heat is radiating into space.[72] Warming reduces average snow cover and forces the retreat of glaciers. At the same time, warming also causes greater evaporation from the oceans, leading to more atmospheric humidity, more and heavier precipitation.[73][74] Plants are flowering earlier in spring, and thousands of animal species have been permanently moving to cooler areas.[75] Different regions of the world warm at different rates. The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global average surface temperature.[76] This is because oceans lose more heat by evaporation and oceans can store a lot of heat.[77] The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has been stored in the ocean.[78][79] The rest has heated the atmosphere, melted ice, and warmed the continents.[80] The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat.[81] Local black carbon deposits on snow and ice also contribute to Arctic warming.[82] Arctic surface temperatures are increasing between three and four times faster than in the rest of the world.[83][84] Melting of ice sheets near the poles weakens both the Atlantic and the Antarctic limb of thermohaline circulation, which further changes the distribution of heat and precipitation around the globe.[85][86][87][88] The World Meteorological Organization estimates there is almost a 50% chance of the five-year average global temperature exceeding +1.5\u00a0\u00b0C between 2024 and 2028.[91] The IPCC expects the 20-year average to exceed +1.5\u00a0\u00b0C in the early 2030s.[92] The IPCC Sixth Assessment Report (2021) included projections that by 2100 global warming is very likely to reach 1.0\u20131.8\u00a0\u00b0C under a scenario with very low emissions of greenhouse gases, 2.1\u20133.5\u00a0\u00b0C under an intermediate emissions scenario,\nor 3.3\u20135.7\u00a0\u00b0C under a very high emissions scenario.[93] The warming will continue past 2100 in the intermediate and high emission scenarios,[94][95] with future projections of global surface temperatures by year 2300 being similar to millions of years ago.[96] The remaining carbon budget for staying beneath certain temperature increases is determined by modelling the carbon cycle and climate sensitivity to greenhouse gases.[97] According to UNEP, global warming can be kept below 2.0\u00a0\u00b0C with a 50% chance if emissions after 2023 do not exceed 900 gigatonnes of CO2. This carbon budget corresponds to around 16 years of current emissions.[98] The climate system experiences various cycles on its own which can last for years, decades or even centuries. For example, El Ni\u00f1o events cause short-term spikes in surface temperature while La Ni\u00f1a events cause short term cooling.[99] Their relative frequency can affect global temperature trends on a decadal timescale.[100] Other changes are caused by an imbalance of energy from external forcings.[101] Examples of these include changes in the concentrations of greenhouse gases, solar luminosity, volcanic eruptions, and variations in the Earth's orbit around the Sun.[102] To determine the human contribution to climate change, unique \"fingerprints\" for all potential causes are developed and compared with both observed patterns and known internal climate variability.[103] For example, solar forcing\u2014whose fingerprint involves warming the entire atmosphere\u2014is ruled out because only the lower atmosphere has warmed.[104] Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes in albedo, are less impactful.[105] Greenhouse gases are transparent to sunlight, and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat, and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time.[106] While water vapour (\u224850%) and clouds (\u224825%) are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature and are therefore mostly considered to be feedbacks that change climate sensitivity. On the other hand, concentrations of gases such as CO2 (\u224820%), tropospheric ozone,[107] CFCs and nitrous oxide are added or removed independently from temperature, and are therefore considered to be external forcings that change global temperatures.[108] Before the Industrial Revolution, naturally occurring amounts of greenhouse gases caused the air near the surface to be about 33\u00a0\u00b0C warmer than it would have been in their absence.[109][110] Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal, oil, and natural gas),[111] has increased the amount of greenhouse gases in the atmosphere. In 2022, the concentrations of CO2 and methane had increased by about 50% and 164%, respectively, since 1750.[112] These CO2 levels are higher than they have been at any time during the last 14 million years.[113] Concentrations of methane are far higher than they were over the last 800,000 years.[114] Global human-caused greenhouse gas emissions in 2019 were equivalent to 59\u00a0billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane, 4% was nitrous oxide, and 2% was fluorinated gases.[115] CO2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity.[5] Additional CO2 emissions come from deforestation and industrial processes, which include the CO2 released by the chemical reactions for making cement, steel, aluminium, and fertilizer.[116][117][118][119] Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, and coal mining, as well as oil and gas extraction.[120][121] Nitrous oxide emissions largely come from the microbial decomposition of fertilizer.[122][123] While methane only lasts in the atmosphere for an average of 12 years,[124] CO2 lasts much longer. The Earth's surface absorbs CO2 as part of the carbon cycle. While plants on land and in the ocean absorb most excess emissions of CO2 every year, that CO2 is returned to the atmosphere when biological matter is digested, burns, or decays.[125] Land-surface carbon sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions.[126] The ocean has absorbed 20 to 30% of emitted CO2 over the last two decades.[127] CO2 is only removed from the atmosphere for the long term when it is stored in the Earth's crust, which is a process that can take millions of years to complete.[125] Around 30% of Earth's land area is largely unusable for humans (glaciers, deserts, etc.), 26% is forests, 10% is shrubland and 34% is agricultural land.[129] Deforestation is the main land use change contributor to global warming,[130] as the destroyed trees release CO2, and are not replaced by new trees, removing that carbon sink.[131] Between 2001 and 2018, 27% of deforestation was from permanent clearing to enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%.[132] Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink.[133] Local vegetation cover impacts how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns.[134] In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler.[133] At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains.[134] Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect.[135] Air pollution, in the form of aerosols, affects the climate on a large scale.[136] Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming,[137] and is primarily attributed to sulfate aerosols produced by the combustion of fossil fuels with heavy sulfur concentrations like coal and bunker fuel.[58] Smaller contributions come from black carbon (from combustion of fossil fuels and biomass), and from dust.[138][139][140] Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much.[141][58] Aerosols also have indirect effects on the Earth's energy budget. Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets.[142] They also reduce the growth of raindrops, which makes clouds more reflective to incoming sunlight.[143] Indirect effects of aerosols are the largest uncertainty in radiative forcing.[144] While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise.[145] Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2\u00a0\u00b0C by 2050.[146] The effect of decreasing sulfur content of fuel oil for ships since 2020[147] is estimated to cause an additional 0.05\u00a0\u00b0C increase in global mean temperature by 2050.[148] As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system.[144] Solar irradiance has been measured directly by satellites,[151] and indirect measurements are available from the early 1600s onwards.[144] Since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (the troposphere).[152] The upper atmosphere (the stratosphere) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling.[104]\nThis is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere.[153] Explosive volcanic eruptions can release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapour into the atmosphere, which adds to greenhouse gases and increases temperatures.[154] These impacts on temperature only last for several years, because both water vapour and volcanic material have low persistence in the atmosphere.[155] volcanic CO2 emissions are more persistent, but they are equivalent to less than 1% of current human-caused CO2 emissions.[156] Volcanic activity still represents the single largest natural impact (forcing) on temperature in the industrial era. Yet, like the other natural forcings, it has had negligible impacts on global temperature trends since the Industrial Revolution.[155] The climate system's response to an initial forcing is shaped by feedbacks, which either amplify or dampen the change. Self-reinforcing or positive feedbacks increase the response, while balancing or negative feedbacks reduce it.[158] The main reinforcing feedbacks are the water-vapour feedback, the ice\u2013albedo feedback, and the net cloud feedback.[159][160] The primary balancing mechanism is radiative cooling, as Earth's surface gives off more heat to space in response to rising temperature.[161] In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO2 on plant growth.[162] Feedbacks are expected to trend in a positive direction as greenhouse gas emissions continue, raising climate sensitivity.[163] These feedback processes alter the pace of global warming. For instance, warmer air can hold more moisture in the form of water vapour, which is itself a potent greenhouse gas.[159] Warmer air can also make clouds higher and thinner, and therefore more insulating, increasing climate warming.[164] The reduction of snow cover and sea ice in the Arctic is another major feedback, this reduces the reflectivity of the Earth's surface in the region and accelerates Arctic warming.[165][166] This additional warming also contributes to permafrost thawing, which releases methane and CO2 into the atmosphere.[167] Around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans.[168] This fraction is not static and if future CO2 emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%.[169] This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plants when they are warmer.[170][171] The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes in thermohaline circulation and phytoplankton distribution.[172][173][86] Uncertainty over feedbacks, particularly cloud cover,[174] is the major reason why different climate models project different magnitudes of warming for a given amount of emissions.[175] A climate model is a representation of the physical, chemical and biological processes that affect the climate system.[176] Models include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing.[177] Models are used to estimate the degree of warming future emissions will cause when accounting for the strength of climate feedbacks.[178][179] Models also predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere.[180] The physical realism of models is tested by examining their ability to simulate current or past climates.[181] Past models have underestimated the rate of Arctic shrinkage[182] and underestimated the rate of precipitation increase.[183] Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations.[184] The 2017 United States-published National Climate Assessment notes that \"climate models may still be underestimating or missing relevant feedback processes\".[185] Additionally, climate models may be unable to adequately predict short-term regional climatic shifts.[186] A subset of climate models add societal factors to a physical climate model. These models simulate how population, economic growth, and energy use affect\u2014and interact with\u2014the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change.[187][188] Depending on the socioeconomic scenario and the mitigation scenario, models produce atmospheric CO2 concentrations that range widely between 380 and 1400 ppm.[189] The environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations.[191] Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency.[192] Extremely wet or dry events within the monsoon period have increased in India and East Asia.[193] Monsoonal precipitation over the Northern Hemisphere has increased since 1980.[194] The rainfall rate and intensity of hurricanes and typhoons is likely increasing,[195] and the geographic range likely expanding poleward in response to climate warming.[196] The frequency of tropical cyclones has not increased as a result of climate change.[197] Global sea level is rising as a consequence of thermal expansion and the melting of glaciers and ice sheets. Sea level rise has increased over time, reaching 4.8\u00a0cm per decade between 2014 and 2023.[199] Over the 21st century, the IPCC projects 32\u201362\u00a0cm of sea level rise under a low emission scenario, 44\u201376\u00a0cm under an intermediate one and 65\u2013101\u00a0cm under a very high emission scenario.[200] Marine ice sheet instability processes in Antarctica may add substantially to these values,[201] including the possibility of a 2-meter sea level rise by 2100 under high emissions.[202] Climate change has led to decades of shrinking and thinning of the Arctic sea ice.[203] While ice-free summers are expected to be rare at 1.5\u00a0\u00b0C degrees of warming, they are set to occur once every three to ten years at a warming level of 2\u00a0\u00b0C.[204] Higher atmospheric CO2 concentrations cause more CO2 to dissolve in the oceans, which is making them more acidic.[205] Because oxygen is less soluble in warmer water,[206] its concentrations in the ocean are decreasing, and dead zones are expanding.[207] Greater degrees of global warming increase the risk of passing through 'tipping points'\u2014thresholds beyond which certain major impacts can no longer be avoided even if temperatures return to their previous state.[210][211] For instance, the Greenland ice sheet is already melting, but if global warming reaches levels between 1.7\u00a0\u00b0C and 2.3\u00a0\u00b0C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5\u00a0\u00b0C or less, it will still lose a lot more ice than if the warming was never allowed to reach the threshold in the first place.[212] While the ice sheets would melt over millennia, other tipping points would occur faster and give societies less time to respond. The collapse of major ocean currents like the Atlantic meridional overturning circulation (AMOC), and irreversible damage to key ecosystems like the Amazon rainforest and coral reefs can unfold in a matter of decades.[209] The collapse of the AMOC would be a severe climate catastrophe, resulting in a cooling of the Northern Hemisphere.[213] The long-term effects of climate change on oceans include further ice melt, ocean warming, sea level rise, ocean acidification and ocean deoxygenation.[214] The timescale of long-term impacts are centuries to millennia due to CO2's long atmospheric lifetime.[215] The result is an estimated total sea level rise of 2.3 metres per degree Celsius (4.2\u00a0ft/\u00b0F) after 2000 years.[216] Oceanic CO2 uptake is slow enough that ocean acidification will also continue for hundreds to thousands of years.[217] Deep oceans (below 2,000 metres (6,600\u00a0ft)) are also already committed to losing over 10% of their dissolved oxygen by the warming which occurred to date.[218] Further, the West Antarctic ice sheet appears committed to practically irreversible melting, which would increase the sea levels by at least 3.3\u00a0m (10\u00a0ft 10\u00a0in) over approximately 2000 years.[209][219][220] Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes.[221] For instance, the range of hundreds of North American birds has shifted northward at an average rate of 1.5\u00a0km/year over the past 55 years.[222] Higher atmospheric CO2 levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear.[223] A related phenomenon driven by climate change is woody plant encroachment, affecting up to 500 million hectares globally.[224] Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics.[225] The size and speed of global warming is making abrupt changes in ecosystems more likely.[226] Overall, it is expected that climate change will result in the extinction of many species.[227] The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land.[228] Just as on land, heat waves in the ocean occur more frequently due to climate change, harming a wide range of organisms such as corals, kelp, and seabirds.[229] Ocean acidification makes it harder for marine calcifying organisms such as mussels, barnacles and corals to produce shells and skeletons; and heatwaves have bleached coral reefs.[230] Harmful algal blooms enhanced by climate change and eutrophication lower oxygen levels, disrupt food webs and cause great loss of marine life.[231] Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts.[232] Plants have come under increased stress from damage by insects.[233] The effects of climate change are impacting humans everywhere in the world.[239] Impacts can be observed on all continents and ocean regions,[240] with low-latitude, less developed areas facing the greatest risk.[241] Continued warming has potentially \"severe, pervasive and irreversible impacts\" for people and ecosystems.[242] The risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries.[243] The World Health Organization calls climate change one of the biggest threats to global health in the 21st century.[14] Scientists have warned about the irreversible harms it poses.[244] Extreme weather events affect public health, and food and water security.[245][246][247] Temperature extremes lead to increased illness and death.[245][246] Climate change increases the intensity and frequency of extreme weather events.[246][247] It can affect transmission of infectious diseases, such as dengue fever and malaria.[244][245] According to the World Economic Forum, 14.5\u00a0million more deaths are expected due to climate change by 2050.[248] 30% of the global population currently live in areas where extreme heat and humidity are already associated with excess deaths.[249][250] By 2100, 50% to 75% of the global population would live in such areas.[249][251] While total crop yields have been increasing in the past 50 years due to agricultural improvements, climate change has already decreased the rate of yield growth.[247] Fisheries have been negatively affected in multiple regions.[247] While agricultural productivity has been positively affected in some high latitude areas, mid- and low-latitude areas have been negatively affected.[247] According to the World Economic Forum, an increase in drought in certain regions could cause 3.2\u00a0million deaths from malnutrition by 2050 and stunting in children.[252] With 2\u00a0\u00b0C warming, global livestock headcounts could decline by 7\u201310% by 2050, as less animal feed will be available.[253] If the emissions continue to increase for the rest of century, then over 9 million climate-related deaths would occur annually by 2100.[254] Economic damages due to climate change may be severe and there is a chance of disastrous consequences.[255] Severe impacts are expected in South-East Asia and sub-Saharan Africa, where most of the local inhabitants are dependent upon natural and agricultural resources.[256][257] Heat stress can prevent outdoor labourers from working. If warming reaches 4\u00a0\u00b0C then labour capacity in those regions could be reduced by 30 to 50%.[258] The World Bank estimates that between 2016 and 2030, climate change could drive over 120 million people into extreme poverty without adaptation.[259] Inequalities based on wealth and social status have worsened due to climate change.[260] Major difficulties in mitigating, adapting to, and recovering from climate shocks are faced by marginalized people who have less control over resources.[261][256] Indigenous people, who are subsistent on their land and ecosystems, will face endangerment to their wellness and lifestyles due to climate change.[262] An expert elicitation concluded that the role of climate change in armed conflict has been small compared to factors such as socio-economic inequality and state capabilities.[263] While women are not inherently more at risk from climate change and shocks, limits on women's resources and discriminatory gender norms constrain their adaptive capacity and resilience.[264] For example, women's work burdens, including hours worked in agriculture, tend to decline less than men's during climate shocks such as heat stress.[264] Low-lying islands and coastal communities are threatened by sea level rise, which makes urban flooding more common. Sometimes, land is permanently lost to the sea.[265] This could lead to statelessness for people in island nations, such as the Maldives and Tuvalu.[266] In some regions, the rise in temperature and humidity may be too severe for humans to adapt to.[267] With worst-case climate change, models project that areas almost one-third of humanity live in might become Sahara-like uninhabitable and extremely hot climates.[268] These factors can drive climate or environmental migration, within and between countries.[269] More people are expected to be displaced because of sea level rise, extreme weather and conflict from increased competition over natural resources. Climate change may also increase vulnerability, leading to \"trapped populations\" who are not able to move due to a lack of resources.[270] Climate change can be mitigated by reducing the rate at which greenhouse gases are emitted into the atmosphere, and by increasing the rate at which carbon dioxide is removed from the atmosphere.[276] To limit global warming to less than 2\u00a0\u00b0C global greenhouse gas emissions need to be net-zero by 2070.[277] This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry.[278] The United Nations Environment Programme estimates that countries need to triple their pledges under the Paris Agreement within the next decade to limit global warming to 2\u00a0\u00b0C.[279] With pledges made under the Paris Agreement as of 2024, there would be a 66% chance that global warming is kept under 2.8\u00a0\u00b0C by the end of the century (range: 1.9\u20133.7\u00a0\u00b0C, depending on exact implementation and technological progress). When only considering current policies, this raises to 3.1\u00a0\u00b0C.[280] Globally, limiting warming to 2\u00a0\u00b0C may result in higher economic benefits than economic costs.[281] Although there is no single pathway to limit global warming to 2\u00a0\u00b0C,[282] most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions.[283] To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in agriculture and forestry,[284] such as preventing deforestation and restoring natural ecosystems by reforestation.[285] Other approaches to mitigating climate change have a higher level of risk. Scenarios that limit global warming to 1.5\u00a0\u00b0C typically project the large-scale use of carbon dioxide removal methods over the 21st century.[286] There are concerns, though, about over-reliance on these technologies, and environmental impacts.[287] Solar radiation modification (SRM) is a proposal for reducing global warming by reflecting some sunlight away from Earth and back into space. Because it does not reduce greenhouse gas concentrations, it would not address ocean acidification[288] and is not considered mitigation.[289] SRM should be considered only as a supplement to mitigation, not a replacement for it,[290]  due to risks such as rapid warming if it were abruptly stopped and not restarted.[291] The most-studied approach is stratospheric aerosol injection.[292] SRM could reduce global warming and some of its impacts, though imperfectly.[293] It poses environmental risks, such as changes to rainfall patterns,[294] as well as political challenges, such as who would decide whether to use it.[292] Renewable energy is key to limiting climate change.[296] For decades, fossil fuels have accounted for roughly 80% of the world's energy use.[297] The remaining share has been split between nuclear power and renewables (including hydropower, bioenergy, wind and solar power and geothermal energy).[298] Fossil fuel use is expected to peak in absolute terms prior to 2030 and then to decline, with coal use experiencing the sharpest reductions.[299] Renewables represented 86% of all new electricity generation installed in 2023.[300] Other forms of clean energy, such as nuclear and hydropower, currently have a larger share of the energy supply. However, their future growth forecasts appear limited in comparison.[301] While solar panels and onshore wind are now among the cheapest forms of adding new power generation capacity in many locations,[302] green energy policies are needed to achieve a rapid transition from fossil fuels to renewables.[303] To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. Investment in coal would be eliminated and coal use nearly phased out by 2050.[304][305] Electricity generated from renewable sources would also need to become the main energy source for heating and transport.[306] Transport can switch away from internal combustion engine vehicles and towards electric vehicles, public transit, and active transport (cycling and walking).[307][308] For shipping and flying, low-carbon fuels would reduce emissions.[307] Heating could be increasingly decarbonized with technologies like heat pumps.[309] There are obstacles to the continued rapid growth of clean energy, including renewables.[310] Wind and solar produce energy intermittently and with seasonal variability. Traditionally, hydro dams with reservoirs and fossil fuel power plants have been used when variable energy production is low. Going forward, battery storage can be expanded, energy demand and supply can be matched, and long-distance transmission can smooth variability of renewable outputs.[296] Bioenergy is often not carbon-neutral and may have negative consequences for food security.[311] The growth of nuclear power is constrained by controversy around radioactive waste, nuclear weapon proliferation, and accidents.[312][313] Hydropower growth is limited by the fact that the best sites have been developed, and new projects are confronting increased social and environmental concerns.[314] Low-carbon energy improves human health by minimizing climate change as well as reducing air pollution deaths,[315] which were estimated at 7 million annually in 2016.[316] Meeting the Paris Agreement goals that limit warming to a 2\u00a0\u00b0C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5\u00a0\u00b0C could save millions and simultaneously increase energy security and reduce poverty.[317] Improving air quality also has economic benefits which may be larger than mitigation costs.[318] Reducing energy demand is another major aspect of reducing emissions.[319] If less energy is needed, there is more flexibility for clean energy development. It also makes it easier to manage the electricity grid, and minimizes carbon-intensive infrastructure development.[320] Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy.[321] Several COVID-19 related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain.[322] Strategies to reduce energy demand vary by sector. In the transport sector, passengers and freight can switch to more efficient travel modes, such as buses and trains, or use electric vehicles.[323] Industrial strategies to reduce energy demand include improving heating systems and motors, designing less energy-intensive products, and increasing product lifetimes.[324] In the building sector the focus is on better design of new buildings, and higher levels of energy efficiency in retrofitting.[325] The use of technologies like heat pumps can also increase building energy efficiency.[326] Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand.[327] A set of actions could reduce agriculture and forestry-based emissions by two-thirds from 2010 levels. These include reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production.[328] On the demand side, a key component of reducing emissions is shifting people towards plant-based diets.[329] Eliminating the production of livestock for meat and dairy would eliminate about 3/4ths of all emissions from agriculture and other land use.[330] Livestock also occupy 37% of ice-free land area on Earth and consume feed from the 12% of land area used for crops, driving deforestation and land degradation.[331] Steel and cement production are responsible for about 13% of industrial CO2 emissions. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production, so that reducing CO2 emissions requires research into alternative chemistries.[332] Where energy production or CO2-intensive heavy industries continue to produce waste CO2, technology can sometimes be used to capture and store most of the gas instead of releasing it to the atmosphere.[333] This technology, carbon capture and storage (CCS), could have a critical but limited role in reducing emissions.[333] It is relatively expensive[334] and has been deployed only to an extent that removes around 0.1% of annual greenhouse gas emissions.[333] Natural carbon sinks can be enhanced to sequester significantly larger amounts of CO2 beyond naturally occurring levels.[335] Reforestation and afforestation (planting forests where there were none before) are among the most mature sequestration techniques, although the latter raises food security concerns.[336] Farmers can promote sequestration of carbon in soils through practices such as use of winter cover crops, reducing the intensity and frequency of tillage, and using compost and manure as soil amendments.[337] Forest and landscape restoration yields many benefits for the climate, including greenhouse gas emissions sequestration and reduction.[133] Restoration/recreation of coastal wetlands, prairie plots and seagrass meadows increases the uptake of carbon into organic matter.[338][339] When carbon is sequestered in soils and in organic matter such as trees, there is a risk of the carbon being re-released into the atmosphere later through changes in land use, fire, or other changes in ecosystems.[340] The use of bioenergy in conjunction with carbon capture and storage (BECCS) can result in net negative emissions as CO2 is drawn from the atmosphere.[341] It remains highly uncertain whether carbon dioxide removal techniques will be able to play a large role in limiting warming to 1.5\u00a0\u00b0C. Policy decisions that rely on carbon dioxide removal increase the risk of global warming rising beyond international goals.[342] Adaptation is \"the process of adjustment to current or expected changes in climate and its effects\".[343]:\u200a5\u200a Without additional mitigation, adaptation cannot avert the risk of \"severe, widespread and irreversible\" impacts.[344] More severe climate change requires more transformative adaptation, which can be prohibitively expensive.[345] The capacity and potential for humans to adapt is unevenly distributed across different regions and populations, and developing countries generally have less.[346] The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basic sanitation and electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance.[347] Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding, and building flood controls. If that fails, managed retreat may be needed.[348] There are economic barriers for tackling dangerous heat impact. Avoiding strenuous work or having air conditioning is not possible for everybody.[349] In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control, and genetic improvements for increased tolerance to a changing climate.[350] Insurance allows for risk-sharing, but is often difficult to get for people on lower incomes.[351] Education, migration and early warning systems can reduce climate vulnerability.[352] Planting mangroves or encouraging other coastal vegetation can buffer storms.[353][354] Ecosystems adapt to climate change, a process that can be supported by human intervention. By increasing connectivity between ecosystems, species can migrate to more favourable climate conditions. Species can also be introduced to areas acquiring a favourable climate. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt via ecosystem-based adaptation. For instance, restoration of natural fire regimes makes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest acts as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts.[355] There are synergies but also trade-offs between adaptation and mitigation.[356] An example for synergy is increased food productivity, which has large benefits for both adaptation and mitigation.[357] An example of a trade-off is that increased use of air conditioning allows people to better cope with heat, but increases energy demand. Another trade-off example is that more compact urban development may reduce emissions from transport and construction, but may also increase the urban heat island effect, exposing people to heat-related health risks.[358] Countries that are most vulnerable to climate change have typically been responsible for a small share of global emissions. This raises questions about justice and fairness.[359] Limiting global warming makes it much easier to achieve the UN's Sustainable Development Goals, such as eradicating poverty and reducing inequalities. The connection is recognized in Sustainable Development Goal 13 which is to \"take urgent action to combat climate change and its impacts\".[360] The goals on food, clean water and ecosystem protection have synergies with climate mitigation.[361] The geopolitics of climate change is complex. It has often been framed as a free-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from switching to a low-carbon economy themselves. Sometimes mitigation also has localized benefits though. For instance, the benefits of a coal phase-out to public health and local environments exceed the costs in almost all regions.[362] Furthermore, net importers of fossil fuels win economically from switching to clean energy, causing net exporters to face stranded assets: fossil fuels they cannot sell.[363] A wide range of policies, regulations, and laws are being used to reduce emissions. As of 2019, carbon pricing covers about 20% of global greenhouse gas emissions.[364] Carbon can be priced with carbon taxes and emissions trading systems.[365] Direct global fossil fuel subsidies reached $319\u00a0billion in 2017, and $5.2\u00a0trillion when indirect costs such as air pollution are priced in.[366] Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths.[367] Money saved on fossil subsidies could be used to support the transition to clean energy instead.[368] More direct methods to reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry.[369] Several countries require utilities to increase the share of renewables in power production.[370] An Open Coalition on Compliance Carbon Markets with the aim of creating a global cap and trade system was established at COP30 (2025). According to some calculations it can increase emissions reduction seven-fold over current policies, deliver $200 billion per year for clean-energy and social programs and even close the gap between current emissions trajectory and the goals of the Paris agreement.[371][372][373] Policy designed through the lens of climate justice tries to address human rights issues and social inequality. According to proponents of climate justice, the costs of climate adaptation should be paid by those most responsible for climate change, while the beneficiaries of payments should be those suffering impacts. One way this can be addressed in practice is to have wealthy nations pay poorer countries to adapt.[374] Oxfam found that in 2023 the wealthiest 10% of people were responsible for 50% of global emissions, while the bottom 50% were responsible for just 8%.[375] Production of emissions is another way to look at responsibility: under that approach, the top 21 fossil fuel companies would owe cumulative climate reparations of $5.4\u00a0trillion over the period 2025\u20132050.[376] To achieve a just transition, people working in the fossil fuel sector would also need other jobs, and their communities would need investments.[377] Nearly all countries in the world are parties to the 1994 United Nations Framework Convention on Climate Change (UNFCCC).[379] The goal of the UNFCCC is to prevent dangerous human interference with the climate system.[380] As stated in the convention, this requires that greenhouse gas concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained.[381] The UNFCCC does not itself restrict emissions but rather provides a framework for protocols that do. Global emissions have risen since the UNFCCC was signed.[382] Its yearly conferences are the stage of global negotiations.[383] The 1997 Kyoto Protocol extended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions.[384] During the negotiations, the G77 (representing developing countries) pushed for a mandate requiring developed countries to \"[take] the lead\" in reducing their emissions,[385] since developed countries contributed most to the accumulation of greenhouse gases in the atmosphere. Per-capita emissions were also still relatively low in developing countries and developing countries would need to emit more to meet their development needs.[386] The 2009 Copenhagen Accord has been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77.[387] Associated parties aimed to limit the global temperature rise to below 2\u00a0\u00b0C.[388]  The accord set the goal of sending $100\u00a0billion per year to developing countries for mitigation and adaptation by 2020, and proposed the founding of the Green Climate Fund.[389] As of 2020[update], only 83.3\u00a0billion were delivered. Only in 2023 the target is expected to be achieved.[390] In 2015 all UN countries negotiated the Paris Agreement, which aims to keep global warming well below 2.0\u00a0\u00b0C and contains an aspirational goal of keeping warming under 1.5\u00a0\u00b0C.[391] The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, a set of procedures was made binding. Countries have to regularly set ever more ambitious goals and reevaluate these goals every five years.[392] The Paris Agreement restated that developing countries must be financially supported.[393] As of March 2025[update], 194 states and the European Union have acceded to or ratified the agreement.[394] The 1987 Montreal Protocol, an international agreement to phase out production of ozone-depleting gases, has had benefits for climate change mitigation.[395] Several ozone-depleting gases like chlorofluorocarbons are powerful greenhouse gases, so banning their production and usage may have avoided a temperature rise of 0.5\u00a0\u00b0C\u20131.0\u00a0\u00b0C,[396] as well as additional warming by preventing damage to vegetation from ultraviolet radiation.[397] It is estimated that the agreement has been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so.[398] The most recent amendment to the Montreal Protocol, the 2016 Kigali Amendment, committed to reducing the emissions of hydrofluorocarbons, which served as a replacement for banned ozone-depleting gases and are also potent greenhouse gases.[399] Should countries comply with the amendment, a warming of 0.3\u00a0\u00b0C\u20130.5\u00a0\u00b0C is estimated to be avoided.[400] In 2019, the United Kingdom parliament became the first national government to declare a climate emergency.[402] Other countries and jurisdictions followed suit.[403] That same year, the European Parliament declared a \"climate and environmental emergency\".[404] The European Commission presented its European Green Deal with the goal of making the EU carbon-neutral by 2050.[405] In 2021, the European Commission released its \"Fit for 55\" legislation package, which contains guidelines for the car industry; all new cars on the European market must be zero-emission vehicles from 2035.[406] Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon-neutral by 2050, and China by 2060.[407] While India has strong incentives for renewables, it also plans a significant expansion of coal in the country.[408] Vietnam is among very few coal-dependent, fast-developing countries that pledged to phase out unabated coal power by the 2040s or as soon as possible thereafter.[409] As of 2021, based on information from 48 national climate plans, which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5\u00a0\u00b0C or 2\u00a0\u00b0C, respectively.[410] Public debate about climate change has been strongly affected by climate change denial and misinformation, which first emerged in the United States and has since spread to other countries, particularly Canada and Australia. It originated from fossil fuel companies, industry groups, conservative think tanks, and contrarian scientists.[412] Like the tobacco industry, the main strategy of these groups has been to manufacture doubt about climate-change related scientific data and results.[413] People who hold unwarranted doubt about climate change are sometimes called climate change \"skeptics\", although \"contrarians\" or \"deniers\" are more appropriate terms.[414] There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimize the negative impacts of climate change.[415] Manufacturing uncertainty about the science later developed into a manufactured controversy: creating the belief that there is significant uncertainty about climate change within the scientific community to delay policy changes.[416] Strategies to promote these ideas include criticism of scientific institutions,[417] and questioning the motives of individual scientists.[415] An echo chamber of climate-denying blogs and media has further fomented misunderstanding of climate change.[418] Climate change came to international public attention in the late 1980s.[422] Due to media coverage in the early 1990s, people often confused climate change with other environmental issues like ozone depletion.[423] In popular culture, the climate fiction movie The Day After Tomorrow (2004) and the Al Gore documentary An Inconvenient Truth (2006) focused on climate change.[422] Significant regional, gender, age and political differences exist in both public concern for, and understanding of, climate change. More highly educated people, and in some countries, women and younger people, were more likely to see climate change as a serious threat.[424] College biology textbooks from the 2010s featured less content on climate change compared to those from the preceding decade, with decreasing emphasis on solutions.[425] Partisan gaps also exist in many countries,[426] and countries with high CO2 emissions tend to be less concerned.[427] Views on causes of climate change vary widely between countries.[428] Media coverage linked to protests has had impacts on public sentiment as well as on which aspects of climate change are focused upon.[429] Higher levels of worry are associated with stronger public support for policies that address climate change.[430] Concern has increased over time,[431] and in 2021 a majority of citizens in 30 countries expressed a high level of worry about climate change, or view it as a global emergency.[432] A 2024 survey across 125 countries found that 89% of the global population demanded intensified political action, but systematically underestimated other peoples' willingness to act.[24][25] Climate protests demand that political leaders take action to prevent climate change. They can take the form of public demonstrations, fossil fuel divestment, lawsuits and other activities.[433][434] Prominent demonstrations include the School Strike for Climate. In this initiative, young people across the globe have been protesting since 2018 by skipping school on Fridays, inspired by Swedish activist and then-teenager Greta Thunberg.[435] Mass civil disobedience actions by groups like Extinction Rebellion have protested by disrupting roads and public transport.[436] Litigation is increasingly used as a tool to strengthen climate action from public institutions and companies. Activists also initiate lawsuits which target governments and demand that they take ambitious action or enforce existing laws on climate change.[437] Lawsuits against fossil-fuel companies generally seek compensation for loss and damage.[438] On 23 July 2025, the UN's International Court of Justice issued its advisory opinion, saying explicitly that states must act to stop climate change, and if they fail to accomplish that duty, other states can sue them. This obligation includes implementing their commitments in international agreements they are parties to, such as the 2015 Paris Climate Accord.[439][440][441] Scientists in the 19th century such as Alexander von Humboldt began to foresee the effects of climate change.[443][444][445][446] In the 1820s, Joseph Fourier proposed the greenhouse effect to explain why Earth's temperature was higher than the Sun's energy alone could explain. Earth's atmosphere is transparent to sunlight, so sunlight reaches the surface where it is converted to heat. However, the atmosphere is not transparent to heat radiating from the surface, and captures some of that heat, which in turn warms the planet.[447]\nIn 1856 Eunice Newton Foote demonstrated that the warming effect of the Sun is greater for air with water vapour than for dry air, and that the effect is even greater with carbon dioxide (CO2). In \"Circumstances Affecting the Heat of the Sun's Rays\" she concluded that \"[a]n atmosphere of that gas would give to our earth a high temperature\".[448][449] Starting in 1859,[451] John Tyndall established that nitrogen and oxygen\u2014together totalling 99% of dry air\u2014are transparent to radiated heat. However, water vapour and gases such as methane and carbon dioxide absorb radiated heat and re-radiate that heat into the atmosphere. Tyndall proposed that changes in the concentrations of these gases may have caused climatic changes in the past, including ice ages.[452] Svante Arrhenius noted that water vapour in air continuously varied, but the CO2 concentration in air was influenced by long-term geological processes. Warming from increased CO2 levels would increase the amount of water vapour, amplifying warming in a positive feedback loop. In 1896, he published the first climate model of its kind, projecting that halving CO2 levels could have produced a drop in temperature initiating an ice age. Arrhenius calculated the temperature increase expected from doubling CO2 to be around 5\u20136\u00a0\u00b0C.[453] Other scientists were initially sceptical and believed that the greenhouse effect was saturated so that adding more CO2 would make no difference, and that the climate would be self-regulating.[454] Beginning in 1938, Guy Stewart Callendar published evidence that climate was warming and CO2 levels were rising,[455] but his calculations met the same objections.[454] In the 1950s, Gilbert Plass created a detailed computer model that included different atmospheric layers and the infrared spectrum. This model predicted that increasing CO2 levels would cause warming. Around the same time, Hans Suess found evidence that CO2 levels had been rising, and Roger Revelle showed that the oceans would not absorb the increase. The two scientists subsequently helped Charles Keeling to begin a record of continued increase\u2014the \"Keeling Curve\"[454]\u2014which was part of continued scientific investigation through the 1960s into possible human causation of global warming.[459] Studies such as the National Research Council's 1979 Charney Report supported the accuracy of climate models that forecast significant warming.[460] Human causation of observed global warming and dangers of unmitigated warming were publicly presented in James Hansen's 1988 testimony before a US Senate committee.[461][39] The Intergovernmental Panel on Climate Change (IPCC), set up in 1988 to provide formal advice to the world's governments, spurred interdisciplinary research.[462] As part of the IPCC reports, scientists assess the scientific discussion that takes place in peer-reviewed journal articles.[463] There is a nearly unanimous scientific consensus that the climate is warming and that this is caused by human activities.[4] No scientific body of national or international standing disagrees with this view.[464] As of 2019, agreement in recent literature reached over 99%.[457][4] The 2021 IPCC Assessment Report stated that it is \"unequivocal\" that climate change is caused by humans.[4] Consensus has further developed that action should be taken to protect people against the impacts of climate change. National science academies have called on world leaders to cut global emissions.[465] Extreme event attribution (EEA), also known as attribution science, was developed in the early decades of the 21st century.[466] EEA uses climate models to identify and quantify the role that human-caused climate change plays in the frequency, intensity, duration, and impacts of specific individual extreme weather events.[467][468] Results of attribution studies allow scientists and journalists to make statements such as, \"this weather event was made at least n times more likely by human-caused climate change\" or \"this heatwave was made m degrees hotter than it would have been in a world without global warming\" or \"this event was effectively impossible without climate change\".[469] Greater computing power in the 2000s and conceptual breakthroughs in the early to mid 2010s[470]  enabled attribution science to detect the effects of climate change on some events with high confidence.[466] Scientists use attribution methods and climate simulations that have already been peer reviewed, allowing \"rapid attribution studies\" to be published within a \"news cycle\" time frame after weather events.[470] This article incorporates text from a free content work. Licensed under CC BY-SA 3.0. Text taken from The status of women in agrifood systems \u2013 Overview\u200b, FAO, FAO. Fourth Assessment Report Fifth Assessment report Special Report: Global Warming of 1.5\u00a0\u00b0C Special Report: Climate change and Land Special Report: The Ocean and Cryosphere in a Changing Climate Sixth Assessment Report Parabothria: Parabothria is a genus of parasitic flies in the family Tachinidae. There is one described species in Parabothria, P.\u00a0punoensis.[1][2] This article related to members of the fly family Tachinidae is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "8_fixed_chunk1",
        "263_random_chunk1"
      ],
      "source_ids": [
        "S008",
        "S463"
      ],
      "category": "comparative",
      "id": 63
    },
    {
      "question": "Compare Ted Scherman and Lisa Carlsen (basketball) in one sentence each: what does each describe or study?",
      "ground_truth": "Ted Scherman: Ted Scherman (born October 3, 1966) is a former professional tennis player from the United States. Scherman was born in San Francisco and in 1985 represented the United States in the Junior Davis Cup competition.[1] In the late 1980s he played at UC Berkeley, where he achieved All-American honors in 1987 and 1988.[2] Following his graduation in 1989 he turned professional. A right-handed player, Scherman played in the main draw of the Queensland Open in 1989, beating Grant Connell in the first round, before being eliminated in the second round by Niclas Kroon.[3] Most of his appearances at the top level of the professional tour were in doubles. He made it to 114 in the world in that format and was a semi-finalist in the ATP Tour tournament at Bordeaux in 1991, with \u0122irts Dzelde.[4] A two-time Challenger title winner, he also competed in the main draw of four Grand Slam tournaments. Lisa Carlsen (basketball): Lisa Carlsen is an American women's basketball coach and former basketball player. She was recently the women's basketball head coach at Northern Illinois University. She previously served as the women's basketball head coach at Lewis University and the University of Nebraska Omaha.[1] Carlson is from Earling, Iowa.[2] She attended Northwest Missouri State University where she played college basketball and was a named all-conference four times.[1] She was named 1992 Champion NCAA Female Athlete of the Year after her senior year. She also played college softball where she earned all-conference and all-region accolades.[2] She earned a bachelor's degree in 1992 and a master's degree in 1994 from Northwest Missouri State.[1] She played basketball professionally in the Women's Basketball Association for three seasons with the Nebraska Express.[2] Prior to coaching basketball, she was a softball coach at St. Mary (Neb.) from 1994 to 1997 where she was the Midlands Collegiate Athletic Conference Coach of the Year twice.[1] At Wayne State College (Neb.) she was the softball head coach and an assistant volleyball coach from 1997 to 1998. Her overall record as a softball coach was 120\u201351.[2] Her first job as a basketball coach was in 1998 as an assistant at Omaha then in NCAA Division II.[3][1] She was promoted to head coach in 2000 where she remained for four seasons with a record of 36\u201375.[2] She was the associate head coach at Winona State for three years before taking the women's basketball head coaching position at Lewis in 2007.[2] In eight seasons at Lewis she posted an overall record of 148\u201389 with appearances in the NCAA Division II women's basketball tournament and two Great Lakes Valley Conference championships.[4] Her 2015 team went 31\u20133 and made the Division II Elite Eight.[5] They began the season 23\u20130.[6] She was named GLVC Coach of the",
      "expected_answer": "Ted Scherman: Ted Scherman (born October 3, 1966) is a former professional tennis player from the United States. Scherman was born in San Francisco and in 1985 represented the United States in the Junior Davis Cup competition.[1] In the late 1980s he played at UC Berkeley, where he achieved All-American honors in 1987 and 1988.[2] Following his graduation in 1989 he turned professional. A right-handed player, Scherman played in the main draw of the Queensland Open in 1989, beating Grant Connell in the first round, before being eliminated in the second round by Niclas Kroon.[3] Most of his appearances at the top level of the professional tour were in doubles. He made it to 114 in the world in that format and was a semi-finalist in the ATP Tour tournament at Bordeaux in 1991, with \u0122irts Dzelde.[4] A two-time Challenger title winner, he also competed in the main draw of four Grand Slam tournaments. Lisa Carlsen (basketball): Lisa Carlsen is an American women's basketball coach and former basketball player. She was recently the women's basketball head coach at Northern Illinois University. She previously served as the women's basketball head coach at Lewis University and the University of Nebraska Omaha.[1] Carlson is from Earling, Iowa.[2] She attended Northwest Missouri State University where she played college basketball and was a named all-conference four times.[1] She was named 1992 Champion NCAA Female Athlete of the Year after her senior year. She also played college softball where she earned all-conference and all-region accolades.[2] She earned a bachelor's degree in 1992 and a master's degree in 1994 from Northwest Missouri State.[1] She played basketball professionally in the Women's Basketball Association for three seasons with the Nebraska Express.[2] Prior to coaching basketball, she was a softball coach at St. Mary (Neb.) from 1994 to 1997 where she was the Midlands Collegiate Athletic Conference Coach of the Year twice.[1] At Wayne State College (Neb.) she was the softball head coach and an assistant volleyball coach from 1997 to 1998. Her overall record as a softball coach was 120\u201351.[2] Her first job as a basketball coach was in 1998 as an assistant at Omaha then in NCAA Division II.[3][1] She was promoted to head coach in 2000 where she remained for four seasons with a record of 36\u201375.[2] She was the associate head coach at Winona State for three years before taking the women's basketball head coaching position at Lewis in 2007.[2] In eight seasons at Lewis she posted an overall record of 148\u201389 with appearances in the NCAA Division II women's basketball tournament and two Great Lakes Valley Conference championships.[4] Her 2015 team went 31\u20133 and made the Division II Elite Eight.[5] They began the season 23\u20130.[6] She was named GLVC Coach of the Year and Division II National Coach of the Year.[7] During her tenure with the Flyers the program had a 100 percent graduation rate.[8] On June 30, 2015 she was named the head coach at Northern Illinois.[9] She led them to the 2017 Women's National Invitation Tournament.[10] She resigned from NIU on March 10, 2025 after a 13\u201317 season and a 147\u2013155 record with the Huskies.[11] National champion\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Postseason invitational champion\u00a0\u00a0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Conference regular season champion\u00a0\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Conference regular season and conference tournament champion\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Division regular season champion\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Division regular season and conference tournament champion\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Conference tournament champion She and her ex-husband, Chris, have four children.[1]",
      "ground_truth_chunk_ids": [
        "50_random_chunk1",
        "29_random_chunk1"
      ],
      "source_ids": [
        "S250",
        "S229"
      ],
      "category": "comparative",
      "id": 64
    },
    {
      "question": "Compare Murder of Patricia Allen and Goold v Collins in one sentence each: what does each describe or study?",
      "ground_truth": "Murder of Patricia Allen: On November 13, 1991, Patricia Allen, a 31-year-old lawyer, was murdered by her estranged husband, Colin McGregor, with a crossbow in downtown Ottawa, Ontario, Canada.[1] McGregor was found guilty of first degree murder and sentenced to life in prison. The murder received national media coverage because it was the first instance, in Canada, of spousal homicide using a crossbow, which was a way to get around gun ownership laws and restrictions, and acquire a deadly weapon (though there have been several similar crossbow crimes since then).[2] Patricia Allen, the only daughter of George and Maisie Allen, grew up with two brothers. Allen earned her bachelor's degree in philosophy at university in Ottawa.[3] Allen attended the McGill University Faculty of Law, where she graduated at the top her class in 1988[4] and won the prize for the highest achievement in civil law.[5][3] In 1989 she moved to Ottawa with McGregor, where she worked as a lawyer for Revenue Canada.[6][7] Colin McGregor (born in 1961) was the eldest of three boys of a Montreal business man, owner of a travel agency, and grew up in Westmount. He attended Westmount High School and at Marianopolis College he was valedictorian and student union president.[8][9][3][4] He played junior (U20) rugby with the Westmount Rugby Club and he was a champion debater at McGill University, having won the Princeton University Debate Tournament in 1982.[9][8] The pair met at McGill University while Allen was studying law and McGregor was studying philosophy. After graduation they married and moved to Ottawa.[3][4] Their relationship was described as rocky by family and friends, as they frequently fought and McGregor was perceived as jealous and possessive.[5] By 1990 they bought a home and Allen was working as a lawyer with Revenue Canada.[5][4][10] During this time McGregor bounced between jobs, working as a Goold v Collins: Goold v Collins and Ors [2004] IESC 38, [2004] 7 JIC 1201 is an Irish Supreme Court case in which the Court ruled that a statutory provision's constitutionality may be reviewed only at the behest of a litigant who is contesting some current application of that provision.[1][2] Eileen Goold had been the subject of a protection order, dated 18 September 2002, compelling her to restrain from violent behaviour towards her husband. In suspected violation of this order she was twice arrested. By agreement with her husband, this order was discharged on 21 November, and, the following day, her husband wrote to the police withdrawing his complaints. On 29 January 2003, the associated criminal charges against Goold were dismissed. On 17 December 2002, Goold obtained leave to apply to the High Court (McKechnie J.) for judicial review of the constitutionality of Section 5 of the Domestic Violence Act, 1996, on the authority of which, the protection order had been issued. The State argued that these proceedings should be disallowed due to their mootness.[3] On succeeding only in part before the High Court, the State appealed to the Supreme Court. In the earlier case of DK v Crowley,[4] the Court had found other provisions of the Domestic Violence Act relating to orders barring a spouse from the family home to be unconstitutional for: \"[F]ailing to prescribe a fixed period of relatively short duration during which an interim barring order made ex parte is to continue in force, deprived the respondents to such applications of the protection of the principle of audi alteram partem in a manner and to an extent which is disproportionate, unreasonable and unnecessary.\"[4] The Court distinguished DK v Crowley on the basis that, at the time of the application for judicial review of the underlying legislation, no agreement had",
      "expected_answer": "Murder of Patricia Allen: On November 13, 1991, Patricia Allen, a 31-year-old lawyer, was murdered by her estranged husband, Colin McGregor, with a crossbow in downtown Ottawa, Ontario, Canada.[1] McGregor was found guilty of first degree murder and sentenced to life in prison. The murder received national media coverage because it was the first instance, in Canada, of spousal homicide using a crossbow, which was a way to get around gun ownership laws and restrictions, and acquire a deadly weapon (though there have been several similar crossbow crimes since then).[2] Patricia Allen, the only daughter of George and Maisie Allen, grew up with two brothers. Allen earned her bachelor's degree in philosophy at university in Ottawa.[3] Allen attended the McGill University Faculty of Law, where she graduated at the top her class in 1988[4] and won the prize for the highest achievement in civil law.[5][3] In 1989 she moved to Ottawa with McGregor, where she worked as a lawyer for Revenue Canada.[6][7] Colin McGregor (born in 1961) was the eldest of three boys of a Montreal business man, owner of a travel agency, and grew up in Westmount. He attended Westmount High School and at Marianopolis College he was valedictorian and student union president.[8][9][3][4] He played junior (U20) rugby with the Westmount Rugby Club and he was a champion debater at McGill University, having won the Princeton University Debate Tournament in 1982.[9][8] The pair met at McGill University while Allen was studying law and McGregor was studying philosophy. After graduation they married and moved to Ottawa.[3][4] Their relationship was described as rocky by family and friends, as they frequently fought and McGregor was perceived as jealous and possessive.[5] By 1990 they bought a home and Allen was working as a lawyer with Revenue Canada.[5][4][10] During this time McGregor bounced between jobs, working as a reporter in Halifax and Montreal, as a media spokesperson for the Canadian Pharmaceutical Association and in the federal government. In the fall of 1990, McGregor started a Master of Public Administration at Carleton University[3] while Allen supported him financially.[5] In the summer of 1991 he was able to secure a prestigious co-op position as an auditor at the Department of National Defence.[5] Later in 1990, when Allen was promoted to senior policy adviser on the Goods and Services Tax, McGregor (as he told a divorce lawyer) felt a shift in their relationship.[5][11] At that time McGregor started acting bizarrely, complaining about pains in his throat, liver, eyes, pancreas and brain.[5] He was convinced he was dying from a herpes infection, believing that swallowing a cold sore had triggered the dormant virus to spread throughout his body.[5][4] After visiting a number of specialists no physical aliment was discovered; on the contrary, the doctors suggested his complaints were psychological.[5] During this time neighbours heard the couple constantly arguing, often loudly.[5] He was admitted to a psychiatric ward for a week in the spring of 1991, and after his release his condition deteriorated and he stopped working at National Defence.[5][12] After threatening suicide in August 1991, McGregor was readmitted to the psychiatric ward for three weeks.[5][13][3] It was determined that his physical symptoms were the result of stress, and he was quite probably using them to control and manipulate others. Doctors noted McGregor was very angry, full of resentment and hostility towards people.[5] During his stay in the psychiatric ward, Allen told McGregor that she wanted a divorce.[5][11][3] Before the second week of September 1991, McGregor was discharged from the psychiatric ward and a close friend picked him up, with whom he lived for a short period.[5] He would frequently call friends and ramble on topics such as suicide, his recent separation, his ailments and how his situation was unfair.[5] After his release and up until her murder, McGregor harassed Allen incessantly.[3] He called her repeatedly and tried to enter their former matrimonial home without her permission.[3] During this time Allen kept a \"diary of threats\" and had a friend stay with her.[4] Leading up to the murder they met several times to divide their possessions as the divorce neared completion.[5] In October 1991, McGregor managed to enter the house through a bathroom window using a pair of garden shears.[5][12][4] On October 23, 1991, McGregor bought a crossbow and a pack of steel-tipped hunting bolts.[4] In the following weeks he would use the walls of his room as a target practice,[4] filling them with holes and continuing until the bolts were dull. A few days prior to the murder he bought a new set of bolts.[5] On the morning of November 13, 1991, Allen drove to a dentist appointment on Argyle Avenue in downtown Ottawa.[3][7] McGregor had been monitoring her movements and covertly followed her to the dentist's office. After Allen finished and was leaving McGregor got out of his vehicle and approached Allen while she was unlocking her vehicle, with a crossbow concealed by a garbage bag.[3][7] Surprised, she asked him why he was there, at which point McGregor shot her in the chest with a steel-tipped hunting bolt, killing her.[5][3][7] McGregor went to a police station shortly after the attack and confessed to killing his wife. During questioning, which began approximately 45 minutes after the incident, he admitted that he had intentionally killed her.[5][7] In his police interview, McGregor expressed extreme distress, stating that he felt suicidal and describing himself as \u201ca monster.\u201d He said he wanted to die and had previously considered killing both his wife and himself. He also disclosed that he believed he was terminally ill due to systemic herpes. McGregor further explained that he had purchased the crossbow with the original intention of using it to take his own life, noting that it was easier to obtain than a firearm.[5][7][3] After being charged with first degree murder and making an initial appearance in court, McGregor was sent to the Royal Ottawa Hospital for a 30-day psychiatric assessment.[14] He later pleaded not guilty by reason of insanity.[12][13][4] During McGregor's trial it was discovered that he informed his psychiatrist, weeks before Allen's murder, that he had wanted to kill her.[8][13][4] In March 1993, Justice Louise Charron delivered her 149-page judgement rejecting McGregor's claim of insanity.[11] He was sentenced to life in prison. McGregor was released from prison after 29 years behind bars.[15] McGregor was sentenced to life imprisonment and served 29 years. He was granted day parole to a halfway house in 2020 and full parole in 2022. While incarcerated, he learned French and became a bilingual writer.[3] He contributed articles and maintained a blog for the online magazine The Social Eyes,[16] co-authored two books with Raymond Viger (including the Quebec Suicide Prevention Handbook),[17] and tutored other inmates. In one of his Social Eyes posts, he stated that prison had not made him bitter, citing his ability to help fellow prisoners, attend church services, and read extensively.[18] Regarding the murder of his wife, Patricia Allen, McGregor has written, \u201cI will never live down what I did.\u201d[18] In a later French-language interview (translated), he said: \u201cIt took me years to become aware of this, but I decided not to blame others and to look at myself in the mirror \u2026 I reflected on the crime, I committed the crime, and I am guilty.\u201d[19] McGregor has not publicly apologized for the killing. Patricia Allen's murder has a legacy that lives on today in several different ways and memorials. In 1992 a scholarships fund was created at Carleton University in her honour called the Patricia Allen Memorial Fund.[20] At first it started with private donations but later made use of other fundraising such as bingo nights and an annual golf tournament.[8] By 1996 it had raised over $220,000 CAD to fund scholarships for graduate students who conduct research into spousal violence.[21][8] Also in 1992 the McGill University Faculty of Law Class of 88 created an annual guest lecture in her honour named the Patricia Allen Memorial Lecture, which consists of a yearly lecture (or two) \"devoted to sensitizing and educating the legal community and others about pressing social and legal issues related to violence, especially against women.\"[6][22][23] In 1994, the Patricia Allen Memorial Fund supported a study with the Institute for Clinical Evaluative Sciences to explore when would it be necessary for doctors to report threats made by their patients.[8] The results of the study assisted medical professionals to make several recommendations, such as compelling doctors to report their patient's serious threats concerning harming a person to the police or risk disciplinary action under professional misconduct regulations.[8] Patricia Allen's name is engraved on Enclave, a monument commemorating the lives of Ottawa women who were murdered by men from 1990 to 2000.[24] Goold v Collins: Goold v Collins and Ors [2004] IESC 38, [2004] 7 JIC 1201 is an Irish Supreme Court case in which the Court ruled that a statutory provision's constitutionality may be reviewed only at the behest of a litigant who is contesting some current application of that provision.[1][2] Eileen Goold had been the subject of a protection order, dated 18 September 2002, compelling her to restrain from violent behaviour towards her husband. In suspected violation of this order she was twice arrested.  By agreement with her husband, this order was discharged on 21 November, and, the following day, her husband wrote to the police withdrawing his complaints.  On 29 January 2003, the associated criminal charges against Goold were dismissed.  On 17 December 2002, Goold obtained leave to apply to the High Court (McKechnie J.) for judicial review of the constitutionality of Section 5 of the Domestic Violence Act, 1996, on the authority of which, the protection order had been issued.  The State argued that these proceedings should be disallowed due to their mootness.[3]  On succeeding only in part before the High Court, the State appealed to the Supreme Court. In the earlier case of DK v Crowley,[4] the Court had found other provisions of the Domestic Violence Act relating to orders barring a spouse from the family home to be unconstitutional for: \"[F]ailing to prescribe a fixed period of relatively short duration during which an interim barring order made ex parte is to continue in force, deprived the respondents to such applications of the protection of the principle of audi alteram partem in a manner and to an extent which is disproportionate, unreasonable and unnecessary.\"[4] The Court distinguished DK v Crowley on the basis that, at the time of the application for judicial review of the underlying legislation, no agreement had been reached regarding the order in question.  Noting the 'vital social purpose' served by domestic violence legislation,[1] the Court referred to the Canadian case of Borowski v. Canada[5] where the Supreme Court of Canada held that: \"[A]n appeal is moot when a decision will not have the effect of resolving some controversy affecting or potentially affecting the rights of the parties. Such a live controversy must be present not only when the action or proceedings is commenced but also when the Court is called upon to reach a decision.\"[5] Writing for the Court, Hardiman J. allowed the State's appeal. He held that since there was not a live \"concrete dispute between the parties\"[1] the case was moot and issued an order staying the proceedings. This case was applied in the cases of LOG v Child and Family Agency  [2017] IEHC 58 and MC v Legal Aid Board [2017] IEHC 26.",
      "ground_truth_chunk_ids": [
        "46_random_chunk1",
        "12_random_chunk1"
      ],
      "source_ids": [
        "S246",
        "S212"
      ],
      "category": "comparative",
      "id": 65
    },
    {
      "question": "Compare Age of Enlightenment and Decision theory in one sentence each: what does each describe or study?",
      "ground_truth": "Age of Enlightenment: The Age of Enlightenment (also the Age of Reason) was a period in the history of Europe and Western civilization[1] during which the Enlightenment,[b] an intellectual[6] and cultural[6] movement, flourished, emerging in the late 17th century[6] in Western Europe[7] and reaching its peak in the 18th century, as its ideas spread more widely across Europe[7] and into the European colonies, in the Americas and Oceania.[8][9][10] Characterized by an emphasis on reason, empirical evidence, and the scientific method, the Enlightenment promoted ideals of individual liberty, religious tolerance, progress, and natural rights. Its thinkers advocated for constitutional government, the separation of church and state, and the application of rational principles to social and political reform.[11][12][13] The Enlightenment emerged from and built upon the Scientific Revolution of the 16th and 17th centuries, which had established new methods of empirical inquiry through the work of figures such as Galileo Galilei, Johannes Kepler, Francis Bacon, Pierre Gassendi, Christiaan Huygens and Isaac Newton. Philosophical foundations were laid by thinkers including Ren\u00e9 Descartes, Thomas Hobbes, Baruch Spinoza, and John Locke, whose ideas about reason, natural rights, and empirical knowledge became central to Enlightenment thought. The dating of the period of the beginning of the Enlightenment can be attributed to the publication of Descartes' Discourse on the Method in 1637, with his method of systematically disbelieving everything unless there was a well-founded reason for accepting it, and featuring his dictum, Cogito, ergo sum ('I think, therefore I am'). Others cite the publication of Newton's Principia Mathematica (1687) as the culmination of the Scientific Revolution and the beginning of the Enlightenment.[14][15][16] European historians traditionally dated its beginning with the death of Louis XIV of France in 1715 and its end with the outbreak of the French Revolution in 1789. Many historians now date the end of the Enlightenment as Decision theory: Decision theory or the theory of rational choice is a branch of probability, economics, and analytic philosophy that uses expected utility and probability to model how individuals would behave rationally under uncertainty.[1][2] It differs from the cognitive and behavioral sciences in that it is mainly prescriptive and concerned with identifying optimal decisions for a rational agent, rather than describing how people actually make decisions. Despite this, the field is important to the study of real human behavior by social scientists, as it lays the foundations to mathematically model and analyze individuals in fields such as sociology, economics, criminology, cognitive science, moral philosophy and political science.[citation needed] The roots of decision theory lie in probability theory, developed by Blaise Pascal and Pierre de Fermat in the 17th century, which was later refined by others like Christiaan Huygens. These developments provided a framework for understanding risk and uncertainty, which are central to decision-making. In the 18th century, Daniel Bernoulli introduced the concept of \"expected utility\" in the context of gambling, which was later formalized by John von Neumann and Oskar Morgenstern in the 1940s. Their work on Game Theory and Expected Utility Theory helped establish a rational basis for decision-making under uncertainty. After World War II, decision theory expanded into economics, particularly with the work of economists like Milton Friedman and others, who applied it to market behavior and consumer choice theory. This era also saw the development of Bayesian decision theory, which incorporates Bayesian probability into decision-making models. By the late 20th century, scholars like Daniel Kahneman and Amos Tversky challenged the assumptions of rational decision-making. Their work in behavioral economics highlighted cognitive biases and heuristics that influence real-world decisions, leading to the development of prospect theory, which modified expected utility theory by accounting for psychological factors. Normative decision theory is",
      "expected_answer": "Age of Enlightenment: The Age of Enlightenment (also the Age of Reason) was a period in the history of Europe and Western civilization[1] during which the Enlightenment,[b] an intellectual[6] and cultural[6] movement, flourished, emerging in the late 17th century[6] in Western Europe[7] and reaching its peak in the 18th century, as its ideas spread more widely across Europe[7] and into the European colonies, in the Americas and Oceania.[8][9][10] Characterized by an emphasis on reason, empirical evidence, and the scientific method, the Enlightenment promoted ideals of individual liberty, religious tolerance, progress, and natural rights. Its thinkers advocated for constitutional government, the separation of church and state, and the application of rational principles to social and political reform.[11][12][13] The Enlightenment emerged from and built upon the Scientific Revolution of the 16th and 17th centuries, which had established new methods of empirical inquiry through the work of figures such as Galileo Galilei, Johannes Kepler, Francis Bacon, Pierre Gassendi, Christiaan Huygens and Isaac Newton. Philosophical foundations were laid by thinkers including Ren\u00e9 Descartes, Thomas Hobbes, Baruch Spinoza, and John Locke, whose ideas about reason, natural rights, and empirical knowledge became central to Enlightenment thought. The dating of the period of the beginning of the Enlightenment can be attributed to the publication of Descartes' Discourse on the Method in 1637, with his method of systematically disbelieving everything unless there was a well-founded reason for accepting it, and featuring his dictum, Cogito, ergo sum ('I think, therefore I am'). Others cite the publication of Newton's Principia Mathematica (1687) as the culmination of the Scientific Revolution and the beginning of the Enlightenment.[14][15][16] European historians traditionally dated its beginning with the death of Louis XIV of France in 1715 and its end with the outbreak of the French Revolution in 1789. Many historians now date the end of the Enlightenment as the start of the 19th century, with the latest proposed year being the death of Immanuel Kant in 1804.[17] The movement was characterized by the widespread circulation of ideas through new institutions: scientific academies, literary salons, coffeehouses, Masonic lodges, and an expanding print culture of books, journals, and pamphlets. The ideas of the Enlightenment undermined the authority of the monarchy and religious officials and paved the way for the political revolutions of the 18th and 19th centuries. A variety of 19th-century movements, including liberalism, socialism,[18] and neoclassicism, trace their intellectual heritage to the Enlightenment.[19] The Enlightenment was marked by an increasing awareness of the relationship between the mind and the everyday media of the world,[20] and by an emphasis on the scientific method and reductionism, along with increased questioning of religious dogma\u2014an attitude captured by Kant's essay Answering the Question: What Is Enlightenment?, where the phrase sapere aude ('dare to know') can be found.[21] The central doctrines of the Enlightenment were individual liberty, representative government, the rule of law, and religious freedom, in contrast to an absolute monarchy or single party state and the religious persecution of faiths other than those formally established and often controlled outright by the State. By contrast, other intellectual currents included arguments in favour of anti-Christianity, Deism and Atheism, accompanied by demands for secular states, bans on religious education, suppression of monasteries, the suppression of the Jesuits, and the expulsion of religious orders. The Enlightenment also faced contemporary criticism, later termed the \"Counter-Enlightenment\" by Sir Isaiah Berlin, which defended traditional religious and political authorities against rationalist critique. The Age of Enlightenment was preceded by and closely associated with the Scientific Revolution.[22] Earlier philosophers whose work influenced the Enlightenment included Francis Bacon, Pierre Gassendi, Ren\u00e9 Descartes, Thomas Hobbes, Baruch Spinoza, John Locke, Pierre Bayle, and Gottfried Wilhelm Leibniz.[23][24] Some of the figures of the Enlightenment included Cesare Beccaria, George Berkeley, Denis Diderot, David Hume, Immanuel Kant, Lord Monboddo, Montesquieu, Jean-Jacques Rousseau, Adam Smith, Hugo Grotius, and Voltaire.[25] One of the most influential Enlightenment publications was the Encyclop\u00e9die (Encyclopedia). Published between 1751 and 1772 in 35 volumes, it was compiled by Diderot, Jean le Rond d'Alembert, and a team of 150 others. The Encyclop\u00e9die helped spread the ideas of the Enlightenment across Europe and beyond.[26] Other publications of the Enlightenment included Locke's A Letter Concerning Toleration (1689) and Two Treatises of Government (1689); Berkeley's A Treatise Concerning the Principles of Human Knowledge (1710), Voltaire's Letters on the English (1733) and Philosophical Dictionary (1764); Hume's A Treatise of Human Nature (1740); Montesquieu's The Spirit of the Laws (1748); Rousseau's Discourse on Inequality (1754) and The Social Contract (1762); Cesare Beccaria's On Crimes and Punishments (1764); Adam Smith's The Theory of Moral Sentiments (1759) and The Wealth of Nations (1776); and Kant's Critique of Pure Reason (1781).[citation needed] Bacon's empiricism and Descartes' rationalist philosophy laid the foundation for enlightenment thinking.[27] Descartes' attempt to construct the sciences on a secure metaphysical foundation was not as successful as his method of doubt applied to philosophy, which led to a dualistic doctrine of mind and matter. His skepticism was refined by Locke's Essay Concerning Human Understanding (1690) and Hume's writings in the 1740s. Descartes' dualism was challenged by Spinoza's uncompromising assertion of the unity of matter in his Tractatus (1670) and Ethics (1677).[28] According to Jonathan Israel, these laid down two distinct lines of Enlightenment thought: first, the moderate variety, following Descartes, Locke, and Christian Wolff, which sought accommodation between reform and the traditional systems of power and faith, and, second, the Radical Enlightenment, inspired by the philosophy of Spinoza, advocating democracy, individual liberty, freedom of expression, and eradication of religious authority.[29][30] The moderate variety tended to be deistic whereas the radical tendency separated the basis of morality entirely from theology. Both lines of thought were eventually opposed by a conservative Counter-Enlightenment which sought a return to faith.[31] In the mid-18th century, Paris became the center of philosophic and scientific activity challenging traditional doctrines and dogmas. After the Edict of Fontainebleau in 1685, the relationship between church and the absolutist government was very strong. The early enlightenment emerged in protest to these circumstances, gaining ground under the support of Madame de Pompadour, the mistress of Louis XV.[32] Called the Si\u00e8cle des Lumi\u00e8res, the philosophical movement of the Enlightenment had already started by the early 18th century, when Pierre Bayle launched the popular and scholarly Enlightenment critique of religion. As a skeptic Bayle only partially accepted the philosophy and principles of rationality. He did draw a strict boundary between morality and religion. The rigor of his Dictionnaire Historique et Critique influenced many of the Enlightenment Encyclop\u00e9distes.[33] By the mid-18th century the French Enlightenment had found a focus in the project of the Encyclop\u00e9die.[32] The philosophical movement was led by Voltaire and Rousseau, who argued for a society based upon reason rather than faith and Catholic doctrine, for a new civil order based on natural law, and for science based on experiments and observation. The political philosopher Montesquieu introduced the idea of a separation of powers in a government, a concept which was enthusiastically adopted by the authors of the United States Constitution. While the philosophes of the French Enlightenment were not revolutionaries and many were members of the nobility, their ideas played an important part in undermining the legitimacy of the Old Regime and shaping the French Revolution.[34] Francis Hutcheson, a moral philosopher and founding figure of the Scottish Enlightenment, described the utilitarian and consequentialist principle that virtue is that which provides, in his words, \"the greatest happiness for the greatest numbers.\" Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by Hutcheson's prot\u00e9g\u00e9s in Edinburgh: David Hume and Adam Smith.[35][36] Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy. Kant tried to reconcile rationalism and religious belief, individual freedom and political authority, as well as map out a view of the public sphere through private and public reason.[37] Kant's work continued to influence German intellectual life and European philosophy more broadly well into the 20th century.[38] Mary Wollstonecraft was one of England's earliest feminist philosophers.[39] She argued for a society based on reason and that women as well as men should be treated as rational beings. She is best known for her 1792 work, A Vindication of the Rights of Woman.[40] Science played an important role in Enlightenment discourse and thought. Many Enlightenment writers and thinkers had backgrounds in the sciences and associated scientific advancement with the overthrow of religion and traditional authority in favour of the development of free speech and thought.[41] There were immediate practical results. The experiments of Antoine Lavoisier were used to create the first modern chemical plants in Paris, and the experiments of the Montgolfier brothers enabled them to launch the first manned flight in a hot air balloon in 1783.[42] Broadly speaking, Enlightenment science greatly valued empiricism and rational thought and was embedded with the Enlightenment ideal of advancement and progress. The study of science, under the heading of natural philosophy, was divided into physics and a conglomerate grouping of chemistry and natural history, which included anatomy, biology, geology, mineralogy, and zoology.[43] As with most Enlightenment views, the benefits of science were not seen universally: Rousseau criticized the sciences for distancing man from nature and not operating to make people happier.[44] Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Scientific academies and societies grew out of the Scientific Revolution as the creators of scientific knowledge, in contrast to the scholasticism of the university.[45] Some societies created or retained links to universities, but contemporary sources distinguished universities from scientific societies by claiming that the university's utility was in the transmission of knowledge while societies functioned to create knowledge.[46] As the role of universities in institutionalized science began to diminish, learned societies became the cornerstone of organized science. Official scientific societies were chartered by the state to provide technical expertise.[47] Most societies were granted permission to oversee their own publications, control the election of new members and the administration of the society.[48] In the 18th century, a very large number of official academies and societies were founded in Europe; by 1789 there were over 70 official scientific societies. In reference to this growth, Bernard de Fontenelle coined the term \"the Age of Academies\" to describe the 18th century.[49] Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the Encyclop\u00e9die and the popularization of Newtonianism by Voltaire and \u00c9milie du Ch\u00e2telet. Some historians have marked the 18th century as a drab period in the history of science.[50] The century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.[citation needed] The influence of science began appearing more commonly in poetry and literature. While some societies were established with ties to universities or maintained existing ones contemporary sources often distinguished between the two, asserting that universities primarily served to transmit knowledge, whereas scientific societies were oriented toward the creation of new knowledge.[51] James Thomson penned his \"A Poem to the Memory of Sir Isaac Newton,\" which mourned the loss of Newton and praised his science and legacy.[52] Hume and other Scottish Enlightenment thinkers developed a \"science of man,\"[53] which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar, and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement,[54] and Hume's philosophical concepts that directly influenced James Madison (and thus the U.S. Constitution), and as popularised by Dugald Stewart was the basis of classical liberalism.[55] In 1776, Adam Smith published The Wealth of Nations, often considered the first work on modern economics as it had an immediate impact on British economic policy that continues into the 21st century.[56] It was immediately preceded and influenced by Anne Robert Jacques Turgot's drafts of Reflections on the Formation and Distribution of Wealth (1766). Smith acknowledged indebtedness and possibly was the original English translator.[57] Beccaria, a jurist, criminologist, philosopher, and politician and one of the great Enlightenment writers, became famous for his masterpiece Dei delitti e delle pene (Of Crimes and Punishments, 1764). His treatise, translated into 22 languages,[58] condemned torture and the death penalty and was a founding work in the field of penology and the classical school of criminology by promoting criminal justice. Francesco Mario Pagano wrote studies such as Saggi politici (Political Essays, 1783); and Considerazioni sul processo criminale (Considerations on the Criminal Trial, 1787), which established him as an international authority on criminal law.[59] The Enlightenment has long been seen as the foundation of modern Western political and intellectual culture.[60] The Enlightenment brought political modernization to the West, in terms of introducing democratic values and institutions and the creation of modern, liberal democracies. This thesis has been widely accepted by scholars and has been reinforced by the large-scale studies by Robert Darnton, Roy Porter, and, most recently, by Jonathan Israel.[61][62] Enlightenment thought was deeply influential in the political realm. European rulers such as Catherine II of Russia, Joseph II of Austria, and Frederick II of Prussia tried to apply Enlightenment thought on religious and political tolerance, which became known as enlightened absolutism.[25] Many of the major political and intellectual figures behind the American Revolution associated themselves closely with the Enlightenment: Benjamin Franklin visited Europe repeatedly and contributed actively to the scientific and political debates there and brought the newest ideas back to Philadelphia; Thomas Jefferson closely followed European ideas and later incorporated some of the ideals of the Enlightenment into the Declaration of Independence; and Madison incorporated these ideals into the U.S. Constitution during its framing in 1787.[63] Locke, one of the most influential Enlightenment thinkers,[64] based his governance philosophy on social contract theory, a subject that permeated Enlightenment political thought. English philosopher Thomas Hobbes ushered in this new debate with his work Leviathan in 1651. Hobbes also developed some of the fundamentals of European liberal thought: the right of the individual, the natural equality of all men, the artificial character of the political order (which led to the later distinction between civil society and the state), the view that all legitimate political power must be \"representative\" and based on the consent of the people, and a liberal interpretation of law which leaves people free to do whatever the law does not explicitly forbid.[65] Both Locke and Rousseau developed social contract theories in Two Treatises of Government and Discourse on Inequality, respectively. While quite different works, Locke, Hobbes, and Rousseau agreed that a social contract, in which the government's authority lies in the consent of the governed,[66] is necessary for man to live in civil society. Locke defines the state of nature as a condition in which humans are rational and follow natural law, in which all men are born equal and with the right to life, liberty, and property. However, when one citizen breaks the law of nature both the transgressor and the victim enter into a state of war, from which it is virtually impossible to break free. Therefore, Locke said that individuals enter into civil society to protect their natural rights via an \"unbiased judge\" or common authority, such as courts. In contrast, Rousseau's conception relies on the supposition that \"civil man\" is corrupted, while \"natural man\" has no want he cannot fulfill himself. Natural man is only taken out of the state of nature when the inequality associated with private property is established.[67] Rousseau said that people join into civil society via the social contract to achieve unity while preserving individual freedom. This is embodied in the sovereignty of the general will, the moral and collective legislative body constituted by citizens.[citation needed] Locke is known for his statement that individuals have a right to \"Life, Liberty, and Property,\" and his belief that the natural right to property is derived from labor. Tutored by Locke, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury, wrote in 1706: \"There is a mighty Light which spreads its self over the world especially in those two free Nations of England and Holland; on whom the Affairs of Europe now turn.\"[68] Locke's theory of natural rights has influenced many political documents, including the U.S. Declaration of Independence and the French National Constituent Assembly's Declaration of the Rights of Man and of the Citizen. Some philosophes argued that the establishment of a contractual basis of rights would lead to the market mechanism and capitalism, the scientific method, religious tolerance, and the organization of states into self-governing republics through democratic means. In this view, the tendency of the philosophes in particular to apply rationality to every problem is considered the essential change.[69] Although much of Enlightenment political thought was dominated by social contract theorists, Hume and Ferguson criticized this camp. Hume's essay Of the Original Contract argues that governments derived from consent are rarely seen and civil government is grounded in a ruler's habitual authority and force. It is precisely because of the ruler's authority over-and-against the subject that the subject tacitly consents, and Hume says that the subjects would \"never imagine that their consent made him sovereign,\" rather the authority did so.[70] Similarly, Ferguson did not believe citizens built the state, rather polities grew out of social development. In his 1767 An Essay on the History of Civil Society, Ferguson uses the four stages of progress, a theory that was popular in Scotland at the time, to explain how humans advance from a hunting and gathering society to a commercial and civil society without agreeing to a social contract. Both Rousseau's and Locke's social contract theories rest on the presupposition of natural rights, which are not a result of law or custom but are things that all men have in pre-political societies and are therefore universal and inalienable. The most famous natural right formulation comes from Locke's Second Treatise, when he introduces the state of nature. For Locke, the law of nature is grounded on mutual security or the idea that one cannot infringe on another's natural rights, as every man is equal and has the same inalienable rights. These natural rights include perfect equality and freedom, as well as the right to preserve life and property. Locke argues against indentured servitude on the basis that enslaving oneself goes against the law of nature because a person cannot surrender their own rights: freedom is absolute, and no one can take it away. Locke argues that one person cannot enslave another because it is morally reprehensible, although he introduces a caveat by saying that enslavement of a lawful captive in time of war would not go against one's natural rights. The leaders of the Enlightenment were not especially democratic, as they more often look to absolute monarchs as the key to imposing reforms designed by the intellectuals. Voltaire despised democracy and said the absolute monarch must be enlightened and must act as dictated by reason and justice\u2014in other words, be a \"philosopher-king.\"[71] In several nations, rulers welcomed leaders of the Enlightenment at court and asked them to help design laws and programs to reform the system, typically to build stronger states. These rulers are called \"enlightened despots\" by historians.[72] They included Frederick the Great of Prussia, Catherine the Great of Russia, Leopold II of Tuscany and Joseph II of Austria. Joseph was over-enthusiastic, announcing many reforms that had little support so that revolts broke out and his regime became a comedy of errors, and nearly all his programs were reversed.[73] Senior ministers Pombal in Portugal and Johann Friedrich Struensee in Denmark also governed according to Enlightenment ideals. In Poland, the model constitution of 1791 expressed Enlightenment ideals, but was in effect for only one year before the nation was partitioned among its neighbors. More enduring were the cultural achievements, which created a nationalist spirit in Poland.[74] Frederick the Great, the king of Prussia from 1740 to 1786, saw himself as a leader of the Enlightenment and patronized philosophers and scientists at his court in Berlin. Voltaire, who had been imprisoned and maltreated by the French government, was eager to accept Frederick's invitation to live at his palace. Frederick explained: \"My principal occupation is to combat ignorance and prejudice... to enlighten minds, cultivate morality, and to make people as happy as it suits human nature, and as the means at my disposal permit.\"[75] The Enlightenment has been frequently linked to the American Revolution of 1776[76] and the French Revolution of 1789\u2014both had some intellectual influence from Thomas Jefferson.[77][78]  A key aspect of this era was a profound shift from the absolute monarchies of Europe, which asserted the \"divine right\" to rule. John Locke rejected this view in his writings on the Two Treatises of Government (1689). He asserted that citizens were seen to possess natural rights, including life, liberty, and property. Therefore governments exist to protect these rights through the \"consent of the governed.\" The clash between these competing ethos often resulted in violent upheaval in differing ways. In France, Ancien r\u00e9gime, with its rigid social hierarchy and absolute monarchical power, was systematically dismantled during the French Revolution. While the American Revolution focused more on breaking free from a government - represented by King George III and Parliament - that colonists felt did not adequately represent their interests. Alexis de Tocqueville proposed the French Revolution as the inevitable result of the radical opposition created in the 18th century between the monarchy and the men of letters of the Enlightenment. These men of letters constituted a sort of \"substitute aristocracy that was both all-powerful and without real power.\" This illusory power came from the rise of \"public opinion,\" born when absolutist centralization removed the nobility and the bourgeoisie from the political sphere. The \"literary politics\" that resulted promoted a discourse of equality and was hence in fundamental opposition to the monarchical regime.[79][80] De Tocqueville \"clearly designates... the cultural effects of transformation in the forms of the exercise of power.\"[81] It does not require great art or magnificently trained eloquence, to prove that Christians should tolerate each other. I, however, am going further: I say that we should regard all men as our brothers. What? The Turk my brother? The Chinaman my brother? The Jew? The Siam? Yes, without doubt; are we not all children of the same father and creatures of the same God? Enlightenment era religious commentary was a response to the preceding century of religious conflict in Europe, especially the Thirty Years' War.[83] Theologians of the Enlightenment wanted to reform their faith to its generally non-confrontational roots and to limit the capacity for religious controversy to spill over into politics and warfare while still maintaining a true faith in God. For moderate Christians, this meant a return to simple Scripture. Locke abandoned the corpus of theological commentary in favor of an \"unprejudiced examination\" of the Word of God alone. He determined the essence of Christianity to be a belief in Christ the redeemer and recommended avoiding more detailed debate.[84] Anthony Collins, one of the English freethinkers, published his \"Essay concerning the Use of Reason in Propositions the Evidence whereof depends on Human Testimony\" (1707), in which he rejects the distinction between \"above reason\" and \"contrary to reason,\" and demands that revelation should conform to man's natural ideas of God. In the Jefferson Bible, Thomas Jefferson went further and dropped any passages dealing with miracles, visitations of angels, and the resurrection of Jesus after his death, as he tried to extract the practical Christian moral code of the New Testament.[85] Enlightenment scholars sought to curtail the political power of organized religion and thereby prevent another age of intolerant religious war.[86] Spinoza determined to remove politics from contemporary and historical theology (e.g., disregarding Judaic law).[87] Moses Mendelssohn advised affording no political weight to any organized religion but instead recommended that each person follow what they found most convincing.[88] They believed a good religion based in instinctive morals and a belief in God should not theoretically need force to maintain order in its believers, and both Mendelssohn and Spinoza judged religion on its moral fruits, not the logic of its theology.[89] Several novel ideas about religion developed with the Enlightenment, including deism and talk of atheism. According to Thomas Paine, deism is the simple belief in God the Creator with no reference to the Bible or any other miraculous source. Instead, the deist relies solely on personal reason to guide his creed,[90] which was eminently agreeable to many thinkers of the time.[91] Atheism was much discussed, but there were few proponents. Wilson and Reill note: \"In fact, very few enlightened intellectuals, even when they were vocal critics of Christianity, were true atheists. Rather, they were critics of orthodox belief, wedded rather to skepticism, deism, vitalism, or perhaps pantheism.\"[92] Some followed Pierre Bayle and argued that atheists could indeed be moral men.[93] Many others like Voltaire held that without belief in a God who punishes evil, the moral order of society was undermined; that is, since atheists gave themselves to no supreme authority and no law and had no fear of eternal consequences, they were far more likely to disrupt society.[94] Bayle observed that, in his day, \"prudent persons will always maintain an appearance of [religion],\" and he believed that even atheists could hold concepts of honor and go beyond their own self-interest to create and interact in society.[95] Locke said that if there were no God and no divine law, the result would be moral anarchy: every individual \"could have no law but his own will, no end but himself. He would be a god to himself, and the satisfaction of his own will the sole measure and end of all his actions.\"[96] The \"Radical Enlightenment\"[97][98] promoted the concept of separating church and state,[99] an idea that is often credited to Locke.[100] According to his principle of the social contract, Locke said that the government lacked authority in the realm of individual conscience, as this was something rational people could not cede to the government for it or others to control. For Locke, this created a natural right in the liberty of conscience, which he said must therefore remain protected from any government authority. These views on religious tolerance and the importance of individual conscience, along with the social contract, became particularly influential in the American colonies and the drafting of the United States Constitution.[101] In a letter to the Danbury Baptist Association in Connecticut, Thomas Jefferson calls for a \"wall of separation between church and state\" at the federal level. He previously had supported successful efforts to disestablish the Church of England in Virginia[102] and authored the Virginia Statute for Religious Freedom.[103] Jefferson's political ideals were greatly influenced by the writings of Locke, Bacon, and Newton,[104] whom he considered the three greatest men that ever lived.[105] The Enlightenment took hold in most European countries and influenced nations globally, often with a specific local emphasis. For example, in France it became associated with anti-government and anti-Church radicalism, while in Germany it reached deep into the middle classes, where it expressed a spiritualistic and nationalistic tone without threatening governments or established churches.[106] Government responses varied widely. In France, the government was hostile, and the philosophes fought against its censorship, sometimes being imprisoned or hounded into exile. The British government, for the most part, ignored the Enlightenment's leaders in England and Scotland, although it did give Newton a knighthood and a very lucrative government office. A common theme among most countries which derived Enlightenment ideas from Europe was the intentional non-inclusion of Enlightenment philosophies pertaining to slavery. Originally during the French Revolution, a revolution deeply inspired by Enlightenment philosophy, \"France's revolutionary government had denounced slavery, but the property-holding 'revolutionaries' then remembered their bank accounts.\"[107] Slavery frequently showed the limitations of the Enlightenment ideology as it pertained to European colonialism, since many colonies of Europe operated on a plantation economy fueled by slave labor. In 1791, the Haitian Revolution, a slave rebellion by emancipated slaves against French colonial rule in the colony of Saint-Domingue, broke out. European nations and the United States, despite the strong support for Enlightenment ideals, refused to \"[give support] to Saint-Domingue's anti-colonial struggle.\"[107] The very existence of an English Enlightenment has been hotly debated by scholars. The majority of textbooks on British history make little or no mention of an English Enlightenment. Some surveys of the entire Enlightenment include England and others ignore it, although they do include coverage of such major intellectuals as Joseph Addison, Edward Gibbon, John Locke, Isaac Newton, Alexander Pope, Joshua Reynolds, and Jonathan Swift.[108] Freethinking, a term describing those who stood in opposition to the institution of the Church, and the literal belief in the Bible, can be said to have begun in England no later than 1713, when Anthony Collins wrote his \"Discourse of Free-thinking,\" which gained substantial popularity. This essay attacked the clergy of all churches and was a plea for deism. Roy Porter argues that the reasons for this neglect were the assumptions that the movement was primarily French-inspired, that it was largely a-religious or anti-clerical, and that it stood in outspoken defiance to the established order.[109] Porter admits that after the 1720s England could claim thinkers to equal Diderot, Voltaire, or Rousseau. However, its leading intellectuals such as Gibbon,[110] Edmund Burke and Samuel Johnson were all quite conservative and supportive of the standing order. Porter says the reason was that Enlightenment had come early to England and had succeeded such that the culture had accepted political liberalism, philosophical empiricism, and religious toleration, positions which intellectuals on the continent had to fight against powerful odds. Furthermore, England rejected the collectivism of the continent and emphasized the improvement of individuals as the main goal of enlightenment.[111] According to Derek Hirst, the 1640s and 1650s saw a revived economy characterised by growth in manufacturing, the elaboration of financial and credit instruments, and the commercialisation of communication. The gentry found time for leisure activities, such as horse racing and bowling. In the high culture important innovations included the development of a mass market for music, increased scientific research, and an expansion of publishing. All the trends were discussed in depth at the newly established coffee houses.[112][113] In the Scottish Enlightenment, the principles of sociability, equality, and utility were disseminated in schools and universities, many of which used sophisticated teaching methods which blended philosophy with daily life.[20] Scotland's major cities created an intellectual infrastructure of mutually supporting institutions such as schools, universities, reading societies, libraries, periodicals, museums, and masonic lodges.[114] The Scottish network was \"predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment.\"[115] In France, Voltaire said \"we look to Scotland for all our ideas of civilization.\"[116] The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of William Cullen, physician and chemist; James Anderson, agronomist; Joseph Black, physicist and chemist; and James Hutton, the first modern geologist.[35][117] Several Americans, especially Benjamin Franklin and Thomas Jefferson, played a major role in bringing Enlightenment ideas to the New World and in influencing British and French thinkers.[118] Franklin was influential for his political activism and for his advances in physics.[119][120]  Franklin also broadly encouraged the individual's rights and responsibilities to serve as an educated and informed citizen.[121]  He published yearly the widely popular, Poor Richard's Almanack, filled with witty quotes encouraging disciplined self-learning, such as \"Early to bed, early to rise, makes a man healthy, wealthy and wise.\"[122] The cultural exchange during the Age of Enlightenment ran in both directions across the Atlantic. Thinkers such as Paine, Locke, and Rousseau all take Native American cultural practices as examples of natural freedom.[123] The Americans closely followed English and Scottish political ideas, as well as some French thinkers such as Montesquieu.[124] As deists, they were influenced by ideas of John Toland and Matthew Tindal. There was a great emphasis upon liberty, republicanism, and religious tolerance. There was no respect for monarchy or inherited political power. Deists reconciled science and religion by rejecting prophecies, miracles, and biblical theology. Leading deists included Thomas Paine in The Age of Reason and Thomas Jefferson in his short Jefferson Bible, from which he removed all supernatural aspects.[125] The Jewish Enlightenment, or Haskalah (Hebrew: \u05d4\u05b7\u05e9\u05b0\u05c2\u05db\u05b8\u05bc\u05dc\u05b8\u05d4, \"education\") was an intellectual movement among the Jews of Central and Eastern Europe, with a certain influence on those in Western Europe and the Muslim world. It arose as a defined ideological worldview during the 1770s, and its last stage ended around 1881, with the rise of Jewish nationalism. The movement advocated against Jewish reclusiveness, encouraged the adoption of prevalent attire over traditional dress, while also working to diminish the authority of traditional community institutions such as rabbinic courts and boards of elders. The Dutch Enlightenment began in 1640.[126] During the Early Dutch Enlightenment (1640\u20131720), many books were translated from Latin, French or English to Dutch, often at the risk of their translators and publishers.[126] By the 1720s, the Dutch Republic had also become a major center for printing and exporting banned books to France.[127] Implanted in Netherlandish culture, vernacular rationalism brought the Dutch to take advantage of the intellectual philosophy the enlightenment spread.[128]The most famous figure of the Dutch Enlightenment was Baruch Spinoza. The French Enlightenment was influenced by England[129] and in turn influenced other national enlightenments. As worded by Sharon A. Stanley, \"the French Enlightenment stands out from other national enlightenments for its unrelenting assault on church leadership and theology.\"[130] Prussia took the lead among the German states in sponsoring the political reforms that Enlightenment thinkers urged absolute rulers to adopt. There were important movements as well in the smaller states of Bavaria, Saxony, Hanover, and the Palatinate. In each case, Enlightenment values became accepted and led to significant political and administrative reforms that laid the groundwork for the creation of modern states.[131] The princes of Saxony, for example, carried out an impressive series of fundamental fiscal, administrative, judicial, educational, cultural, and general economic reforms. The reforms were aided by the country's strong urban structure and influential commercial groups and modernized pre-1789 Saxony along the lines of classic Enlightenment principles.[132][133] Before 1750, the German upper classes looked to France for intellectual, cultural, and architectural leadership, as French was the language of high society. By the mid-18th century, the Aufkl\u00e4rung (The Enlightenment) had transformed German high culture in music, philosophy, science, and literature. Christian Wolff was the pioneer as a writer who expounded the Enlightenment to German readers and legitimized German as a philosophic language.[134] Johann Gottfried von Herder broke new ground in philosophy and poetry, as a leader of the Sturm und Drang movement of proto-Romanticism. Weimar Classicism (Weimarer Klassik) was a cultural and literary movement based in Weimar that sought to establish a new humanism by synthesizing Romantic, classical, and Enlightenment ideas. The movement (from 1772 until 1805) involved Herder as well as polymath Johann Wolfgang von Goethe and Friedrich Schiller, a poet and historian. The theatre principal Abel Seyler greatly influenced the development of German theatre and promoted serious German opera, new works and experimental productions, and the concept of a national theatre.[135] Herder argued that every group of people had its own particular identity, which was expressed in its language and culture. This legitimized the promotion of German language and culture and helped shape the development of German nationalism. Schiller's plays expressed the restless spirit of his generation, depicting the hero's struggle against social pressures and the force of destiny.[136] German music, sponsored by the upper classes, came of age under composers Johann Sebastian Bach, Joseph Haydn, and Wolfgang Amadeus Mozart.[137] In remote K\u00f6nigsberg, Kant tried to reconcile rationalism and religious belief, individual freedom, and political authority. Kant's work contained basic tensions that would continue to shape German thought\u2014and indeed all of European philosophy\u2014well into the 20th century.[138] German Enlightenment won the support of princes, aristocrats, and the middle classes, and it permanently reshaped the culture.[139] However, there was a conservatism among the elites that warned against going too far.[140] In 1788, Prussia issued an \"Edict on Religion\" that forbade preaching any sermon that undermined popular belief in the Holy Trinity or the Bible. The goal was to avoid theological disputes that might impinge on domestic tranquility. Men who doubted the value of Enlightenment favoured the measure, but so too did many supporters. German universities had created a closed elite that could debate controversial issues among themselves, but spreading them to the public was seen as too risky. This intellectual elite was favoured by the state, but that might be reversed if the process of the Enlightenment proved politically or socially destabilizing.[141] During the 18th century, Austria was under Habsburg rule. The reign of Maria Theresa, the first Habsburg monarch to be considered influenced by the Enlightenment in some areas, was marked by a mix of enlightenment and conservatism. Her son Joseph II's brief reign was marked by this conflict, with his ideology of Josephinism facing opposition. Joseph II carried out numerous reforms in the spirit of the Enlightenment, which affected, for example, the school system, monasteries and the legal system. Emperor Leopold II, who was an early opponent of capital punishment, had a brief and contentious rule that was mostly marked by relations with France. Similarly, Emperor Francis II's rule was primarily marked by relations with France. The ideas of the Enlightenment also appeared in literature and theater works. Joseph von Sonnenfels was an important representative. In music, Austrian musicians such as Joseph Haydn and Wolfgang Amadeus Mozart were associated with the Enlightenment. The Modern Greek Enlightenment (Greek: \u0394\u03b9\u03b1\u03c6\u03c9\u03c4\u03b9\u03c3\u03bc\u03cc\u03c2, Diafotism\u00f3s) was the Greek expression of the Age of Enlightenment, characterized by an intellectual and philosophical movement within the Greek community. At this time, many Greeks were dispersed across the Ottoman Empire, with some residing on the Ionian Islands, in Venice, and other parts of Italy. The Hungarian Enlightenment[142] emerged during the 18th century, while Hungary was part of the Habsburg Empire. The Hungarian Enlightenment is usually said to have begun in 1772 and was greatly influenced by French Enlightenment (through Vienna).[142] The Romanian Enlightenment[143] emerged during the 18th century across the three major historical regions inhabited by Romanians: Transylvania, Wallachia, and Moldavia. At that time, Transylvania was in the Habsburg Empire while Wallachia and Moldavia were vassals of the Ottoman Empire. The Transylvanian Enlightenment was represented by the Transylvanian School, a group of thinkers who promoted a cultural revival and rights for Romanians (who were marginalized by the Habsburgs). The Wallachian Enlightenment was represented by such figures as Dinicu Golescu (1777\u20131830), while the Moldavian Englightenment was headed by prince Dimitrie Cantemir (1673-1723). The Enlightenment arrived relatively late in Switzerland, spreading from England, the Netherlands, and France toward the end of the 17th century. The movement initially took hold in Protestant regions, where it gradually replaced orthodox religious thinking. The 1712 victory of the reformed cantons of Zurich and Bern over the five Catholic cantons of central Switzerland in the Second War of Villmergen marked both a Protestant triumph and a victory for Enlightenment ideas in the economically more developed regions.[144] In Switzerland, which lacked a central court or academy, the Enlightenment spread through the intellectual elite of reformed cities, particularly pastors educated in academies and colleges with strong humanist traditions. The theological \"Helvetic triumvirate\" of Jean-Alphonse Turrettini (Geneva), Jean-Fr\u00e9d\u00e9ric Ostervald (Neuch\u00e2tel), and Samuel Werenfels (Basel) led their churches toward a humanistic Christianity beginning in 1697, creating what Paul Wernle termed \"reasoned orthodoxy\" that balanced rational thought with Christian ethics.[144] Swiss Enlightenment thinkers made significant contributions across multiple fields. The Romand school developed influential theories of natural law, with scholars like Jean Barbeyrac (Lausanne), Jean-Jacques Burlamaqui (Geneva), and Emer de Vattel (Neuch\u00e2tel) promoting concepts of inalienable rights and justified resistance to tyranny that influenced the American independence movement. In literature, Johann Jakob Bodmer and Johann Jakob Breitinger made Zurich a center of German literary innovation, while Albert von Haller's poetry represented the peak of Swiss Enlightenment literature. Jean-Jacques Rousseau, considering himself both a Genevan and Swiss citizen, developed democratic republican theories that extended Genevan models to broader European federalist principles.[144] The movement was characterized by what scholars term \"Helvetism\" \u2013 specifically Swiss aspects including a Christian conception of natural law, patriotic ethics, and philosophical approaches grounded in practical pedagogy and economics. Most distinctively, Swiss Enlightenment thought celebrated Alpine nature, viewing Switzerland as the \"land of shepherds\" whose republican and federalist traditions were shaped by its mountain environment. The movement organized through numerous societies and publications, including the Encyclop\u00e9die d'Yverdon (1770-1780), which offered a moderate alternative to the French Encyclop\u00e9die. Swiss intellectuals gained international prominence, with many serving in foreign academies, particularly in Berlin under Frederick II and in St. Petersburg under Catherine II.[144] In Italy the main centers of diffusion of the Enlightenment were Naples and Milan:[145] in both cities the intellectuals took public office and collaborated with the Bourbon and Habsburg administrations. In Naples, Antonio Genovesi, Ferdinando Galiani, and Gaetano Filangieri were active under the tolerant King Charles of Bourbon. However, the Neapolitan Enlightenment, like Vico's philosophy, remained almost always in the theoretical field.[146] Only later, many Enlighteners animated the unfortunate experience of the Parthenopean Republic. In Milan, however, the movement strove to find concrete solutions to problems. The center of discussions was the magazine Il Caff\u00e8 (1762\u20131766), founded by brothers Pietro and Alessandro Verri (famous philosophers and writers, as well as their brother Giovanni), who also gave life to the Accademia dei Pugni, founded in 1761. Minor centers were Tuscany, Veneto, and Piedmont, where among others, Pompeo Neri worked. From Naples, Genovesi influenced a generation of southern Italian intellectuals and university students. His textbook Della diceosina, o sia della Filosofia del Giusto e dell'Onesto (1766) was a controversial attempt to mediate between the history of moral philosophy on the one hand and the specific problems encountered by 18th-century commercial society on the other. It contained the greater part of Genovesi's political, philosophical, and economic thought, which became a guidebook for Neapolitan economic and social development.[147] Science flourished as Alessandro Volta and Luigi Galvani made break-through discoveries in electricity. Pietro Verri was a leading economist in Lombardy. Historian Joseph Schumpeter states he was \"the most important pre-Smithian authority on Cheapness-and-Plenty.\"[148] The most influential scholar on the Italian Enlightenment has been Franco Venturi.[149][150] Italy also produced some of the Enlightenment's greatest legal theorists, including Cesare Beccaria, Giambattista Vico, and Francesco Mario Pagano. When Charles II, the last Spanish Habsburg monarch, died his successor was from the French House of Bourbon, initiating a period of French Enlightenment influence in Spain and the Spanish Empire.[151][152] In the 18th Century, the Spanish continued to expand their empire in the Americas with the Spanish missions in California and established missions deeper inland in South America. Under Charles III, the crown began to implement serious structural changes. The monarchy curtailed the power of the Catholic Church, and established a standing military in Spanish America. Freer trade was promoted under comercio libre in which regions could trade with companies sailing from any other Spanish port, rather than the restrictive mercantile system. The crown sent out scientific expeditions to assert Spanish sovereignty over territories it claimed but did not control, but also importantly to discover the economic potential of its far-flung empire. Botanical expeditions sought plants that could be of use to the empire.[153] Charles IV gave Prussian scientist Alexander von Humboldt free rein to travel in Spanish America, usually closed to foreigners, and more importantly, access to crown officials to aid the success of his scientific expedition.[154] When Napoleon invaded Spain in 1808, Ferdinand VII abdicated and Napoleon placed his brother Joseph Bonaparte on the throne. To add legitimacy to this move, the Bayonne Constitution was promulgated, which included representation from Spain's overseas components, but most Spaniards rejected the whole Napoleonic project. A war of national resistance erupted. The Cortes de C\u00e1diz (parliament) was convened to rule Spain in the absence of the legitimate monarch, Ferdinand. It created a new governing document, the Constitution of 1812, which laid out three branches of government: executive, legislative, and judicial; put limits on the king by creating a constitutional monarchy; defined citizens as those in the Spanish Empire without African ancestry; established universal manhood suffrage; and established public education starting with primary school through university as well as freedom of expression. The constitution was in effect from 1812 until 1814, when Napoleon was defeated and Ferdinand was restored to the throne of Spain. Upon his return, Ferdinand repudiated the constitution and reestablished absolutist rule.[155] The Haitian Revolution began in 1791 and ended in 1804 and shows how Enlightenment ideas \"were part of complex transcultural flows.\"[10] Radical ideas in Paris during and after the French Revolution were mobilized in Haiti, such as by Toussaint Louverture.[10] Toussaint had read the critique of European colonialism in Guillaume Thomas Fran\u00e7ois Raynal's book Histoire des deux Indes and \"was particularly impressed by Raynal's prediction of the coming of a 'Black Spartacus.'\"[10] The revolution combined Enlightenment ideas with the experiences of the slaves in Haiti, two-thirds of whom had been born in Africa and could \"draw on specific notions of kingdom and just government from West and Central Africa, and to employ religious practices such as voodoo for the formation of revolutionary communities.\"[10] The revolution also affected France and \"forced the French National Convention to abolish slavery in 1794.\"[10] The Portuguese Enlightenment was heavily marked by the rule of Prime Minister Marquis of Pombal under King Joseph I from 1756 to 1777. Following the 1755 Lisbon earthquake which destroyed a large part of Lisbon, the Marquis of Pombal implemented important economic policies to regulate commercial activity (in particular with Brazil and England), and to standardise quality throughout the country (for example by introducing the first integrated industries in Portugal). His reconstruction of Lisbon's riverside district in straight and perpendicular streets (the Lisbon Baixa), methodically organized to facilitate commerce and exchange (for example by assigning to each street a different product or service), can be seen as a direct application of the Enlightenment ideas to governance and urbanism. His urbanistic ideas, also being the first large-scale example of earthquake engineering, became collectively known as Pombaline style, and were implemented throughout the kingdom during his stay in office. His governance was as enlightened as ruthless, see for example the T\u00e1vora affair. In literature, the first Enlightenment ideas in Portugal can be traced back to the diplomat, philosopher, and writer Ant\u00f3nio Vieira[156] who spent a considerable amount of his life in colonial Brazil denouncing discriminations against New Christians and the indigenous peoples in Brazil. During the 18th century, enlightened literary movements such as the Arc\u00e1dia Lusitana (lasting from 1756 until 1776, then replaced by the Nova Arc\u00e1dia in 1790 until 1794) surfaced in the academic medium, in particular involving former students of the University of Coimbra. A distinct member of this group was the poet Manuel Maria Barbosa du Bocage. The physician Ant\u00f3nio Nunes Ribeiro Sanches was also an important Enlightenment figure, contributing to the Encyclop\u00e9die and being part of the Russian court. The ideas of the Enlightenment influenced various economists and anti-colonial intellectuals throughout the Portuguese Empire, such as Jos\u00e9 de Azeredo Coutinho, Jos\u00e9 da Silva Lisboa, Cl\u00e1udio Manoel da Costa, and Tom\u00e1s Ant\u00f4nio Gonzaga. The Napoleonic invasion of Portugal had consequences for the Portuguese monarchy. With the aid of the British navy, the Portuguese royal family was evacuated to Brazil, its most important colony. Even though Napoleon had been defeated, the royal court remained in Brazil. The Liberal Revolution of 1820 forced the return of the royal family to Portugal. The terms by which the restored king was to rule was a constitutional monarchy under the Constitution of Portugal. Brazil declared its independence from Portugal in 1822 and became a monarchy. The existence of a Swedish Enlightenment has been debated by scholars. According to Tore Fr\u00e4ngsmyr, the Swedish Enlightenment \"never formed a truly coherent current of ideas or became a unified movement.\"[157] As worded by Max Skj\u00f6nsberg, Fr\u00e4ngsmyr's main arguments against a Swedish Enlightenment were that religious criticism in Sweden was reserved for foreign Catholicism rather than the domestic Lutheran Church and that the debate about freedom in the 1750s and 1760s focused on political economy and freedom to trade rather than freedom to 'philosophize'. The fact that political economy is now a much more important aspect of Enlightenment historiography, to a great degree thanks to research on the Scottish Enlightenment, is a clear example of why Fr\u00e4ngsmyr's case is in need of revision.[158] Between 1718 and 1772, the Swedish Enlightenment overlapped with the period of parliamentary rule known in Swedish history as the Age of Liberty. In Russia, the government began to actively encourage the proliferation of arts and sciences in the mid-18th century. This era produced the first Russian university, library, theatre, public museum, and independent press. Like other enlightened despots, Catherine the Great played a key role in fostering the arts, sciences and education. She used her own interpretation of Enlightenment ideals, assisted by notable international experts such as Voltaire (by correspondence) and in residence world class scientists such as Leonhard Euler and Peter Simon Pallas. The national Enlightenment differed from its Western European counterpart in that it promoted further modernization of all aspects of Russian life and was concerned with attacking the institution of serfdom in Russia. The Russian Enlightenment centered on the individual instead of societal enlightenment and encouraged the living of an enlightened life.[159][160] A powerful element was prosveshchenie which combined religious piety, erudition, and commitment to the spread of learning. However, it lacked the skeptical and critical spirit of the Western European Enlightenment.[161] Enlightenment ideas (o\u015bwiecenie) emerged late in Poland, as the Polish middle class was weaker and szlachta (nobility) culture (Sarmatism) together with the Polish\u2013Lithuanian Commonwealth political system (Golden Liberty) were in deep crisis. The political system was built on aristocratic republicanism, but was unable to defend itself against powerful neighbors Russia, Prussia, and Austria as they repeatedly sliced off regions until nothing was left of independent Poland. The Polish Enlightenment began in the 1730s\u201340s and especially in theatre and the arts peaked in the reign of King Stanis\u0142aw August Poniatowski (second half of the 18th century). Warsaw was a main centre after 1750, with an expansion of schools and educational institutions and the arts patronage held at the Royal Castle.[162] Leaders promoted tolerance and more education. They included King Stanislaw II August and reformers Piotr Switkowski, Antoni Poplawski, Josef Niemcewicz, and J\u00f3sef Pawlinkowski, as well as Baudouin de Cortenay, a Polonized dramatist. Opponents included Florian Jaroszewicz, Gracjan Piotrowski, Karol Wyrwicz, and Wojciech Skarszewski.[163] The movement went into decline with the Third Partition of Poland (1795) \u2013 a national tragedy inspiring a short period of sentimental writing \u2013 and ended in 1822, replaced by Romanticism.[164] Eighteenth-century China experienced \"a trend towards seeing fewer dragons and miracles, not unlike the disenchantment that began to spread across the Europe of the Enlightenment.\"[10] Furthermore, \"some of the developments that we associate with Europe's Enlightenment resemble events in China remarkably.\"[10] During this time, ideals of Chinese society were reflected in \"the reign of the Qing emperors Kangxi and Qianlong; China was posited as the incarnation of an enlightened and meritocratic society\u2014and instrumentalized for criticisms of absolutist rule in Europe.\"[10] From 1641 to 1853, the Tokugawa shogunate of Japan enforced a policy called kaikin. The policy prohibited foreign contact with most outside countries.[165] Robert Bellah found \"origins of modern Japan in certain strands of Confucian thinking, a 'functional analogue to the Protestant Ethic' that Max Weber singled out as the driving force behind Western capitalism.\"[10] Japanese Confucian and Enlightenment ideas were brought together, for example, in the work of the Japanese reformer Tsuda Mamichi in the 1870s, who said, \"Whenever we open our mouths...it is to speak of 'enlightenment.'\"[10] In Japan and much of East Asia, Confucian ideas were not replaced but \"ideas associated with the Enlightenment were instead fused with the existing cosmology\u2014which in turn was refashioned under conditions of global interaction.\"[10] In Japan in particular, the term ri, which is the Confucian idea of \"order and harmony on human society\" also came to represent \"the idea of laissez-faire and the rationality of market exchange.\"[10] By the 1880s, the slogan \"Civilization and Enlightenment\" became potent throughout Japan, China, and Korea and was employed to address challenges of globalization.[10] During this time, Korea \"aimed at isolation\" and was known as the \"hermit kingdom\" but became awakened to Enlightenment ideas by the 1890s such as with the activities of the Independence Club.[10] Korea was influenced by China and Japan but also found its own Enlightenment path with the Korean intellectual Yu Kilchun who popularized the term Enlightenment throughout Korea.[10] The use of Enlightenment ideas was a \"response to a specific situation in Korea in the 1890s, and not a belated answer to Voltaire.\"[10] In 18th-century India, Tipu Sultan was an enlightened monarch, who \"was one of the founding members of the (French) Jacobin Club in Seringapatam, had planted a liberty tree, and asked to be addressed as 'Tipu Citoyen,'\" which means Citizen Tipu.[10] In parts of India, an important movement called the \"Bengal Renaissance\" led to Enlightenment reforms beginning in the 1820s.[10] Ram Mohan Roy was a reformer who \"fused different traditions in his project of social reform that made him a proponent of a 'religion of reason.'\"[10] Eighteenth-century Egypt had \"a form of 'cultural revival' in the making\u2014specifically Islamic origins of modernization long before Napoleon's Egyptian campaign.\"[10] Napoleon's expedition into Egypt further encouraged \"social transformations that harkened back to debates about inner-Islamic reform, but now were also legitimized by referring to the authority of the Enlightenment.\"[10] A major intellectual influence on Islamic modernism and expanding the Enlightenment in Egypt, Rifa al-Tahtawi \"oversaw the publication of hundreds of European works in the Arabic language.\"[10] The Enlightenment began to influence the Ottoman Empire in the 1830s and continued into the late 19th century.[10]\nThe Tanzimat was a period of reform in the Ottoman Empire that began with the G\u00fclhane Hatt-\u0131 \u015eerif in 1839 and ended with the First Constitutional Era in 1876. Namik Kemal, a political activist and member of the Young Ottomans, drew on major Enlightenment thinkers and \"a variety of intellectual resources in his quest for social and political reform.\"[10] In 1893, Kemal responded to Ernest Renan, who had indicted the Islamic religion, with his own version of the Enlightenment, which \"was not a poor copy of French debates in the eighteenth century, but an original position responding to the exigencies of Ottoman society in the late nineteenth century.\"[10] The Arab Enlightenment, or Nahda (Arabic: \u0627\u0644\u0646\u0651\u0647\u0636\u0629, \"the awakening\"), was a cultural movement in Arab-populated regions of the Ottoman Empire such as Egypt, Lebanon, Syria, and Tunisia, during the second half of the 19th century and the early 20th century. The Nahda is often traced to the cultural shock of the French invasion of Egypt and Syria in 1798, and the reformist drive of subsequent rulers such as Muhammad Ali of Egypt. The idea of the Enlightenment has always been contested territory. According to Keith Thomas, its supporters \"hail it as the source of everything that is progressive about the modern world. For them, it stands for freedom of thought, rational inquiry, critical thinking, religious tolerance, political liberty, scientific achievement, the pursuit of happiness, and hope for the future.\"[166] Thomas adds that its detractors accuse it of shallow rationalism, na\u00efve optimism, unrealistic universalism, and moral darkness. From the start, conservative and clerical defenders of traditional religion attacked materialism and skepticism as evil forces that encouraged immorality. By 1794, they pointed to the Reign of Terror during the French Revolution as confirmation of their predictions. Romantic philosophers argued that the Enlightenment's excessive dependence on reason was a mistake that it perpetuated, disregarding the bonds of history, myth, faith, and tradition that were necessary to hold society together.[166] Ritchie Robertson portrays it as a grand intellectual and political program, offering a \"science\" of society modeled on the powerful physical laws of Newton. \"Social science\" was seen as the instrument of human improvement. It would expose truth and expand human happiness.[167] The rights of women and nonwhite people were generally overlooked in Enlightenment philosophy, which is often explicitly Eurocentric.[15] Scientific racism first emerged at this time, bringing together traditional racism and new research methods.[168][169]  During the Enlightenment, concepts of monogenism and polygenism became popular, though they would only be systematized epistemologically during the 19th century. Monogenism contends that all races have a single origin, while polygenism is the idea that each race has a separate origin. Until the 18th century, the words \"race\" and \"species\" were interchangeable.[168] The classification of non-European peoples as sub-human and irrational served to justify European dominance.[c][170]:\u200a4,\u200a10 The term \"Enlightenment\" emerged in English in the latter part of the 19th century,[171] with particular reference to French philosophy, as the equivalent of the French term Lumi\u00e8res (used first by Jean-Baptiste Dubos in 1733 and already well established by 1751). From Kant's 1784 essay \"Beantwortung der Frage: Was ist Aufkl\u00e4rung?\" (\"Answering the Question: What is Enlightenment?\"), the German term became Aufkl\u00e4rung (aufkl\u00e4ren = to illuminate; sich aufkl\u00e4ren = to clear up). However, scholars have never agreed on a definition of the Enlightenment or on its chronological or geographical extent. Terms like les Lumi\u00e8res (French), illuminismo (Italian), ilustraci\u00f3n (Spanish) and Aufkl\u00e4rung (German) referred to partly overlapping movements. Not until the late 19th century did English scholars agree they were talking about \"the Enlightenment.\"[166][172] Enlightenment historiography began in the period itself, from what Enlightenment figures said about their work. A dominant element was the intellectual angle they took. Jean le Rond d'Alembert's Preliminary Discourse of l'Encyclop\u00e9die provides a history of the Enlightenment which comprises a chronological list of developments in the realm of knowledge\u2014of which the Encyclop\u00e9die forms the pinnacle.[173] In 1783, Mendelssohn referred to Enlightenment as a process by which man was educated in the use of reason.[174][d] Kant called Enlightenment \"man's release from his self-incurred tutelage,\" tutelage being \"man's inability to make use of his understanding without direction from another.\"[175] \"For Kant, Enlightenment was mankind's final coming of age, the emancipation of the human consciousness from an immature state of ignorance.\"[176] The German scholar Ernst Cassirer called the Enlightenment \"a part and a special phase of that whole intellectual development through which modern philosophic thought gained its characteristic self-confidence and self-consciousness.\"[177] According to historian Roy Porter, the liberation of the human mind from a dogmatic state of ignorance, is the epitome of what the Age of Enlightenment was trying to capture.[176] Bertrand Russell saw the Enlightenment as a phase in a progressive development which began in antiquity and that reason and challenges to the established order were constant ideals throughout that time.[178] Russell said that the Enlightenment was ultimately born out of the Protestant reaction against the Catholic Counter-Reformation and that philosophical views such as affinity for democracy against monarchy originated among 16th-century Protestants to justify their desire to break away from the Catholic Church. Although many of these philosophical ideals were picked up by Catholics, Russell argues that by the 18th century the Enlightenment was the principal manifestation of the schism that began with Martin Luther.[178] Jonathan Israel rejects the attempts of postmodern and Marxian historians to understand the revolutionary ideas of the period purely as by-products of social and economic transformations.[179] He instead focuses on the history of ideas in the period from 1650 to the end of the 18th century and claims that it was the ideas themselves that caused the change that eventually led to the revolutions of the latter half of the 18th century and the early 19th century.[180] Israel argues that until the 1650s Western civilization \"was based on a largely shared core of faith, tradition, and authority.\"[181] There is little consensus on the beginning of the Age of Enlightenment, though several historians and philosophers argue that it was marked by Descartes' 1637 philosophy of Cogito, ergo sum (\"I think, therefore I am\"), which shifted the epistemological basis from external authority to internal certainty.[182][183][184] In France, many cited the publication of Newton's Principia Mathematica (1687),[185] which built upon the work of earlier scientists and formulated the laws of motion and universal gravitation.[186] French historians usually place the Si\u00e8cle des Lumi\u00e8res (\"Century of Enlightenments\") between 1715 and 1789: from the beginning of the reign of Louis XV until the French Revolution.[187] Most scholars use the last years of the century, often choosing the French Revolution or the beginning of the Napoleonic Wars (1804) as a convenient point in time with which to date the end of the Enlightenment.[188] In recent years, scholars have expanded the time span and global perspective of the Enlightenment by examining: (1) how European intellectuals did not work alone and other people helped spread and adapt Enlightenment ideas, (2) how Enlightenment ideas were \"a response to cross-border interaction and global integration,\" and (3) how the Enlightenment \"continued throughout the nineteenth century and beyond.\"[10] The Enlightenment \"was not merely a history of diffusion\" and \"was the work of historical actors around the world... who invoked the term... for their own specific purposes.\"[10] In their 1947 book Dialectic of Enlightenment, Frankfurt School philosophers Max Horkheimer and Theodor W. Adorno, both wartime exiles from Nazi Germany, critiqued the supposed rational basis of the modern world: Enlightenment, understood in the widest sense as the advance of thought, has always aimed at liberating human beings from fear and installing them as masters. Yet the wholly enlightened earth radiates under the sign of disaster triumphant.[189] Extending Horkheimer and Adorno's argument, intellectual historian Jason Josephson Storm argues that any idea of the Age of Enlightenment as a clearly defined period that is separate from the earlier Renaissance and later Romanticism or Counter-Enlightenment constitutes a myth. Storm points out that there are vastly different and mutually contradictory periodizations of the Enlightenment depending on nation, field of study, and school of thought; that the term and category of \"Enlightenment\" referring to the Scientific Revolution was actually applied after the fact; that the Enlightenment did not see an increase in disenchantment or the dominance of the mechanistic worldview; and that a blur in the early modern ideas of the humanities and natural sciences makes it hard to circumscribe a Scientific Revolution.[190] Storm defends his categorization of the Enlightenment as \"myth\" by noting the regulative role ideas of a period of Enlightenment and disenchantment play in modern Western culture, such that belief in magic, spiritualism, and even religion appears somewhat taboo in intellectual strata.[191] In the 1970s, study of the Enlightenment expanded to include the ways Enlightenment ideas spread to European colonies and how they interacted with indigenous cultures and how the Enlightenment took place in formerly unstudied areas such as Italy, Greece, the Balkans, Poland, Hungary, and Russia.[192][193] Intellectuals such as Robert Darnton and J\u00fcrgen Habermas have focused on the social conditions of the Enlightenment. Habermas described the creation of the \"bourgeois public sphere\" in 18th-century Europe, containing the new venues and modes of communication allowing for rational exchange. Habermas said that the public sphere was bourgeois, egalitarian, rational, and independent from the state, making it the ideal venue for intellectuals to critically examine contemporary politics and society, away from the interference of established authority. While the public sphere is generally an integral component of the social study of the Enlightenment, other historians[e] have questioned whether the public sphere had these characteristics. In contrast to the intellectual historiographical approach of the Enlightenment, which examines the various currents or discourses of intellectual thought within the European context during the 17th and 18th centuries, the cultural (or social) approach examines the changes that occurred in European society and culture. This approach studies the process of changing sociabilities and cultural practices during the Enlightenment. One of the primary elements of the culture of the Enlightenment was the rise of the public sphere, a \"realm of communication marked by new arenas of debate, more open and accessible forms of urban public space and sociability, and an explosion of print culture,\" in the late 17th century and 18th century.[194] Elements of the public sphere included that it was egalitarian, that it discussed the domain of \"common concern,\" and that argument was founded on reason.[195] Habermas uses the term \"common concern\" to describe those areas of political/social knowledge and discussion that were previously the exclusive territory of the state and religious authorities, now open to critical examination by the public sphere. The values of this bourgeois public sphere included holding reason to be supreme, considering everything to be open to criticism (the public sphere is critical), and the opposition of secrecy of all sorts.[196] The creation of the public sphere has been associated with two long-term historical trends: the rise of the modern nation state and the rise of capitalism. The modern nation state in its consolidation of public power created by counterpoint a private realm of society independent of the state, which allowed for the public sphere. Capitalism also increased society's autonomy and self-awareness, as well as an increasing need for the exchange of information. As the nascent public sphere expanded, it embraced a large variety of institutions, and the most commonly cited were coffee houses and caf\u00e9s, salons and the literary public sphere, figuratively localized in the Republic of Letters.[198][199] In France, the creation of the public sphere was helped by the aristocracy's move from the king's palace at Versailles to Paris in about 1720, since their rich spending stimulated the trade in luxuries and artistic creations, especially fine paintings.[200] The context for the rise of the public sphere was the economic and social change commonly associated with the Industrial Revolution: \"Economic expansion, increasing urbanization, rising population and improving communications in comparison to the stagnation of the previous century.\"[201] Rising efficiency in production techniques and communication lowered the prices of consumer goods and increased the amount and variety of goods available to consumers (including the literature essential to the public sphere). Meanwhile, the colonial experience (most European states had colonial empires in the 18th century) began to expose European society to extremely heterogeneous cultures, leading to the breaking down of \"barriers between cultural systems, religious divides, gender differences and geographical areas.\"[202] The word \"public\" implies the highest level of inclusivity\u2014the public sphere by definition should be open to all. However, this sphere was only public to relative degrees. Enlightenment thinkers frequently contrasted their conception of the \"public\" with that of the people: Condorcet contrasted \"opinion\" with populace, Marmontel \"the opinion of men of letters\" with \"the opinion of the multitude\" and d'Alembert the \"truly enlightened public\" with \"the blind and noisy multitude.\"[203] Additionally, most institutions of the public sphere excluded both women and the lower classes.[204] Cross-class influences occurred through noble and lower class participation in areas such as the coffeehouses and the Masonic lodges. Because of the focus on reason over superstition, the Enlightenment cultivated the arts.[205] Emphasis on learning, art, and music became more widespread, especially with the growing middle class. Areas of study such as literature, philosophy, science, and the fine arts increasingly explored subject matter to which the general public, in addition to the previously more segregated professionals and patrons, could relate.[206] As musicians depended more on public support, public concerts became increasingly popular and helped supplement performers' and composers' incomes. The concerts also helped them to reach a wider audience. Handel, for example, epitomized this with his highly public musical activities in London. He gained considerable fame there with performances of his operas and oratorios. The music of Haydn and Mozart, with their Viennese Classical styles, are usually regarded as being the most in line with the Enlightenment ideals.[207] The desire to explore, record, and systematize knowledge had a meaningful impact on music publications. Rousseau's Dictionnaire de musique (published 1767 in Geneva and 1768 in Paris) was a leading text in the late 18th century.[207] This widely available dictionary gave short definitions of words like genius and taste and was clearly influenced by the Enlightenment movement. Another text influenced by Enlightenment values was Charles Burney's A General History of Music: From the Earliest Ages to the Present Period (1776), which was a historical survey and an attempt to rationalize elements in music systematically over time.[208] Recently, musicologists have shown renewed interest in the ideas and consequences of the Enlightenment. For example, Rose Rosengard Subotnik's Deconstructive Variations (subtitled Music and Reason in Western Society) compares Mozart's Die Zauberfl\u00f6te (1791) using the Enlightenment and Romantic perspectives and concludes that the work is \"an ideal musical representation of the Enlightenment.\"[208] As the economy and the middle class expanded, there was an increasing number of amateur musicians. One manifestation of this involved women, who became more involved with music on a social level. Women were already engaged in professional roles as singers and increased their presence in the amateur performers' scene, especially with keyboard music.[206] Music publishers began to print music that amateurs could understand and play. The majority of the works that were published were for keyboard, voice and keyboard, and chamber ensemble.[206] After these initial genres were popularized, from the mid-century on, amateur groups sang choral music, which then became a new trend for publishers to capitalize on. The increasing study of the fine arts, as well as access to amateur-friendly published works, led to more people becoming interested in reading and discussing music. Music magazines, reviews, and critical works which suited amateurs as well as connoisseurs began to surface.[206] The philosophes spent a great deal of energy disseminating their ideas among educated men and women in cosmopolitan cities. They used many venues, some of them quite new. The term \"Republic of Letters\" was coined in 1664 by Pierre Bayle in his journal Nouvelles de la Republique des Lettres. Towards the end of the 18th century, the editor of Histoire de la R\u00e9publique des Lettres en France, a literary survey, described the Republic of Letters as being: In the midst of all the governments that decide the fate of men; in the bosom of so many states, the majority of them despotic\u00a0... there exists a certain realm which holds sway only over the mind\u00a0... that we honor with the name Republic, because it preserves a measure of independence, and because it is almost its essence to be free. It is the realm of talent and of thought.[209] The Republic of Letters was the sum of a number of Enlightenment ideals: an egalitarian realm governed by knowledge that could act across political boundaries and rival state power.[209] It was a forum that supported \"free public examination of questions regarding religion or legislation.\"[210] Kant considered written communication essential to his conception of the public sphere; once everyone was a part of the \"reading public,\" then society could be said to be enlightened.[210][211] The people who participated in the Republic of Letters, such as Diderot and Voltaire, are frequently known today as important Enlightenment figures. Indeed, the men who wrote Diderot's Encyclop\u00e9die arguably formed a microcosm of the larger \"republic.\"[212] Many women played an essential part in the French Enlightenment because of the role they played as salonni\u00e8res in Parisian salons, as the contrast to the male philosophes. The salon was the principal social institution of the republic[213] and \"became the civil working spaces of the project of Enlightenment.\" Women, as salonni\u00e8res, were \"the legitimate governors of [the] potentially unruly discourse\" that took place within.[214] While women were marginalized in the public culture of the Old Regime, the French Revolution destroyed the old cultural and economic restraints of patronage and corporatism (medieval guilds), opening French society to female participation, particularly in the literary sphere.[215] In France, the established men of letters (gens de lettres) had fused with the elites (les grands) of French society by the mid-18th century. This led to the creation of an oppositional literary sphere, Grub Street, the domain of a \"multitude of versifiers and would-be authors.\"[216] These men came to London to become authors only to discover that the literary market could not support large numbers of writers, who in any case were very poorly remunerated by the publishing-bookselling guilds.[217] The writers of Grub Street, the Grub Street Hacks, were left feeling bitter about the relative success of the men of letters[218] and found an outlet for their literature which was typified by the libelle. Written mostly in the form of pamphlets, the libelles \"slandered the court, the Church, the aristocracy, the academies, the salons, everything elevated and respectable, including the monarchy itself.\"[219] Le Gazetier cuirass\u00e9 by Charles Th\u00e9veneau de Morande was a prototype of the genre. It was Grub Street literature that was most read by the public during the Enlightenment.[220] According to Darnton, more importantly the Grub Street hacks inherited the \"revolutionary spirit\" once displayed by the philosophes and paved the way for the French Revolution by desacralizing figures of political, moral, and religious authority in France.[221] The increased consumption of reading materials of all sorts was one of the key features of the \"social\" Enlightenment. Developments in the Industrial Revolution allowed consumer goods to be produced in greater quantities at lower prices, encouraging the spread of books, pamphlets, newspapers, and journals \u2013 \"media of the transmission of ideas and attitudes.\" Commercial development likewise increased the demand for information, along with rising populations and increased urbanisation.[222] However, demand for reading material extended outside of the realm of the commercial and outside the realm of the upper and middle classes, as evidenced by the biblioth\u00e8que bleue. Literacy rates are difficult to gauge, but in France the rates doubled over the course of the 18th century.[223] Reflecting the decreasing influence of religion, the number of books about science and art published in Paris doubled from 1720 to 1780, while the number of books about religion dropped to just one-tenth of the total.[34] Reading underwent serious changes in the 18th century. In particular, Rolf Engelsing has argued for the existence of a reading revolution. Until 1750, reading was done intensively: people tended to own a small number of books and read them repeatedly, often to small audience. After 1750, people began to read \"extensively,\" finding as many books as they could, increasingly reading them alone.[224][225] This is supported by increasing literacy rates, particularly among women.[226] The vast majority of the reading public could not afford to own a private library, and while most of the state-run \"universal libraries\" set up in the 17th and 18th centuries were open to the public, they were not the only sources of reading material. On one end of the spectrum was the biblioth\u00e8que bleue, a collection of cheaply produced books published in Troyes, France. Intended for a largely rural and semi-literate audience these books included almanacs, retellings of medieval romances and condensed versions of popular novels, among other things. While some historians have argued against the Enlightenment's penetration into the lower classes, the biblioth\u00e8que bleue represents at least a desire to participate in Enlightenment sociability.[227] Moving up the classes, a variety of institutions offered readers access to material without needing to buy anything. Libraries that lent out their material for a small price started to appear, and occasionally bookstores would offer a small lending library to their patrons. Coffee houses commonly offered books, journals, and sometimes even popular novels to their customers. Tatler and The Spectator, two influential periodicals sold from 1709 to 1714, were closely associated with coffee house culture in London, being both read and produced in various establishments in the city.[228] This is an example of the triple or even quadruple function of the coffee house: reading material was often obtained, read, discussed, and even produced on the premises.[229] It is difficult to determine what people actually read during the Enlightenment. For example, examining the catalogs of private libraries gives an image skewed in favor of the classes wealthy enough to afford libraries and also ignores censored works unlikely to be publicly acknowledged. For this reason, a study of publishing would be much more fruitful for discerning reading habits.[230] Across continental Europe, but in France especially, booksellers and publishers had to negotiate censorship laws of varying strictness. For example, the Encyclop\u00e9die narrowly escaped seizure and had to be saved by Malesherbes, the man in charge of the French censor. Indeed, many publishing companies were conveniently located outside France so as to avoid overzealous French censors. They would smuggle their merchandise across the border, where it would then be transported to clandestine booksellers or small-time peddlers.[231] The records of clandestine booksellers may give a better representation of what literate Frenchmen might have truly read, since their clandestine nature provided a less restrictive product choice.[232] In one case, political books were the most popular category, primarily libels and pamphlets. Readers were more interested in sensationalist stories about criminals and political corruption than they were in political theory itself. The second most popular category, \"general works\" (those books \"that did not have a dominant motif and that contained something to offend almost everyone in authority\"), demonstrated a high demand for generally low-brow subversive literature. However, these works never became part of literary canon and are largely forgotten today as a result.[232] A healthy, legal publishing industry existed throughout Europe, although established publishers and book sellers occasionally ran afoul of the law. For example, the Encyclop\u00e9die, condemned by both the King and Clement XII, nevertheless found its way into print with the help of the aforementioned Malesherbes and creative use of French censorship law.[233] However, many works were sold without running into any legal trouble at all. Borrowing records from libraries in England, Germany, and North America indicate that more than 70% of books borrowed were novels. Less than 1% of the books were of a religious nature, indicating the general trend of declining religiosity.[209] A genre that greatly rose in importance was that of scientific literature. Natural history in particular became increasingly popular among the upper classes. Works of natural history include Ren\u00e9-Antoine Ferchault de R\u00e9aumur's Histoire naturelle des insectes and Jacques Gautier d'Agoty's La Myologie compl\u00e8te, ou description de tous les muscles du corps humain (1746). Outside Ancien R\u00e9gime France, natural history was an important part of medicine and industry, encompassing the fields of botany, zoology, meteorology, hydrology, and mineralogy. Students in Enlightenment universities and academies were taught these subjects to prepare them for careers as diverse as medicine and theology. As shown by Matthew Daniel Eddy, natural history in this context was a very middle class pursuit and operated as a fertile trading zone for the interdisciplinary exchange of diverse scientific ideas.[234] The target audience of natural history was French upper class, evidenced more by the specific discourse of the genre than by the generally high prices of its works. Naturalists catered to upper class desire for erudition: many texts had an explicit instructive purpose. However, natural history was often a political affair. As Emma Spary writes, the classifications used by naturalists \"slipped between the natural world and the social\u00a0... to establish not only the expertise of the naturalists over the natural, but also the dominance of the natural over the social.\"[235] The idea of taste (le go\u00fbt) was a social indicator: to truly be able to categorize nature, one had to have the proper taste, an ability of discretion shared by all members of the upper class. In this way, natural history spread many of the scientific developments of the time but also provided a new source of legitimacy for the dominant class.[236] From this basis, naturalists could then develop their own social ideals based on their scientific works.[237] The first scientific and literary journals were established during the Enlightenment. The first journal, the Parisian Journal des s\u00e7avans, appeared in 1665. However, it was not until 1682 that periodicals began to be more widely produced. French and Latin were the dominant languages of publication, but there was also a steady demand for material in German and Dutch. There was generally low demand for English publications on the continent, which was echoed by England's similar lack of desire for French works. Languages commanding less of an international market\u2014such as Danish, Spanish, and Portuguese\u2014found journal success more difficult, and a more international language was used instead. French slowly took over Latin's status as the lingua franca of learned circles. This in turn gave precedence to the publishing industry in Holland, where the vast majority of these French language periodicals were produced.[238] Jonathan Israel called the journals the most influential cultural innovation of European intellectual culture.[239] They shifted the attention of the \"cultivated public\" away from established authorities to novelty and innovation, and instead promoted the Enlightened ideals of toleration and intellectual objectivity. Being a source of knowledge derived from science and reason, they were an implicit critique of existing notions of universal truth monopolized by monarchies, parliaments, and religious authorities. They also advanced Christian Enlightenment that upheld \"the legitimacy of God-ordained authority\"\u2014the Bible\u2014in which there had to be agreement between the biblical and natural theories.[240] Although the existence of dictionaries and encyclopedias spanned into ancient times, the texts changed from defining words in a long running list to far more detailed discussions of those words in 18th-century encyclopedic dictionaries.[241] The works were part of an Enlightenment movement to systematize knowledge and provide education to a wider audience than the elite. As the 18th century progressed, the content of encyclopedias also changed according to readers' tastes. Volumes tended to focus more strongly on secular affairs, particularly science and technology, rather than matters of theology. Along with secular matters, readers also favoured an alphabetical ordering scheme over cumbersome works arranged along thematic lines.[242] Commenting on alphabetization, the historian Charles Porset has said that \"as the zero degree of taxonomy, alphabetical order authorizes all reading strategies; in this respect it could be considered an emblem of the Enlightenment.\" For Porset, the avoidance of thematic and hierarchical systems thus allows free interpretation of the works and becomes an example of egalitarianism.[243] Encyclopedias and dictionaries also became more popular during the Age of Enlightenment as the number of educated consumers who could afford such texts began to multiply.[241] In the latter half of the 18th century, the number of dictionaries and encyclopedias published by decade increased from 63 between 1760 and 1769 to approximately 148 in the decade proceeding the French Revolution.[244] Along with growth in numbers, dictionaries and encyclopedias also grew in length, often having multiple print runs that sometimes included in supplemented editions.[242] The first technical dictionary was drafted by John Harris and entitled Lexicon Technicum: Or, An Universal English Dictionary of Arts and Sciences. Harris' book avoids theological and biographical entries and instead concentrates on science and technology. Published in 1704, the Lexicon Technicum was the first book to be written in English that took a methodical approach to describing mathematics and commercial arithmetic along with the physical sciences and navigation. Other technical dictionaries followed Harris' model, including Ephraim Chambers' Cyclopaedia (1728), which included five editions and is a substantially larger work than Harris'. The folio edition of the work even included foldout engravings. The Cyclopaedia emphasized Newtonian theories, Lockean philosophy and contained thorough examinations of technologies, such as engraving, brewing, and dyeing. In Germany, practical reference works intended for the uneducated majority became popular in the 18th century. The Marperger Curieuses Natur-, Kunst-, Berg-, Gewerk- und Handlungs-Lexicon (1712) explained terms that usefully described the trades and scientific and commercial education. Jablonksi Allgemeines Lexicon (1721) was better known than the Handlungs-Lexicon and underscored technical subjects rather than scientific theory. For example, over five columns of text were dedicated to wine while geometry and logic were allocated only twenty-two and seventeen lines, respectively. The first edition of the Encyclop\u00e6dia Britannica (1771) was modelled along the same lines as the German lexicons.[245] However, the prime example of reference works that systematized scientific knowledge in the Age of Enlightenment were universal encyclopedias rather than technical dictionaries. It was the goal of universal encyclopedias to record all human knowledge in a comprehensive reference work.[246] The most well-known of these works is Diderot and d'Alembert's Encyclop\u00e9die, ou dictionnaire raisonn\u00e9 des sciences, des arts et des m\u00e9tiers. The work, which began publication in 1751, was composed of 35 volumes and over 71,000 separate entries. A great number of the entries were dedicated to describing the sciences and crafts in detail and provided intellectuals across Europe with a high-quality survey of human knowledge. In d'Alembert's Preliminary Discourse to the Encyclopedia of Diderot, the work's goal to record the extent of human knowledge in the arts and sciences is outlined: As an Encyclop\u00e9die, it is to set forth as well as possible the order and connection of the parts of human knowledge. As a Reasoned Dictionary of the Sciences, Arts, and Trades, it is to contain the general principles that form the basis of each science and each art, liberal or mechanical, and the most essential facts that make up the body and substance of each.[247] The massive work was arranged according to a \"tree of knowledge.\" The tree reflected the marked division between the arts and sciences, which was largely a result of the rise of empiricism. Both areas of knowledge were united by philosophy, or the trunk of the tree of knowledge. The Enlightenment's desacrilization of religion was pronounced in the tree's design, particularly where theology accounted for a peripheral branch, with black magic as a close neighbour.[248] As the Encyclop\u00e9die gained popularity, it was published in quarto and octavo editions after 1777. The quarto and octavo editions were much less expensive than previous editions, making the Encyclop\u00e9die more accessible to the non-elite. Robert Darnton estimates that there were approximately 25,000 copies of the Encyclop\u00e9die in circulation throughout France and Europe before the French Revolution.[249] The extensive yet affordable encyclopedia came to represent the transmission of Enlightenment and scientific education to an expanding audience.[250] One of the most important developments that the Enlightenment era brought to the discipline of science was its popularization. An increasingly literate population seeking knowledge and education in both the arts and the sciences drove the expansion of print culture and the dissemination of scientific learning. The new literate population was precipitated by a high rise in the availability of food; this enabled many people to rise out of poverty, and instead of paying more for food, they had money for education.[251] Popularization was generally part of an overarching Enlightenment ideal that endeavoured \"to make information available to the greatest number of people.\"[252] As public interest in natural philosophy grew during the 18th century, public lecture courses and the publication of popular texts opened up new roads to money and fame for amateurs and scientists who remained on the periphery of universities and academies.[253] More formal works included explanations of scientific theories for individuals lacking the educational background to comprehend the original scientific text. Newton's celebrated Philosophiae Naturalis Principia Mathematica was published in Latin and remained inaccessible to readers without education in the classics until Enlightenment writers began to translate and analyze the text in the vernacular. The first significant work that expressed scientific theory and knowledge expressly for the laity, in the vernacular and with the entertainment of readers in mind, was Bernard de Fontenelle's Conversations on the Plurality of Worlds (1686). The book was produced specifically for women with an interest in scientific writing and inspired a variety of similar works.[254] These popular works were written in a discursive style, which was laid out much more clearly for the reader than the complicated articles, treatises, and books published by the academies and scientists. Charles Leadbetter's Astronomy (1727) was advertised as \"a Work entirely New\" that would include \"short and easie  [sic] Rules and Astronomical Tables.\"[255] The first French introduction to Newtonianism and the Principia was El\u00e9ments de la philosophie de Newton, published by Voltaire in 1738.[256] \u00c9milie du Ch\u00e2telet's translation of the Principia, published after her death in 1756, also helped to spread Newton's theories beyond scientific academies and the university.[257] Writing for a growing female audience, Francesco Algarotti published Il Newtonianism per le dame, which was a tremendously popular work and was translated from Italian into English by Elizabeth Carter. A similar introduction to Newtonianism for women was produced by Henry Pemberton. His A View of Sir Isaac Newton's Philosophy was published by subscription. Extant records of subscribers show that women from a wide range of social standings purchased the book, indicating the growing number of scientifically inclined female readers among the middling class.[258] During the Enlightenment, women also began producing popular scientific works. Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years in eleven editions.[259] Most work on the Enlightenment emphasizes the ideals discussed by intellectuals, rather than the actual state of education at the time. Leading educational theorists like England's John Locke and Switzerland's Jean Jacques Rousseau both emphasized the importance of shaping young minds early. By the late Enlightenment, there was a rising demand for a more universal approach to education, particularly after the American Revolution and the French Revolution. The predominant educational psychology from the 1750s onward, especially in northern European countries, was associationism: the notion that the mind associates or dissociates ideas through repeated routines. In addition to being conducive to Enlightenment ideologies of liberty, self-determination, and personal responsibility, it offered a practical theory of the mind that allowed teachers to transform longstanding forms of print and manuscript culture into effective graphic tools of learning for the lower and middle orders of society.[260] Children were taught to memorize facts through oral and graphic methods that originated during the Renaissance.[261] Many of the leading universities associated with Enlightenment progressive principles were located in northern Europe, with the most renowned being the universities of Leiden, G\u00f6ttingen, Halle, Montpellier, Uppsala, and Edinburgh. These universities, especially Edinburgh, produced professors whose ideas had a significant impact on Britain's North American colonies and later the American Republic. Within the natural sciences, Edinburgh's medical school also led the way in chemistry, anatomy, and pharmacology.[234] In other parts of Europe, the universities and schools of France and most of Europe were bastions of traditionalism and were not hospitable to the Enlightenment. In France, the major exception was the medical university at Montpellier.[262] The history of Academies in France during the Enlightenment begins with the Academy of Science, founded in 1666 in Paris. It was closely tied to the French state, acting as an extension of a government seriously lacking in scientists. It helped promote and organize new disciplines and it trained new scientists. It also contributed to the enhancement of scientists' social status, considering them to be the \"most useful of all citizens.\" Academies demonstrate the rising interest in science along with its increasing secularization, as evidenced by the small number of clerics who were members (13%).[264] The presence of the French academies in the public sphere cannot be attributed to their membership, as although the majority of their members were bourgeois, the exclusive institution was only open to elite Parisian scholars. They perceived themselves as \"interpreters of the sciences for the people.\" For example, it was with this in mind that academicians took it upon themselves to disprove the popular pseudo-science of mesmerism.[265] The strongest contribution of the French Academies to the public sphere comes from the concours acad\u00e9miques (roughly translated as \"academic contests\") they sponsored throughout France. These academic contests were perhaps the most public of any institution during the Enlightenment.[266] The practice of contests dated back to the Middle Ages and was revived in the mid-17th century. The subject matter had previously been generally religious and/or monarchical, featuring essays, poetry, and painting. However, by roughly 1725 this subject matter had radically expanded and diversified, including \"royal propaganda, philosophical battles, and critical ruminations on the social and political institutions of the Old Regime.\" Topics of public controversy were also discussed such as the theories of Newton and Descartes, the slave trade, women's education, and justice in France.[267] More importantly, the contests were open to all, and the enforced anonymity of each submission guaranteed that neither gender nor social rank would determine the judging. Indeed, although the \"vast majority\" of participants belonged to the wealthier strata of society (\"the liberal arts, the clergy, the judiciary and the medical profession\"), there were some cases of the popular classes submitting essays and even winning.[268] Similarly, a significant number of women participated\u2014and won\u2014the competitions. Of a total of 2,300 prize competitions offered in France, women won 49\u2014perhaps a small number by modern standards but very significant in an age in which very few women had any academic training. Indeed, the majority of the winning entries were for poetry competitions, a genre commonly stressed in women's education.[269] In England, the Royal Society of London played a significant role in the public sphere and the spread of Enlightenment ideas. It was founded by a group of independent scientists and given a royal charter in 1662.[270] The society played a large role in spreading Robert Boyle's experimental philosophy around Europe and acted as a clearinghouse for intellectual correspondence and exchange.[271] Boyle was \"a founder of the experimental world in which scientists now live and operate\" and his method based knowledge on experimentation, which had to be witnessed to provide proper empirical legitimacy. This is where the Royal Society came into play: witnessing had to be a \"collective act\" and the Royal Society's assembly rooms were ideal locations for relatively public demonstrations.[272] However, not just any witness was considered to be credible: \"Oxford professors were accounted more reliable witnesses than Oxfordshire peasants.\" Two factors were taken into account: a witness's knowledge in the area and a witness's \"moral constitution.\" In other words, only civil society were considered for Boyle's public.[273] Salons were places where philosophes were reunited and discussed old, actual, or new ideas. This led to salons being the birthplace of intellectual and enlightened ideas. Coffeehouses were especially important to the spread of knowledge during the Enlightenment because they created a unique environment in which people from many different walks of life gathered and shared ideas. They were frequently criticized by nobles who feared the possibility of an environment in which class and its accompanying titles and privileges were disregarded. Such an environment was especially intimidating to monarchs who derived much of their power from the disparity between classes of people. If the different classes joined together under the influence of Enlightenment thinking, they might recognize the all-encompassing oppression and abuses of their monarchs and because of the numbers of their members might be able to successfully revolt. Monarchs also resented the idea of their subjects convening as one to discuss political matters, especially matters of foreign affairs. Rulers thought political affairs were their business only, a result of their divine right to rule.[274] Coffeeshops became homes away from home for many who sought to engage in discourse with their neighbors and discuss intriguing and thought-provoking matters, from philosophy to politics. Coffeehouses were essential to the Enlightenment, for they were centers of free-thinking and self-discovery. Although many coffeehouse patrons were scholars, many were not. Coffeehouses attracted a diverse set of people, including the educated wealthy and bourgeois as well as the lower classes. Patrons, being doctors, lawyers, merchants, represented almost all classes, so the coffeeshop environment sparked fear in those who wanted to preserve class distinction. One of the most popular critiques of the coffeehouse said that it \"allowed promiscuous association among people from different rungs of the social ladder, from the artisan to the aristocrat\" and was therefore compared to Noah's Ark, receiving all types of animals, clean and unclean.[275] This unique culture served as a catalyst for journalism, when Joseph Addison and Richard Steele recognized its potential as an audience. Together, Steele and Addison published The Spectator (1711), a daily publication which aimed, through fictional narrator Mr. Spectator, to both entertain and provoke discussion on serious philosophical matters. The first English coffeehouse opened in Oxford in 1650. Brian Cowan said that Oxford coffeehouses developed into \"penny universities,\" offering a locus of learning that was less formal than at structured institutions. These penny universities occupied a significant position in Oxford academic life, as they were frequented by those consequently referred to as the virtuosi, who conducted their research on some of the premises. According to Cowan, \"the coffeehouse was a place for like-minded scholars to congregate, to read, as well as learn from and to debate with each other, but was emphatically not a university institution, and the discourse there was of a far different order than any university tutorial.\"[276] The Caf\u00e9 Procope was established in Paris in 1686, and by the 1720s there were around 400 caf\u00e9s in the city. The Caf\u00e9 Procope in particular became a center of Enlightenment, welcoming such celebrities as Voltaire and Rousseau. The Caf\u00e9 Procope was where Diderot and D'Alembert decided to create the Encyclop\u00e9die.[277] The caf\u00e9s were one of the various \"nerve centers\" for bruits publics, public noise or rumour. These bruits were allegedly a much better source of information than were the actual newspapers available at the time.[278] The debating societies are an example of the public sphere during the Enlightenment.[279] Their origins include: In the late 1770s, popular debating societies began to move into more \"genteel\" rooms, a change which helped establish a new standard of sociability.[281] The backdrop to these developments was \"an explosion of interest in the theory and practice of public elocution.\" The debating societies were commercial enterprises that responded to this demand, sometimes very successfully. Some societies welcomed from 800 to 1,200 spectators per night.[282] The debating societies discussed an extremely wide range of topics. Before the Enlightenment, most intellectual debates revolved around \"confessional\"\u2014that is, Catholic, Lutheran, Reformed (Calvinist) or Anglican issues, debated primarily to establish which bloc of faith ought to have the \"monopoly of truth and a God-given title to authority.\"[283] After Enlightenment, everything that previously had been rooted in tradition was questioned, and often replaced by new concepts. After the second half of the 17th century and during the 18th century, a \"general process of rationalization and secularization set in\" and confessional disputes were reduced to a secondary status in favor of the \"escalating contest between faith and incredulity.\"[283] In addition to debates on religion, societies discussed issues such as politics and the role of women. However, the critical subject matter of these debates did not necessarily translate into opposition to the government; the results of the debate quite frequently upheld the status quo.[284] From a historical standpoint, one of the most important features of the debating society was their openness to the public, as women attended and even participated in almost every debating society, which were likewise open to all classes providing they could pay the entrance fee. Once inside, spectators were able to participate in a largely egalitarian form of sociability that helped spread Enlightenment ideas.[285] Historians have debated the extent to which the secret network of Freemasonry was a main factor in the Enlightenment.[286] Leaders of the Enlightenment included Freemasons such as Diderot, Montesquieu, Voltaire, Lessing, Pope,[287] Horace Walpole, Robert Walpole, Mozart, Goethe, Frederick the Great, Benjamin Franklin[288] and George Washington.[289] Norman Davies said Freemasonry was a powerful force on behalf of liberalism in Europe from about 1700 to the twentieth century. It expanded during the Enlightenment, reaching practically every country in Europe. It was especially attractive to powerful aristocrats and politicians as well as intellectuals, artists, and political activists.[290] During the Enlightenment, Freemasons comprised an international network of like-minded men, often meeting in secret in ritualistic programs at their lodges. They promoted the ideals of the Enlightenment and helped diffuse these values across Britain, France, and other places. Freemasonry as a systematic creed with its own myths, values, and rituals originated in Scotland c.\u20091600 and spread to England and then across the Continent in the 18th century. They fostered new codes of conduct\u2014including a communal understanding of liberty and equality inherited from guild sociability\u2014\"liberty, fraternity, and equality.\"[291] Scottish soldiers and Jacobite Scots brought to the Continent ideals of fraternity, which reflected not the local system of Scottish customs, but the institutions and ideals originating in the English Revolution against royal absolutism.[292] Freemasonry was particularly prevalent in France\u2014by 1789, there were perhaps as many as 100,000 French Masons, making Freemasonry the most popular of all Enlightenment associations.[293] The Freemasons displayed a passion for secrecy and created new degrees and ceremonies. Similar societies, partially imitating Freemasonry, emerged in France, Germany, Sweden, and Russia. One example was the Illuminati, founded in Bavaria in 1776, which was copied after the Freemasons, but was never part of the movement. The name itself translates to \"enlightened,\" chosen to reflect their original intent to promote the values of the movement. The Illuminati was an overtly political group, which most Masonic lodges decidedly were not.[294] Masonic lodges created a private model for public affairs. They \"reconstituted the polity and established a constitutional form of self-government, complete with constitutions and laws, elections, and representatives.\" In other words, the micro-society set up within the lodges constituted a normative model for society as a whole. This was especially true on the continent: when the first lodges began to appear in the 1730s, their embodiment of British values was often seen as threatening by state authorities. For example, the Parisian lodge that met in the mid 1720s was composed of English Jacobite exiles.[295] Furthermore, freemasons across Europe explicitly linked themselves to the Enlightenment as a whole. For example, in French lodges the line \"As the means to be enlightened I search for the enlightened\" was a part of their initiation rites. British lodges assigned themselves the duty to \"initiate the unenlightened.\" This did not necessarily link lodges to the irreligious, but neither did this exclude them from the occasional heresy. In fact, many lodges praised the Grand Architect, the masonic terminology for the deistic divine being who created a scientifically ordered universe.[296] German historian Reinhart Koselleck claimed: \"On the Continent there were two social structures that left a decisive imprint on the Age of Enlightenment: the Republic of Letters and the Masonic lodges.\"[297] Scottish professor Thomas Munck argues that \"although the Masons did promote international and cross-social contacts which were essentially non-religious and broadly in agreement with enlightened values, they can hardly be described as a major radical or reformist network in their own right.\"[298] Many of the Masons values seemed to greatly appeal to Enlightenment values and thinkers. Diderot discusses the link between Freemason ideals and the enlightenment in D'Alembert's Dream, exploring masonry as a way of spreading enlightenment beliefs.[299] Historian Margaret Jacob stresses the importance of the Masons in indirectly inspiring enlightened political thought.[300] On the negative side, Daniel Roche contests claims that Masonry promoted egalitarianism and he argues the lodges only attracted men of similar social backgrounds.[301] The presence of noble women in the French \"lodges of adoption\" that formed in the 1780s was largely due to the close ties shared between these lodges and aristocratic society.[302] The major opponent of Freemasonry was the Catholic Church so in countries with a large Catholic element, such as France, Italy, Spain, and Mexico, much of the ferocity of the political battles involve the confrontation between what Davies calls the reactionary Church and enlightened Freemasonry.[303][304] Even in France, Masons did not act as a group.[305] American historians, while noting that Benjamin Franklin and George Washington were indeed active Masons, have downplayed the importance of Freemasonry in causing the American Revolution because the Masonic order was non-political and included both Patriots and their enemy the Loyalists.[306] At the same time, the Classical art of Greece and Rome became interesting to people again, since archaeological teams discovered Pompeii and Herculaneum.[307] For up to Descartes ... a particular sub-iectum ... lies at the foundation of its own fixed qualities and changing circumstances. The superiority of a sub-iectum ... arises out of the claim of man to a ... self-supported, unshakeable foundation of truth, in the sense of certainty. Why and how does this claim acquire its decisive authority? The claim originates in that emancipation of man in which he frees himself from obligation to Christian revelational truth and Church doctrine to a legislating for himself that takes its stand upon itself. Decision theory: Decision theory or the theory of rational choice is a branch of probability, economics, and analytic philosophy that uses expected utility and probability to model how individuals would behave rationally under uncertainty.[1][2] It differs from the cognitive and behavioral sciences in that it is mainly prescriptive and concerned with identifying optimal decisions for a rational agent, rather than describing how people actually make decisions. Despite this, the field is important to the study of real human behavior by social scientists, as it lays the foundations to mathematically model and analyze individuals in fields such as sociology, economics, criminology, cognitive science, moral philosophy and political science.[citation needed] The roots of decision theory lie in probability theory, developed by Blaise Pascal and Pierre de Fermat in the 17th century, which was later refined by others like Christiaan Huygens. These developments provided a framework for understanding risk and uncertainty, which are central to decision-making. In the 18th century, Daniel Bernoulli introduced the concept of \"expected utility\" in the context of gambling, which was later formalized by John von Neumann and Oskar Morgenstern in the 1940s. Their work on Game Theory and Expected Utility Theory helped establish a rational basis for decision-making under uncertainty. After World War II, decision theory expanded into economics, particularly with the work of economists like Milton Friedman and others, who applied it to market behavior and consumer choice theory. This era also saw the development of Bayesian decision theory, which incorporates Bayesian probability into decision-making models. By the late 20th century, scholars like Daniel Kahneman and Amos Tversky challenged the assumptions of rational decision-making. Their work in behavioral economics highlighted cognitive biases and heuristics that influence real-world decisions, leading to the development of prospect theory, which modified expected utility theory by accounting for psychological factors. Normative decision theory is concerned with identification of optimal decisions where optimality is often determined by considering an ideal decision maker who is able to calculate with perfect accuracy and is in some sense fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis and is aimed at finding tools, methodologies, and software (decision support systems) to help people make better decisions.[3][4] In contrast, descriptive decision theory is concerned with describing observed behaviors often under the assumption that those making decisions are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework (e.g. stochastic transitivity axioms), reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).[3][4] Prescriptive decision theory is concerned with predictions about behavior that positive decision theory produces to allow for further tests of the kind of decision-making that occurs in practice. In recent decades, there has also been increasing interest in \"behavioral decision theory\", contributing to a re-evaluation of what useful decision-making requires.[5][6] The area of choice under uncertainty represents the heart of decision theory. Known from the 17th century (Blaise Pascal invoked it in his famous wager, which is contained in his Pens\u00e9es, published in 1670), the idea of expected value is that, when faced with a number of actions, each of which could give rise to more than one possible outcome with different probabilities, the rational procedure is to identify all possible outcomes, determine their values (positive or negative) and the probabilities that will result from each course of action, and multiply the two to give an \"expected value\", or the average expectation for an outcome; the action to be chosen should be the one that gives rise to the highest total expected value. In 1738, Daniel Bernoulli published an influential paper entitled Exposition of a New Theory on the Measurement of Risk, in which he uses the St. Petersburg paradox to show that expected value theory must be normatively wrong. He gives an example in which a Dutch merchant is trying to decide whether to insure a cargo being sent from Amsterdam to St. Petersburg in winter. In his solution, he defines a utility function and computes expected utility rather than expected financial value.[7] In the 20th century, interest was reignited by Abraham Wald's 1939 paper pointing out that the two central procedures of sampling-distribution-based statistical-theory, namely hypothesis testing and parameter estimation, are special cases of the general decision problem.[8] Wald's paper renewed and synthesized many concepts of statistical theory, including loss functions, risk functions, admissible decision rules, antecedent distributions, Bayesian procedures, and minimax procedures. The phrase \"decision theory\" itself was used in 1950 by E. L. Lehmann.[9] The revival of subjective probability theory, from the work of Frank Ramsey, Bruno de Finetti, Leonard Savage and others, extended the scope of expected utility theory to situations where subjective probabilities can be used. At the time, von Neumann and Morgenstern's theory of expected utility[10] proved that expected utility maximization followed from basic postulates about rational behavior. The work of Maurice Allais and Daniel Ellsberg showed that human behavior has systematic and sometimes important departures from expected-utility maximization (Allais paradox and Ellsberg paradox).[11] The prospect theory of Daniel Kahneman and Amos Tversky renewed the empirical study of economic behavior with less emphasis on rationality presuppositions. It describes a way by which people make decisions when all of the outcomes carry a risk.[12] Kahneman and Tversky found three regularities \u2013 in actual human decision-making, \"losses loom larger than gains\"; people focus more on changes in their utility-states than they focus on absolute utilities; and the estimation of subjective probabilities is severely biased by anchoring. Intertemporal choice is concerned with the kind of choice where different actions lead to outcomes that are realized at different stages over time.[13] It is also described as cost-benefit decision making since it involves the choices between rewards that vary according to magnitude and time of arrival.[14] If someone received a windfall of several thousand dollars, they could spend it on an expensive holiday, giving them immediate pleasure, or they could invest it in a pension scheme, giving them an income at some time in the future. What is the optimal thing to do? The answer depends partly on factors such as the expected rates of interest and inflation, the person's life expectancy, and their confidence in the pensions industry. However even with all those factors taken into account, human behavior again deviates greatly from the predictions of prescriptive decision theory, leading to alternative models in which, for example, objective interest rates are replaced by subjective discount rates.[citation needed] Some decisions are difficult because of the need to take into account how other people in the situation will respond to the decision that is taken. The analysis of such social decisions is often treated under decision theory, though it involves mathematical methods. In the emerging field of socio-cognitive engineering, the research is especially focused on the different types of distributed decision-making in human organizations, in normal and abnormal/emergency/crisis situations.[15] Other areas of decision theory are concerned with decisions that are difficult simply because of their complexity, or the complexity of the organization that has to make them. Individuals making decisions are limited in resources (i.e. time and intelligence) and are therefore boundedly rational; the issue is thus, more than the deviation between real and optimal behavior, the difficulty of determining the optimal behavior in the first place. Decisions are also affected by whether options are framed together or separately; this is known as the distinction bias.[citation needed] Heuristics are procedures for making a decision without working out the consequences of every option. Heuristics decrease the amount of evaluative thinking required for decisions, focusing on some aspects of the decision while ignoring others.[16] While quicker than step-by-step processing, heuristic thinking is also more likely to involve fallacies or inaccuracies.[17] One example of a common and erroneous thought process that arises through heuristic thinking is the gambler's fallacy \u2014 believing that an isolated random event is affected by previous isolated random events. For example, if flips of a fair coin give repeated tails, the coin still has the same probability (i.e., 0.5) of tails in future turns, though intuitively it might seems that heads becomes more likely.[18] In the long run, heads and tails should occur equally often; people commit the gambler's fallacy when they use this heuristic to predict that a result of heads is \"due\" after a run of tails.[19] Another example is that decision-makers may be biased towards preferring moderate alternatives to extreme ones. The compromise effect operates under a mindset that the most moderate option carries the most benefit. In an incomplete information scenario, as in most daily decisions, the moderate option will look more appealing than either extreme, independent of the context, based only on the fact that it has characteristics that can be found at either extreme.[20] A highly controversial issue is whether one can replace the use of probability in decision theory with something else. Advocates for the use of probability theory point to: The proponents of fuzzy logic, possibility theory, Dempster\u2013Shafer theory, and info-gap decision theory maintain that probability is only one of many alternatives and point to many examples where non-standard alternatives have been implemented with apparent success. Notably, probabilistic decision theory can sometimes be sensitive to assumptions about the probabilities of various events, whereas non-probabilistic rules, such as minimax, are robust in that they do not make such assumptions. A general criticism of decision theory based on a fixed universe of possibilities is that it considers the \"known unknowns\", not the \"unknown unknowns\":[21] it focuses on expected variations, not on unforeseen events, which some argue have outsized impact and must be considered \u2013 significant events may be \"outside model\". This[which?] line of argument, called the ludic fallacy, is that there are inevitable imperfections in modeling the real world by particular models, and that unquestioning reliance on models blinds one to their limits.",
      "ground_truth_chunk_ids": [
        "130_fixed_chunk1",
        "163_fixed_chunk1"
      ],
      "source_ids": [
        "S130",
        "S163"
      ],
      "category": "comparative",
      "id": 66
    },
    {
      "question": "Compare Tanya Plibersek and World Wide Web in one sentence each: what does each describe or study?",
      "ground_truth": "Tanya Plibersek: Tanya Joan Pliber\u0161ek[a] (born 2 December 1969) is an Australian politician who has served as Minister for Social Services since 2025 and the member of parliament (MP) for the New South Wales division of Sydney since 1998. Previously, she served as the Minister for the Environment and Water from 2022 to 2025, deputy leader of the Labor Party from 2013 to 2019, and held ministerial offices in the Rudd and Gillard governments. Plibersek was born in Sydney to Slovenian immigrant parents and grew up in Sutherland Shire. She has degrees from the University of Technology Sydney and Macquarie University, and worked in the NSW Government's Domestic Violence Unit before entering parliament. Plibersek was elected to the Division of Sydney at the 1998 federal election, aged 28. She joined the shadow cabinet in 2004, and when Labor won the 2007 election was made Minister for Housing and Minister for the Status of Women. In a cabinet reshuffle in 2010, Plibersek was made Minister for Human Services and Minister for Social Inclusion. She was promoted to Minister for Health the following year, and held that position until Labor's defeat at the 2013 election. Plibersek was then elected as deputy to new ALP leader Bill Shorten. Plibersek served as deputy opposition leader until Labor's defeat at the 2019 Australian federal election. She was subsequently made shadow minister for education under new opposition leader Anthony Albanese. Upon Labor's victory at the 2022 Australian federal election, she was appointed Minister for the Environment and Water. After the 2025 federal election she was made Minister for Social Services in the Second Albanese ministry. She is a senior figure in the Labor Left faction. Plibersek was born in Sydney, the youngest of three children born to Joseph and Rose Plibersek. Her elder brother Ray is a lawyer, World Wide Web: The World Wide Web (also known as WWW, W3, or simply the Web)[2] is a public interconnected information system that enables content sharing over the Internet.[3] It facilitates access to documents and other web resources according to specific rules of the Hypertext Transfer Protocol (HTTP).[4] The Web was invented by English computer scientist Tim Berners-Lee while at CERN in 1989 and opened to the public in 1993. It was conceived as a \"universal linked information system\".[5][6][7] Documents and other media content are made available to the network through web servers and can be accessed by programs such as web browsers. Servers and resources on the World Wide Web are identified and located through a character string called uniform resource locator (URL). The original and still very common document type is a web page formatted in Hypertext Markup Language (HTML). This markup language supports plain text, images, embedded video and audio contents, and scripts (short programs) that implement complex user interaction. The HTML language also supports hyperlinks (embedded URLs), which provide immediate access to other web resources. Web navigation, or web surfing, is the common practice of following such hyperlinks across multiple websites. Web applications are web pages that function as application software. The information on the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common domain name make up a website. A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organisations, government agencies, and individual users; and comprises an enormous amount of educational, entertainment, commercial, and government information. The World Wide Web has become the world's dominant information systems platform.[8][9][10][11] It is the primary tool that billions of",
      "expected_answer": "Tanya Plibersek: Tanya Joan Pliber\u0161ek[a] (born 2 December 1969) is an Australian politician who has served as Minister for Social Services since 2025 and the member of parliament (MP) for the New South Wales division of Sydney since 1998. Previously, she served as the Minister for the Environment and Water from 2022 to 2025, deputy leader of the Labor Party from 2013 to 2019, and held ministerial offices in the Rudd and Gillard governments. Plibersek was born in Sydney to Slovenian immigrant parents and grew up in Sutherland Shire. She has degrees from the University of Technology Sydney and Macquarie University, and worked in the NSW Government's Domestic Violence Unit before entering parliament. Plibersek was elected to the Division of Sydney at the 1998 federal election, aged 28. She joined the shadow cabinet in 2004, and when Labor won the 2007 election was made Minister for Housing and Minister for the Status of Women. In a cabinet reshuffle in 2010, Plibersek was made Minister for Human Services and Minister for Social Inclusion. She was promoted to Minister for Health the following year, and held that position until Labor's defeat at the 2013 election. Plibersek was then elected as deputy to new ALP leader Bill Shorten. Plibersek served as deputy opposition leader until Labor's defeat at the 2019 Australian federal election. She was subsequently made shadow minister for education under new opposition leader Anthony Albanese. Upon Labor's victory at the 2022 Australian federal election, she was appointed Minister for the Environment and Water. After the 2025 federal election she was made Minister for Social Services in the Second Albanese ministry. She is a senior figure in the Labor Left faction. Plibersek was born in Sydney, the youngest of three children born to Joseph and Rose Plibersek. Her elder brother Ray is a lawyer, and her eldest brother Phillip (d. 1997) was a geologist. Her parents were born in small Slovenian villages, arriving in Australia unknown to each other as part of the post-war immigration scheme. Her mother (n\u00e9e Rosalija Repi\u010d) was born in Podvinci, and came to Australia via Italy. Her father (n\u00e9 Jo\u017ee Pliber\u0161ek) was born in Ko\u010dno pri Polskavi, and came to Australia via Austria. He found work as a labourer on the Snowy Mountains Scheme, and later spent 20 years working for Qantas as a plumber and gas fitter.[1][2] Plibersek grew up in the suburb of Oyster Bay in Sydney's Sutherland Shire. She attended Oyster Bay Public School and Jannali Girls High School, where she was the dux. She joined the Labor Party at the age of 15.[1] Plibersek studied journalism at the University of Technology Sydney, graduating with a Bachelor of Arts in communications. She then took a Masters in Public Policy and Politics at Macquarie University.[3][4] After a failed attempt to secure a cadetship with the Australian Broadcasting Corporation (ABC), she found work with the Domestic Violence Unit at the New South Wales Government's Office for the Status and Advancement of Women.[3] She found working with the state women's minister Kerry Chikarovski \"demoralising\" and later criticised her for focusing on the glass ceiling rather than other women's issues. Plibersek subsequently joined the office of Senator Bruce Childs, before switching to work for Senator George Campbell as a research officer.[5] Plibersek was elected to the House of Representatives at the 1998 federal election, aged 28,[6] retaining the Division of Sydney for the ALP following the retirement of Peter Baldwin. With the support of George Campbell's \"hard left\" faction, she won preselection for the seat against twelve other candidates, including ten other women. In the lead-up to the ballot she \"wrote to each branch member three or four times, attended branch meetings virtually every night, gave talks to community groups, and contributed to three candidates' debates\".[5] Plibersek supported Kim Beazley's unsuccessful candidacies in the 2003 ALP leadership votes, where he initially lost to Simon Crean and then later to Mark Latham.[7] In July 2003 she and Anthony Albanese publicly criticised Crean for his rejection of the party's policy on a Second Sydney Airport.[8] After the 2004 election, Plibersek was elected to Latham's shadow ministry and allocated three portfolios \u2013 youth; the status of women; and work and family, community and early childhood education. In June 2005, after Latham was succeeded as opposition leader by Beazley, she retained the youth and status of women portfolios and was given responsibility for childcare.[6] Upon the release of The Latham Diaries she described him as \"a negative and critical person\".[9] Plibersek publicly supported Beazley against Kevin Rudd in the 2006 leadership spill, though was retained in Rudd's shadow ministry after his defeat of Beazley, with the portfolios of youth; the status of women; and human services and housing.[6] Following the 2007 federal election, Plibersek was appointed Minister for Housing and Minister for the Status of Women in the First Rudd Ministry. Following the 2010 federal election, Plibersek was appointed Minister for Human Services and Minister for Social Inclusion. Her appointment took effect following the birth of Plibersek's youngest son Louis, and soon afterwards Plibersek directed the Human Services response to the 2010\u201311 Queensland floods. As Minister for Human Services, Plibersek established emergency and recovery centres to provide urgent support to flood-affected communities.[citation needed] As Minister for Housing, Plibersek established the National Rental Affordability Scheme to build 50,000 affordable rental homes, invested $6\u00a0billion in social housing to build 21,600 new homes and repair 80,000 homes, and provided $550\u00a0million for homelessness services. The new housing was built ahead of time and under budget. She also established the Housing Affordability Fund and First Home Saver Accounts.[10] In December 2008, along with Kevin Rudd, Prime Minister at that time, Plibersek released the Government's White Paper on Homelessness, The Road Home, which expressed a goal of halving homelessness by 2020.[11] This goal was abandoned by the incoming Abbott government which cut homelessness funding and ended the National Rental Affordability Scheme and First Home Saver Accounts.[citation needed] As Minister for the Status of Women, Plibersek convened the National Council to Reduce Violence against Women and their Children in May 2008, and released the National Council's Plan for Australia to Reduce Violence Against Women and their Children in March 2009 \u2013 the first in Australian history.[12][13] Plibersek also addressed the 2009 United Nations International Women's Day event, attended by United Nations Secretary General Ban Ki-moon, and announced Australia's formal accession to the United Nations Optional Protocol to the Convention on the Elimination of All Forms of Discrimination Against Women (CEDAW).[14] Plibersek said that acceding to the Optional Protocol \"will send a strong message that Australia is serious about promoting gender equality and that we are prepared to be judged by international human rights standards.\"[15] Plibersek played a key role in designing and securing Australia's first paid parental leave scheme. Plibersek also strengthened Australia's anti-people trafficking strategy, with a focus on better supporting victims including women in the sex industry.[16] As Shadow Minister for Women, Plibersek also committed the Labor Opposition to expanding access to abortions by making termination services legal in all taxpayer-funded hospitals. This policy was later dumped from Labor's election commitments under Albanese.[17] In Opposition, Plibersek supported women's economic security by committing the Labor Opposition to the removal of the $450 per month minimum income threshold for eligibility for the superannuation guarantee, a policy later adopted by the Liberal Government.[18] Plibersek also committed to forcing companies with over one thousand employees to disclose the gap in salaries between male and female staff. This was dismissed by the Morrison Liberal Government at the time as setting \"one set of employees against another\".[19] As Minister for Health Plibersek established Grow Up Smiling, a $4\u00a0billion package to support better dental care for children, which expanded Medicare-subsided dental check-ups for children from age 2 to 17. She introduced free Gardasil vaccinations, previously only available for girls, for boys to protect against cancers caused by HPV \u2013 a world first.[20] Along with the previous Health Minister, Nicola Roxon, Plibersek also implemented world-leading plain packaging of tobacco laws which saw smoking rates drop to 13%.[citation needed] Plibersek also added the RU486 abortion pill to the PBS, improving access to reproductive healthcare.[21] When Plibersek was Minister for Health, Australia achieved the best 5-year cancer survival rates in the world. Plibersek also delivered 1,300 more hospital beds and 60,000 additional doctors, nurses and allied health professionals. She also oversaw the funding, construction and/or opening of a number of new facilities, including the Chris O\u2019Brien Lifehouse Cancer Centre, the Kinghorn Cancer Centre, the Launceston Multi-Purpose Health Centre, and a new medical and dental school as well as new facilities for the South Australian Health and Medical Research Institute (SAHMRI) in Adelaide.[citation needed] Plibersek further added\u202f87 new medicines\u202fto the\u202fPharmaceutical Benefits Scheme (PBS)\u202fbetween\u202f2012\u20132013. introduced the\u202fLiving Longer Living Better reforms,[22] allocating\u202f$1.2 billion\u202fto improve\u202faged care services\u202fbetween\u202f2012\u20132013, approved the\u202ffirst rapid HIV test\u202ffor point-of-care use in Australia, providing results within an hour.[23] Plibersek was unanimously elected deputy leader of the Labor Party (and thus Deputy Leader of the Opposition) on 14 October 2013, following the leadership election that had seen Bill Shorten succeed Kevin Rudd as leader.[24] She was Shadow Minister for Foreign Affairs and International Development until July 2016. Following the 2016 election, she was made Shadow Minister for Education and Shadow Minister for Women.[25] Shorten said handing Plibersek the education portfolio was \"about putting a great policy thinker on the political frontline\".[26] Plibersek was re-elected at the 2019 election with a swing of 5.7 points to the Labor Party. Following the party's defeat at the federal election of 2019 and Bill Shorten's immediate resignation as party leader, Plibersek made it known that she was interested in standing in the leadership election, and was supported by Shorten and former prime minister Julia Gillard; however, she concluded that \"now is not my time\", citing family responsibilities.[27][28][29] After Anthony Albanese's victory in the leadership contest, Plibersek was appointed Shadow Minister for Education and Training in his new shadow cabinet.[30] In January 2021 Albanese also appointed her Shadow Minister for Women.[31] Plibersek was appointed Minister for the Environment and Water in the first Albanese ministry.[32] Upon Peter Dutton's election as Leader of the Opposition, Plibersek compared Dutton to Lord Voldemort. In a radio interview, she stated: \"I think there will be a lot of children who have watched a lot of Harry Potter films who will be very frightened of what they are seeing on TV at night. I am saying he looks a bit like Voldemort. We will see whether he can do what he promised he would do when he was last running for leader, which is smile more.\"[33] Plibersek later apologised. In a radio interview, Dutton called the claims \"unfortunate\" but \"water off a duck's back\", also noting that he wasn't \"bald by choice\" and was diagnosed with a skin condition several years ago.[33] Plibersek's apology was welcomed by newly elected Prime Minister Anthony Albanese in an interview with ABC News.[33] In November 2024, Plibersek reached an agreement with the Greens and independent senator David Pocock to pass legislation to create a national environment protection agency. However, the legislation was scrapped at the last minute after intervention from Prime Minister Albanese.[34][35][36] After the re-election of the Albanese government in the 2025 federal election, Plibersek was moved to the role of Minister for Social Services in the second Albanese ministry.[37] Plibersek is a member of the Labor Party's left faction. Plibersek has argued that government should actively invest in the economy to promote growth and equality, calling for a federal commitment to a policy of full employment where \"Australians who can work, can get a job\". During the coronavirus recession of 2020, she has advocated government stimulus over tax cuts for high income earners. In September 2020, she explained her priorities for economic recovery: \"We need to be building things in Australia to support both the skilled trades people and the apprentices that we should be training right now. We need to build things. We need to make things. We need to care for people. We need secure jobs with decent pay\". Education Plibersek is a long-term supporter of investing in education. As the shadow Minister for Education and Training she has developed policies across schools, universities, TAFE and vocational education. Prior to the 2019 election, these policies included increasing school funding by $14 billion over a decade, to the creation of a new Evidence Institute for Schools and instituting a review of the country's NAPLAN testing system. In higher education Plibersek promised to reintroduce the demand driven system of university funding, creating an extra 200,000 places for students. Energy Plibersek is a strong supporter of renewable energy and transitioning towards clean energy production. She has argued that the renewables industry is key to promoting new jobs, assisting local manufacturing, lowering carbon emissions and reducing power prices. She has also endorsed programs to help households install solar panels on their homes \u2013 which have been adopted by over two and a half million Australia households. In 2018, Plibersek argued against providing federal subsidies for new coal fired power plants. Housing\nAfter the 2007 election, Plibersek was made Federal Minister for Housing in the Rudd government. As part of the government's response to the 2008 financial crisis, Plibersek implemented several policies that both grew the housing stock and stimulated the Australian economy. These policies included the First Home Owners Boost, providing up to $21,000 for people buying new dwellings, the National Rental Affordability Scheme, providing incentives for investors to build properties for low and middle income Australians, and $6 billion for the construction, repair and improvement of social housing. Plibersek also released the Homelessness White Paper, which set out a comprehensive national plan to tackle homelessness in Australia with significant funding attached. Plibersek has argued that significant new investment in social and public housing should be part of Australia's response to the coronavirus economic downturn. Welfare Plibersek supports an increase to Newstart, Australia's then-unemployment benefit, arguing that the current rate is too low, \"trapping people in poverty\" who are \"just surviving\" on an allowance of $40 a day. Women's reproductive rights Plibersek is pro-choice. As Minister for Health, Plibersek approved listing the abortion drug RU-486 on the Pharmaceutical Benefits Scheme. Plibersek described the provision of the medicine as \"a good thing in the situation where women are faced with one of the most difficult decisions that they will ever make\".[38] Anti-abortion groups criticised the move, with one campaigner, Margaret Tighe, labelling it a \"gross abuse of power.\"[39] Other commentators, including Clementine Ford, labelled the decision \"progressive\".[40][41] First Nations people Plibersek supports instituting an Indigenous Voice to Parliament, based on the recommendations made in the Uluru Statement from the Heart. As Deputy Leader of the Labor Party, she stated that implementing the Indigenous Voice was her party's priority in Indigenous affairs, alongside \u2018closing the gap\u2019, particularly in health and education. An Australian republic Plibersek supports Australia becoming a Republic. As a first term Member of Parliament in 1999, she campaigned for the yes vote in the constitutional referendum to replace the Queen and Governor-General with a President appointed by a two-thirds majority of the members of the Commonwealth Parliament. LGBT rights From the 1990s onward, Plibersek campaigned for the removal of discrimination against same-sex de facto couples from federal legislation, raising the issue formally in Parliament on multiple occasions during her parliamentary career (including 1999,[42] 2006,[43] and 2008[44]). In her regular paid advertisement in the South Sydney Herald, Plibersek wrote that \"The passing of these reforms to federal legislation was one of the proudest moments of my time in the Australian Parliament\"[45] and she has marched in the Sydney Gay and Lesbian Mardi Gras Parade almost every year for three decades.[46] As deputy leader, Plibersek led the push to make support for same-sex marriage binding Labor policy[47] which resulted in many Labor MPs speaking out publicly in support of same-sex marriage. Plibersek seconded a private members bill to legalise same sex marriage, moved by Labor leader Bill Shorten. She opposed the 2017 postal plebiscite, arguing it was unnecessary and divisive, but campaigned strongly for the \u2018yes\u2019 vote during the plebiscite campaign. Multiculturalism and citizenship As an Australian with Slovenian heritage, Plibersek is vocal advocate for multiculturalism. On an episode of Q&A in 2018 she clashed with conservative Senator Matthew Canavan on the topic of immigration, arguing against Canavan's claim that immigrants congregated through \u2018ghettoisation\u2019. Before the 2019 election, Plibersek pledged $8\u00a0million towards community language schools, to expand language training for children. In January 2020 Plibersek aroused controversy in an Australia Day speech, calling for children to learn the Australian citizenship pledge at school.[48][49] In the speech, Plibersek argued that progressives should feel more comfortable with the concept of patriotism: \"You can be proud of your citizenship and dedicated to progress. You can cherish this nation and want to make it better. You can be a progressive and love your country: I certainly do.\" Foreign aid As Shadow Minister for Foreign Affairs, Plibersek opposed the cuts to foreign aid made by the Abbott Liberal government. At the 2016 election Labor promised to reverse those cuts if elected. Iraq Plibersek opposed the 2003 invasion of Iraq.[50] In 2003, when then-US President George W. Bush visited Australia, Tanya presented national security adviser Condoleezza Rice with a letter, signed by 43 Labor MPs, explaining why Labor parliamentarians opposed Australia invading Iraq without United Nations approval.[51] She also stated in Parliament, \"I do not support an attack on Iraq. I particularly do not support a pre-emptive first strike. Nor do I support any action that is initiated by the US alone rather than being sanctioned by the United Nations.\"[50] East Timor While Shadow Minister for Foreign Affairs in 2016, Plibersek proposed that Australia redraw its maritime border with East Timor. According to the Sydney Morning Herald, \"Ms Plibersek lamented that Australia's pivotal role in securing East Timor's independence \u2013 \"a proud moment\" \u2013 was being tarnished by its refusal to negotiate a new, permanent maritime boundary with East Timor. \"The maritime boundary dispute has poisoned relations with our newest neighbour. This must change for their sake and ours,\" Ms Plibersek said.\"[52]\nThis position was later adopted by the Liberal government, and a new border agreement treaty was signed in 2018. Israel Speaking in the House of Representatives on 17 September 2002, Plibersek said: \"I can think of a rogue state which consistently ignores UN resolutions, whose ruler is a war criminal responsible for the massacres of civilians in refugee camps outside its borders. The US supports and funds this country. This year it gave it a blank cheque to continue its repression of its enemies. It uses US military hardware to bulldoze homes and kill civilians. It is called Israel, and the war criminal is Ariel Sharon. Needless to say, the US does not mention the UN resolutions that Israel has ignored for 30 years; it just continues sending the money...\"[53] Plibersek's remarks again gained prominence in October 2013, after she and Bill Shorten were elected as deputy leader and leader of the Labor Party, respectively. After choosing to take on the foreign affairs portfolio while in opposition, Liberal Party MP Julie Bishop, then Minister for Foreign Affairs, said Plibersek should \"publicly retract those statements\". The Australian noted that Plibersek's appointment was likely to be criticised by the Jewish community in Australia.[54] But the Executive Council of Australian Jewry expressed satisfaction in Plibersek's elevation to the deputy leadership, noting that she had \u2018developed friendly relations with the Jewish Community\u2019. Plibersek visited Israel and the State of Palestine in February 2014, meeting with the Prime Minister of Palestine, Rami Hamdallah.[55] Plibersek has held the following portfolios and parliamentary party positions since her election in 1998 (both shadow and government appointments are listed):[6] Plibersek lives in Sydney with her husband Michael Coutts-Trotter, who is a senior NSW public servant, and three children.[56] Following the 2010 federal election, when Labor retained government with the support of the Australian Greens and independents, parliamentary numbers were finely balanced. After some controversy, Plibersek was granted a pair by the Coalition so that her absence from the House of Representatives while on maternity leave did not affect the result of votes.[57] She gave birth to her son later that year.[58][59] In September 2016, her older brother Ray Plibersek was elected to Sutherland Shire council representing C Ward for the Australian Labor Party.[60] Plibersek is fond of bushwalking and Jane Austen[61] novels. World Wide Web: The World Wide Web (also known as WWW, W3, or simply the Web)[2] is a public interconnected information system that enables content sharing over the Internet.[3] It facilitates access to documents and other web resources according to specific rules of the Hypertext Transfer Protocol (HTTP).[4] The Web was invented by English computer scientist Tim Berners-Lee while at CERN in 1989 and opened to the public in 1993. It was conceived as a \"universal linked information system\".[5][6][7] Documents and other media content are made available to the network through web servers and can be accessed by programs such as web browsers. Servers and resources on the World Wide Web are identified and located through a character string called uniform resource locator (URL). The original and still very common document type is a web page formatted in Hypertext Markup Language (HTML). This markup language supports plain text, images, embedded video and audio contents, and scripts (short programs) that implement complex user interaction. The HTML language also supports hyperlinks (embedded URLs), which provide immediate access to other web resources. Web navigation, or web surfing, is the common practice of following such hyperlinks across multiple websites. Web applications are web pages that function as application software. The information on the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common domain name make up a website. A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organisations, government agencies, and individual users; and comprises an enormous amount of educational, entertainment, commercial, and government information. The World Wide Web has become the world's dominant information systems platform.[8][9][10][11] It is the primary tool that billions of people worldwide use to interact with the Internet.[4] The Web was invented by English computer scientist Tim Berners-Lee while working at CERN.[12][13] He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organisation, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common tree structure approach, used for instance in the existing CERNDOC documentation system and in the Unix filesystem, as well as approaches that relied on tagging files with keywords, as in the VAX/NOTES system. Instead, he adopted concepts he had put into practice with his private ENQUIRE system (1980), built at CERN. When he became aware of Ted Nelson's hypertext model (1965), in which documents can be linked in unconstrained ways through hyperlinks associated with \"hot spots\" embedded in the text, it helped to confirm the validity of his concept.[14][15] The model was later popularised by Apple's HyperCard system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to mutable data files, or even fire up programs on their server computer. He also conceived \"gateways\" that would allow access through the new system to documents organised in other ways (such as traditional computer file systems or the Usenet). Moreover, he insisted that the system should be decentralised, without any central control or coordination over the creation of links.[6][16][12][13] Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name.[6] He got a working system implemented by the end of 1990, including a browser called  WorldWideWeb (which became the name of the project and of the network) and an HTTP server running at CERN. As part of that development, he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format.[17] The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN and began to spread to other scientific and academic institutions. Within the next two years, there were 50 websites created.[18][19] CERN made the Web protocol and code available royalty free on 30 April 1993, enabling its widespread use.[20][21][22] After the NCSA released the Mosaic web browser later that year, the Web's popularity grew rapidly as thousands of websites sprang up in less than a year.[23][24] Mosaic was a graphical browser that could display inline images and submit forms that  were processed by the HTTPd server.[25][26] Marc Andreessen and Jim Clark founded Netscape the following year and released the Navigator browser, which introduced Java and JavaScript to the Web. It quickly became the dominant browser. Netscape became a public company in 1995, which triggered a frenzy for the Web and started the dot-com bubble.[27] Microsoft responded by developing its own browser, Internet Explorer, starting the browser wars. By bundling it with Windows, it became the dominant browser for 14 years.[28] Berners-Lee founded the World Wide Web Consortium (W3C) which created XML in 1996 and recommended replacing HTML with stricter XHTML.[29] In the meantime, developers began exploiting an IE feature called XMLHttpRequest to make Ajax applications and launched the Web 2.0 revolution. Mozilla, Opera, and Apple rejected XHTML and created the WHATWG which developed HTML5.[30] In 2009, the W3C conceded and abandoned XHTML.[31] In 2019, it ceded control of the HTML specification to the WHATWG.[32] The World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet.[33][34][35][11] Gopher was run by the University of Minnesota and the alternative to the World Wide Web. Tim Berners-Lee states that World Wide Web is officially spelled as three separate words, each capitalised, with no intervening hyphens.[44] Use of the www prefix has been declining, especially when web applications sought to brand their domain names and make them easily pronounceable. As the mobile web grew in popularity,[45] services like Gmail.com, Outlook.com, Myspace.com, Facebook.com and Twitter.com are most often mentioned without adding \"www.\" (or, indeed, \".com\") to the domain.[46] In English, www is usually read as double-u double-u double-u.[47] Some users pronounce it dub-dub-dub, particularly in New Zealand.[48] Stephen Fry, in his \"Podgrams\" series of podcasts, pronounces it wuh wuh wuh.[49] The English writer Douglas Adams once quipped in The Independent on Sunday (1999): \"The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for\".[50] The terms Internet and World Wide Web are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of computer networks interconnected through telecommunications and optical networking. In contrast, the World Wide Web is a global collection of documents and other resources, linked by hyperlinks and URIs. Web resources are accessed using HTTP or HTTPS, which are application-level Internet protocols that use the Internet transport protocols.[4] Viewing a web page on the World Wide Web normally begins either by typing the URL of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages\u2014and to move from one web page to another through hyperlinks\u2014came to be known as 'browsing,' 'web surfing' (after channel surfing), or 'navigating the Web'. Early studies of this new behaviour investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation, and targeted navigation.[51] The following example demonstrates the functioning of a web browser when accessing a page at the URL http://example.org/home.html. The browser resolves the server name of the URL (example.org) into an Internet Protocol address using the globally distributed Domain Name System (DNS). This lookup returns an IP address such as 203.0.113.4 or 2001:db8:2e::7334. The browser then requests the resource by sending an HTTP request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service, so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses port number 80 and, for HTTPS, it normally uses port number 443. The content of the HTTP request can be as simple as two lines of text: The computer receiving the HTTP request delivers it to the web server software listening for requests on port 80. If the web server can fulfil the request, it sends an HTTP response back to the browser indicating success: Followed by the content of the requested page. Hypertext Markup Language (HTML) for a basic web page might look like this: The web browser parses the HTML and interprets the markup (<title>, <p> for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media, scripts that affect page behaviour, and Cascading Style Sheets that affect page layout. The browser makes additional HTTP requests to the web server for these other Internet media types. As it receives its content from the web server, the browser progressively renders the page onto the screen as specified by its HTML and these additional resources. Hypertext Markup Language (HTML) is the standard markup language for creating web pages and web applications. With Cascading Style Sheets (CSS) and JavaScript, it forms a triad of cornerstone technologies for the World Wide Web.[52] Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document. HTML elements are the building blocks of HTML pages. With HTML constructs, images and other objects such as interactive forms may be embedded into the rendered page. HTML provides a means to create structured documents by denoting structural semantics for text such as headings, paragraphs, lists, links, quotes, and other items. HTML elements are delineated by tags, written using angle brackets. Tags such as <img /> and <input /> directly introduce content into the page. Other tags, such as <p>, surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page. HTML can embed programs written in a scripting language such as JavaScript, which affects the behaviour and content of web pages. Inclusion of CSS defines the look and layout of content. The World Wide Web Consortium (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997.[update][53] Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions, and other web resources. In the underlying HTML, a hyperlink is coded like this:\n<a href=\"http://example.org/home.html\">Example.org Homepage</a>. Such a collection of useful, related resources interconnected via hypertext links is dubbed a web of information. Publication on the Internet created what Tim Berners-Lee first called the WorldWideWeb (in its original CamelCase, which was subsequently discarded) in November 1990.[54] The hyperlink structure of the web is described by the webgraph: the nodes of the web graph correspond to the web pages (or URLs), the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called \"dead\" links. The ephemeral nature of the Web has prompted many efforts to archive websites. The Internet Archive, active since 1996, is the best known of such efforts. Many hostnames used for the World Wide Web begin with www because of the long-standing practice of naming Internet hosts according to the services they provide. The hostname of a web server is often www, in the same way that it may be ftp for an FTP server, and news or nntp for a Usenet news server. These hostnames appear as Domain Name System (DNS) or subdomain names, as in www.example.com. The use of www is not required by any technical or policy standard and many websites do not use it; the first web server was nxoc01.cern.ch.[55] According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of www as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending www to an institution's website domain name was subsequently copied.[56][better\u00a0source\u00a0needed] Many established websites still use the prefix, or they employ other subdomain names such as www2, secure or en for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the www subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different websites. The use of a subdomain name is useful for load balancing incoming web traffic by creating a CNAME record that points to a cluster of web servers. Since, currently[as of?], only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root.[57][dubious \u2013 discuss] When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix \"www\" to the beginning of it and possibly \".com\", \".org\" and \".net\" at the end, depending on what might be missing. For example, entering \"microsoft\" may be transformed to http://www.microsoft.com/ and \"openoffice\" to http://www.openoffice.org. This feature started appearing in early versions of Firefox, when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as Lynx.[58][unreliable source?] It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices.[59] The scheme specifiers http:// and https:// at the start of a web URI refer to Hypertext Transfer Protocol or HTTP Secure, respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted.[citation needed] A web page (also written as webpage) is a document that is suitable for the World Wide Web and web browsers. A web browser displays a web page on a monitor or mobile device. The term web page usually refers to what is visible, but may also refer to the contents of the computer file itself, which is usually a text file containing hypertext written in HTML or a comparable markup language. Typical web pages provide hypertext for browsing to other web pages via hyperlinks, often referred to as links. Web browsers will frequently have to access multiple web resource elements, such as reading style sheets, scripts, and images, while presenting each web page. On a network, a web browser can retrieve a web page from a remote web server. The web server may restrict access to a private network, such as a corporate intranet. The web browser uses the Hypertext Transfer Protocol (HTTP) to make such requests to the web server. A static web page is delivered exactly as stored, as web content in the web server's file system. In contrast, a dynamic web page is generated by a web application, usually driven by server-side software. Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email, etc. A static web page (sometimes called a flat page/stationary page) is a web page that is delivered to the user exactly as stored, in contrast to dynamic web pages which are generated by a web application. Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a web server to negotiate content-type or language of the document where such versions are available and the server is configured to do so. A server-side dynamic web page is a web page whose construction is controlled by an application server processing server-side scripts. In server-side scripting, parameters determine how the assembly of every new web page proceeds, including the setting up of more client-side processing. A client-side dynamic web page processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via Document Object Model, or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way. A dynamic web page is then reloaded by the user or by a computer program to change some variable content. The updated information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a dynamic web page update using Ajax technologies will neither create a page to go back to nor truncate the web browsing history forward of the displayed page. Using Ajax technologies, the end user gets one dynamic page managed as a single page in the web browser while the actual web content rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM, the DOM, for its client, from an application server. Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not static web pages, though it has fallen out of common use since the popularisation of AJAX, a term which is now itself rarely used. Client-side scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.[citation needed] JavaScript is a scripting language that was initially developed in 1995 by Brendan Eich, then of Netscape, for use within web pages.[60] The standardised version is ECMAScript.[60] To make web pages more interactive, some web applications also use JavaScript techniques such as Ajax (asynchronous JavaScript and XML). Client-side script is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly poll the server to check whether new information is available.[61] A website[62] is a collection of related web resources including web pages, multimedia content, typically identified with a common domain name, and published on at least one web server. Notable examples are wikipedia.org, google.com, and amazon.com. A website may be accessible via a public Internet Protocol (IP) network, such as the Internet, or a private local area network (LAN), by referencing a uniform resource locator (URL) that identifies the site. Websites can have many functions and can be used in various fashions; a website can be a personal website, a corporate website for a company, a government website, an organisation website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and social networking to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an intranet. Web pages, which are the building blocks of websites, are documents, typically composed in plain text interspersed with formatting instructions of Hypertext Markup Language (HTML, XHTML). They may incorporate elements from other websites with suitable markup anchors. Web pages are accessed and transported with the Hypertext Transfer Protocol (HTTP), which may optionally employ encryption (HTTP Secure, HTTPS) to provide security and privacy for the user. The user's application, often a web browser, renders the page content according to its HTML markup instructions onto a display terminal. Hyperlinking between web pages conveys to the reader the site structure and guides the navigation of the site, which often starts with a home page containing a directory of the site web content. Some websites require user registration or subscription to access content. Examples of subscription websites include many business sites, news websites, academic journal websites, gaming websites, file-sharing websites, message boards, web-based email, social networking websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services. End users can access websites on a range of devices, including desktop and laptop computers, tablet computers, smartphones, and smart TVs. A web browser (commonly referred to as a browser) is a software user agent for accessing information on the World Wide Web. To connect to a website's server and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer. In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into websites. The most popular browsers are Chrome, Safari, Edge, Samsung Internet and Firefox.[63] A Web server is server software, or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over HTTP and several other related protocols. The primary function of a web server is to store, process and deliver web pages to clients.[64] The communication between client and server takes place using the Hypertext Transfer Protocol (HTTP). Pages delivered are most frequently HTML documents, which may include images, style sheets and scripts in addition to the text content. A user agent, commonly a web browser or web crawler, initiates communication by making a request for a specific resource using HTTP and the server responds with the content of that resource or an error message if unable to do so. The resource is typically a real file on the server's secondary storage, but this is not necessarily the case and depends on how the web server is implemented. While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting web forms, including uploading of files. Many generic web servers also support  scripting using Active Server Pages (ASP), PHP (Hypertext Preprocessor), or other scripting languages. This means that the behaviour of the web server can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents dynamically (\"on-the-fly\") as opposed to returning static documents. The former is primarily used for retrieving or modifying information from databases. The latter is typically much faster and more easily cached but cannot deliver dynamic content. Web servers can also frequently be found embedded in devices such as printers, routers, webcams and serving only a local network. The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most operating systems). Optical networking is a sophisticated infrastructure that utilises optical fibre to transmit data over long distances, connecting countries, cities, and even private residences. The technology uses optical microsystems like tunable lasers, filters, attenuators, switches, and wavelength-selective switches to manage and operate these networks.[65][66] The large quantity of optical fibre installed throughout the world at the end of the twentieth century set the foundation of the Internet as it is used today. The information highway relies heavily on optical networking, a method of sending messages encoded in light to relay information in various telecommunication networks.[67] The Advanced Research Projects Agency Network (ARPANET) was one of the first iterations of the Internet, created in collaboration with universities and researchers in 1969.[68][69][70][71] However, access to the ARPANET was limited to researchers, and in 1985, the National Science Foundation founded the National Science Foundation Network (NSFNET), a program that provided supercomputer access to researchers.[71] Limited public access to the Internet led to pressure from consumers and corporations to privatise the network. In 1993, the US passed the National Information Infrastructure Act, which dictated that the National Science Foundation must hand over control of the optical capabilities to commercial operators.[72][73] The privatisation of the Internet and the release of the World Wide Web to the public in 1993 led to an increased demand for Internet capabilities. This spurred developers to seek solutions to reduce the time and cost of laying new fibre and increase the amount of information that can be sent on a single fibre, to meet the growing needs of the public.[74][75][76][77] In 1994, Pirelli S.p.A.'s optical components division introduced a wavelength-division multiplexing (WDM) system to meet growing demand for increased data transmission. This four-channel WDM technology allowed more information to be sent simultaneously over a single optical fibre, effectively boosting network capacity.[78][79] Pirelli wasn't the only company that developed a WDM system; another company, the Ciena Corporation (Ciena), created its own technology to transmit data more efficiently. David Huber, an optical networking engineer and entrepreneur Kevin Kimberlin founded Ciena in 1992.[80][81][82] Drawing on laser technology from Gordon Gould and William Culver of Optelecom, Inc., the company focused on utilising optical amplifiers to transmit data via light.[83][84][85] Under chief executive officer Pat Nettles, Ciena developed a dual-stage optical amplifier for dense wavelength-division multiplexing (DWDM), patented in 1997 and deployed on the Sprint network in 1996.[86][87][88][89][90] An HTTP cookie (also called web cookie, Internet cookie, browser cookie, or simply cookie) is a small piece of data sent from a website and stored on the user's computer by the user's web browser while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons, logging in, or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields, such as names, addresses, passwords, and credit card numbers. Cookies perform essential functions in the modern web. Perhaps most importantly, authentication cookies are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's web browser, and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a hacker, used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see cross-site scripting and cross-site request forgery for examples).[91] Tracking cookies, and especially third-party tracking cookies, are commonly used as ways to compile long-term records of individuals' browsing histories \u2013 a potential privacy concern that prompted European[92] and U.S. lawmakers to take action in 2011.[93][94] European law requires that all websites targeting European Union member states gain \"informed consent\" from users before storing non-essential cookies on their device. Google Project Zero researcher Jann Horn describes ways cookies can be read by intermediaries, like Wi-Fi hotspot providers. When in such circumstances, he recommends using the browser in private browsing mode (widely known as Incognito mode in Google Chrome).[95] A web search engine or Internet search engine is a software system that is designed to carry out web search (Internet search), which means to search the World Wide Web in a systematic way for particular information specified in a web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). The information may be a mix of web pages, images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories. Unlike web directories, which are maintained only by human editors, search engines also maintain real-time information by running an algorithm on a web crawler. Internet content that is not capable of being searched by a web search engine is generally described as the deep web. In 1990, Archie, the world's first search engine, was released. The technology was originally an index of File Transfer Protocol (FTP) sites, which was a method for moving files between a client and a server network.[96][97] This early search tool was superseded by more advanced engines like Yahoo! in 1995 and Google in 1998.[98][99] The deep web,[100] invisible web,[101] or hidden web[102] are parts of the World Wide Web whose contents are not indexed by standard web search engines. The opposite term to the deep web is the surface web, which is accessible to anyone using the Internet.[103] Computer scientist Michael K. Bergman is credited with coining the term deep web in 2001 as a search indexing term.[104] The content of the deep web is hidden behind HTTP forms,[105][106] and includes many very common uses such as web mail, online banking, and services that users must pay for, and which is protected by a paywall, such as video on demand, some online magazines and newspapers, among others. The content of the deep web can be located and accessed by a direct URL or IP address and may require a password or other security access past the public website page. A web cache is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a browser cache by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in online banking, or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Website designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise firewalls often cache Web resources requested by one user for the benefit of many users. Some search engines store cached content of frequently accessed websites. For criminals, the Web has become a venue to spread malware and engage in a range of cybercrime, including (but not limited to) identity theft, fraud, espionage, and intelligence gathering.[107] Web-based vulnerabilities now outnumber traditional computer security concerns,[108][109] and as measured by Google, about one in ten web pages may contain malicious code.[110] Most web-based attacks take place on legitimate websites, and most, as measured by Sophos, are hosted in the United States, China and Russia.[111] The most common of all malware threats is SQL injection attacks against websites.[112] Through HTML and URIs, the Web was vulnerable to attacks like cross-site scripting (XSS) that came with the introduction of JavaScript[113] and were exacerbated to some degree by Web 2.0 and Ajax web design that favours the use of scripts.[114] In one 2007 estimate, 70% of all websites are open to XSS attacks on their users.[115] Phishing is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5\u00a0billion in 2012.[116] Two of the well-known phishing methods are Covert Redirect and Open Redirect. Proposed solutions vary. Large security companies like McAfee already design governance and compliance suites to meet post-9/11 regulations,[117] and some, like Finjan Holdings have recommended active real-time inspection of programming code and all content regardless of its source.[107] Some have argued that for enterprises to see Web security as a business opportunity rather than a cost centre,[118] while others call for \"ubiquitous, always-on digital rights management\" enforced in the infrastructure to replace the hundreds of companies that secure data and networks.[119] Jonathan Zittrain has said users sharing responsibility for computing safety is far preferable to locking down the Internet.[120] Every time a client requests a web page, the server can identify the request's IP address. Web servers usually log IP addresses in a log file. Also, unless set not to do so, most web browsers record requested web pages in a viewable history feature, and usually cache much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide personally identifiable information is by using a virtual private network. A VPN encrypts traffic between the client and VPN server, and masks the original IP address, lowering the chance of user identification. When a web page asks for, and the user supplies, personally identifiable information\u2014such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses HTTP cookies, username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organisation can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their demographic profile. These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's terms and conditions and the local laws that apply, information from these profiles may be sold, shared, or passed to other organisations without the user being informed. For many ordinary people, this means little more than some unexpected emails in their inbox or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web. Social networking sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified with an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences. Online bullies may make use of personal information to harass or stalk users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners.[121] Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential facial recognition technology, it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web. Web standards include many interdependent standards and specifications, some of which govern aspects of the Internet, not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and web services. Considerations include the interoperability, accessibility and usability of web pages and websites. Web standards, in the broader sense, consist of the following: Web standards are not fixed sets of rules but are constantly evolving sets of finalised technical specifications of web technologies.[128] Web standards are developed by standards organisations\u2014groups of interested and often competing parties chartered with the task of standardisation\u2014not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of W3C specifications, the highest maturity level). There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with disabilities. These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change.[129] The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and equal opportunity to people with disabilities.[130] Tim Berners-Lee once noted, \"The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect.\"[129] Many countries regulate web accessibility as a requirement for websites.[131] International co-operation in the W3C Web Accessibility Initiative led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using assistive technology.[129][132] The W3C Internationalisation Activity assures that web technology works in all languages, scripts, and cultures.[133] Beginning in 2004 or 2005, Unicode gained ground and eventually in December 2007 surpassed both ASCII and Western European as the Web's most frequently used character map.[134] Originally RFC\u00a03986 allowed resources to be identified by URI in a subset of US-ASCII. RFC\u00a03987 allows more characters\u2014any character in the Universal Character Set\u2014and now a resource can be identified by IRI in any language.[135]",
      "ground_truth_chunk_ids": [
        "24_random_chunk1",
        "42_fixed_chunk1"
      ],
      "source_ids": [
        "S224",
        "S042"
      ],
      "category": "comparative",
      "id": 67
    },
    {
      "question": "Compare Longwood Bowl and French Revolution in one sentence each: what does each describe or study?",
      "ground_truth": "Longwood Bowl: The Longwood Bowl[1] was a men's and women's tennis tournament first played at the Longwood Cricket Club courts at Brookline, Massachusetts, United States from 1882 to 1949. The men's tournament was also known as the Longwood Challenge Bowl.[2] The first women's event was the Longwood Tennis Cup it later became known as the Longwood Bowl Invitational. In 1877 the Longwood Cricket Club was founded.[3] In 1881 the club held its first tennis tournament.[4] In 1882 the club held its first important tennis event the Longwood Cricket Club Tournament it was the precursor event to the Longwood Bowl also known as the Longwood Challenge Bowl tournament founded in 1891.[5] The men's event was held through till 1942 when it was discontinued, and the women's event continued on till 1949 before it was also abolished. The tournament was played for the entire time at Brookline, Massachusetts where Longwood Cricket Club's tennis courts are located.[6] In 1922 the club house and administrative center was moved to Newton, Massachusetts.[7] The winners of the men's tournament retain a permanent replica of the Longwood Bowl Trophy if they win it three times.[8] Incomplete roll included:[9] Incomplete roll French Revolution: The French Revolution[a] was a period of political and societal change in France that began with the Estates General of 1789 and ended with the Coup of 18 Brumaire on 9 November 1799. Many of the revolution's ideas are considered fundamental principles of liberal democracy,[1] and its values remain central to modern French political discourse.[2] It was caused by a combination of social, political, and economic factors which the existing regime proved unable to manage. Financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614. The representatives of the Third Estate broke away and re-constituted themselves as a National Assembly in June. The Storming of the Bastille in Paris on 14 July led to a series of radical measures by the Assembly, including the abolition of feudalism, state control over the Catholic Church in France, and issuing the Declaration of the Rights of Man and of the Citizen. The next three years were dominated by a struggle for political control. King Louis XVI's attempted flight to Varennes in June 1791 further discredited the monarchy, and military defeats after the outbreak of the French Revolutionary Wars in April 1792 led to the insurrection of 10 August 1792. As a result, the monarchy was replaced by the French First Republic in September, followed by the execution of Louis XVI himself in January 1793. After another revolt in June 1793, the constitution was suspended, and political power passed from the National Convention to the Committee of Public Safety, dominated by radical Jacobins led by Maximilien Robespierre. About 16,000 people were sentenced by the Revolutionary Tribunal and executed in the Reign of Terror, which ended in July 1794 with the Thermidorian Reaction. Weakened by external threats and internal opposition, the Committee of",
      "expected_answer": "Longwood Bowl: The Longwood Bowl[1] was a men's and women's tennis tournament first played at the Longwood Cricket Club courts at Brookline, Massachusetts, United States from 1882 to 1949. The men's tournament was also known as the Longwood Challenge Bowl.[2] The first women's event was the Longwood Tennis Cup it later became known as the Longwood Bowl Invitational. In 1877 the Longwood Cricket Club was founded.[3] In 1881 the club held its first tennis tournament.[4] In 1882 the club held its first important tennis event the Longwood Cricket Club Tournament it was the precursor event to the Longwood Bowl also known as the Longwood Challenge Bowl tournament founded in 1891.[5] The men's event was held through till 1942 when it was discontinued, and the women's event continued on till 1949 before it was also abolished. The tournament was played for the entire time at Brookline, Massachusetts where Longwood Cricket Club's tennis courts are located.[6] In 1922 the club house and administrative center was moved to Newton, Massachusetts.[7] The winners of the men's tournament retain a permanent replica of the Longwood Bowl Trophy if they win it three times.[8] Incomplete roll included:[9] Incomplete roll French Revolution: The French Revolution[a] was a period of political and societal change in France that began with the Estates General of 1789 and ended with the Coup of 18 Brumaire on 9 November 1799. Many of the revolution's ideas are considered fundamental principles of liberal democracy,[1] and its values remain central to modern French political discourse.[2] It was caused by a combination of social, political, and economic factors which the existing regime proved unable to manage. Financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614. The representatives of the Third Estate broke away and re-constituted themselves as a National Assembly in June. The Storming of the Bastille in Paris on 14 July led to a series of radical measures by the Assembly, including the abolition of feudalism, state control over the Catholic Church in France, and issuing the Declaration of the Rights of Man and of the Citizen. The next three years were dominated by a struggle for political control. King Louis XVI's attempted flight to Varennes in June 1791 further discredited the monarchy, and military defeats after the outbreak of the French Revolutionary Wars in April 1792 led to the insurrection of 10 August 1792. As a result, the monarchy was replaced by the French First Republic in September, followed by the execution of Louis XVI himself in January 1793. After another revolt in June 1793, the constitution was suspended, and political power passed from the National Convention to the Committee of Public Safety, dominated by radical Jacobins led by Maximilien Robespierre. About 16,000 people were sentenced by the Revolutionary Tribunal and executed in the Reign of Terror, which ended in July 1794 with the Thermidorian Reaction. Weakened by external threats and internal opposition, the Committee of Public Safety was replaced in November 1795 by the Directory. Its instability ended in 1799 with the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul. The Revolution resulted from multiple long-term and short-term factors, culminating in a social, economic, financial and political crisis in the late 1780s.[3][4][5] Combined with resistance to reform by the ruling elite and indecisive policy by Louis XVI and his ministers, the result was a crisis the state was unable to manage.[6][7] Between 1715 and 1789, the French population grew from 21 to 28 million, 20% of whom lived in towns or cities, Paris alone having over 600,000 inhabitants.[8] This was accompanied by a tripling in the size of the middle class, which comprised almost 10% of the population by 1789.[9] Despite increases in overall prosperity, its benefits were largely restricted to the rentier and mercantile classes, while the living standards fell for wage labourers and peasant farmers who rented their land.[10][11] Economic recession from 1785, combined with bad harvests in 1787 and 1788, led to high unemployment and food prices, causing a financial and political crisis.[3][12][13][14] While the state also experienced a debt crisis, the level of debt itself was not high compared with Britain's.[15] A significant problem was that tax rates varied widely from one region to another, were often different from the official amounts, and were collected inconsistently. The complexity and lack of accountability caused resentment among all taxpayers.[16][b] Attempts to simplify the system were blocked by the regional Parlements which approved financial policy. The resulting impasse led to the calling of the Estates General of 1789, which became radicalised by the struggle for control of public finances.[18] Louis XVI was willing to consider reforms, but he often backed down when faced with opposition from conservative elements within the nobility. Enlightenment critiques of social institutions were widely discussed among the educated French elite. At the same time, the American Revolution and the European revolts of the 1780s inspired public debate on issues such as patriotism, liberty, equality, and democracy. These shaped the response of the educated public to the crisis,[19] while scandals such as the Affair of the Diamond Necklace fuelled widespread anger at the court, nobility, and church officials.[20] France faced a series of budgetary crises during the 18th century as revenues failed to keep pace with expenditure.[21][22] Despite solid economic growth, the use of tax farmers meant this was not reflected in a proportional growth in state tax income.[21] As the nobility and Church benefited from a variety of exemptions, the tax burden fell mainly on the lower classes.[23] Reform was difficult because new tax laws had to be registered with regional judicial bodies or parlements that were able to block them. The king could impose laws by decree, but this risked open conflict with the parlements, the nobility, and those subject to new taxes.[24] France primarily used loans to fund the 1778 to 1783 Anglo-French War. Even after it ended, the monarchy continued to borrow heavily, and by 1788, half of state revenue went on servicing its debt.[25] In 1786, the French finance minister, Calonne, proposed reforms including a universal land tax, the abolition of grain controls and internal tariffs, and new provincial assemblies appointed by the king. The new taxes were rejected, first by a hand-picked Assembly of Notables dominated by the nobility, then by the parlements when submitted by Calonne's successor Brienne. The notables and parlements argued that the proposed taxes could only be approved by an Estates-General, a representative body that last met in 1614.[26] The conflict between the Crown and the parlements became a national political crisis. Both sides issued a series of public statements, the government arguing that it was combating privilege, and the parlement defending the ancient rights of the nation. Public opinion was firmly on the side of the parlements, and riots broke out in several towns. Brienne's attempts to raise new loans failed, and on 8 August 1788, he announced that the king would summon an Estates-General to convene the following May. Brienne resigned and was replaced by Jacques Necker.[27] In September 1788, the Parlement of Paris ruled that the Estates-General should convene in the same form as in 1614, meaning that the three estates would meet and vote separately, with votes counted by estate rather than by head. As a result, the clergy and nobility could combine to outvote the Third Estate, despite representing less than 5% of the population.[28][29] With the relaxation of censorship and laws against political clubs, a group of liberal nobles and middle class activists known as the Society of Thirty launched a campaign for the doubling of Third Estate representation and individual voting. The public debate sparked an average of 25 new political pamphlets published each week from 25 September 1788.[30] One of the most influential was written by Abb\u00e9 Siey\u00e8s. Titled What Is the Third Estate?, it denounced the privilege of the clergy and nobility, and argued the Third Estate represented the nation and should sit alone as a National Assembly. Activists such as Jean Joseph Mounier, Antoine Barnave and Maximilien Robespierre organised regional meetings, petitions and literature in support of these demands.[31] In December, the king agreed to double the representation of the Third Estate, but left the question of counting votes for the Estates-General to decide.[32] The Catholic Church in France was wealthy, owning nearly 10% of all land, as well as receiving annual tithes.[33] However, three-quarters of the 303 clergy elected were parish priests, many of whom earned less than unskilled labourers and had more in common with their poor parishioners than with the bishops of the first estate.[34] The Second Estate elected 322 deputies, representing about 400,000 men and women, who owned about 25% of the land and collected seigneurial dues and rents from their tenants. Most delegates were town-dwelling members of the noblesse d'\u00e9p\u00e9e, or traditional aristocracy. Courtiers and representatives of the noblesse de robe (those who derived rank from judicial or administrative posts) were underrepresented.[35] Of the 610 deputies of the Third Estate, about two-thirds held legal qualifications and almost half were venal office holders. Less than 100 were in trade or industry, and none were peasants or artisans.[36] To assist delegates, each region completed a list of grievances, known as Cahiers de dol\u00e9ances.[37] Tax inequality and seigneurial dues (feudal payments owed to landowners) headed the grievances in the cahiers de doleances for the estate.[38] On 5 May 1789, the Estates-General convened at Versailles, with Necker reiterating that each estate should decide separately how and when it would meet and vote in common with the other estates. On the following day, each estate was to separately verify the credentials of their representatives. The Third Estate, however, voted to invite the other estates to join them in verifying all the representatives of the Estates-General in common, and to agree that votes should be counted by head. Negotiations continued until 12 June when the Third Estate unilaterally began verifying its own members. On 17th, the Third Estate declared itself to be the National Assembly of France and that all existing taxes were illegal.[39] By 19 June, they had been joined by more than 100 members of the clergy.[40] Shaken by this challenge to his authority, the king agreed to a reform package he would present personally to the Estates-General. The Salle des \u00c9tats was closed to prepare for the joint session, but the members of the Estates-General were not informed in advance. Finding their meeting place closed next day, they took the so-called Tennis Court Oath, undertaking not to disperse until a constitution had been agreed.[41] At the royal session, Louis XVI announced a series of reforms and stated no new taxes or loans would be implemented without the consent of the Estates-General. However, he then undermined this by re-stating his original demand for all three to sit and vote separately. The Third Estate refused to leave the hall and reiterated their oath not to disperse until a constitution had been agreed. Over the next days more members of the clergy joined the National Assembly. On 27 June, faced with popular demonstrations and mutinies in his French Guards, Louis XVI commanded the members of the first and second estates to join the third in the National Assembly.[42] Even the limited reforms the king had announced went too far for Marie Antoinette and Louis' younger brother the Comte d'Artois. On their advice, Louis dismissed Necker again as chief minister on 11 July.[43] On 12 July, the Assembly went into a non-stop session following rumours that the king was planning to use the Swiss Guards to force it to close. The news brought crowds of protestors into the streets, and soldiers of the elite Gardes Fran\u00e7aises refused to disperse them.[44] On 14 July many of these soldiers joined a crowd attacking the Bastille, a royal fortress with large stores of arms and ammunition. Its governor, Bernard-Ren\u00e9 de Launay, surrendered after several hours of fighting that cost the lives of 83 attackers. Launay was taken to the H\u00f4tel de Ville, where he was killed and his head placed on a pike and paraded around the city. Although rumoured to hold many prisoners, the Bastille held only seven: four forgers, a lunatic, a failed assassin, and a deviant nobleman. Nevertheless, it was a potent symbol of the Ancien R\u00e9gime and it was demolished in the following weeks.[45] Bastille Day has become the French national holiday.[46] Alarmed by the prospect of losing control of the capital, Louis appointed the Marquis de Lafayette commander of the National Guard, with Jean-Sylvain Bailly as head of a new administrative structure known as the Commune. On 17 July, Louis visited Paris accompanied by 100 deputies, where he was greeted by Bailly and accepted a tricolore cockade to loud cheers. However, it was clear power had shifted from his court; he was welcomed as 'Louis XVI, father of the French and king of a free people.'[47] The short-lived unity enforced on the Assembly by a common threat quickly dissipated. Deputies argued over constitutional forms, while civil authority rapidly deteriorated. On 22 July, former Finance Minister Joseph Foullon and his son were lynched by a Parisian mob, and neither Bailly nor Lafayette could prevent it. In rural areas, wild rumours and paranoia resulted in the formation of militia and an agrarian insurrection known as the Great Fear.[48] The breakdown of law and order and frequent attacks on aristocratic property led much of the nobility to flee abroad. These \u00e9migr\u00e9s funded reactionary forces within France and urged foreign monarchs to back a counter-revolution.[49] In response, the Assembly published the August Decrees which abolished feudalism. Over 25% of French farmland was subject to feudal dues, providing the nobility with most of their income; these were now cancelled, along with church tithes. While their former tenants were supposed to pay them compensation, collecting it proved impossible, and the obligation was annulled in 1793.[50] Other decrees included equality before the law, opening public office to all, freedom of worship, and cancellation of special privileges held by provinces and towns.[51] With the suspension of the 13 regional parlements in November, the key institutional pillars of the old regime had all been abolished in less than four months. From its early stages, the Revolution therefore displayed signs of its radical nature; what remained unclear was the constitutional mechanism for turning intentions into practical applications.[52] On 9 July, the National Assembly declared itself the National Constituent Assembly[53] and appointed a committee to draft a constitution and statement of rights.[54] Twenty drafts were submitted, which were used by a sub-committee to create a Declaration of the Rights of Man and of the Citizen, with Mirabeau being the most prominent member.[55] The declaration was approved by the Assembly and published on 26 August as a statement of principle.[56] The Assembly now concentrated on the constitution. Mounier and his monarchist supporters advocated a bicameral system, with an upper house appointed by the king, who would also have the right to appoint ministers and veto legislation. On 10 September, the majority of the Assembly, led by Siey\u00e8s and Talleyrand, voted in favour of a single body, and the following day approved a \"suspensive veto\" for the king, meaning Louis could delay implementation of a law but not block it indefinitely. In October, the Assembly voted to restrict political rights, including voting rights, to \"active citizens\", defined as French males over the age of 25 who paid direct taxes equal to three days' labour. The remainder were designated \"passive citizens\", restricted to \"civil rights\", a distinction opposed by a significant minority, including the Jacobin clubs.[57][58] By mid-1790, the main elements of a constitutional monarchy were in place, although the constitution was not accepted by Louis until 1791.[59] Food shortages and the worsening economy caused frustration at the lack of progress and led to popular unrest in Paris. This came to a head in late September 1789, when the Flanders Regiment arrived in Versailles to reinforce the royal bodyguard and were welcomed with a formal banquet as was common practice. The radical press described this as a 'gluttonous orgy' and claimed the tricolour cockade had been abused, while the Assembly viewed their arrival as an attempt to intimidate them.[60] On 5 October, crowds of women assembled outside the H\u00f4tel de Ville, agitating against high food prices and shortages.[61] These protests quickly turned political, and after seizing weapons stored at the H\u00f4tel de Ville, some 7,000 of them marched on Versailles, where they entered the Assembly to present their demands. They were followed to Versailles by 15,000 members of the National Guard under Lafayette, who was virtually \"a prisoner of his own troops\".[62] When the National Guard arrived later that evening, Lafayette persuaded Louis that the safety of his family required their relocation to Paris. Next morning, some of the protestors broke into the royal apartments, searching for Marie Antoinette, who had escaped. They ransacked the palace, killing several guards. Order was eventually restored, and the royal family and Assembly left for Paris, escorted by the National Guard.[63] Louis had announced his acceptance of the August Decrees and the declaration, and his official title changed from 'King of France' to 'King of the French'.[64] Historian John McManners argues \"in eighteenth-century France, throne and altar were commonly spoken of as in close alliance; their simultaneous collapse ... would one day provide the final proof of their interdependence.\" One suggestion is that after a century of persecution, some French Protestants actively supported an anti-Catholic regime, a resentment fuelled by Enlightenment thinkers such as Voltaire.[65] Jean-Jacques Rousseau, considered a philosophical founder of the revolution,[66][67][68] wrote it was \"manifestly contrary to the law of nature... that a handful of people should gorge themselves with superfluities, while the hungry multitude goes in want of necessities.\"[69] The Revolution caused a massive shift of power from the Catholic Church to the state; although the extent of religious belief has been questioned, elimination of tolerance for religious minorities meant by 1789 being French also meant being Catholic.[70] The church was the largest individual landowner in France, controlling nearly 10% of all estates and levied tithes, effectively a 10% tax on income, collected from peasant farmers in the form of crops. In return, it provided a minimal level of social support.[71] The August Decrees abolished tithes, and on 2 November the Assembly confiscated all church property, the value of which was used to back a new paper currency known as assignats. In return, the state assumed responsibilities such as paying the clergy and caring for the poor, the sick and the orphaned.[72] On 13 February 1790, religious orders and monasteries were dissolved, while monks and nuns were encouraged to return to private life.[73] The Civil Constitution of the Clergy of 12 July 1790 made them employees of the state, established rates of pay, and developed a system for electing priests and bishops. Pope Pius VI and many French Catholics objected to this since it denied the authority of the Pope over the French church. In October, 30 bishops wrote a declaration denouncing the law, further fuelling opposition.[74] When clergy were required to swear loyalty to the Civil Constitution in November, it split the church between the 24% who complied and the majority who refused.[75] This stiffened popular resistance against state interference, especially in traditionally Catholic areas such as Normandy, Brittany and the Vend\u00e9e, where only a few priests took the oath and the civilian population turned against the revolution.[74] The result was state-led persecution of \"refractory clergy\", many of whom were forced into exile, deported, or executed.[76] The period from October 1789 to spring 1791 is usually seen as one of relative tranquility, when some of the most important legislative reforms were enacted. However, conflict over the source of legitimate authority was more apparent in the provinces, where officers of the Ancien R\u00e9gime had been swept away but not yet replaced by new structures. This was less obvious in Paris, since the National Guard made it the best policed city in Europe, but disorder in the provinces inevitably affected members of the Assembly.[77] Centrists led by Siey\u00e8s, Lafayette, Mirabeau and Bailly created a majority by forging consensus with monarchiens like Mounier, and independents including Adrien Duport, Barnave and Alexandre Lameth. At one end of the political spectrum, reactionaries like Cazal\u00e8s and Maury denounced the Revolution in all its forms, with radicals like Maximilien Robespierre at the other. He and Jean-Paul Marat opposed the criteria for \"active citizens\", gaining them substantial support among the Parisian proletariat, many of whom had been disenfranchised by the measure.[78] On 14 July 1790, celebrations were held throughout France commemorating the fall of the Bastille, with participants swearing an oath of fidelity to \"the nation, the law and the king.\" The F\u00eate de la F\u00e9d\u00e9ration in Paris was attended by the royal family, with Talleyrand performing a mass. Despite this show of unity, the Assembly was increasingly divided, while external players like the Paris Commune and National Guard competed for power. One of the most significant was the Jacobin club; originally a forum for general debate, by August 1790 it had over 150 members, split into different factions.[79] The Assembly continued to develop new institutions; in September 1790, the regional Parlements were abolished and their legal functions replaced by a new independent judiciary, with jury trials for criminal cases. However, moderate deputies were uneasy at popular demands for universal suffrage, labour unions and cheap bread, and over the winter of 1790 and 1791, they passed a series of measures intended to disarm popular radicalism. These included exclusion of poorer citizens from the National Guard, limits on use of petitions and posters, and the June 1791 Le Chapelier Law suppressing trade guilds and any form of worker organisation.[80] The traditional force for preserving law and order was the army, which was increasingly divided between officers, who largely came from the nobility, and ordinary soldiers. In August 1790, the loyalist General Bouill\u00e9 suppressed a serious mutiny at Nancy; although congratulated by the Assembly, he was criticised by Jacobin radicals for the severity of his actions. Growing disorder meant many professional officers either left or became \u00e9migr\u00e9s, further destabilising the institution.[81] Held in the Tuileries Palace under virtual house arrest, Louis XVI was urged by his brother and wife to re-assert his independence by taking refuge with Bouill\u00e9, who was based at Montm\u00e9dy with 10,000 soldiers considered loyal to the Crown.[82] The royal family left the palace in disguise on the night of 20 June 1791; late the next day, Louis was recognised as he passed through Varennes, arrested and taken back to Paris. The attempted escape had a profound impact on public opinion; since it was clear Louis had been seeking refuge in Austria, the Assembly now demanded oaths of loyalty to the regime and began preparing for war, while fear of 'spies and traitors' became pervasive.[83] Despite calls to replace the monarchy with a republic, Louis retained his position but was generally regarded with acute suspicion and forced to swear allegiance to the constitution. A new decree stated retracting this oath, making war upon the nation, or permitting anyone to do so in his name would be considered abdication. However, radicals led by Jacques Pierre Brissot prepared a petition demanding his deposition, and on 17 July, an immense crowd gathered in the Champ de Mars to sign. Led by Lafayette, the National Guard was ordered to \"preserve public order\" and responded to a barrage of stones by firing into the crowd, killing between 13 and 50 people.[84] The massacre badly damaged Lafayette's reputation; the authorities responded by closing radical clubs and newspapers, while their leaders went into exile or hiding, including Marat.[85] On 27 August, Emperor Leopold II and King Frederick William II of Prussia issued the Declaration of Pillnitz declaring their support for Louis and hinting at an invasion of France on his behalf. In reality, the meeting between Leopold and Frederick was primarily to discuss the partitions of Poland; the declaration was intended to satisfy Comte d'Artois and other French \u00e9migr\u00e9s, but the threat rallied popular support behind the regime.[86] Based on a motion proposed by Robespierre, existing deputies were barred from elections held in September for the French Legislative Assembly. Although Robespierre was one of those excluded, his support in the clubs gave him a political power base not available to Lafayette and Bailly, who resigned respectively as head of the National Guard and the Paris Commune. The new laws were gathered together in the 1791 Constitution, and submitted to Louis XVI, who pledged to defend it \"from enemies at home and abroad\". On 30 September, the Constituent Assembly was dissolved, and the Legislative Assembly convened the next day.[87] The Legislative Assembly is often dismissed by historians as an ineffective body, compromised by divisions over the role of the monarchy, an issue exacerbated when Louis attempted to prevent or reverse limitations on his powers.[88] At the same time, restricting the vote to those who paid a minimal amount of tax disenfranchised a significant proportion of the 6\u00a0million Frenchmen over 25, while only 10% of those able to vote actually did so. Finally, poor harvests and rising food prices led to unrest among the urban class known as sans-culottes, who saw the new regime as failing to meet their demands for bread and work.[89] This meant the new constitution was opposed by significant elements inside and outside the Assembly, itself split into three main groups. 264 members were affiliated with Barnave's Feuillants, constitutional monarchists who considered the Revolution had gone far enough, while another 136 were Jacobin leftists who supported a republic, led by Brissot and usually referred to as Brissotins.[90] The remaining 345 belonged to La Plaine, a centrist faction who switched votes depending on the issue, but many of whom shared doubts as to whether Louis was committed to the Revolution.[90] After he officially accepted the new Constitution, one recorded response was \"Vive le roi, s'il est de bon foi!\", or \"Long live the king \u2013 if he keeps his word\".[91] Although a minority in the Assembly, control of key committees allowed the Brissotins to provoke Louis into using his veto. They first managed to pass decrees confiscating \u00e9migr\u00e9 property and threatening them with the death penalty.[92] This was followed by measures against non-juring priests, whose opposition to the Civil Constitution led to a state of near civil war in southern France, which Barnave tried to defuse by relaxing the more punitive provisions. On 29 November, the Assembly approved a decree giving refractory clergy eight days to comply, or face charges of 'conspiracy against the nation', an act opposed even by Robespierre.[93] When Louis vetoed both, his opponents were able to portray him as opposed to reform in general.[94] Brissot accompanied this with a campaign for war against Austria and Prussia, often interpreted as a mixture of calculation and idealism. While exploiting popular anti-Austrianism, it reflected a genuine belief in exporting the values of political liberty and popular sovereignty.[95] Simultaneously, conservatives headed by Marie Antoinette also favoured war, seeing it as a way to regain control of the military, and restore royal authority. In December 1791, Louis made a speech in the Assembly giving foreign powers a month to disband the \u00e9migr\u00e9s or face war, an act greeted with enthusiasm by supporters, but suspicion from opponents.[96] Barnave's inability to build a consensus in the Assembly resulted in the appointment of a new government, chiefly composed of Brissotins. On 20 April 1792, the French Revolutionary Wars began when French armies attacked Austrian and Prussian forces along their borders, before suffering a series of disastrous defeats. In an effort to mobilise popular support, the government ordered non-juring priests to swear the oath or be deported, dissolved the Constitutional Guard and replaced it with 20,000 f\u00e9d\u00e9r\u00e9s; Louis agreed to disband the Guard, but vetoed the other two proposals, while Lafayette called on the Assembly to suppress the clubs.[97] Popular anger increased when details of the Brunswick Manifesto reached Paris on 1 August, threatening 'unforgettable vengeance' should any oppose the Allies in seeking to restore the power of the monarchy. On the morning of 10 August, a combined force of the Paris National Guard and provincial f\u00e9d\u00e9r\u00e9s attacked the Tuileries Palace, killing many of the Swiss Guards protecting it.[98] Louis and his family took refuge with the Assembly and shortly after 11:00 am, the deputies present voted to 'temporarily relieve the king', effectively suspending the monarchy.[99] In late August, elections were held for the National Convention. Restrictions on the franchise meant the number of votes cast fell to 3.3\u00a0million, versus 4\u00a0million in 1791, while intimidation was widespread.[100] The Brissotins split between moderate Girondins led by Brissot, and radical Montagnards, headed by Robespierre, Georges Danton and Jean-Paul Marat. While loyalties constantly shifted, voting patterns suggest roughly 160 of the 749 deputies can generally be categorised as Girondists, with another 200 Montagnards. The remainder were part of a centrist faction known as La Plaine, headed by Bertrand Bar\u00e8re, Pierre Joseph Cambon and Lazare Carnot.[101] In the September Massacres, between 1,100 and 1,600 prisoners held in Parisian jails were summarily executed, the vast majority being common criminals.[102] A response to the capture of Longwy and Verdun by Prussia, the perpetrators were largely National Guard members and f\u00e9d\u00e9r\u00e9s on their way to the front. While responsibility is still disputed, even moderates expressed sympathy for the action, which soon spread to the provinces. One suggestion is that the killings stemmed from concern over growing lawlessness, rather than political ideology.[103] On 20 September, the French defeated the Prussians at the Battle of Valmy, in what was the first major victory by the army of France during the Revolutionary Wars. Emboldened by this, on 22 September the Convention replaced the monarchy with the French First Republic and introduced a new calendar, with 1792 becoming \"Year One\".[104] The next few months were taken up with the trial of Citoyen Louis Capet, formerly Louis XVI. While evenly divided on the question of his guilt, members of the convention were increasingly influenced by radicals based within the Jacobin clubs and Paris Commune. The Brunswick Manifesto made it easy to portray Louis as a threat to the Revolution, especially when extracts from his personal correspondence showed him conspiring with Royalist exiles.[105] On 17 January 1793, Louis was sentenced to death for \"conspiracy against public liberty and general safety\". 361 deputies were in favour, 288 against, while another 72 voted to execute him, subject to delaying conditions. The sentence was carried out on 21 January on the Place de la R\u00e9volution, now the Place de la Concorde.[106] Conservatives across Europe called for the destruction of revolutionary France, and in February the Convention responded by declaring war on Britain and the Dutch Republic. Together with Austria and Prussia, these two countries were later joined by Spain, Portugal, Naples, and Tuscany in the War of the First Coalition.[107] The Girondins hoped war would unite the people behind the government and provide an excuse for rising prices and food shortages. Instead, they found themselves the target of popular anger and in what proved a disastrous strategic move, many left Paris for the provinces. The first conscription measure or lev\u00e9e en masse on 24 February sparked riots in the capital and other regional centres. Already unsettled by changes imposed on the church, in March the traditionally conservative and royalist Vend\u00e9e rose in revolt. On 18th, General Charles Fran\u00e7ois Dumouriez was defeated at Neerwinden and defected to the Austrians. Uprisings followed in Bordeaux, Lyon, Toulon, Marseille and Caen. The Republic seemed on the verge of collapse.[108] The crisis led to the creation on 6 April 1793 of the Committee of Public Safety, an executive committee accountable to the convention.[109] The Girondins made a fatal political error by indicting Marat before the Revolutionary Tribunal for allegedly directing the September massacres; he was quickly acquitted, further isolating the Girondins from the sans-culottes. When Jacques H\u00e9bert called for a popular revolt against the \"henchmen of Louis Capet\" on 24 May, he was arrested by the Commission of Twelve, a Girondin-dominated tribunal set up to expose 'plots'. In response to protests by the Commune, the Commission warned \"if by your incessant rebellions something befalls the representatives of the nation, Paris will be obliterated\".[108] Growing discontent allowed the clubs to mobilise against the Girondins. Backed by the Commune and elements of the National Guard, on 31 May they attempted to seize power in a coup. Although the coup failed, on 2 June the convention was surrounded by a crowd of up to 80,000, demanding cheap bread, unemployment pay and political reforms, including restriction of the vote to the sans-culottes, and the right to remove deputies at will.[110] Ten members of the commission and another twenty-nine members of the Girondin faction were arrested, and on 10 June, the Montagnards took over the Committee of Public Safety.[111] Meanwhile, a committee led by Robespierre's close ally Louis Antoine de Saint-Just was tasked with preparing a new constitution. Completed in only eight days, it was ratified by the convention on 24 June and contained radical reforms, including universal male suffrage. However, normal legal processes were suspended following the assassination of Marat on 13 July by the Girondist Charlotte Corday, which the Committee of Public Safety used as an excuse to take control. The 1793 Constitution was suspended indefinitely in October.[112] Key areas of focus for the new government included creating a new state ideology, economic regulation and winning the war.[113] They were helped by divisions among their internal opponents; while areas like the Vend\u00e9e and Brittany wanted to restore the monarchy, most supported the Republic but opposed the regime in Paris. On 17 August, the Convention voted a second lev\u00e9e en masse; despite initial problems in equipping and supplying such large numbers, by mid-October Republican forces had re-taken Lyon, Marseille and Bordeaux, while defeating Coalition armies at Hondschoote and Wattignies.[114] The new class of military leaders included a young colonel named Napoleon Bonaparte, who was appointed commander of artillery at the siege of Toulon thanks to his friendship with Augustin Robespierre. His success in that role resulted in promotion to the Army of Italy in April 1794, and the beginning of his rise to military and political power.[115] Although intended to bolster revolutionary fervour, the Reign of Terror rapidly degenerated into the settlement of personal grievances. At the end of July, the Convention set price controls on a wide range of goods, with the death penalty for hoarders. On 9 September, 'revolutionary groups' were established to enforce these controls, while the Law of Suspects on 17 September approved the arrest of suspected \"enemies of freedom\". This initiated what has become known as the \"Terror\". From September 1793 to July 1794, around 300,000 were arrested,[116] with some 16,600 people executed on charges of counter-revolutionary activity, while another 40,000 may have been summarily executed, or died awaiting trial.[117] Price controls made farmers reluctant to sell their produce in Parisian markets, and by early September the city was suffering acute food shortages. At the same time, the war increased public debt, which the Assembly tried to finance by selling confiscated property. However, few would buy assets that might be repossessed by their former owners, a concern that could only be alleviated by military victory. This meant the financial position worsened as threats to the Republic increased, while printing assignats to deal with the deficit further increased inflation.[118] On 10 October, the Convention recognised the Committee of Public Safety as the supreme Revolutionary Government and suspended the constitution until peace was achieved.[112] In mid-October, Marie Antoinette was convicted of a long list of crimes and guillotined; two weeks later, the Girondist leaders arrested in June were also executed, along with Philippe \u00c9galit\u00e9. The \"Terror\" was not confined to Paris, with over 2,000 killed in Lyons after its recapture.[119] At Cholet on 17 October, the Republican army won a decisive victory over the Vend\u00e9e rebels, and the survivors escaped into Brittany. A defeat at Le Mans on 23 December ended the rebellion as a major threat, although the insurgency continued until 1796. The extent of the repression that followed has been debated by French historians since the mid-19th century.[120] Between November 1793 and February 1794, over 4,000 were drowned in the Loire at Nantes under the supervision of Jean-Baptiste Carrier. Historian Reynald Secher claims that as many as 117,000 died between 1793 and 1796. Although those numbers have been challenged, Fran\u00e7ois Furet concludes it \"not only revealed massacre and destruction on an unprecedented scale, but a zeal so violent that it has bestowed as its legacy much of the region's identity.\"[121][c] At the height of the Terror, not even its supporters were immune from suspicion, leading to divisions within the Montagnard faction between radical H\u00e9bertists and moderates led by Danton.[d] Robespierre saw their dispute as de-stabilising the regime, and, as a deist, objected to the anti-religious policies advocated by the atheist H\u00e9bert, who was arrested and executed on 24 March with 19 of his colleagues.[125] To retain the loyalty of the remaining H\u00e9bertists, Danton was arrested and executed on 5 April with Camille Desmoulins, after a show trial that arguably did more damage to Robespierre than any other act in this period.[126] The Law of 22 Prairial (10 June) denied \"enemies of the people\" the right to defend themselves. Those arrested in the provinces were sent to Paris for judgment; from March to July, executions in Paris increased from 5 to 26 per day.[127] Many Jacobins ridiculed the festival of the Cult of the Supreme Being on 8 June, a lavish and expensive ceremony led by Robespierre, who was also accused of circulating claims he was a second Messiah. Relaxation of price controls and rampant inflation caused increasing unrest among the sans-culottes, but the improved military situation reduced fears the Republic was in danger. Fearing their own survival depended on Robespierre's removal, on 29 June three members of the Committee of Public Safety openly accused him of being a dictator.[128] Robespierre responded by refusing to attend Committee meetings, allowing his opponents to build a coalition against him. In a speech made to the convention on 26 July, he claimed certain members were conspiring against the Republic, an almost certain death sentence if confirmed. When he refused to provide names, the session broke up in confusion. That evening he repeated these claims at the Jacobins club, where it was greeted with demands for execution of the 'traitors'. Fearing the consequences if they did not act first, his opponents attacked Robespierre and his allies in the Convention next day. When Robespierre attempted to speak, his voice failed, one deputy crying \"The blood of Danton chokes him!\"[129] After the Convention authorised his arrest, he and his supporters took refuge in the Hotel de Ville, which was defended by elements of the National Guard. Other units loyal to the Convention stormed the building that evening and detained Robespierre, who severely injured himself attempting suicide. He was executed on 28 July with 19 colleagues, including Saint-Just and Georges Couthon, followed by 83 members of the Commune.[130] The Law of 22 Prairial was repealed, any surviving Girondists reinstated as deputies, and the Jacobin Club was closed and banned.[131] There are various interpretations of the Terror and the violence with which it was conducted. Furet argues that the intense ideological commitment of the revolutionaries and their utopian goals required the extermination of any opposition.[132] A middle position suggests violence was not inevitable but the product of a series of complex internal events, exacerbated by war.[133] The bloodshed did not end with the death of Robespierre; southern France saw a wave of revenge killings, directed against alleged Jacobins, Republican officials and Protestants. Although the victors of Thermidor asserted control over the Commune by executing their leaders, some of those closely involved in the \"Terror\" retained their positions. They included Paul Barras, later chief executive of the French Directory, and Joseph Fouch\u00e9, director of the killings in Lyon who served as Minister of Police under the Directory, the Consulate and Empire.[134] Despite his links to Augustin Robespierre, military success in Italy meant Bonaparte escaped censure.[135] The December 1794 Treaty of La Jaunaye ended the Chouannerie in western France by allowing freedom of worship and the return of non-juring priests.[136] This was accompanied by military success; in January 1795, French forces helped the Dutch Patriots set up the Batavian Republic, securing their northern border.[137] The war with Prussia was concluded in favour of France by the Peace of Basel in April 1795, while Spain made peace shortly thereafter.[138] However, the Republic still faced a crisis at home. Food shortages arising from a poor 1794 harvest were exacerbated in northern France by the need to supply the army in Flanders, while the winter was the worst since 1709.[139] By April 1795, people were starving, and the assignat was worth only 8% of its face value; in desperation, the Parisian poor rose again.[140] They were quickly dispersed and the main impact was another round of arrests, while Jacobin prisoners in Lyon were summarily executed.[141] A committee drafted the Constitution of the Year III, approved by plebiscite on 23 September 1795 and put into place on 27 September.[142] Largely designed by Pierre Daunou and Boissy d'Anglas, it established a bicameral legislature, intended to slow down the legislative process, ending the wild swings of policy under the previous unicameral systems. The Council of 500 was responsible for drafting legislation, which was reviewed and approved by the Council of Ancients, an upper house containing 250 men over the age of 40. Executive power was in the hands of five directors, selected by the Council of Ancients from a list provided by the lower house, with a five-year mandate.[143] Deputies were chosen by indirect election, a total franchise of around 5 million voting in primaries for 30,000 electors, or 0.6% of the population. Since they were also subject to stringent property qualification, it guaranteed the return of conservative or moderate deputies. In addition, rather than dissolving the previous legislature as in 1791 and 1792, the so-called 'law of two-thirds' ruled only 150 new deputies would be elected each year. The remaining 600 Conventionnels kept their seats, a move intended to ensure stability.[144] Jacobin sympathisers viewed the French Directory as a betrayal of the Revolution, while Bonapartists later justified Napoleon's coup by emphasising its corruption.[145] The regime also faced internal unrest, a weak economy, and an expensive war, while the Council of 500 could block legislation at will. Since the directors had no power to call new elections, the only way to break a deadlock was rule by decree or use force. As a result, the directory was characterised by \"chronic violence, ambivalent forms of justice, and repeated recourse to heavy-handed repression.\"[146] Retention of the Conventionnels ensured the Thermidorians held a majority in the legislature and three of the five directors, but they were increasingly challenged by the right. On 5 October, Convention troops led by Napoleon put down a royalist rising in Paris; when the first legislative elections were held two weeks later, over 100 of the 150 new deputies were royalists of some sort.[147] The power of the Parisian sans-culottes had been broken by the suppression of the May 1795 revolt; relieved of pressure from below, the Jacobin clubs became supporters of the directory, largely to prevent restoration of the monarchy.[148] Removal of price controls and a collapse in the value of the assignat led to inflation and soaring food prices. By April 1796, over 500,000 Parisians were unemployed, resulting in the May insurrection known as the Conspiracy of the Equals. Led by the revolutionary Fran\u00e7ois-No\u00ebl Babeuf, their demands included immediate implementation of the 1793 Constitution, and a more equitable distribution of wealth. Despite support from sections of the military, the revolt was easily crushed, while Babeuf and other leaders were executed.[149] Nevertheless, by 1799 the economy had been stabilised, and important reforms made allowing steady expansion of French industry. Many of these remained in place for much of the 19th century.[150] Prior to 1797, three of the five directors were firmly Republican; Barras, R\u00e9velli\u00e8re-L\u00e9peaux and Jean-Fran\u00e7ois Rewbell, as were around 40% of the legislature. The same percentage were broadly centrist or unaffiliated, along with two directors, \u00c9tienne-Fran\u00e7ois Letourneur and Lazare Carnot. Although only 20% were committed Royalists, many centrists supported the restoration of the exiled Louis XVIII in the belief this would bring peace.[151] The elections of May 1797 resulted in significant gains for the right, with Royalists Jean-Charles Pichegru elected president of the Council of 500, and Barth\u00e9lemy appointed a director.[152] With Royalists apparently on the verge of power, Republicans attempted a pre-emptive coup on 4 September. Using troops from Napoleon's Army of Italy under Pierre Augereau, the Council of 500 was forced to approve the arrest of Barth\u00e9lemy, Pichegru and Carnot. The elections were annulled, 63 leading Royalists deported to French Guiana, and laws were passed against \u00e9migr\u00e9s, Royalists and ultra-Jacobins. The removal of his conservative opponents opened the way for direct conflict between Barras and those on the left.[153] Fighting continued despite general war weariness, and the 1798 elections resulted in a resurgence in Jacobin strength. Napoleon's invasion of Egypt in July 1798 confirmed European fears of French expansionism, and the War of the Second Coalition began in November. Without a majority in the legislature, the directors relied on the army to enforce decrees and extract revenue from conquered territories. Generals like Napoleon and Barth\u00e9lemy Catherine Joubert became central to the political process, while both the army and directory became notorious for their corruption.[154] It has been suggested the directory collapsed because by 1799, many 'preferred the uncertainties of authoritarian rule to the continuing ambiguities of parliamentary politics'.[155] The architect of its end was Siey\u00e8s, who when asked what he had done during the Terror allegedly answered \"I survived\". Nominated to the directory, his first action was to remove Barras, with the help of allies including Talleyrand, and Napoleon's brother Lucien, president of the Council of 500.[156] On 9 November 1799, the coup of 18 Brumaire replaced the five directors with the French Consulate, which consisted of three members, Napoleon, Siey\u00e8s, and Roger Ducos. Most historians consider this the end point of the French Revolution.[157] The role of ideology in the Revolution is controversial with Jonathan Israel stating that the \"radical Enlightenment\" was the primary driving force of the Revolution.[158] Cobban, however, argues \"[t]he actions of the revolutionaries were most often prescribed by the need to find practical solutions to immediate problems, using the resources at hand, not by pre-conceived theories.\"[159] The identification of ideologies is complicated by the profusion of revolutionary clubs, factions and publications, absence of formal political parties, and individual flexibility in the face of changing circumstances.[160] In addition, although the Declaration of the Rights of Man was a fundamental document for all revolutionary factions, its interpretation varied widely.[161] While all revolutionaries professed their devotion to liberty in principle, \"it appeared to mean whatever those in power wanted.\"[162] For example, the liberties specified in the Rights of Man were limited by law when they might \"cause harm to others, or be abused\". Prior to 1792, Jacobins and others frequently opposed press restrictions on the grounds these violated a basic right.[163] However, the radical National Convention passed laws in September 1793 and July 1794 imposing the death penalty for offences such as \"disparaging the National Convention\", and \"misleading public opinion.\"[164] While revolutionaries also endorsed the principle of equality, few advocated equality of wealth since property was also viewed as a right.[165] The National Assembly opposed equal political rights for women,[166] while the abolition of slavery in the colonies was delayed until February 1794 because it conflicted with the property rights of slave owners, and many feared it would disrupt trade.[167] Political equality for male citizens was another divisive issue, with the 1791 constitution limiting the right to vote and stand for office to males over 25 who met a property qualification, so-called \"active citizens\". This restriction was opposed by many activists, including Robespierre, the Jacobins, and Cordeliers.[168] The principle that sovereignty resided in the nation was a key concept of the Revolution.[169] However, Israel argues this obscures ideological differences over whether the will of the nation was best expressed through representative assemblies and constitutions, or direct action by revolutionary crowds, and popular assemblies such as the sections of the Paris commune.[170] Many considered constitutional monarchy as incompatible with the principle of popular sovereignty,[171] but prior to 1792, there was a strong bloc with an ideological commitment to such a system, based on the writings of Thomas Hobbes, John Locke, Montesquieu and Voltaire.[172] Israel argues the nationalisation of church property and the establishment of the Constitutional Church reflected an ideological commitment to secularism, and a determination to undermine a bastion of old regime privilege.[173] While Cobban agrees the Constitutional Church was motivated by ideology, he sees its origins in the anti-clericalism of Voltaire and other Enlightenment figures.[174] Jacobins were hostile to formal political parties and factions which they saw as a threat to national unity and the general will, with \"political virtue\" and \"love of country\" key elements of their ideology.[175][176] They viewed the ideal revolutionary as selfless, sincere, free of political ambition, and devoted to the nation.[177] The disputes leading to the departure first of the Feuillants, then later the Girondists, were conducted in terms of the relative political virtue and patriotism of the disputants. In December 1793, all members of the Jacobin clubs were subject to a \"purifying scrutiny\", to determine whether they were \"men of virtue\".[178] The Revolution initiated a series of conflicts that began in 1792 and ended with Napoleon's defeat at Waterloo in 1815. In its early stages, this seemed unlikely; the 1791 Constitution specifically disavowed \"war for the purpose of conquest\", and although traditional tensions between France and Austria re-emerged in the 1780s, Emperor Joseph II cautiously welcomed the reforms. Austria was at war with the Ottomans, as were the Russians, while both were negotiating with Prussia over partitioning Poland. Most importantly, Britain preferred peace, and as Emperor Leopold II stated after the Declaration of Pillnitz, \"without England, there is no case\".[179] In late 1791, factions within the Assembly came to see war as a way to unite the country and secure the Revolution by eliminating hostile forces on its borders and establishing its \"natural frontiers\".[180] France declared war on Austria in April 1792 and issued the first conscription orders, with recruits serving for twelve months. By the time peace finally came in 1815, the conflict had involved every major European power as well as the United States, redrawn the map of Europe and expanded into the Americas, the Middle East, and the Indian Ocean.[181] From 1701 to 1801, the population of Europe grew from 118 to 187\u00a0million; combined with new mass production techniques, this allowed belligerents to support large armies, requiring the mobilisation of national resources. It was a different kind of war, fought by nations rather than kings, intended to destroy their opponents' ability to resist, but also to implement deep-ranging social change. While all wars are political to some degree, this period was remarkable for the emphasis placed on reshaping boundaries and the creation of entirely new European states.[182] In April 1792, French armies invaded the Austrian Netherlands but suffered a series of setbacks before victory over an Austrian-Prussian army at Valmy in September. After defeating a second Austrian army at Jemappes on 6 November, they occupied the Netherlands, areas of the Rhineland, Nice and Savoy. Emboldened by this success, in February 1793 France declared war on the Dutch Republic, Spain and Britain, beginning the War of the First Coalition.[183] However, the expiration of the 12-month term for the 1792 recruits forced the French to relinquish their conquests. In August, new conscription measures were passed, and by May 1794 the French army had between 750,000 and 800,000 men.[184] Despite high rates of desertion, this was large enough to manage multiple internal and external threats; for comparison, the combined Prussian-Austrian army was less than 90,000.[185] By February 1795, France had annexed the Austrian Netherlands, established their frontier on the left bank of the Rhine and replaced the Dutch Republic with the Batavian Republic, a satellite state. These victories led to the collapse of the anti-French coalition; Prussia made peace in April 1795, followed soon after by Spain, leaving Britain and Austria as the only major powers still in the war.[186] In October 1797, a series of defeats by Bonaparte in Italy led Austria to agree to the Treaty of Campo Formio, in which they formally ceded the Netherlands and recognised the Cisalpine Republic.[187] Fighting continued for two reasons; first, French state finances had come to rely on indemnities levied on their defeated opponents. Second, armies were primarily loyal to their generals, for whom the wealth achieved by victory and the status it conferred became objectives in themselves. Leading soldiers like Lazare Hoche, Jean-Charles Pichegru and Lazare Carnot wielded significant political influence and often set policy; Campo Formio was approved by Bonaparte, not the Directory, which strongly objected to terms it considered too lenient.[187] Despite these concerns, the Directory never developed a realistic peace programme, fearing the destabilising effects of peace and the consequent demobilisation of hundreds of thousands of young men. As long as the generals and their armies stayed away from Paris, they were happy to allow them to continue fighting, a key factor behind sanctioning Bonaparte's invasion of Egypt. This resulted in aggressive and opportunistic policies, leading to the War of the Second Coalition in November 1798.[188] In 1789, the most populous French colonies were Saint-Domingue (today Haiti), Martinique, Guadeloupe, the \u00cele Bourbon (R\u00e9union) and the \u00cele\u00a0de la France. These colonies produced commodities such as sugar, coffee and cotton for exclusive export to France. There were about 700,000 slaves in the colonies, of which about 500,000 were in Saint-Domingue. Colonial products accounted for about a third of France's exports.[189] In February 1788, the Society of the Friends of the Blacks was formed in France with the aim of abolishing slavery in the empire. In August 1789, colonial slave owners and merchants formed the rival Club de Massiac to represent their interests. When the Constituent Assembly adopted the Declaration of the Rights of Man and of the Citizen in August 1789, delegates representing the colonial landowners successfully argued that the principles should not apply in the colonies as they would bring economic ruin and disrupt trade. Colonial landowners also gained control of the Colonial Committee of the Assembly from where they exerted a powerful influence against abolition.[190][191] People of colour also faced social and legal discrimination in mainland France and its colonies, including a bar on their access to professions such as law, medicine and pharmacy.[192][e] In 1789\u201390, a delegation of free coloureds, led by Vincent Og\u00e9 and Julien Raimond, unsuccessfully lobbied the Assembly to end discrimination against this group. Og\u00e9 left for Saint-Domingue where an uprising against white landowners broke out in October 1790. The revolt failed, and Og\u00e9 was killed.[196][191] In May 1791, the National Assembly granted full political rights to coloureds born of two free parents but left the rights of freed slaves to be determined by the colonial assemblies. The assemblies refused to implement the decree and fighting broke out between the coloured population of Saint-Domingue and white colonists, each side recruiting slaves to their forces. A major slave revolt followed in August.[197] In March 1792, the Legislative Assembly responded to the revolt by granting citizenship to all free coloureds and sending two commissioners, L\u00e9ger-F\u00e9licit\u00e9 Sonthonax and \u00c9tienne Polverel, and 6,000 troops to Saint-Domingue to enforce the decree. On arrival in September, the commissioners announced that slavery would remain in force. Over 72,000 slaves were still in revolt, mostly in the north.[198] Brissot and his supporters envisaged an eventual abolition of slavery but their immediate concern was securing trade and the support of merchants for the revolutionary wars. After Brissot's fall, the new constitution of June 1793 included a new Declaration of the Rights of Man and the Citizen but excluded the colonies from its provisions. In any event, the new constitution was suspended until France was at peace.[199] In early 1793, royalist planters from Guadeloupe and Saint-Domingue formed an alliance with Britain. The Spanish supported insurgent slaves, led by Jean-Fran\u00e7ois Papillon and Georges Biassou, in the north of Saint-Domingue. White planters loyal to the republic sent representatives to Paris to convince the Jacobin controlled Convention that those calling for the abolition of slavery were British agents and supporters of Brissot, hoping to disrupt trade.[200] In June, the commissioners in Saint-Domingue freed 10,000 slaves fighting for the republic. As the royalists and their British and Spanish supporters were also offering freedom for slaves willing to fight for their cause, the commissioners outbid them by abolishing slavery in the north in August, and throughout the colony in October. Representatives were sent to Paris to gain the approval of the convention for the decision.[200][201] The Convention voted for the abolition of slavery in the colonies on 4 February 1794 and decreed that all residents of the colonies had the full rights of French citizens irrespective of colour.[202] An army of 1,000 sans-culottes led by Victor Hugues was sent to Guadeloupe to expel the British and enforce the decree. The army recruited former slaves and eventually numbered 11,000, capturing Guadeloupe and other smaller islands. Abolition was also proclaimed on Guyane. Martinique remained under British occupation, while colonial landowners in R\u00e9union and the \u00celes Mascareignes repulsed the republicans.[203] Black armies drove the Spanish out of Saint-Domingue in 1795, and the British troops withdrew in 1798.[204] In republican controlled areas from 1793 to 1799, freed slaves were required to work on their former plantations or for their former masters if they were in domestic service. They were paid a wage and gained property rights. Black and coloured generals were effectively in control of large areas of Guadeloupe and Saint-Domingue, including Toussaint Louverture in the north of Saint-Domingue, and Andr\u00e9 Rigaud in the south. Historian Fr\u00e9deric R\u00e9gent states that the restrictions on the freedom of employment and movement of former slaves meant that, \"only whites, persons of color already freed before the decree, and former slaves in the army or on warships really benefited from general emancipation.\"[203] Newspapers and pamphlets played a central role in stimulating and defining the Revolution. Prior to 1789, there have been a small number of heavily censored newspapers that needed a royal licence to operate, but the Estates General created an enormous demand for news, and over 130 newspapers appeared by the end of the year. Among the most significant were Marat's L'Ami du peuple and Elys\u00e9e Loustallot's Revolutions de Paris\u00a0[fr].[205] Over the next decade, more than 2,000 newspapers were founded, 500 in Paris alone. Most lasted only a matter of weeks but they became the main communication medium, combined with the very large pamphlet literature.[206] Newspapers were read aloud in taverns and clubs and circulated hand to hand. There was a widespread assumption that writing was a vocation, not a business, and the role of the press was the advancement of civic republicanism.[207] By 1793 the radicals were most active but initially the royalists flooded the country with their publication the \"L'Ami du Roi\u00a0[fr]\" (Friends of the King) until they were suppressed.[208] To illustrate the differences between the new Republic and the old regime, the leaders needed to implement a new set of symbols to be celebrated instead of the old religious and monarchical symbols. To this end, symbols were borrowed from historic cultures and redefined, while those of the old regime were either destroyed or reattributed acceptable characteristics. These revised symbols were used to instil in the public a new sense of tradition and reverence for the Enlightenment and the Republic.[209] \"La Marseillaise\" (French pronunciation: [la ma\u0281s\u025bj\u025b\u02d0z]) became the national anthem of France. The song was written and composed in 1792 by Claude Joseph Rouget de Lisle, and was originally titled \"Chant de guerre pour l'Arm\u00e9e du Rhin\". The French National Convention adopted it as the First Republic's anthem in 1795. It acquired its nickname after being sung in Paris by volunteers from Marseille marching on the capital. The song is the first example of the \"European march\" anthemic style, while the evocative melody and lyrics led to its widespread use as a song of revolution and incorporation into many pieces of classical and popular music. De Lisle was instructed to 'produce a hymn which conveys to the soul of the people the enthusiasm which it (the music) suggests.'[211] The guillotine remains \"the principal symbol of the Terror in the French Revolution.\"[212] Invented by a physician during the Revolution as a quicker, more efficient and more distinctive form of execution, the guillotine became a part of popular culture and historic memory. It was celebrated on the left as the people's avenger, for example in the revolutionary song La guillotine permanente,[213] and cursed as the symbol of the Terror by the right.[214] Its operation became a popular entertainment that attracted great crowds of spectators. Vendors sold programmes listing the names of those scheduled to die. Many people came day after day and vied for the best locations from which to observe the proceedings; knitting women (tricoteuses) formed a cadre of hardcore regulars, inciting the crowd. Parents often brought their children. By the end of the Terror, the crowds had thinned drastically. Repetition had staled even this most grisly of entertainments, and audiences grew bored.[215] Cockades were widely worn by revolutionaries beginning in 1789. They pinned the blue-and-red cockade of Paris onto the white cockade of the Ancien R\u00e9gime. Camille Desmoulins asked his followers to wear green cockades on 12 July 1789. The Paris militia, formed on 13 July, adopted a blue and red cockade. Blue and red are the traditional colours of Paris, and they are used on the city's coat of arms. Cockades with various colour schemes were used during the storming of the Bastille on 14 July.[216] The Liberty cap, also known as the Phrygian cap, or pileus, is a brimless, felt cap that is conical in shape with the tip pulled forward. It reflects Roman republicanism and liberty, alluding to the Roman ritual of manumission, in which a freed slave receives the bonnet as a symbol of his newfound liberty.[217] In August and October 1793, revolutionary authorities ordered the exhumation of the remains of members of the royal family buried at the Saint-Denis basilica, which had served as the principal burial site of French royalty since the early Middle Ages.[218][page\u00a0needed]The remains were reburied in mass graves.[219][page\u00a0needed] These acts reflected revolutionary hostility toward royal authority and the symbolic power of dynastic memory.[220][page\u00a0needed] Deprived of political rights by the Ancien R\u00e9gime, the Revolution initially allowed women to participate, although only to a limited degree. Activists included Girondists like Olympe de Gouges, author of the Declaration of the Rights of Woman and of the Female Citizen, and Charlotte Corday, killer of Marat. Others like Th\u00e9roigne de M\u00e9ricourt, Pauline L\u00e9on and the Society of Revolutionary Republican Women supported the Jacobins, staged demonstrations in the National Assembly and took part in the October 1789 March to Versailles. Despite this, the 1791 and 1793 constitutions denied them political rights and democratic citizenship.[221] In 1793, the Society of Revolutionary Republican Women campaigned for strict price controls on bread, and a law that would compel all women to wear the tricolour cockade. Although both demands were successful, in October the male-dominated Jacobins who then controlled the government denounced the Society as dangerous rabble-rousers and made all women's clubs and associations illegal. Organised women were permanently shut out of the French Revolution after 30 October 1793.[222] At the same time, especially in the provinces, women played a prominent role in resisting social changes introduced by the Revolution. This was particularly so in terms of the reduced role of the Catholic Church; for those living in rural areas, closing of the churches meant a loss of normality.[223] This sparked a counter-revolutionary movement led by women; while supporting other political and social changes, they opposed the dissolution of the Catholic Church and revolutionary cults like the Cult of the Supreme Being.[224] Olwen Hufton argues some wanted to protect the Church from heretical changes enforced by revolutionaries, viewing themselves as \"defenders of faith\".[225] Olympe de Gouges was an author whose publications emphasised that while women and men were different, this should not prevent equality under the law. In her Declaration of the Rights of Woman and of the Female Citizen she insisted women deserved rights, especially in areas concerning them directly, such as divorce and recognition of illegitimate children. Along with other Girondists, she was executed in November 1793 during the Terror. Madame Roland, also known as Manon or Marie Roland, was another important female activist whose political focus was not specifically women but other aspects of the government. A Girondist, her personal letters to leaders of the Revolution influenced policy; in addition, she often hosted political gatherings of the Brissotins, a political group which allowed women to join. She too was executed in November 1793.[226] The Revolution abolished many economic constraints imposed by the Ancien R\u00e9gime, including church tithes and feudal dues although tenants often paid higher rents and taxes.[227] All church lands were nationalised, along with those owned by Royalist exiles, which were used to back paper currency known as assignats, and the feudal guild system eliminated.[228] It also abolished the highly inefficient system of tax farming, whereby private individuals would collect taxes for a hefty fee. The government seized the foundations that had been set up (starting in the 13th century) to provide an annual stream of revenue for hospitals, poor relief, and education. The state sold the lands but typically local authorities did not replace the funding and so most of the nation's charitable and school systems were massively disrupted.[229] Between 1790 and 1796, industrial and agricultural output dropped, foreign trade plunged, and prices soared, forcing the government to finance expenditure by issuing ever increasing quantities assignats. When this resulted in escalating inflation, the response was to impose price controls and persecute private speculators and traders, creating a black market. Between 1789 and 1793, the annual deficit increased from 10% to 64% of gross national product, while annual inflation reached 3,500% after a poor harvest in 1794 and the removal of price controls. The assignats were withdrawn in 1796 but inflation continued until the introduction of the gold-based Franc germinal in 1803.[230] The French Revolution had a major impact on western history by ending feudalism in France and creating a path for advances in individual freedoms throughout Europe.[231][2] The revolution represented the most significant challenge to political absolutism up to that point in history and spread democratic ideals throughout Europe and ultimately the world.[232] Its impact on French nationalism was profound, while also stimulating nationalist movements throughout Europe.[233] Some modern historians argue the concept of the nation state was a direct consequence of the revolution.[234] As such, the revolution is often seen as marking the start of modernity and the modern period.[235] The long-term impact on France was profound, shaping politics, society, religion and ideas, and polarising politics for more than a century. Historian Fran\u00e7ois Aulard writes: \"From the social point of view, the Revolution consisted in the suppression of what was called the feudal system, in the emancipation of the individual, in greater division of landed property, the abolition of the privileges of noble birth, the establishment of equality, the simplification of life.... The French Revolution differed from other revolutions in being not merely national, for it aimed at benefiting all humanity.\"[236][title\u00a0missing] The revolution permanently crippled the power of the aristocracy and drained the wealth of the Church, although the two institutions survived. Hanson suggests the French underwent a fundamental transformation in self-identity, evidenced by the elimination of privileges and their replacement by intrinsic human rights.[237] After the collapse of the First French Empire in 1815, the French public lost many of the rights and privileges earned since the revolution, but remembered the participatory politics that characterised the period. According to Paul Hanson, \"Revolution became a tradition, and republicanism an enduring option.\"[238] The Revolution meant an end to arbitrary royal rule and held out the promise of rule by law under a constitutional order. Napoleon as emperor set up a constitutional system and the restored Bourbons were forced to retain one. After the abdication of Napoleon III in 1871, the French Third Republic was launched with a deep commitment to upholding the ideals of the Revolution.[239][240] The Vichy regime (1940\u20131944) tried to undo the revolutionary heritage but retained the republic. However, there were no efforts by the Bourbons, Vichy or any other government to restore the privileges that had been stripped away from the nobility in 1789. France permanently became a society of equals under the law.[238] Agriculture was transformed by the Revolution. With the breakup of large estates controlled by the Church and the nobility and worked by hired hands, rural France became more a land of small independent farms. Harvest taxes were ended, such as the tithe and seigneurial dues. Primogeniture was ended both for nobles and peasants, thereby weakening the family patriarch, and led to a fall in the birth rate since all children had a share in the family property.[241] Cobban argues the Revolution bequeathed to the nation \"a ruling class of landowners.\"[242] Economic historians are divided on the economic impact of the Revolution. One suggestion is the resulting fragmentation of agricultural holdings had a significant negative impact in the early years of 19th century, then became positive in the second half of the century because it facilitated the rise in human capital investments.[243] Others argue the redistribution of land had an immediate positive impact on agricultural productivity, before the scale of these gains gradually declined over the course of the 19th century.[244] In the cities, entrepreneurship on a small scale flourished, as restrictive monopolies, privileges, barriers, rules, taxes and guilds gave way. However, the British blockade virtually ended overseas and colonial trade, hurting the cities and their supply chains. Overall, the Revolution did not greatly change the French business system, and probably helped freeze in place the horizons of the small business owner. The typical businessman owned a small store, mill or shop, with family help and a few paid employees; large-scale industry was less common than in other industrialising nations.[245] Historians often see the impact of the Revolution as through the institutions and ideas exported by Napoleon. Economic historians Dan Bogart, Mauricio Drelichman, Oscar Gelderblom, and Jean-Laurent Rosenthal describe Napoleon's codified law as the French Revolution's \"most significant export.\"[246] According to Daron Acemoglu, Davide Cantoni, Simon Johnson, and James A. Robinson the French Revolution had long-term effects in Europe. They suggest that \"areas that were occupied by the French and that underwent radical institutional reform experienced more rapid urbanization and economic growth, especially after 1850. There is no evidence of a negative effect of French invasion.\"[247] The Revolution sparked intense debate in Britain. The Revolution Controversy was a \"pamphlet war\" set off by the publication of A Discourse on the Love of Our Country, a speech given by Richard Price to the Revolution Society on 4 November 1789, supporting the French Revolution. Edmund Burke responded in November 1790 with his own pamphlet, Reflections on the Revolution in France, attacking the French Revolution as a threat to the aristocracy of all countries.[248][249] William Coxe opposed Price's premise that one's country is principles and people, not the State itself.[250] Conversely, two seminal political pieces of political history were written in Price's favour, supporting the general right of the French people to replace their State. One of the first of these \"pamphlets\" into print was A Vindication of the Rights of Men by Mary Wollstonecraft. Wollstonecraft's title was echoed by Thomas Paine's Rights of Man, published a few months later. In 1792 Christopher Wyvill published Defence of Dr. Price and the Reformers of England, a plea for reform and moderation.[251] This exchange of ideas has been described as \"one of the great political debates in British history\".[252] In Ireland, the effect was to transform what had been an attempt by Protestant settlers to gain some autonomy into a mass movement led by the Society of United Irishmen involving Catholics and Protestants. It stimulated the demand for further reform throughout Ireland, especially in Ulster, and led to the Irish Rebellion of 1798, which was brutally suppressed by government troops.[253] The German reaction to the Revolution swung from favourable to antagonistic. At first it brought liberal and democratic ideas, the end of guilds, serfdom and the Jewish ghetto. It brought economic freedoms and agrarian and legal reform. Above all the antagonism helped stimulate and shape German nationalism.[254] France invaded Switzerland and turned it into the \"Helvetic Republic\" (1798\u20131803), a French puppet state. French interference with localism and traditions was deeply resented in Switzerland, although some reforms took hold and survived in the later period of restoration.[255][256] France invaded and occupied the region now known as Belgium between 1794 and 1814. The new government enforced reforms, incorporating the region into France. Resistance was strong in every sector, as Belgian nationalism emerged to oppose French rule. The French legal system, however, was adopted, with its equal legal rights, and abolition of class distinctions.[257] The Kingdom of Denmark adopted liberalising reforms in line with those of the French Revolution. Reform was gradual and the regime itself carried out agrarian reforms that had the effect of weakening absolutism by creating a class of independent peasant freeholders. Much of the initiative came from well-organised liberals who directed political change in the first half of the 19th century.[258] The Constitution of Norway of 1814 was inspired by the French Revolution[259] and was considered to be one of the most liberal and democratic constitutions at the time.[260] Initially, most people in the Province of Quebec were favourable toward the revolutionaries' aims. The Revolution took place against the background of an ongoing campaign for constitutional reform in the colony by Loyalist emigrants from the United States.[261] Public opinion began to shift against the Revolution after the Flight to Varennes and further soured after the September Massacres and the subsequent execution of Louis XVI.[262] French migration to the Canadas experienced a substantial decline during and after the Revolution. Only a limited number of artisans, professionals, and religious emigres were allowed to settle in the region during this period.[263] Most emigres settled in Montreal or Quebec City.[263] The influx of religious emigres also revitalised the local Catholic Church, with exiled priests establishing a number of parishes across the Canadas.[263] In the United States, the French Revolution deeply polarised American politics, and this polarisation led to the creation of the First Party System. In 1793, as war broke out in Europe, the Democratic-Republican Party led by former American minister to France Thomas Jefferson favored revolutionary France and pointed to the 1778 treaty that was still in effect. George Washington and his unanimous cabinet, including Jefferson, decided that the treaty did not bind the United States to enter the war. Washington proclaimed neutrality instead.[264] The first writings on the French revolution were near contemporaneous with events and mainly divided along ideological lines. These included Edmund Burke's conservative critique Reflections on the Revolution in France (1790) and Thomas Paine's response Rights of Man (1791).[265] From 1815, narrative histories dominated, often based on first-hand experience of the revolutionary years. By the mid-nineteenth century, more scholarly histories appeared, written by specialists and based on original documents and a more critical assessment of contemporary accounts.[266] Dupuy identifies three main strands in nineteenth century historiography of the Revolution. The first is represented by reactionary writers who rejected the revolutionary ideals of popular sovereignty, civil equality, and the promotion of rationality, progress and personal happiness over religious faith. The second stream is those writers who celebrated its democratic, and republican values. The third were liberals like Germaine de Sta\u00ebl and Guizot, who accepted the necessity of reforms establishing a constitution and the rights of man, but rejected state interference with private property and individual rights, even when supported by a democratic majority.[267] Jules Michelet was a leading 19th-century historian of the democratic republican strand, and Thiers, Mignet and Tocqueville were prominent in the liberal strand.[268] Hippolyte Taine's Origins of Contemporary France (1875\u20131894) was modern in its use of departmental archives, but Dupuy sees him as reactionary, given his contempt for the crowd, and Revolutionary values.[269] The broad distinction between conservative, democratic-republican and liberal interpretations of the Revolution persisted in the 20th-century, although historiography became more nuanced, with greater attention to critical analysis of documentary evidence.[269][270] Alphonse Aulard (1849\u20131928) was the first professional historian of the Revolution; he promoted graduate studies, scholarly editions, and learned journals.[271][272] His major work, The French Revolution, a Political History, 1789\u20131804 (1905), was a democratic and republican interpretation of the Revolution.[273] Socio-economic analysis and a focus on the experiences of ordinary people dominated French studies of the Revolution from the 1930s.[274] Georges Lefebvre elaborated a Marxist socio-economic analysis of the revolution with detailed studies of peasants, the rural panic of 1789, and the behaviour of revolutionary crowds.[275][276] Albert Soboul, also writing in the Marxist-Republican tradition, published a major study of the sans-culottes in 1958.[277] Alfred Cobban challenged Jacobin-Marxist social and economic explanations of the revolution in two important works, The Myth of the French Revolution (1955) and Social Interpretation of the French Revolution (1964). He argued the Revolution was primarily a political conflict, which ended in a victory for conservative property owners, a result which retarded economic development.[278][279] In their 1965 work, La Revolution fran\u00e7aise, Fran\u00e7ois Furet and Denis Richet also argued for the primacy of political decisions, contrasting the reformist period of 1789 to 1790 with the following interventions of the urban masses which led to radicalisation and an ungovernable situation.[280] From the 1990s, Western scholars largely abandoned Marxist interpretations of the revolution in terms of bourgeoisie-proletarian class struggle as anachronistic. However, no new explanatory model has gained widespread support.[235][281] The historiography of the Revolution has expanded into areas such as cultural and regional histories, visual representations, transnational interpretations, and decolonisation.[280]",
      "ground_truth_chunk_ids": [
        "79_random_chunk1",
        "22_fixed_chunk1"
      ],
      "source_ids": [
        "S279",
        "S022"
      ],
      "category": "comparative",
      "id": 68
    },
    {
      "question": "Compare List of census-designated places in West Virginia and Nutrition in one sentence each: what does each describe or study?",
      "ground_truth": "List of census-designated places in West Virginia: The United States Census Bureau separates places by incorporation for statistical purposes during its decennial census. To incorporate, communities may need to meet statutory requirements made by their respective state, such as thresholds in population or specificities relative to location.[a] Federally, the Census Bureau defines incorporated places as areas, whose boundaries do not cross state lines, that \"provide governmental functions for a concentration of people\", as opposed to \"minor civil [divisions], which generally ... provide services or administer an area without regard, necessarily, to population\".[5] Unincorporated communities, classified as census-designated places (CDPs), lack elected municipal officers and boundaries with legal status.[5] The Bureau identified 205 CDPs in the state of West Virginia at the 2020 census. The Municipal Code of West Virginia, which governs incorporation, requires applicant municipal corporations (places for incorporation) that cover an area more than 1 square mile (2.6 km2) to have a minimum of 500 inhabitants or freeholders per square mile, and those under 1 square mile to have at least 100 inhabitants or freeholders. Applicant areas must not reside within a municipality \"urban in character\", nor claim an area \"disproportionate to its number of inhabitants\".[6] Upon approval, the state classifies municipal corporations as a Class I city, with a population of more than fifty thousand, a Class II city, with a population between ten thousand and fifty thousand, a Class III city, with a population between two thousand and ten thousand, or a Class IV town or village, with a population of less than two thousand.[7] All municipalities can \"use a common seal\", defend, maintain, or institute a proceeding in court, and hold, take, purchase, or lease, as lessee, property for municipal purposes.[8] Of the fifty-five counties in West Virginia, Logan is home to the most CDPs, with twenty-five, followed by Fayette, with twenty, and Nutrition: Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life. The intake of these substances provides organisms with nutrients (divided into macro- and micro-) which can be metabolized to create energy and chemical structures; too much or too little of an essential nutrient can cause malnutrition. Nutritional science, the study of nutrition as a hard science, typically emphasizes human nutrition. The type of organism determines what nutrients it needs and how it obtains them. Organisms obtain nutrients by consuming organic matter, consuming inorganic matter, absorbing light, or some combination of these. Some can produce nutrients internally by consuming basic elements, while others must consume other organisms to obtain pre-existing nutrients. All forms of life require carbon, energy, and water as well as various other molecules. Animals require complex nutrients such as carbohydrates, lipids, and proteins, obtaining them by consuming other organisms. Humans have developed agriculture and cooking to replace foraging and advance human nutrition. Plants acquire nutrients through the soil and the atmosphere. Fungi absorb nutrients around them by breaking them down and absorbing them through the mycelium. Scientific analysis of food and nutrients began during the chemical revolution in the late 18th century. Chemists in the 18th and 19th centuries experimented with different elements and food sources to develop theories of nutrition.[1] Modern nutrition science began in the 1910s as individual micronutrients began to be identified. The first vitamin to be chemically identified was thiamine in 1926, and vitamin C was identified as a protection against scurvy in 1932.[2] The role of vitamins in nutrition was studied in the following decades. The first recommended dietary allowances for humans were developed to address fears of disease caused by food deficiencies during the Great Depression and the Second World War.[3] Due to",
      "expected_answer": "List of census-designated places in West Virginia: The United States Census Bureau separates places by incorporation for statistical purposes during its decennial census. To incorporate, communities may need to meet statutory requirements made by their respective state, such as thresholds in population or specificities relative to location.[a] Federally, the Census Bureau defines incorporated places as areas, whose boundaries do not cross state lines, that \"provide governmental functions for a concentration of people\", as opposed to \"minor civil [divisions], which generally ... provide services or administer an area without regard, necessarily, to population\".[5] Unincorporated communities, classified as census-designated places (CDPs), lack elected municipal officers and boundaries with legal status.[5] The Bureau identified 205 CDPs in the state of West Virginia at the 2020 census. The Municipal Code of West Virginia, which governs incorporation, requires applicant municipal corporations (places for incorporation) that cover an area more than 1 square mile (2.6\u00a0km2) to have a minimum of 500 inhabitants or freeholders per square mile, and those under 1 square mile to have at least 100 inhabitants or freeholders. Applicant areas must not reside within a municipality \"urban in character\", nor claim an area \"disproportionate to its number of inhabitants\".[6] Upon approval, the state classifies municipal corporations as a Class I city, with a population of more than fifty thousand, a Class II city, with a population between ten thousand and fifty thousand, a Class III city, with a population between two thousand and ten thousand, or a Class IV town or village, with a population of less than two thousand.[7] All municipalities can \"use a common seal\", defend, maintain, or institute a proceeding in court, and hold, take, purchase, or lease, as lessee, property for municipal purposes.[8] Of the fifty-five counties in West Virginia, Logan is home to the most CDPs, with twenty-five, followed by Fayette, with twenty, and Raleigh, with eighteen. The largest CDP by population is Teays Valley, with 14,350 residents, while Bowden, with 0 residents over 0.12 square miles (0.31\u00a0km2), represents the state's smallest CDP by both population and area.[9] Nutrition: Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life. The intake of these substances provides organisms with nutrients (divided into macro- and micro-) which can be metabolized to create energy and chemical structures; too much or too little of an essential nutrient can cause malnutrition. Nutritional science, the study of nutrition as a hard science, typically emphasizes human nutrition. The type of organism determines what nutrients it needs and how it obtains them. Organisms obtain nutrients by consuming organic matter, consuming inorganic matter, absorbing light, or some combination of these. Some can produce nutrients internally by consuming basic elements, while others must consume other organisms to obtain pre-existing nutrients. All forms of life require carbon, energy, and water as well as various other molecules. Animals require complex nutrients such as carbohydrates, lipids, and proteins, obtaining them by consuming other organisms. Humans have developed agriculture and cooking to replace foraging and advance human nutrition. Plants acquire nutrients through the soil and the atmosphere. Fungi absorb nutrients around them by breaking them down and absorbing them through the mycelium. Scientific analysis of food and nutrients began during the chemical revolution in the late 18th century. Chemists in the 18th and 19th centuries experimented with different elements and food sources to develop theories of nutrition.[1] Modern nutrition science began in the 1910s as individual micronutrients began to be identified. The first vitamin to be chemically identified was thiamine in 1926, and vitamin C was identified as a protection against scurvy in 1932.[2] The role of vitamins in nutrition was studied in the following decades. The first recommended dietary allowances for humans were developed to address fears of disease caused by food deficiencies during the Great Depression and the Second World War.[3] Due to its importance in human health, the study of nutrition has heavily emphasized human nutrition and agriculture, while ecology is a secondary concern.[4] Nutrients are substances that provide energy and physical components to the organism, allowing it to survive, grow, and reproduce. Nutrients can be basic elements or complex macromolecules. Approximately 30 elements are found in organic matter, with nitrogen, carbon, and phosphorus being the most important.[5] Macronutrients are the primary substances required by an organism, and micronutrients are substances required by an organism in trace amounts. Organic micronutrients are classified as vitamins, and inorganic micronutrients are classified as minerals. Over-nutrition of macronutrients is a major cause of obesity and increases the risk of developing various non-communicable diseases (NCDs), including type 2 diabetes, stroke, hypertension, coronary heart disease, osteoporosis, and some forms of cancer.[6] Nutrients can also be classified as essential or nonessential, with essential meaning the body cannot synthesize the nutrient on its own.[7] Nutrients are absorbed by the cells and used in metabolic biochemical reactions. These include fueling reactions that create precursor metabolites and energy, biosynthetic reactions that convert precursor metabolites into building block molecules, polymerizations that combine these molecules into macromolecule polymers, and assembly reactions that use these polymers to construct cellular structures.[5] Organisms can be classified by how they obtain carbon and energy. Heterotrophs are organisms that obtain nutrients by consuming the carbon of other organisms, while autotrophs are organisms that produce their own nutrients from the carbon of inorganic substances like carbon dioxide. Mixotrophs are organisms that can be heterotrophs and autotrophs, including some plankton and carnivorous plants. Phototrophs obtain energy from light, while chemotrophs obtain energy by consuming chemical energy from matter. Organotrophs consume other organisms to obtain electrons, while lithotrophs obtain electrons from inorganic substances, such as water, hydrogen sulfide, dihydrogen, iron(II), sulfur, or ammonium.[8] Prototrophs can create essential nutrients from other compounds, while auxotrophs must consume preexisting nutrients.[9] In nutrition, the diet of an organism is the sum of the foods it eats.[10] A healthy diet improves the physical and mental health of an organism. This requires ingestion and absorption of vitamins, minerals, essential amino acids from protein and essential fatty acids from fat-containing food. Carbohydrates, protein and fat play major roles in ensuring the quality of life, health and longevity of the organism.[11] Some cultures and religions have restrictions on what is acceptable for their diet.[12] A nutrient cycle is a biogeochemical cycle involving the movement of inorganic matter through a combination of soil, organisms, air or water, where they are exchanged in organic matter.[13] Energy flow is a unidirectional and noncyclic pathway, whereas the movement of mineral nutrients is cyclic. Mineral cycles include the carbon cycle, sulfur cycle, nitrogen cycle, water cycle, phosphorus cycle, and oxygen cycle, among others that continually recycle along with other mineral nutrients into productive ecological nutrition.[13] Biogeochemical cycles that are performed by living organisms and natural processes are water, carbon, nitrogen, phosphorus, and sulfur cycles.[14] Nutrient cycles allow these essential elements to return to the environment after being absorbed or consumed.[15] Without proper nutrient cycling, there would be a risk of change in oxygen levels, climate, and ecosystem function.[citation needed] Foraging is the process of seeking out nutrients in the environment. It may also be defined to include the subsequent use of the resources. Some organisms, such as animals and bacteria, can navigate to find nutrients, while others, such as plants and fungi, extend outward to find nutrients. Foraging may be random, in which the organism seeks nutrients without method, or it may be systematic, in which the organism can go directly to a food source.[16] Organisms are able to detect nutrients through taste or other forms of nutrient sensing, allowing them to regulate nutrient intake.[17] Optimal foraging theory is a model that explains foraging behavior as a cost\u2013benefit analysis in which an animal must maximize the gain of nutrients while minimizing the amount of time and energy spent foraging. It was created to analyze the foraging habits of animals, but it can also be extended to other organisms.[18] Some organisms are specialists that are adapted to forage for a single food source, while others are generalists that can consume a variety of food sources.[19] Nutrient deficiencies, known as malnutrition, occur when an organism does not have the nutrients that it needs. A deficiency is not the same as a nutrient inadequacy which occurs when the intake of nutrients is above the level of deficiency, but below the recommended dietary level. This may lead to hidden symptoms of nutrient deficiency that are difficult to identify.[20] Nutrient deficiency may be caused by a sudden decrease in nutrient intake or by an inability to absorb essential nutrients. Not only is malnutrition the result of a lack of necessary nutrients,[21] but it can also be a result of other illnesses and health conditions. When this occurs, an organism will adapt by reducing energy consumption and expenditure to prolong the use of stored nutrients. It will use stored energy reserves until they are depleted.[22] A balanced diet includes appropriate amounts of all essential and non-essential nutrients. These can vary by age, weight, sex, physical activity levels, and more. A lack of just one essential nutrient can cause bodily harm, just as an overabundance can cause toxicity. The Daily Reference Values keep the majority of people from nutrient deficiencies.[23] DRVs are not recommendations but a combination of nutrient references to educate professionals and policymakers on what the maximum and minimum nutrient intakes are for the average person.[24] Food labels also use DRVs as a reference to create safe nutritional guidelines for the average healthy person.[25] Animals are heterotrophs that consume other organisms to obtain nutrients. Herbivores are animals that eat plants, carnivores are animals that eat other animals, and omnivores are animals that eat both plants and other animals.[26] Many herbivores rely on bacterial fermentation to create digestible nutrients from indigestible plant cellulose, while obligate carnivores must eat animal meats to obtain certain vitamins or nutrients their bodies cannot otherwise synthesize. Animals generally have a higher requirement of energy in comparison to plants.[27] The macronutrients essential to animal life are carbohydrates, amino acids, and fatty acids.[7][28] All macronutrients except water are required by the body for energy, however, this is not their sole physiological function. The energy provided by macronutrients in food is measured in kilocalories, usually called Calories, where 1 Calorie is the amount of energy required to raise 1 kilogram of water by 1 degree Celsius.[29] Carbohydrates are molecules that store significant amounts of energy. Animals digest and metabolize carbohydrates to obtain this energy. Carbohydrates are typically synthesized by plants during metabolism, and animals have to obtain most carbohydrates from nature, as they have only a limited ability to generate them. They include sugars, oligosaccharides, and polysaccharides. Glucose is the simplest form of carbohydrate.[30] Carbohydrates are broken down to produce glucose and short-chain fatty acids, and they are the most abundant nutrients for herbivorous land animals.[31] Carbohydrates contain 4 calories per gram. Lipids provide animals with fats and oils. They are not soluble in water, and they can store energy for an extended period of time. They can be obtained from many different plant and animal sources. Most dietary lipids are triglycerides, composed of glycerol and fatty acids. Phospholipids and sterols are found in smaller amounts.[32] An animal's body will reduce the amount of fatty acids it produces as dietary fat intake increases, while it increases the amount of fatty acids it produces as carbohydrate intake increases.[33] Fats contain 9 calories per gram. Protein consumed by animals is broken down to amino acids, which would be later used to synthesize new proteins. Protein is used to form cellular structures, fluids,[34] and enzymes (biological catalysts). Enzymes are essential to most metabolic processes, as well as DNA replication, repair, and transcription.[35] Protein contains 4 calories per gram. Much of animal behavior is governed by nutrition. Migration patterns and seasonal breeding take place in conjunction with food availability, and courtship displays are used to display an animal's health.[36] Animals develop positive and negative associations with foods that affect their health, and they can instinctively avoid foods that have caused toxic injury or nutritional imbalances through a conditioned food aversion. Some animals, such as rats, do not seek out new types of foods unless they have a nutrient deficiency.[37] Early human nutrition consisted of foraging for nutrients, like other animals, but it diverged at the beginning of the Holocene with the Neolithic Revolution, in which humans developed agriculture to produce food. The Chemical Revolution in the 18th century allowed humans to study the nutrients in foods and develop more advanced methods of food preparation. Major advances in economics and technology during the 20th century allowed mass production and food fortification to better meet the nutritional needs of humans.[38] Human behavior is closely related to human nutrition, making it a subject of social science in addition to biology. Nutrition in humans is balanced with eating for pleasure, and optimal diet may vary depending on the demographics and health concerns of each person.[39] Social determinants of health (SDOH) and structural factors drive nutrition and diet-related health disparities.[40] Humans are omnivores that eat a variety of foods. Cultivation of cereals and production of bread has made up a key component of human nutrition since the beginning of agriculture. Early humans hunted animals for meat, and modern humans domesticate animals to consume their meat and eggs. The development of animal husbandry has also allowed humans in some cultures to consume the milk of other animals and process it into foods such as cheese. Other foods eaten by humans include nuts, seeds, fruits, and vegetables. Access to domesticated animals as well as vegetable oils has caused a significant increase in human intake of fats and oils. Humans have developed advanced methods of food processing that prevent contamination of pathogenic microorganisms and simplify the production of food. These include drying, freezing, heating, milling, pressing, packaging, refrigeration, and irradiation. Most cultures add herbs and spices to foods before eating to add flavor, though most do not significantly affect nutrition. Other additives are also used to improve the safety, quality, flavor, and nutritional content of food.[41] Humans obtain most carbohydrates as starch from cereals, though sugar has grown in importance.[30] Lipids can be found in animal fat, butterfat, vegetable oil, and leaf vegetables, and they are also used to increase flavor in foods.[32] Protein can be found in virtually all foods, as it makes up cellular material, though certain methods of food processing may reduce the amount of protein in a food.[42] Humans can also obtain energy from ethanol, which is both a food and a drug, but it provides relatively few essential nutrients and is associated with nutritional deficiencies and other health risks.[43] In humans, poor nutrition can cause deficiency-related diseases, such as blindness, anemia, scurvy, preterm birth, stillbirth and cretinism,[44] or nutrient-excess conditions, such as obesity[45] and metabolic syndrome.[46] Other conditions possibly affected by nutrition disorders include cardiovascular diseases,[47] diabetes,[48][49] and osteoporosis.[50] Undernutrition can lead to wasting in acute cases, and stunting of marasmus in chronic cases of malnutrition.[44] In domesticated animals, such as pets, livestock, and working animals, as well as other animals in captivity, nutrition is managed by humans through animal feed. Fodder and forage are provided to livestock. Specialized pet food has been manufactured since 1860, and subsequent research and development have addressed the nutritional needs of pets. Dog food and cat food in particular are heavily studied and typically include all essential nutrients for these animals. Cats are sensitive to some common nutrients, such as taurine, and require additional nutrients derived from meat. Large-breed puppies are susceptible to overnutrition, as small-breed dog food is more energy dense than they can absorb.[51] Most plants obtain nutrients through inorganic substances absorbed from the soil or the atmosphere. Carbon, hydrogen, oxygen, nitrogen, and sulfur are essential nutrients that make up organic material in a plant and allow enzymic processes. These are absorbed ions in the soil, such as bicarbonate, nitrate, ammonium, and sulfate, or they are absorbed as gases, such as carbon dioxide, water, oxygen gas, and sulfur dioxide. Phosphorus, boron, and silicon are used for esterification. They are obtained through the soil as phosphates, boric acid, and silicic acid, respectively. Other nutrients used by plants are potassium, sodium, calcium, magnesium, manganese, chlorine, iron, copper, zinc, and molybdenum.[52] Plants uptake essential elements from the soil through their roots and from the air (consisting of mainly nitrogen and oxygen) through their leaves. Nutrient uptake in the soil is achieved by cation exchange, wherein root hairs pump hydrogen ions (H+) into the soil through proton pumps. These hydrogen ions displace cations attached to negatively charged soil particles so that the cations are available for uptake by the root. In the leaves, stomata open to take in carbon dioxide and expel oxygen.[53] Although nitrogen is plentiful in the Earth's atmosphere, very few plants can use this directly. Most plants, therefore, require nitrogen compounds to be present in the soil in which they grow. This is made possible by the fact that largely inert atmospheric nitrogen is changed in a nitrogen fixation process to biologically usable forms in the soil by bacteria.[54] As these nutrients do not provide the plant with energy, they must obtain energy by other means. Green plants absorb energy from sunlight with chloroplasts and convert it to usable energy through photosynthesis.[55] Fungi are chemoheterotrophs that consume external matter for energy. Most fungi absorb matter through the root-like mycelium, which grows through the organism's source of nutrients and can extend indefinitely. The fungus excretes extracellular enzymes to break down surrounding matter and then absorbs the nutrients through the cell wall. Fungi can be parasitic, saprophytic, or symbiotic. Parasitic fungi attach and feed on living hosts, such as animals, plants, or other fungi. Saprophytic fungi feed on dead and decomposing organisms. Symbiotic fungi grow around other organisms and exchange nutrients with them.[56] Protists include all eukaryotes that are not animals, plants, or fungi, resulting in great diversity between them. Algae are photosynthetic protists that can produce energy from light. Several types of protists use mycelium similar to those of fungi. Protozoa are heterotrophic protists, and different protozoa seek nutrients in different ways. Flagellate protozoa use a flagellum to assist in hunting for food, and some protozoa travel via infectious spores to act as parasites.[57] Many protists are mixotrophic, having both phototrophic and heterotrophic characteristics. Mixotrophic protists will typically depend on one source of nutrients while using the other as a supplemental source or a temporary alternative when its primary source is unavailable.[58] Prokaryotes, including bacteria and archaea, vary greatly in how they obtain nutrients across nutritional groups. Prokaryotes can only transport soluble compounds across their cell envelopes, but they can break down chemical components around them. Some lithotrophic prokaryotes are extremophiles that can survive in nutrient-deprived environments by breaking down inorganic matter.[59] Phototrophic prokaryotes, such as cyanobacteria and Chloroflexia, can engage in photosynthesis to obtain energy from sunlight. This is common among bacteria that form in mats atop geothermal springs. Phototrophic prokaryotes typically obtain carbon from assimilating carbon dioxide through the Calvin cycle.[60] Some prokaryotes, such as Bdellovibrio and Ensifer, are predatory and feed on other single-celled organisms. Predatory prokaryotes seek out other organisms through chemotaxis or random collision, merge with the organism, degrade it, and absorb the released nutrients. Predatory strategies of prokaryotes include attaching to the outer surface of the organism and degrading it externally, entering the cytoplasm of the organism, or by entering the periplasmic space of the organism. Groups of predatory prokaryotes may forgo attachment by collectively producing hydrolytic enzymes.[61]",
      "ground_truth_chunk_ids": [
        "235_random_chunk1",
        "69_fixed_chunk1"
      ],
      "source_ids": [
        "S435",
        "S069"
      ],
      "category": "comparative",
      "id": 69
    },
    {
      "question": "Compare Abbas Shareef and Compiler in one sentence each: what does each describe or study?",
      "ground_truth": "Abbas Shareef: Abbas Shareef (Dhivehi: \u07a2\u07a6\u0787\u07b0\u0784\u07a7\u0790\u07b0 \u079d\u07a6\u0783\u07a9\u078a\u07b0; born 19??) is a Maldivian lawyer who is currently serving as Prosecutor General since September 2024 and had previously served as the Secretary to the President on Legal Affairs under president Mohamed Muizzu and a Judge at the High Court of the Maldives from March 2011 to October 2015. Abbas Shareef was born in Mal\u00e9, Maldives. He studied Bachelor of Laws at the Australian University of Tasmania in 2000.[1] Shareef was appointed a Judge at the High Court of the Maldives by then-president Mohamed Nasheed on 26 March 2011.[2] Prior to his appointment as a High Court judge, he held positions such as Assistant Legal Officer, Legal Officer and Assistant Director General at the President's Office. He further served as a member of the Judicial Service Commission appointed by the President, Deputy President of the Judicial Service Commission and Vice President of the Appeals Committee of the Football Association of Maldives.[3] On 22 June 2015, Shareef along with Azmiralda Zahir and Shuaib Hussain Zakariyya were transferred to the southern branch of the appellate court.[4] In October 2015, Shareef retired as a High Court judge.[5][6] Shareef was appointed as Secretary to the President on Legal Affairs on 17 November 2023.[7] On 12 September 2024, following the resignation of Hussain Shameem, President Mohamed Muizzu submitted the name of Shareef to the People's Majlis for parliamentary approval to appoint him as the Prosecutor General of the Maldives.[8][9] Shareef was approved by the parliament on 16 September 2024.[10][11][12] On 18 September 2024, Abbas was appointed as the Prosecutor General by President Mohamed Muizzu.[13][14] Compiler: In computing, a compiler is software that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.[1][2]: p1 [3] There are many different types of compilers which produce output in different useful forms. A cross-compiler produces code for a different CPU or operating system than the one on which the cross-compiler itself runs. A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimized compiler for a language. Related software include decompilers, programs that translate from low-level languages to higher level ones; programs that translate between high-level languages, usually called source-to-source compilers or transpilers; language rewriters, usually programs that translate the form of expressions without a change of language; and compiler-compilers, compilers that produce compilers (or parts of them), often in a generic and reusable way so as to be able to produce many differing compilers. A compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and machine specific code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[4] With respect to making source code runnable, an interpreter provides a similar function as a compiler, but via a different mechanism. An interpreter executes code without converting it to",
      "expected_answer": "Abbas Shareef: Abbas Shareef (Dhivehi: \u07a2\u07a6\u0787\u07b0\u0784\u07a7\u0790\u07b0 \u079d\u07a6\u0783\u07a9\u078a\u07b0; born 19??) is a Maldivian lawyer who is currently serving as Prosecutor General since September 2024 and had previously served as the Secretary to the President on Legal Affairs under president Mohamed Muizzu and a Judge at the High Court of the Maldives from March 2011 to October 2015. Abbas Shareef was born in Mal\u00e9, Maldives. He studied Bachelor of Laws at the Australian University of Tasmania in 2000.[1] Shareef was appointed a Judge at the High Court of the Maldives by then-president Mohamed Nasheed on 26 March 2011.[2] Prior to his appointment as a High Court judge, he held positions such as Assistant Legal Officer, Legal Officer and Assistant Director General at the President's Office. He further served as a member of the Judicial Service Commission appointed by the President, Deputy President of the Judicial Service Commission and Vice President of the Appeals Committee of the Football Association of Maldives.[3] On 22 June 2015, Shareef along with Azmiralda Zahir and Shuaib Hussain Zakariyya were transferred to the southern branch of the appellate court.[4] In October 2015, Shareef retired as a High Court judge.[5][6] Shareef was appointed as Secretary to the President on Legal Affairs on 17 November 2023.[7] On 12 September 2024, following the resignation of Hussain Shameem, President Mohamed Muizzu submitted the name of Shareef to the People's Majlis for parliamentary approval to appoint him as the Prosecutor General of the Maldives.[8][9] Shareef was approved by the parliament on 16 September 2024.[10][11][12] On 18 September 2024, Abbas was appointed as the Prosecutor General by President Mohamed Muizzu.[13][14] Compiler: In computing, a compiler is software that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.[1][2]:\u200ap1\u200a[3] There are many different types of compilers which produce output in different useful forms. A cross-compiler  produces code for a different CPU or operating system than the one on which the cross-compiler itself runs.  A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimized compiler for a language. Related software include decompilers, programs that translate from low-level languages to higher level ones; programs that translate between high-level languages, usually called source-to-source compilers or transpilers; language rewriters, usually programs that translate the form of expressions without a change of language; and compiler-compilers, compilers that produce compilers (or parts of them), often in a generic and reusable way so as to be able to produce many differing compilers. A compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and machine specific code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[4] With respect to making source code runnable, an interpreter provides a similar function as a compiler, but via a different mechanism. An interpreter executes code without converting it to machine code.[2]:\u200ap2\u200a Therefore, some interpreters execute source code while others execute an intermediate form such as bytecode. Hence a program compiled to native code tends to run faster than when interpreted. Environments with a bytecode-intermediate-form tends toward intermediate-speed. While Just-in-time compilation allows for native execution speed with a one-time startup processing time cost. For low-level programming languages, such as assembly and C, it is typical that they are compiled, especially when speed is a significant concern, rather than being cross-platform supported. So that for such languages, there are more one-to-one correspondences between the source code and the resulting machine code, making it easier for programmers to control the use of hardware. In theory; a programming language can be used via either a compiler or an interpreter, but in practice, each language tends to be used with only one or the other. Nonetheless, it is possible to write a compiler for a language that is commonly interpreted. For example, Common Lisp can be compiled to Java bytecode (and then interpreted by the Java virtual machine), as well as C code (then compiled to native machine code), or directly to native code. Theoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures.[5] Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process. It is usually more productive for a programmer to use a high-level language, so the development of high-level languages followed naturally from the capabilities offered by digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include: The sentences in a language may be defined by a set of rules called a grammar.[6] Backus\u2013Naur form (BNF) describes the syntax of \"sentences\" of a language. It was developed by John Backus and used for the syntax of Algol 60.[7] The ideas derive from the context-free grammar concepts by linguist Noam Chomsky.[8] \"BNF and its extensions have become standard tools for describing the syntax of programming notations. In many cases, parts of compilers are generated automatically from a BNF description.\"[9] Between 1942 and 1945, Konrad Zuse designed the first (algorithmic) programming language for computers called Plankalk\u00fcl (\"Plan Calculus\").  Zuse also envisioned a Planfertigungsger\u00e4t (\"Plan assembly device\") to automatically translate the mathematical formulation of a program into machine-readable punched film stock.[10] While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s.[11] APL is a language for mathematical computations. Between 1949 and 1951, Heinz Rutishauser proposed Superplan, a high-level language and automatic translator.[12] His ideas were later refined by Friedrich L. Bauer and Klaus Samelson.[13] High-level language design during the formative years of digital computing provided useful programming tools for a variety of applications: Compiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with the analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.[17] Some early milestones in the development of compiler technology: Early operating systems and software were written in assembly language. In the 1960s and early 1970s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C. BCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool.[30] Several compilers have been implemented, Richards' book provides insights to the language and its compiler.[31] BCPL was not only an influential systems programming language that is still used in research[32] but also provided a basis for the design of B and C languages. BLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W. A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970. Multics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbat\u00f3 from MIT.[33] Multics was written in the PL/I language developed by IBM and IBM User Group.[34] IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented.[35] For the first few years of the Multics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs.[36] EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.[37] Bell Labs left the Multics project in 1969, and developed a system programming language B based on BCPL concepts, written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix. Bell Labs started the development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs.[38] Initially, a front-end program to Bell Labs' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.[39][40] Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science.[41] Bell Labs became interested in OOP with the development of C++.[42] C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983.[43] The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew. In many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex. DARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf's CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target.[44] PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator. PQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure.[45] The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation.[46] Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the (since 1995, object-oriented) programming language Ada. The Ada STONEMAN document[a] formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with the American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the STONEMAN document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overall effort on Ada development.[47] Other Ada compiler efforts got underway in Britain at the University of York and in Germany at the University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation.[48] There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment. High-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.[49] \"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security.\"[50] The \"Compiler Research: The Next 50 Years\" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets. A compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end-to-end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets. In the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once. A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process. Classifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing much work and early computers did not have enough memory to contain one program that did all of this work. As a result, compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations. The ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal). In some cases, the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass. The disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once. Splitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program. Regardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end. This front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end.[51] Practical examples of this approach are the GNU Compiler Collection, Clang (LLVM-based C/C++ compiler),[52] and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends. The front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope. While the frontend can be a single monolithic function or program, as in a scannerless parser, it was traditionally implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly, the frontend is broken into three phases: lexical analysis (also known as lexing or scanning), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases, these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare. The main phases of the front end include the following: The middle end, also known as optimizer, performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code.[56] The middle end contains those optimizations that are independent of the CPU architecture being targeted. The main phases of the middle end include the following: Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation. The scope of compiler analysis and optimizations vary greatly; their scope may range from operating within a basic block, to whole procedures, or even the whole program.  There is a trade-off between the granularity of the optimizations and the cost of compilation.  For example, peephole optimizations are fast to perform during compilation but only affect a small local fragment of the code, and can be performed independently of the context in which the code fragment appears.  In contrast, interprocedural optimization requires more compilation time and memory space, but enable optimizations that are only possible by considering the behavior of multiple functions simultaneously. Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes. Due to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled. The back end is responsible for the CPU architecture specific optimizations and for code generation.[56] The main phases of the back end include the following: Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification.[58] Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler. Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language \u2013 for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.[59] Interpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a set of directly executed machine instructions is needed somewhere at the bottom of the execution stack (see machine language). Furthermore, for optimization compilers can contain interpreter functionality, and interpreters may include ahead of time compilation techniques. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further. Meta-tracing is an automated compiler synthesis approach which takes this further and can be used to synthesize a compiler from a language interpreter. Some language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4,[60] and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself. One classification of compilers is by the platform on which their generated code executes. This is known as the target platform. A native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment. The output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason, such compilers are not usually classified as native or cross compilers. The lower level language that is the target of a compiler may itself be a high-level programming language. C, viewed by some as a sort of portable assembly language, is frequently the target language of such compilers. For example, Cfront, the original compiler for C++, used C as its target language. The C code generated by such a compiler is usually not intended to be readable and maintained by humans, so indent style and creating pretty C intermediate code are ignored. Some of the features of C that make it a good target language include the #line directive, which can be generated by the compiler to support debugging of the original source, and the wide platform support available with C compilers. While a common compiler type outputs machine code, there are many other types: Assemblers, which translate human readable assembly language to the machine code instructions executed by hardware, are not considered compilers.[69][b] (The inverse program that translates machine code to assembly language is called a disassembler.)",
      "ground_truth_chunk_ids": [
        "28_random_chunk1",
        "152_fixed_chunk1"
      ],
      "source_ids": [
        "S228",
        "S152"
      ],
      "category": "comparative",
      "id": 70
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Richelieu is an unincorporated community in Logan County and Butler County, Kentucky,'?",
      "ground_truth": "Richelieu is an unincorporated community in Logan County and Butler County, Kentucky, United States.[1] Richelieu is located near Logan County's northeastern boundary with Butler County along Kentucky Route 1038. It is also located near the tripoint where Logan and Butler County boundaries meet with those of Warren County.[2] This Logan County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information. This Butler County, Kentucky state location article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Richelieu, Kentucky",
      "ground_truth_chunk_ids": [
        "52_random_chunk1"
      ],
      "source_ids": [
        "S252"
      ],
      "category": "inferential",
      "id": 71
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Bengaluru,[b] also known as Bangalore (its official name until 1 November 2014),'?",
      "ground_truth": "Bengaluru,[b] also known as Bangalore (its official name until 1 November 2014), is the capital and largest city of the southern Indian state of Karnataka. As per the 2011 census, the city had a population of 8.4 million, making it the third most populous city in India and the most populous in South India. The Bengaluru metropolitan area had a population of around 8.5 million, making it the fifth most populous urban agglomeration in the country. It is located towards the southern end of the Deccan Plateau, at an altitude of 900 m (3,000 ft) above sea level. The city is known as India's \"Garden City\", due to its parks and greenery. Archaeological artefacts indicate that the human settlement in the region happened as early as 4000 BCE. The first mention of the name \"Bengaluru\" is from an old Kannada stone inscription from 890 CE found at the Nageshwara Temple in Begur. The region was ruled by the Western Ganga dynasty since 350 CE, and became part of the Chola Empire in the early eleventh century CE. In the late Middle Ages, it formed a part of the Hoysala kingdom and then the Vijayanagara Empire. In 1537 CE, Kempe Gowda I, a feudal ruler under the Vijayanagara Empire, established a mud fort which is considered the foundation of the modern city of Bengaluru with the earlier established areas (petes) still in existence. After the fall of the Vijayanagara Empire, Kempe Gowda declared independence, and the city was expanded by his successors. In 1638 CE, an Adil Shahi army defeated Kempe Gowda III, and the city became a jagir (feudal estate) of Shahaji. The Mughals later captured the city and sold it to Chikka Devaraja Wodeyar, the Maharaja of the Kingdom of Mysore. After the death of Krishnaraja Wodeyar II in",
      "expected_answer": "Bengaluru",
      "ground_truth_chunk_ids": [
        "200_fixed_chunk1"
      ],
      "source_ids": [
        "S200"
      ],
      "category": "inferential",
      "id": 72
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'The Mobira Talkman 450 is a brick phone which is discontinued.[1] This'?",
      "ground_truth": "The Mobira Talkman 450 is a brick phone which is discontinued.[1] This mobile phone related article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "Mobira Talkman 450",
      "ground_truth_chunk_ids": [
        "51_random_chunk1"
      ],
      "source_ids": [
        "S251"
      ],
      "category": "inferential",
      "id": 73
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Myth is a genre of folklore consisting primarily of narratives that play'?",
      "ground_truth": "Myth is a genre of folklore consisting primarily of narratives that play a fundamental role in a society. For scholars, this is totally different from the ordinary sense of the term myth, meaning a belief that is not true, as the veracity of a piece of folklore is entirely irrelevant to determining whether it constitutes a myth.[1] Myths are often endorsed by religious and secular authorities, and may be natural or supernatural in character.[2] Many societies group their myths, legends, and history together, considering myths and legends to be factual accounts of their remote past.[6] In particular, creation myths take place in a primordial age when the world had not achieved its later form.[10] Origin myths explain how a society's customs, institutions, and taboos were established and sanctified.[2][8] National myths are narratives about a nation's past that symbolize the nation's values. There is a complex relationship between recital of myths and the enactment of rituals. The word myth comes from Ancient Greek \u03bc\u1fe6\u03b8\u03bf\u03c2 (m\u0233thos),[11] meaning 'speech', 'narrative', or 'fiction'. In turn, Ancient Greek \u03bc\u03c5\u03b8\u03bf\u03bb\u03bf\u03b3\u03af\u03b1 (mytholog\u00eda 'story', 'legends', or 'story-telling') combines the word m\u0233thos with the suffix -\u03bb\u03bf\u03b3\u03af\u03b1 (-logia 'study').[12] Accordingly, Plato used mytholog\u00eda as a general term for fiction or story-telling of any kind. This word was adapted into other European languages in the early 19th century, in a much narrower sense, as a scholarly term for \"[a] traditional story, especially one concerning the early history of a people or explaining a natural or social phenomenon, and typically involving supernatural beings or events.\"[13][14] The Greek term mytholog\u00eda was then borrowed into Late Latin, occurring in the title of Latin author Fabius Planciades Fulgentius' 5th-century Mythologi\u00e6 to denote what is now referred to as classical mythology\u2014i.e., Greco-Roman etiological stories involving their gods. Fulgentius's Mythologi\u00e6 explicitly treated its subject matter as allegories",
      "expected_answer": "Mythology",
      "ground_truth_chunk_ids": [
        "87_fixed_chunk1"
      ],
      "source_ids": [
        "S087"
      ],
      "category": "inferential",
      "id": 74
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Les Boullereaux-Champigny is a French railway station in Champigny-sur-Marne, Val-de-Marne department, at'?",
      "ground_truth": "Les Boullereaux-Champigny is a French railway station in Champigny-sur-Marne, Val-de-Marne department, at kilometric point 18.473 of the Paris-Est\u2013Mulhouse-Ville railway. The station opened on 13 January 1974 as the railway became electrified between Noisy-le-Sec and Tournan. It was renovated and platforms were raised when RER E service started on branch E4, on 30 August 1999. It is named after a district of Champigny-sur-Marne. The station is served in both directions by one train every 15 minutes off-peak, during peak times and at evening. More than 79 trains toward Haussmann\u2013Saint-Lazare and 78 trains toward Villiers-sur-Marne\u2013Le Plessis-Tr\u00e9vise station and Tournan (at evening) call at the station. Several buses stop near the station:",
      "expected_answer": "Les Boullereaux-Champigny station",
      "ground_truth_chunk_ids": [
        "85_random_chunk1"
      ],
      "source_ids": [
        "S285"
      ],
      "category": "inferential",
      "id": 75
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Vanda wightii is a species of orchid from southern India and Sri'?",
      "ground_truth": "Vanda wightii is a species of orchid from southern India and Sri Lanka. For some time it was thought to be extinct after being described in 1849. The species was however rediscovered and it is now known from India (Kerala, Karnataka, Tamil Nadu). It is closely related to Vanda thwaitesii.[3] The species was named by Reichenback after the botanist and collector Robert Wight.[4] 1. They are epiphytes with stem 15\u201322 cm long, profusely rooting at the base, leaves closely arranged above. Leaves 7-35 x 1.1\u20132 cm, jointed at base, deeply channeled in the middle, V-shaped in cross section, keeled, thick, fleshy and coriaceous, unequally bilobed at apex. Inflorescence a simple and axillary raceme, 6\u201317 cm long with 2.5\u20133 mm thick peduncle; sterile bracts 1\u20132, tubular, covering the inflorescence axis; pedicel+ovary 4\u20135 cm long; floral bracts 2-2.5 x 4.5-5.5, triangular ovate, gland dotted, acute at apex. Flowers 2\u20133, pleasantly fragrant at dusk, reminding strongly the smell of Vanda tessellata /Cestrum nocturnum, 4-4.5 cm across, dirty brownish greenish yellow with light tessellations on sepals and petals, white at base and back side; dorsal sepal 2.1 -2.4 x 1.1-1.5 cm, ovate-oblong, clawed and obtuse at base, slightly wavy on margins, 7-veined, side veins branched, all veins connected by intravenal bridges; lateral sepals 2.1-2.5 x 1.4-1.5 cm, elliptic, obovate, obtuse at apex, 7-veined, margin at the basal portion backwardly folded; petals 2-2.4 x 1.2-1.3 cm, obovate-oblong, 5-veined, clawed at base, obtusely acute at apex; lip white throughout, with a flash of yellow at inner back wall of sac between the side lobes; 1.3 cm long with 1 cm long saccate spur, immovably attached to the base of a short column foot; side lobes 0.5x 0.4 cm, oblong, midlobe 1.1 x 0.9 cm, thick, fleshy with 2 keels, medianly deep channeled; spur 0.9-1 x 0.6",
      "expected_answer": "Vanda wightii",
      "ground_truth_chunk_ids": [
        "181_random_chunk1"
      ],
      "source_ids": [
        "S381"
      ],
      "category": "inferential",
      "id": 76
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Georg M\u00fcller may refer to:'?",
      "ground_truth": "Georg M\u00fcller may refer to:",
      "expected_answer": "Georg M\u00fcller",
      "ground_truth_chunk_ids": [
        "150_random_chunk1"
      ],
      "source_ids": [
        "S350"
      ],
      "category": "inferential",
      "id": 77
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Existentialism is a family of philosophical views and inquiry that explore the'?",
      "ground_truth": "Existentialism is a family of philosophical views and inquiry that explore the human individual's struggle to lead an authentic life despite the apparent absurdity or incomprehensibility of existence.[1][2][3] In examining meaning, purpose, and value, existentialist thought often includes concepts such as existential crises, angst, courage, and freedom.[4] Existentialism is associated with several 19th- and 20th-century European philosophers who shared an emphasis on the human subject, despite often profound differences in thought.[5][6][7] Among the 19th-century figures now associated with existentialism are philosophers S\u00f8ren Kierkegaard and Friedrich Nietzsche, as well as novelist Fyodor Dostoevsky, all of whom critiqued rationalism and concerned themselves with the problem of meaning. The word existentialism, however, was not coined until the mid 20th century, during which it became most associated with contemporaneous philosophers Jean-Paul Sartre, Martin Heidegger, Simone de Beauvoir, Karl Jaspers, Gabriel Marcel, Paul Tillich, and more controversially Albert Camus. Many existentialists considered traditional systematic or academic philosophies, in style and content, to be too abstract and removed from concrete human experience.[8][9] A primary virtue in existentialist thought is authenticity.[10] Existentialism would influence many disciplines outside of philosophy, including theology, drama, art, literature, and psychology.[11] Existentialist philosophy encompasses a range of perspectives, but it shares certain underlying concepts. Among these, a central tenet of existentialism is that personal freedom, individual responsibility, and deliberate choice are essential to the pursuit of self-discovery and the determination of life's meaning.[12] The term existentialism (French: L'existentialisme) was coined by the French Catholic philosopher Gabriel Marcel in the mid-1940s.[13][14][15] When Marcel first applied the term to Jean-Paul Sartre, at a colloquium in 1945, Sartre rejected it.[16] Sartre subsequently changed his mind and, on October 29, 1945, publicly adopted the existentialist label in a lecture to the Club Maintenant in Paris, published as L'existentialisme est un humanisme (Existentialism Is a Humanism), a",
      "expected_answer": "Existentialism",
      "ground_truth_chunk_ids": [
        "131_fixed_chunk1"
      ],
      "source_ids": [
        "S131"
      ],
      "category": "inferential",
      "id": 78
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'The 1980 World's Strongest Man was the fourth edition of World's Strongest'?",
      "ground_truth": "The 1980 World's Strongest Man was the fourth edition of World's Strongest Man and was won by Bill Kazmaier from the United States. It was his first title after finishing third the previous year. Lars Hedlund from Sweden finished second after finishing second the previous year, and Geoff Capes from the United Kingdom finished third. Defending champion Don Reinhoudt was forced to retire from the competition due to injury in his final World's Strongest Man. The contest was held in the United States for the fourth consecutive year, at the Vernon Valley/Great Gorge Resort in Vernon, New Jersey.[1] There were a total of 10 different events used in the competition: The American Bill Kazmaier dominated the competition by winning 6 events, getting two second places, one third and one fourth place: He tied for first in the Log lift, together with Lars Hedlund. He won the engine race, the steel bar bend, the Girl Squat lift, the Silver Dollar Deadlift and the final tug of war. He came in second in the weight toss and the fridge Carry. He took third in the battery hold and got fourth in the truck pull.",
      "expected_answer": "1980 World's Strongest Man",
      "ground_truth_chunk_ids": [
        "162_random_chunk1"
      ],
      "source_ids": [
        "S362"
      ],
      "category": "inferential",
      "id": 79
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'United Nations Security Council Resolution 74 was adopted on 16 September 1949.'?",
      "ground_truth": "United Nations Security Council Resolution 74 was adopted on 16 September 1949. Having received and examined a letter from the Chairman of the Atomic Energy Commission (AEC) transmitting two resolutions, the Security Council directed the Secretary-General to transmit this letter and the accompanying resolutions, along with records of the discussion of the issue within the AEC, to the General Assembly and the Member States. The resolution passed by nine votes in favour, with abstentions from the Ukrainian SSR and Soviet Union.",
      "expected_answer": "United Nations Security Council Resolution 74",
      "ground_truth_chunk_ids": [
        "77_random_chunk1"
      ],
      "source_ids": [
        "S277"
      ],
      "category": "inferential",
      "id": 80
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'God Loves Ugly is the second studio album by American hip hop'?",
      "ground_truth": "God Loves Ugly is the second studio album by American hip hop group Atmosphere. It was released on Rhymesayers Entertainment on June 11, 2002.[1] The album, via distribution with Fat Beats, went on to sell more than 130,000 copies in the United States.[2] The single from the album, titled \"Modern Man's Hustle\", reached #18 on the Billboard Hot Rap Tracks chart.[2] On January 20, 2009, God Loves Ugly was re-released with a bonus DVD, featuring cameos from Eyedea, Aesop Rock and Crescent Moon, among others.[3] Originally released as Sad Clown Bad Dub 4, the DVD features two hours of live performance footage, special guest appearances, and music videos for \"GodLovesUgly\", \"Summer Song\", and \"Say Shh\".[3] At Metacritic, which assigns a weighted average score out of 100 to reviews from mainstream critics, God Loves Ugly received an average score of 76% based on 9 reviews, indicating \"generally favorable reviews\".[4] In 2011, \"Modern Man's Hustle\" ranked at number 6 on Complex's \"25 Best Rhymesayers Songs\" list.[15] In 2015, God Loves Ugly was listed by HipHopDX as one of the \"30 Best Underground Hip Hop Albums Since 2000\".[16]",
      "expected_answer": "God Loves Ugly",
      "ground_truth_chunk_ids": [
        "182_random_chunk1"
      ],
      "source_ids": [
        "S382"
      ],
      "category": "inferential",
      "id": 81
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Ka\u010dinskas is a Lithuanian language family name. It corresponds to Polish language'?",
      "ground_truth": "Ka\u010dinskas is a Lithuanian language family name. It corresponds to Polish language surname Kaczy\u0144ski. The surname may refer to:",
      "expected_answer": "Ka\u010dinskas",
      "ground_truth_chunk_ids": [
        "159_random_chunk1"
      ],
      "source_ids": [
        "S359"
      ],
      "category": "inferential",
      "id": 82
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Biodiversity is the variability of life on Earth. It can be measured'?",
      "ground_truth": "Biodiversity is the variability of life on Earth. It can be measured on various levels, for example, genetic variability, species diversity, ecosystem diversity and phylogenetic diversity.[1] Diversity is not distributed evenly on Earth\u2014it is greater in the tropics as a result of the warm climate and high primary productivity in the region near the equator. Tropical forest ecosystems cover less than one-fifth of Earth's terrestrial area and contain about 50% of the world's species.[2] There are latitudinal gradients in species diversity for both marine and terrestrial taxa.[3] Since life began on Earth, six major mass extinctions and several minor events have led to large and sudden drops in biodiversity. The Phanerozoic aeon (the last 540 million years) marked a rapid growth in biodiversity via the Cambrian explosion. In this period, the majority of multicellular phyla first appeared. The next 400 million years included repeated, massive biodiversity losses. Those events have been classified as mass extinction events. In the Carboniferous, rainforest collapse may have led to a great loss of plant and animal life. The Permian\u2013Triassic extinction event, 251 million years ago, was the worst; vertebrate recovery took 30 million years. Human activities have led to an ongoing biodiversity loss and an accompanying loss of genetic diversity. This process is often referred to as Holocene extinction, or the sixth mass extinction. For example, it was estimated in 2007 that up to 30% of all species will be extinct by 2050.[4] Destroying habitats for farming is a key reason why biodiversity is decreasing today. Climate change also plays a role.[5][6] This can be seen for example in the effects of climate change on biomes. This anthropogenic extinction may have started toward the end of the Pleistocene, as some studies suggest that the megafaunal extinction event that took place around the end of",
      "expected_answer": "Biodiversity",
      "ground_truth_chunk_ids": [
        "61_fixed_chunk1"
      ],
      "source_ids": [
        "S061"
      ],
      "category": "inferential",
      "id": 83
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Ma\u00eetresse Fran\u00e7oise (pen name Annick Foucault) is a publicly known dominatrix in'?",
      "ground_truth": "Ma\u00eetresse Fran\u00e7oise (pen name Annick Foucault) is a publicly known dominatrix in Paris, France. Her autobiography was published in France by publisher \u00c9ditions Gallimard. Annick Foucault was born in the South of France.[1][2] Her childhood was marred by the loss of her father.[1][2] After having been the victim of an accident, she consecutively had no real other alternative but dropping out of university.[3] Thus she chiefly became a self-taught woman.[4] She first ran a ready-to-wear shop[3] and then became interested in Minitel,[5] which she considered to be \"an excellent means of communication, a kind of no-man's-land where anything is allowed\".[6] She subsequently participated in various forums and set up a \"debate forum\", 3615 Fetish[citation needed], which was devoted to sadomasochism, before launching in 1994 her own specialized \"minitel network\", under the name of Miss M.[3] Marc Daum described her as \"a major messaging authorizing officer 3615 on behalf of a foremost player in telematics, given that the good intellectual performance of her forums [had built] the loyalty of her knowledgeable readers\".[7] During the same period, she was the chief editor of a magazine\u2014La Sc\u00e8ne\u2014with a circulation of 5,000 copies.[8] In October 1996, she provided a \"precious collaboration\" to the journalist Monique Ayoun by the time she was writing an article dedicated to Minitel on behalf of the newspaper Biba [fr].[9] In 1994, Annick Foucault\u2014who already enjoyed a strong reputation in specialized circles[10]\u2014published a writing under her real name:[11] Fran\u00e7oise Ma\u00eetresse. This book\u2014which, as pointed out by Anne-\u00c9lisabeth Moutet [fr], was edited by Gallimard[12]\u2014 is an autobiographical narrative which content, according to Jean Pache, appears to be \"strange and meaningful\".[4] The common thread unfolds over \"her history, her childhood, the discovery of her cerebral sexuality, her experience as a dominatrix, in a style as precise as lively and literary\".[13] The script",
      "expected_answer": "Ma\u00eetresse Fran\u00e7oise",
      "ground_truth_chunk_ids": [
        "189_random_chunk1"
      ],
      "source_ids": [
        "S389"
      ],
      "category": "inferential",
      "id": 84
    },
    {
      "question": "Based on its description, which field or concept is being defined here: 'Software engineering is a branch of both computer science and engineering focused'?",
      "ground_truth": "Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications,[1] It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs,[2][3][4][5] In the tech industry, the title software engineer is often used aspirationally, even though many such roles are fundamentally programming positions and lack the formal regulation associated with traditional engineering,[6] A software engineer applies a software development process,[2][7] that involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself Beginning in the 1960s, software engineering was recognized as a separate field of engineering.[8] The development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968, NATO organized the first conference on software engineering, which addressed emerging challenges in software development. The event played a key role in formalizing guidelines and best practices for creating reliable and maintainable software.[9] The origins of the term software engineering have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of \"Computers and Automation\"[10] and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) in \"President's Letter to the ACM Membership\" by Anthony A. Oettinger.[11][12][13] It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer.[14] Margaret Hamilton described the discipline of \"software engineering\" during the Apollo missions to give what they were doing legitimacy.[15] At the time, there was perceived to be a \"software crisis\".[16][17][18] The 40th International Conference on Software Engineering (ICSE 2018) celebrates",
      "expected_answer": "Software engineering",
      "ground_truth_chunk_ids": [
        "44_fixed_chunk1"
      ],
      "source_ids": [
        "S044"
      ],
      "category": "inferential",
      "id": 85
    },
    {
      "question": "What is Developmental psychology?",
      "ground_truth": "Developmental psychology is the scientific study of how and why humans grow, change, and adapt across the course of their lives. Originally concerned with infants and children, the field has expanded to include adolescence, adult development, aging, and the entire lifespan.[1] Developmental psychologists aim to explain how thinking, feeling, and behaviors change throughout life. This field examines change[2] across three major dimensions, which are physical development, cognitive development, and social emotional development.[3][4] Within these three dimensions are a broad range of topics including motor skills, executive functions, moral understanding, language acquisition, social change, personality, emotional development, self-concept, and identity formation. Developmental psychology explores the influence of both nature and nurture on human development, as well as the processes of change that occur across different contexts over time. Many researchers are interested in the interactions among personal characteristics, the individual's behavior, and environmental factors, including the social context and the built environment. Ongoing debates in regards to developmental psychology include biological essentialism vs. neuroplasticity, and stages of development vs. dynamic systems of development. While research in developmental psychology has certain limitations, ongoing studies aim to understand how life stage transitions and biological factors influence human behavior and development.[5] Developmental psychology involves a range of fields,[2] such as educational psychology, child psychopathology, forensic developmental psychology, child development, cognitive psychology, ecological psychology, and cultural psychology. Influential developmental psychologists from the 20th century include Urie Bronfenbrenner, Erik Erikson, Sigmund Freud, Anna Freud, Jean Piaget, Barbara Rogoff, Esther Thelen, and Lev Vygotsky.[6] Jean-Jacques Rousseau and John B. Watson are typically cited as providing the foundation for modern developmental psychology.[7] In the mid-18th century, Jean Jacques Rousseau described three stages of development: infants (infancy), puer (childhood) and adolescence in Emile: Or, On Education. Rousseau's ideas were adopted and supported by educators at the time. Developmental psychology",
      "expected_answer": "Developmental psychology is the scientific study of how and why humans grow, change, and adapt across the course of their lives. Originally concerned with infants and children, the field has expanded to include adolescence, adult development, aging, and the entire lifespan.[1] Developmental psychologists aim to explain how thinking, feeling, and behaviors change throughout life. This field examines change[2] across three major dimensions, which are physical development, cognitive development, and social emotional development.[3][4] Within these three dimensions are a broad range of topics including motor skills, executive functions, moral understanding, language acquisition, social change, personality, emotional development, self-concept, and identity formation. Developmental psychology explores the influence of both nature and nurture on human development, as well as the processes of change that occur across different contexts over time. Many researchers are interested in the interactions among personal characteristics, the individual's behavior, and environmental factors, including the social context and the built environment. Ongoing debates in regards to developmental psychology include biological essentialism vs. neuroplasticity, and stages of development vs. dynamic systems of development. While research in developmental psychology has certain limitations, ongoing studies aim to understand how life stage transitions and biological factors influence human behavior and development.[5] Developmental psychology involves a range of fields,[2] such as educational psychology, child psychopathology, forensic developmental psychology, child development, cognitive psychology, ecological psychology, and cultural psychology. Influential developmental psychologists from the 20th century include Urie Bronfenbrenner, Erik Erikson, Sigmund Freud, Anna Freud, Jean Piaget, Barbara Rogoff, Esther Thelen, and Lev Vygotsky.[6] Jean-Jacques Rousseau and John B. Watson are typically cited as providing the foundation for modern developmental psychology.[7] In the mid-18th century, Jean Jacques Rousseau described three stages of development: infants (infancy), puer (childhood) and adolescence in Emile: Or, On Education. Rousseau's ideas were adopted and supported by educators at the time. Developmental psychology generally focuses on how and why certain changes (cognitive, social, intellectual, personality) occur over time in the course of a human life. Many theorists have made a profound contribution to this area of psychology. One of them is the psychologist Erik Erikson,[8] who created a model of eight phases of psychosocial development.[8] According to his theory, people go through different phases in their lives, each of which has its own developmental crisis that shapes a person's personality and behavior.[9] In the late 19th century, psychologists familiar with the evolutionary theory of Darwin began seeking an evolutionary description of psychological development;[7] prominent here was the pioneering psychologist G. Stanley Hall,[7] who attempted to correlate ages of childhood with previous ages of humanity. James Mark Baldwin, who wrote essays on topics that included Imitation: A Chapter in the Natural History of Consciousness and Mental Development in the Child and the Race: Methods and Processes, was significantly involved in the theory of developmental psychology.[7] Sigmund Freud, whose concepts were developmental, significantly affected public perceptions.[7] Sigmund Freud developed a theory that suggested that humans behave as they do because they are constantly seeking pleasure. This process of seeking pleasure changes through stages because people evolve. Each period of seeking pleasure that a person experiences is represented by a stage of psychosexual development. These stages symbolize the process of arriving at becoming a maturing adult.[10] The first is the oral stage, which begins at birth and ends around a year and a half of age. During the oral stage, the child finds pleasure in behaviors like sucking or other behaviors with the mouth. The second is the anal stage, from about a year or a year and a half to three years of age. During the anal stage, the child defecates from the anus and is often fascinated with its defecation. This period of development often occurs during the time when the child is being toilet-trained. The child becomes interested in feces and urine. Children begin to see themselves as independent from their parents. They begin to desire assertiveness and autonomy. The third is the phallic stage, which occurs from three to five years of age (most of a person's personality forms by this age). During the phallic stage, the child becomes aware of its sexual organs. Pleasure comes from finding acceptance and love from the opposite sex. The fourth is the latency stage, which occurs from age five until puberty. During the latency stage, the child's sexual interests are repressed. Stage five is the genital stage, which takes place from puberty until adulthood. During the genital stage, puberty begins to occur.[11] Children have now matured, and begin to think about other people instead of just themselves. Pleasure comes from feelings of affection from other people. Freud believed there is tension between the conscious and unconscious because the conscious tries to hold back what the unconscious tries to express. To explain this, he developed three personality structures: id, ego, and superego. The id, the most primitive of the three, functions according to the pleasure principle: seek pleasure and avoid pain.[12] The superego plays the critical and moralizing role, while the ego is the organized, realistic part that mediates between the desires of the id and the superego.[13] Jean Piaget, a Swiss theorist, posited that children learn by actively constructing knowledge through their interactions with their physical and social environments.[14] He suggested that the adult's role in helping the child learn was to provide appropriate materials. In his interview techniques with children that formed an empirical basis for his theories, he used something similar to Socratic questioning to get children to reveal their thinking. He argued that a principal source of development was through the child's inevitable generation of contradictions through their interactions with their physical and social worlds. The child's resolution of these contradictions led to more integrated and advanced forms of interaction, a developmental process that he called \"equilibration.\" Piaget argued that intellectual development takes place through a series of stages generated through the equilibration process. Each stage consists of steps the child must master before moving to the next step. He believed that these stages are not separate from one another, but rather that each stage builds on the previous one in a continuous learning process. He proposed four stages: sensorimotor, pre-operational, concrete operational, and formal operational. Though he did not believe these stages occurred at any given age, many studies have determined when these cognitive abilities should take place.[15] Piaget claimed that logic and morality develop through constructive stages.[16] Expanding on Piaget's work, Lawrence Kohlberg determined that the process of moral development was principally concerned with justice, and that it continued throughout the individual's lifetime.[17] He suggested three levels of moral reasoning: pre-conventional moral reasoning, conventional moral reasoning, and post-conventional moral reasoning. The pre-conventional moral reasoning is typical of children and is characterized by reasoning that is based on rewards and punishments associated with different courses of action. Conventional moral reasoning occurs during late childhood and early adolescence and is characterized by reasoning based on the rules and conventions of society. Lastly, post-conventional moral reasoning is a stage during which the individual sees society's rules and conventions as relative and subjective, rather than as authoritative.[18] Kohlberg used the Heinz Dilemma to apply to his stages of moral development. The Heinz Dilemma involves Heinz's wife dying from cancer and Heinz having the dilemma to save his wife by stealing a drug. Preconventional morality, conventional morality, and post-conventional morality applies to Heinz's situation.[19] German-American psychologist Erik Erikson and his collaborator and wife, Joan Erikson, posits eight stages of individual human development influenced by biological, psychological, and social factors throughout the lifespan.[8] At each stage the person must resolve a challenge, or an existential dilemma. Successful resolution of the dilemma results in the person ingraining a positive virtue, but failure to resolve the fundamental challenge of that stage reinforces negative perceptions of the person or the world around them and the person's personal development is unable to progress.[8] The first stage, \"Trust vs. Mistrust\", takes place in infancy. The positive virtue for the first stage is hope, in the infant learning whom to trust and having hope for a supportive group of people to be there for him/her. The second stage is \"Autonomy vs. Shame and Doubt\" with the positive virtue being will. This takes place in early childhood when the child learns to become more independent by discovering what they are capable of whereas if the child is overly controlled, feelings of inadequacy are reinforced, which can lead to low self-esteem and doubt. The third stage is \"Initiative vs. Guilt\". The virtue of being gained is a sense of purpose. This takes place primarily via play. This is the stage where the child will be curious and have many interactions with other kids. They will ask many questions as their curiosity grows. If too much guilt is present, the child may have a slower and harder time interacting with their world and other children in it. The fourth stage is \"Industry (competence) vs. Inferiority\". The virtue for this stage is competency and is the result of the child's early experiences in school. This stage is when the child will try to win the approval of others and understand the value of their accomplishments. The fifth stage is \"Identity vs. Role Confusion\". The virtue gained is fidelity and it takes place in adolescence. This is when the child ideally starts to identify their place in society, particularly in terms of their gender role. The sixth stage is \"Intimacy vs. Isolation\", which happens in young adults and the virtue gained is love. This is when the person starts to share his/her life with someone else intimately and emotionally. Not doing so can reinforce feelings of isolation. The seventh stage is \"Generativity vs. Stagnation\". This happens in adulthood and the virtue gained is care. A person becomes stable and starts to give back by raising a family and becoming involved in the community. The eighth stage is \"Ego Integrity vs. Despair\". When one grows old, they look back on their life and contemplate their successes and failures. If they resolve this positively, the virtue of wisdom is gained. This is also the stage when one can gain a sense of closure and accept death without regret or fear.[20] Michael Commons enhanced and simplified B\u00e4rbel Inhelder and Piaget's developmental theory and offers a standard method of examining the universal pattern of development. The Model of Hierarchical Complexity (MHC) is not based on the assessment of domain-specific information, It divides the Order of Hierarchical Complexity of tasks to be addressed from the Stage performance on those tasks. A stage is the order hierarchical complexity of the tasks the participant's successfully addresses. He expanded Piaget's original eight stage (counting the half stages) to seventeen stages. The stages are: The order of hierarchical complexity of tasks predicts how difficult the performance is with an R ranging from 0.9 to 0.98. In the MHC, there are three main axioms for an order to meet in order for the higher order task to coordinate the next lower order task. Axioms are rules that are followed to determine how the MHC orders actions to form a hierarchy. These axioms are: a) defined in terms of tasks at the next lower order of hierarchical complexity task action; b) defined as the higher order task action that organizes two or more less complex actions; that is, the more complex action specifies the way in which the less complex actions combine; c) defined as the lower order task actions have to be carried out non-arbitrarily.Commons, Michael L.; Gane-McCalla, Rebecca; Barker, Christopher D.; Li, Ellen Y. (2014). \"The model of hierarchical complexity as a measurement system\". Behavioral Development Bulletin. 19 (3). American Psychological Association: 9\u201368. doi:10.1037/h0100589. Ecological systems theory, originally formulated by Urie Bronfenbrenner, specifies four types of nested environmental systems, with bi-directional influences within and between the systems. The four systems are microsystem, mesosystem, exosystem, and macrosystem. Each system contains roles, norms and rules that can powerfully shape development. The microsystem is the direct environment in our lives such as our home and school. Mesosystem is how relationships connect to the microsystem. Exosystem is a larger social system where the child plays no role. Macrosystem refers to the cultural values, customs and laws of society.[21] The microsystem is the immediate environment surrounding and influencing the individual (example: school or the home setting). The mesosystem is the combination of two microsystems and how they influence each other (example: sibling relationships at home vs. peer relationships at school). The exosystem is the interaction among two or more settings that are indirectly linked (example: a father's job requiring more overtime ends up influencing his daughter's performance in school because he can no longer help with her homework). The macrosystem is broader taking into account social economic status, culture, beliefs, customs and morals (example: a child from a wealthier family sees a peer from a less wealthy family as inferior for that reason). Lastly, the chronosystem refers to the chronological nature of life events and how they interact and change the individual and their circumstances through transition (example: a mother losing her own mother to illness and no longer having that support in her life).[15] Since its publication in 1979, Bronfenbrenner's major statement of this theory, The Ecology of Human Development,[22] has had widespread influence on the way psychologists and others approach the study of human beings and their environments. As a result of this conceptualization of development, these environments\u2014from the family to economic and political structures\u2014have come to be viewed as part of the life course from childhood through to adulthood.[23] Lev Vygotsky was a Russian theorist from the Soviet era, who posited that children learn through hands-on experience and social interactions with members of their culture.[24] Vygotsky believed that a child's development should be examined during problem-solving activities.[25] Unlike Piaget, he claimed that timely and sensitive intervention by adults when a child is on the edge of learning a new task (called the \"zone of proximal development\") could help children learn new tasks. Zone of proximal development is a tool used to explain the learning of children and collaborating problem solving activities with an adult or peer.[25] This adult role is often referred to as the skilled \"master\", whereas the child is considered the learning apprentice through an educational process often termed \"cognitive apprenticeship\" Martin Hill stated that \"The world of reality does not apply to the mind of a child.\" This technique is called \"scaffolding\", because it builds upon knowledge children already have with new knowledge that adults can help the child learn.[26] Vygotsky was strongly focused on the role of culture in determining the child's pattern of development, arguing that development moves from the social level to the individual level.[26] In other words, Vygotsky claimed that psychology should focus on the progress of human consciousness through the relationship of an individual and their environment.[27] He felt that if scholars continued to disregard this connection, then this disregard would inhibit the full comprehension of the human consciousness.[27] Constructivism is a paradigm in psychology that characterizes learning as a process of actively constructing knowledge. Individuals create meaning for themselves or make sense of new information by selecting, organizing, and integrating information with other knowledge, often in the context of social interactions. Constructivism can occur in two ways: individual and social. Individual constructivism is when a person constructs knowledge through cognitive processes of their own experiences rather than by memorizing facts provided by others. Social constructivism is when individuals construct knowledge through an interaction between the knowledge they bring to a situation and social or cultural exchanges within that content.[15] A foundational concept of constructivism is that the purpose of cognition is to organize one's experiential world, instead of the ontological world around them.[28] Jean Piaget, a Swiss developmental psychologist, proposed that learning is an active process because children learn through experience and make mistakes and solve problems. Piaget proposed that learning should be whole by helping students understand that meaning is constructed.[29] Evolutionary developmental psychology is a research paradigm that applies the basic principles of Darwinian evolution, particularly natural selection, to understand the development of human behavior and cognition. It involves the study of both the genetic and environmental mechanisms that underlie the development of social and cognitive competencies, as well as the epigenetic (gene-environment interactions) processes that adapt these competencies to local conditions.[30] EDP considers both the reliably developing, species-typical features of ontogeny (developmental adaptations), as well as individual differences in behavior, from an evolutionary perspective. While evolutionary views tend to regard most individual differences as the result of either random genetic noise (evolutionary byproducts)[31] and/or idiosyncrasies (for example, peer groups, education, neighborhoods, and chance encounters)[32] rather than products of natural selection, EDP asserts that natural selection can favor the emergence of individual differences via \"adaptive developmental plasticity\".[30][33] From this perspective, human development follows alternative life-history strategies in response to environmental variability, rather than following one species-typical pattern of development.[30] EDP is closely linked to the theoretical framework of evolutionary psychology (EP), but is also distinct from EP in several domains, including research emphasis (EDP focuses on adaptations of ontogeny, as opposed to adaptations of adulthood) and consideration of proximate ontogenetic and environmental factors (i.e., how development happens) in addition to more ultimate factors (i.e., why development happens), which are the focus of mainstream evolutionary psychology.[34] Attachment theory, originally developed by John Bowlby, focuses on the importance of open, intimate, emotionally meaningful relationships.[35] Attachment is described as a biological system or powerful survival impulse that evolved to ensure the survival of the infant. A threatened or stressed child will move toward caregivers who create a sense of physical, emotional, and psychological safety for the individual. Attachment feeds on body contact and familiarity. Psychologist Harry Harlow's research with infant rhesus monkeys in the mid-20th century provided pivotal experimental support for attachment theory. His studies found that infant monkeys consistently preferred cloth surrogate mothers that provided comfort over wire ones that offered only food. These results demonstrated that emotional security and physical comfort are more critical to attachment than nourishment alone. Harlow's findings reinforced Bowlby's view that early caregiving relationships are biologically essential for healthy emotional development and social bonding later in life.[36] Later Mary Ainsworth developed the Strange Situation protocol and the concept of the secure base. This tool has been found to help understand attachment, such as the Strange Situation Test and the Adult Attachment Interview. Both of which help determine factors to certain attachment styles. The Strange Situation Test helps find \"disturbances in attachment\" and whether certain attributes are found to contribute to a certain attachment issue.[37] The Adult Attachment Interview is a tool that is similar to the Strange Situation Test but instead focuses attachment issues found in adults.[37] Both tests have helped many researchers gain more information on the risks and how to identify them.[37] Theorists have proposed four types of attachment styles:[38] secure, anxious-avoidant, anxious-resistant,[18] and disorganized.[38] Secure attachment is a healthy attachment between the infant and the caregiver. It is characterized by trust. Anxious-avoidant is an insecure attachment between an infant and a caregiver. This is characterized by the infant's indifference toward the caregiver. Anxious-resistant is an insecure attachment between the infant and the caregiver characterized by distress from the infant when separated and anger when reunited.[18] Disorganized is an attachment style without a consistent pattern of responses upon return of the parent.[38] It is possible to prevent a child's innate propensity to develop bonds. Some infants are kept in isolation or subjected to severe neglect or abuse, or they are raised without the stimulation and care of a regular caregiver. This deprivation may cause short-term consequences such as separation, rage, despair, and a brief lag in cerebral growth. Increased aggression, clinging behavior, alienation, psychosomatic illnesses, and an elevated risk of adult depression are among the long-term consequences.[39][page\u00a0needed][40][page\u00a0needed]\\ According to attachment theory, which is a psychological concept, people's capacity to develop healthy social and emotional ties later in life is greatly impacted by their early relationships with their primary caregivers, especially during infancy. This suggests that humans have an inbuilt need to develop strong bonds with caregivers in order to survive and be healthy. Childhood attachment styles can have an impact on how people behave in adult social situations, including romantic partnerships.[41] A significant concern of developmental psychology is the relationship between innateness and environmental influences on development. This is often referred to as \"nature and nurture\" or nativism versus empiricism. A nativist account of development would argue that the processes in question are innate, that is, they are specified by the organism's genes.[42] What makes a person who they are? Is it their environment or their genetics? This is the debate of nature vs nurture.[43] According to an empiricist viewpoint, those processes are learned through interaction with the environment. Today most developmental psychologists take a more holistic approach, emphasizing the interaction between genetic and environmental influences. One of the ways this relationship has been explored in recent years is through the emerging field of evolutionary developmental psychology. The dispute over innateness has been well represented in the field of language acquisition studies. A major question in this area is whether or not certain properties of human language are specified genetically or can be acquired through learning. The empiricist position on the issue of language acquisition suggests that the language input provides the necessary information required for learning the structure of language and that infants acquire language through a process of statistical learning. From this perspective, language can be acquired via general learning methods that also apply to other aspects of development, such as perceptual learning.[44] The nativist position argues that the input from language is too impoverished for infants and children to acquire the structure of language. Linguist Noam Chomsky asserts that, evidenced by the lack of sufficient information in the language input, there is a universal grammar that applies to all human languages and is pre-specified. This has led to the idea that there is a special cognitive module suited for learning language, often called the language acquisition device. Chomsky's critique of the behaviorist model of language acquisition is regarded by many as a key turning point in the decline in the prominence of the theory of behaviorism generally.[45] But Skinner's conception of \"Verbal Behavior\" has not died, perhaps in part because it has generated successful practical applications.[45] Maybe there could be \"strong interactions of both nature and nurture\".[46] Many researchers now emphasize that development results from a continuous, dynamic interaction between genetic predispositions and environmental influences. Rather than acting independently, nature and nurture are seen as intertwined forces, where genetic factors can shape sensitivity to environmental inputs, and environmental conditions can influence how genes are expressed across development.[47] One of the major discussions in developmental psychology includes whether development is discontinuous or continuous. Continuous development is quantifiable and quantitative, whereas discontinuous development is qualitative. Quantitative estimations of development can be measuring the stature of a child, and measuring their memory or consideration span. \"Particularly dramatic examples of qualitative changes are metamorphoses, such as the emergence of a caterpillar into a butterfly.\"[48] Those psychologists who bolster the continuous view of improvement propose that improvement includes slow and progressing changes all through the life span, with behavior within the prior stages of advancement giving the premise of abilities and capacities required for the other stages. \"To many, the concept of continuous, quantifiable measurement seems to be the essence of science\".[48] However, not all psychologists concur that advancement could be a continuous process. A few see advancement as a discontinuous process. They accept advancement includes unmistakable and partitioned stages with diverse sorts of behavior happening in each organization. This proposes that the development of certain capacities in each arrange, such as particular feelings or ways of considering, has a definite beginning and ending point. Nevertheless, there is no exact moment when a capacity suddenly appears or disappears. Although some sorts of considering, feeling or carrying on could seem to seem abruptly, it is more than likely that this has been developing gradually for some time.[49] Stage theories of development rest on the suspicion that development may be a discontinuous process including particular stages which are characterized by subjective contrasts in behavior. They moreover assume that the structure of the stages is not variable concurring to each person, in any case, the time of each arrangement may shift separately. Stage theories can be differentiated with ceaseless hypotheses, which set that development is an incremental process.[50] This issue involves the degree to which one becomes older renditions of their early experience or whether they develop into something different from who they were at an earlier point in development.[51] It considers the extent to which early experiences (especially infancy) or later experiences are the key determinants of a person's development. Stability is defined as the consistent ordering of individual differences with respect to some attribute.[52] Change is altering someone/something. Most human development lifespan developmentalists recognize that extreme positions are unwise. Therefore, the key to a comprehensive understanding of development at any stage requires the interaction of different factors and not only one.[53] Theory of mind is the ability to attribute mental states to ourselves and others.[54] It is a complex but vital process in which children begin to understand the emotions, motives, and feelings of not only themselves but also others. Theory of mind allows individuals to understand that others have unique beliefs and desires different from their own. This ability enables successful social interactions by recognizing and interpreting the mental states of others. If a child does not fully develop theory of mind within this crucial 5-year period, they can suffer from communication barriers that follow them into adolescence and adulthood.[55] Exposure to more people and the availability of stimuli that encourages social-cognitive growth is a factor that relies heavily on family.[56] Developmental psychology is concerned not only with describing the characteristics of psychological change over time but also seeks to explain the principles and internal workings underlying these changes. Psychologists have attempted to better understand these factors by using models. A model must simply account for the means by which a process takes place. This is sometimes done in reference to changes in the brain that may correspond to changes in behavior over the course of the development. Mathematical modeling is useful in developmental psychology for implementing theory in a precise and easy-to-study manner, allowing generation, explanation, integration, and prediction of diverse phenomena. Several modeling techniques are applied to development: symbolic, connectionist (neural network), or dynamical systems models. Dynamic systems models illustrate how many different features of a complex system may interact to yield emergent behaviors and abilities. Nonlinear dynamics has been applied to human systems specifically to address issues that require attention to temporality such as life transitions, human development, and behavioral or emotional change over time. Nonlinear dynamic systems is currently being explored as a way to explain discrete phenomena of human development such as affect,[57] second language acquisition,[58] and locomotion.[59] One critical aspect of developmental psychology is the study of neural development, which investigates how the brain changes and develops during different stages of life. Neural development focuses on how the brain changes and develops during different stages of life. Studies have shown that the human brain undergoes rapid changes during prenatal and early postnatal periods. These changes include the formation of neurons, the development of neural networks, and the establishment of synaptic connections.[60] The formation of neurons and the establishment of basic neural circuits in the developing brain are crucial for laying the foundation of the brain's structure and function, and disruptions during this period can have long-term effects on cognitive and emotional development.[61] Experiences and environmental factors play a crucial role in shaping neural development. Early sensory experiences, such as exposure to language and visual stimuli, can influence the development of neural pathways related to perception and language processing.[62] Genetic factors play a huge roll in neural development. Genetic factors can influence the timing and pattern of neural development, as well as the susceptibility to certain developmental disorders, such as autism spectrum disorder and attention-deficit/hyperactivity disorder.[63] Research finds that the adolescent brain undergoes significant changes in neural connectivity and plasticity. During this period, there is a pruning process where certain neural connections are strengthened while others are eliminated, resulting in more efficient neural networks and increased cognitive abilities, such as decision-making and impulse control.[64] The study of neural development provides crucial insights into the complex interplay between genetics, environment, and experiences in shaping the developing brain. By understanding the neural processes underlying developmental changes, researchers gain a better understanding of cognitive, emotional, and social development in humans. Cognitive development is primarily concerned with how infants and children acquire, develop, and use internal mental capabilities such as: problem-solving, memory, and language. Major topics in cognitive development are the study of language acquisition and the development of perceptual and motor skills. Piaget was one of the influential early psychologists to study the development of cognitive abilities. His theory suggests that development proceeds through a set of stages from infancy to adulthood and that there is an end point or goal. Other accounts, such as that of Lev Vygotsky, have suggested that development does not progress through stages, but rather that the developmental process that begins at birth and continues until death is too complex for such structure and finality. Rather, from this viewpoint, developmental processes proceed more continuously. Thus, development should be analyzed, instead of treated as a product to obtain. K. Warner Schaie has expanded the study of cognitive development into adulthood. Rather than being stable from adolescence, Schaie sees adults as progressing in the application of their cognitive abilities.[65] Modern cognitive development has integrated the considerations of cognitive psychology and the psychology of individual differences into the interpretation and modeling of development.[66] Specifically, the neo-Piagetian theories of cognitive development showed that the successive levels or stages of cognitive development are associated with increasing processing efficiency and working memory capacity. These increases explain differences between stages, progression to higher stages, and individual differences of children who are the same-age and of the same grade-level. However, other theories have moved away from Piagetian stage theories, and are influenced by accounts of domain-specific information processing, which posit that development is guided by innate evolutionarily-specified and content-specific information processing mechanisms. Developmental psychologists who are interested in social development examine how individuals develop social and emotional competencies. For example, they study how children form friendships, how they understand and deal with emotions, and how identity develops. Research in this area may involve study of the relationship between cognition or cognitive development and social behavior. Emotional regulation or ER refers to an individual's ability to modulate emotional responses across a variety of contexts. In young children, this modulation is in part controlled externally, by parents and other authority figures. As children develop, they take on more and more responsibility for their internal state. Studies have shown that the development of ER is affected by the emotional regulation children observe in parents and caretakers, the emotional climate in the home, and the reaction of parents and caretakers to the child's emotions.[67] Music also has an influence on stimulating and enhancing the senses of a child through self-expression.[68] A child's social and emotional development can be disrupted by motor coordination problems, evidenced by the environmental stress hypothesis. The environmental hypothesis explains how children with coordination problems and developmental coordination disorder are exposed to several psychosocial consequences which act as secondary stressors, leading to an increase in internalizing symptoms such as depression and anxiety.[69] Motor coordination problems affect fine and gross motor movement as well as perceptual-motor skills. Secondary stressors commonly identified include the tendency for children with poor motor skills to be less likely to participate in organized play with other children and more likely to feel socially isolated.[69] Social and emotional development focuses on five keys areas: Self-Awareness, Self Management, Social Awareness, Relationship Skills and Responsible Decision Making.[70] Physical development concerns the physical maturation of an individual's body until it reaches the adult stature. Although physical growth is a highly regular process, all children differ tremendously in the timing of their growth spurts.[71] Studies are being done to analyze how the differences in these timings affect and are related to other variables of developmental psychology such as information processing speed. Traditional measures of physical maturity using x-rays are less in practice nowadays, compared to simple measurements of body parts such as height, weight, head circumference, and arm span.[71] A few other studies and practices with physical developmental psychology are the phonological abilities of mature 5- to 11-year-olds, and the controversial hypotheses of left-handers being maturationally delayed compared to right-handers. A study by Eaton, Chipperfield, Ritchot, and Kostiuk in 1996 found in three different samples that there was no difference between right- and left-handers.[71] Researchers interested in memory development look at the way our memory develops from childhood and onward. According to fuzzy-trace theory, a theory of cognition originally proposed by Valerie F. Reyna and Charles Brainerd, people have two separate memory processes: verbatim and gist. These two traces begin to develop at different times as well as at a different pace. Children as young as four years old have verbatim memory, memory for surface information, which increases up to early adulthood, at which point it begins to decline. On the other hand, our capacity for gist memory, memory for semantic information, increases up to early adulthood, at which point it is consistent through old age. Furthermore, one's reliance on gist memory traces increases as one ages.[72] Neuroscientific research has contributed to understanding the biological mechanisms behind memory development. A study using diffusion MRI in children aged four to twelve found that greater maturity in white matter tracts, specifically the uncinate fasciculus and dorsal cingulum bundle, was associated with stronger episodic memory recall. These findings suggest that the structural development of white matter pathways plays a significant role in memory function during childhood.[73] Developmental psychology employs many of the research methods used in other areas of psychology. However, infants and children cannot be tested in the same ways as adults, so different methods are often used to study their development. Developmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation.[74] These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study.[75] Every developmental investigation can be characterized in terms of whether its underlying strategy involves the experimental, correlational, or case study approach.[76][77] The experimental method involves \"actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed;[77] the experimental design points to cause-and-effect relationships.[78] This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable.[77] The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables.[78] On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability.[78] The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention.[77][78] The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment;[78] however, the limitation is that it does not permit determination of cause-and-effect relationships among variables.[78] The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores.[78] Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.[77] Most developmental studies, regardless of whether they employ the experimental, correlational, or case study method, can also be constructed using research designs.[75] Research designs are logical frameworks used to make key comparisons within research studies such as: In a longitudinal study, a researcher observes many individuals born at or around the same time (a cohort) and carries out new observations as members of the cohort age. This method can be used to draw conclusions about which types of development are universal (or normative) and occur in most members of a cohort. As an example a longitudinal study of early literacy development examined in detail the early literacy experiences of one child in each of 30 families.[79] Researchers may also observe ways that development varies between individuals, and hypothesize about the causes of variation in their data. Longitudinal studies often require large amounts of time and funding, making them unfeasible in some situations. Also, because members of a cohort all experience historical events unique to their generation, apparently normative developmental trends may, in fact, be universal only to their cohort.[80] In a cross-sectional study, a researcher observes differences between individuals of different ages at the same time. This generally requires fewer resources than the longitudinal method, and because the individuals come from different cohorts, shared historical events are not so much of a confounding factor. By the same token, however, cross-sectional research may not be the most effective way to study differences between participants, as these differences may result not from their different ages but from their exposure to different historical events.[81] A third study design, the sequential design, combines both methodologies. Here, a researcher observes members of different birth cohorts at the same time, and then tracks all participants over time, charting changes in the groups. While much more resource-intensive, the format aids in a clearer distinction between what changes can be attributed to an individual or historical environment from those that are truly universal.[82] Because every method has some weaknesses, developmental psychologists rarely rely on one study or even one method to reach conclusions by finding consistent evidence from as many converging sources as possible.[77] Prenatal development is of interest to psychologists investigating the context of early psychological development. The whole prenatal development involves three main stages: germinal stage, embryonic stage and fetal stage. Germinal stage begins at conception until 2 weeks; embryonic stage means the development from 2 weeks to 8 weeks; fetal stage represents 9 weeks until birth of the baby.[83] The senses develop in the womb itself: a fetus can both see and hear by the second trimester (13 to 24 weeks of age). The sense of touch develops in the embryonic stage (5 to 8 weeks).[84] Most of the brain's billions of neurons also are developed by the second trimester.[85] Babies are hence born with some odor, taste and sound preferences, largely related to the mother's environment.[86] Some primitive reflexes too arise before birth and are still present in newborns. One hypothesis is that these reflexes are vestigial and have limited use in early human life. Piaget's theory of cognitive development suggested that some early reflexes are building blocks for infant sensorimotor development. For example, the tonic neck reflex may help development by bringing objects into the infant's field of view.[87] Other reflexes, such as the walking reflex, appear to be replaced by more sophisticated voluntary control later in infancy. This may be because the infant gains too much weight after birth to be strong enough to use the reflex, or because the reflex and subsequent development are functionally different.[88] It has also been suggested that some reflexes (for example the moro and walking reflexes) are predominantly adaptations to life in the womb with little connection to early infant development.[87] Primitive reflexes reappear in adults under certain conditions, such as neurological conditions like dementia or traumatic lesions. Ultrasounds have shown that infants are capable of a range of movements in the womb, many of which appear to be more than simple reflexes.[88] By the time they are born, infants can recognize and have a preference for their mother's voice suggesting some prenatal development of auditory perception.[88] Prenatal development and birth complications may also be connected to neurodevelopmental disorders, for example in schizophrenia. With the advent of cognitive neuroscience, embryology and the neuroscience of prenatal development is of increasing interest to developmental psychology research. Several environmental agents\u2014teratogens\u2014can cause damage during the prenatal period. These include prescription and nonprescription drugs, illegal drugs, tobacco, alcohol, environmental pollutants, infectious disease agents such as the rubella virus and the toxoplasmosis parasite, maternal malnutrition, maternal emotional stress, and Rh factor blood incompatibility between mother and child.[89] There are many statistics which prove the effects of the aforementioned substances. A leading example of this would be that at least 100,000 \"cocaine babies\" were born in the United States annually in the late 1980s. \"Cocaine babies\" are proven to have quite severe and lasting difficulties which persist throughout infancy and right throughout childhood. The drug also encourages behavioural problems in the affected children and defects of various vital organs.[90] From birth until the first year, children are referred to as infants. As they grow, children respond to their environment in unique ways.[91] Developmental psychologists vary widely in their assessment of infant psychology, and the influence the outside world has upon it. The majority of a newborn infant's time is spent sleeping.[92] At first, their sleep cycles are evenly spread throughout the day and night, but after a couple of months, infants generally become diurnal.[93] In human or rodent infants, there is always the observation of a diurnal cortisol rhythm, which is sometimes entrained with a maternal substance.[94] Nevertheless, the circadian rhythm starts to take shape, and a 24-hour rhythm is observed in just some few months after birth.[93][94] Infants can be seen to have six states, grouped into pairs: Infant perception is what a newborn can see, hear, smell, taste, and touch. These five features are considered as the \"five senses\".[97] Because of these different senses, infants respond to stimuli differently.[88] Babies are born with the ability to discriminate virtually all sounds of all human languages.[105] Infants of around six months can differentiate between phonemes in their own language, but not between similar phonemes in another language. Notably, infants are able to differentiate between various durations and sound levels and can easily differentiate all the languages they have encountered, hence easy for infants to understand a certain language compared to an adult.[106] At this stage infants also start to babble, whereby they start making vowel consonant sound as they try to understand the true meaning of language and copy whatever they are hearing in their surrounding producing their own phonemes. In various cultures, a distinct form of speech called \"babytalk\" is used when communicating with newborns and young children. This register consists of simplified terms for common topics such as family members, food, hygiene, and familiar animals. It also exhibits specific phonological patterns, such as substituting alveolar sounds with initial velar sounds, especially in languages like English. Furthermore, babytalk often involves morphological simplifications, such as regularizing verb conjugations (for instance, saying \"corned\" instead of \"cornered\" or \"goed\" instead of \"went\"). This language is typically taught to children and is perceived as their natural way of communication. Interestingly, in mythology and popular culture, certain characters, such as the \"Hausa trickster\" or the Warner Bros cartoon character \"Tweety Pie\", are portrayed as speaking in a babytalk-like manner.[107] Piaget suggested that an infant's perception and understanding of the world depended on their motor development, which was required for the infant to link visual, tactile and motor representations of objects.[108] The concept of object permanence refers to the knowledge that an object exists even when it is not directly perceived or visible; in other words, something is still there even if it is not visible. This is a crucial developmental milestone for infants, who learn that something is not necessarily lost forever just because it is hidden. When a child displays object permanence, they will look for a toy that is hidden, showing that they are aware that the item is still there even when it is covered by a blanket. Most babies start to exhibit symptoms of object permanence around the age of eight months. According to this theory, infants develop object permanence through touching and handling objects.[88] Piaget's sensorimotor stage comprised six sub-stages (see sensorimotor stages for more detail). In the early stages, development arises out of movements caused by primitive reflexes.[109] Discovery of new behaviors results from classical and operant conditioning, and the formation of habits.[109] From eight months the infant is able to uncover a hidden object but will persevere when the object is moved. Piaget concluded that infants lacked object permanence before 18 months when infants' before this age failed to look for an object where it had last been seen. Instead, infants continued to look for an object where it was first seen, committing the \"A-not-B error\". Some researchers have suggested that before the age of 8\u20139 months, infants' inability to understand object permanence extends to people, which explains why infants at this age do not cry when their mothers are gone (\"Out of sight, out of mind\"). In the 1980s and 1990s, researchers developed new methods of assessing infants' understanding of the world with far more precision and subtlety than Piaget was able to do in his time. Since then, many studies based on these methods suggest that young infants understand far more about the world than first thought. Based on recent findings, some researchers (such as Elizabeth Spelke and Renee Baillargeon) have proposed that an understanding of object permanence is not learned at all, but rather comprises part of the innate cognitive capacities of our species. According to Jean Piaget's developmental psychology, object permanence, or the awareness that objects exist even when they are no longer visible, was thought to emerge gradually between the ages of 8 and 12 months. However, experts such as Elizabeth Spelke and Renee Baillargeon have questioned this notion. They studied infants' comprehension of object permanence at a young age using novel experimental approaches such as violation-of-expectation paradigms. These findings imply that children as young as 3 to 4 months old may have an innate awareness of object permanence. Baillargeon's \"drawbridge\" experiment, for example, showed that infants were surprised when they saw occurrences that contradicted object permanence expectations. This proposition has important consequences for our understanding of infant cognition, implying that infants may be born with core cognitive abilities rather than developing them via experience and learning.[110] Other research has suggested that young infants in their first six months of life may possess an understanding of numerous aspects of the world around them, including: There are critical periods in infancy and childhood during which development of certain perceptual, sensorimotor, social and language systems depends crucially on environmental stimulation.[114] Feral children such as Genie, deprived of adequate stimulation, fail to acquire important skills and are unable to learn in later childhood. In this case, Genie is used to represent the case of a feral child because she was socially neglected and abused while she was just a young girl. She underwent abnormal child psychology which involved problems with her linguistics. This happened because she was neglected while she was very young with no one to care about her and had less human contact. The concept of critical periods is also well-established in neurophysiology, from the work of Hubel and Wiesel among others. Neurophysiology in infants generally provides correlating details that exists between neurophysiological details and clinical features and also focuses on vital information on rare and common neurological disorders that affect infants. Studies have been done to look at the differences in children who have developmental delays versus typical development. Normally when being compared to one another, mental age (MA) is not taken into consideration. There still may be differences in developmentally delayed (DD) children vs. typical development (TD) behavioral, emotional and other mental disorders. When compared to MA children there is a bigger difference between normal developmental behaviors overall. DDs can cause lower MA, so comparing DDs with TDs may not be as accurate. Pairing DDs specifically with TD children at similar MA can be more accurate. There are levels of behavioral differences that are considered as normal at certain ages. When evaluating DDs and MA in children, consider whether those with DDs have a larger amount of behavior that is not typical for their MA group. Developmental delays tend to contribute to other disorders or difficulties than their TD counterparts.[115] Infants shift between ages of one and two to a developmental stage known as toddlerhood. In this stage, an infant's transition into toddlerhood is highlighted through self-awareness, developing maturity in language use, and presence of memory and imagination. During toddlerhood, babies begin learning how to walk, talk, and make decisions for themselves. An important characteristic of this age period is the development of language, where children are learning how to communicate and express their emotions and desires through the use of vocal sounds, babbling, and eventually words.[116] Self-control also begins to develop. At this age, children take initiative to explore, experiment and learn from making mistakes. Caretakers who encourage toddlers to try new things and test their limits, help the child become autonomous, self-reliant, and confident.[117] If the caretaker is overprotective or disapproving of independent actions, the toddler may begin to doubt their abilities and feel ashamed of the desire for independence. The child's autonomic development is inhibited, leaving them less prepared to deal with the world in the future. Toddlers also begin to identify themselves in gender roles, acting according to their perception of what a man or woman should do.[118] Socially, the period of toddler-hood is commonly called the \"terrible twos\".[119] Toddlers often use their new-found language abilities to voice their desires, but are often misunderstood by parents due to their language skills just beginning to develop. A person at this stage testing their independence is another reason behind the stage's infamous label. Tantrums in a fit of frustration are also common. Erik Erikson divides childhood into four stages, each with its distinct social crisis:[120] As stated, the psychosocial crisis for Erikson is Trust versus Mistrust. Needs are the foundation for gaining or losing trust in the infant. If the needs are met, trust in the guardian and the world forms. If the needs are not met, or the infant is neglected, mistrust forms alongside feelings of anxiety and fear.[122] Autonomy versus shame follows trust in infancy. The child begins to explore their world in this stage and discovers preferences in what they like. If autonomy is allowed, the child grows in independence and their abilities. If freedom of exploration is hindered, it leads to feelings of shame and low self-esteem.[122] In the earliest years, children are \"completely dependent on the care of others\". Therefore, they develop a \"social relationship\" with their care givers and, later, with family members. During their preschool years (3\u20135), they \"enlarge their social horizons\" to include people outside the family.[123] Preoperational and then operational thinking develops, which means actions are reversible, and egocentric thought diminishes.[124] The motor skills of preschoolers increase so they can do more things for themselves. They become more independent. No longer completely dependent on the care of others, the world of this age group expands. More people have a role in shaping their individual personalities. Preschoolers explore and question their world.[125] For Jean Piaget, the child is \"a little scientist exploring and reflecting on these explorations to increase competence\" and this is done in \"a very independent way\".[126] Play is a major activity for ages 3\u20135. For Piaget, through play \"a child reaches higher levels of cognitive development.\"[127] In their expanded world, children in the 3\u20135 age group attempt to find their own way. If this is done in a socially acceptable way, the child develops the initiative. If not, the child develops guilt.[128] Children who develop \"guilt\" rather than \"initiative\" have failed Erikson's psychosocial crisis for the 3\u20135 age group. For Erik Erikson, the psychosocial crisis during middle childhood is Industry vs. Inferiority which, if successfully met, instills a sense of Competency in the child.[120] In all cultures, middle childhood is a time for developing \"skills that will be needed in their society.\"[129] School offers an arena in which children can gain a view of themselves as \"industrious (and worthy)\". They are \"graded for their school work and often for their industry\". They can also develop industry outside of school in sports, games, and doing volunteer work.[130] Children who achieve \"success in school or games might develop a feeling of competence.\" The \"peril during this period is that feelings of inadequacy and inferiority will develop.[129] Parents and teachers can \"undermine\" a child's development by failing to recognize accomplishments or being overly critical of a child's efforts.[130]\nChildren who are \"encouraged and praised\" develop a belief in their competence. Lack of encouragement or ability to excel lead to \"feelings of inadequacy and inferiority\".[131] The Centers for Disease Control (CDC) divides Middle Childhood into two stages, 6\u20138 years and 9\u201311 years, and gives \"developmental milestones for each stage\".[132][133] Entering elementary school, children in this age group begin to thinks about the future and their \"place in the world\". Working with other students and wanting their friendship and acceptance become more important. This leads to \"more independence from parents and family\". As students, they develop the mental and verbal skills \"to describe experiences and talk about thoughts and feelings\". They become less self-centered and show \"more concern for others\".[132] For children ages 9\u201311 \"friendships and peer relationships\" increase in strength, complexity, and importance. This results in greater \"peer pressure\". They grow even less dependent on their families and they are challenged academically. To meet this challenge, they increase their attention span and learn to see other points of view.[133] Adolescence is the period of life between the onset of puberty and the full commitment to an adult social role, such as worker, parent, and/or citizen. It is the period known for the formation of personal and social identity (see Erik Erikson) and the discovery of moral purpose (see William Damon). Intelligence is demonstrated through the logical use of symbols related to abstract concepts and formal reasoning. A return to egocentric thought often occurs early in the period. Only 35% develop the capacity to reason formally during adolescence or adulthood. (Huitt, W. and Hummel, J. January 1998)[134] Erik Erikson labels this stage identity versus role confusion. Erikson emphasizes the importance of developing a sense of identity in adolescence because it affects the individual throughout their life. Identity is a lifelong process and is related with curiosity and active engagement. Role confusion is often considered the current state of identity of the individual. Identity exploration is the process of changing from role confusion to resolution.[135] During Erik Erikson's identity versus role uncertainty stage, which occurs in adolescence, people struggle to form a cohesive sense of self while exploring many social roles and prospective life routes. This time is characterized by deep introspection, self-examination, and the pursuit of self-understanding. Adolescents are confronted with questions regarding their identity, beliefs, and future goals. The major problem is building a strong sense of identity in the face of society standards, peer pressure, and personal preferences. Adolescents participate in identity exploration, commitment, and synthesis, actively seeking out new experiences, embracing ideals and aspirations, and merging their changing sense of self into a coherent identity. Successfully navigating this stage builds the groundwork for good psychological development in adulthood, allowing people to pursue meaningful relationships, make positive contributions to society, and handle life's adversities with perseverance and purpose.[9] It is divided into three parts, namely: The adolescent unconsciously explores questions such as \"Who am I? Who do I want to be?\" Like toddlers, adolescents must explore, test limits, become autonomous, and commit to an identity, or sense of self. Different roles, behaviors and ideologies must be tried out to select an identity. Role confusion and inability to choose vocation can result from a failure to achieve a sense of identity through, for example, friends.[136] Early adulthood generally refers to the period between ages 18 to 39,[137] and according to theorists such as Erik Erikson, is a stage where development is mainly focused on maintaining relationships.[138] Erikson shows the importance of relationships by labeling this stage intimacy vs isolation. Intimacy suggests a process of becoming part of something larger than oneself by sacrificing in romantic relationships and working for both life and career goals.[139] Other examples include creating bonds of intimacy, sustaining friendships, and starting a family. Some theorists state that development of intimacy skills rely on the resolution of previous developmental stages. A sense of identity gained in the previous stages is also necessary for intimacy to develop. If this skill is not learned the alternative is alienation, isolation, a fear of commitment, and the inability to depend on others. Isolation, on the other hand, suggests something different than most might expect. Erikson defined it as a delay of commitment in order to maintain freedom. Yet, this decision does not come without consequences. Erikson explained that choosing isolation may affect one's chances of getting married, progressing in a career, and overall development.[139] A related framework for studying this part of the lifespan is that of emerging adulthood. Scholars of emerging adulthood, such as Jeffrey Arnett, are not necessarily interested in relationship development. Instead, this concept suggests that people transition after their teenage years into a period, not characterized as relationship building and an overall sense of constancy with life, but with years of living with parents, phases of self-discovery, and experimentation.[140] Middle adulthood generally refers to the period between ages 40 to 64. During this period, middle-aged adults experience a conflict between generativity and stagnation. Generativity is the sense of contributing to society, the next generation, or their immediate community. On the other hand, stagnation results in a lack of purpose.[141] The adult's identity continues to develop in middle-adulthood. Middle-aged adults often adopt opposite gender characteristics. The adult realizes they are half-way through their life and often reevaluate vocational and social roles. Life circumstances can also cause a reexamination of identity.[142] Physically, the middle-aged experience a decline in muscular strength, reaction time, sensory keenness, and cardiac output. Also, women experience menopause at an average age of 48.8 and a sharp drop in the hormone estrogen.[143] Men experience an equivalent endocrine system event to menopause. Andropause in males is a hormone fluctuation with physical and psychological effects that can be similar to those seen in menopausal females. As men age lowered testosterone levels can contribute to mood swings and a decline in sperm count. Sexual responsiveness can also be affected, including delays in erection and longer periods of penile stimulation required to achieve ejaculation. The important influence of biological and social changes experienced by women and men in middle adulthood is reflected in the fact that depression is highest at age 48.5 around the world.[144] The World Health Organization finds \"no general agreement on the age at which a person becomes old.\" Most \"developed countries\" set the age as 65 or 70. However, in developing countries inability to make \"active contribution\" to society, not chronological age, marks the beginning of old age.[145][146] According to Erikson's stages of psychosocial development, old age is the stage in which individuals assess the quality of their lives.[147] Erikson labels this stage as integrity versus despair. For integrated persons, there is a sense of fulfillment in life. They have become self-aware and optimistic due to life's commitments and connection to others. While reflecting on life, people in this stage develop feelings of contentment with their experiences. If a person falls into despair, they are often disappointed about failures or missed chances in life. They may feel that the time left in life is an insufficient amount to turn things around.[148] Physically, older people experience a decline in muscular strength, reaction time, stamina, hearing, distance perception, and the sense of smell.[149] They also are more susceptible to diseases such as cancer and pneumonia due to a weakened immune system.[150] Programs aimed at balance, muscle strength, and mobility have been shown to reduce disability among mildly (but not more severely) disabled elderly.[151] Sexual expression depends in large part upon the emotional and physical health of the individual. Many older adults continue to be sexually active and satisfied with their sexual activity.[152] Mental disintegration may also occur, leading to dementia or ailments such as Alzheimer's disease. The average age of onset for dementia in males is 78.8 and 81.9 for women.[153] It is generally believed that crystallized intelligence increases up to old age, while fluid intelligence decreases with age.[154] Whether or not normal intelligence increases or decreases with age depends on the measure and study. Longitudinal studies show that perceptual speed, inductive reasoning, and spatial orientation decline.[155] An article on adult cognitive development reports that cross-sectional studies show that \"some abilities remained stable into early old age\".[155] Parenting variables alone have typically accounted for 20 to 50 percent of the variance in child outcomes.[156] All parents have their own parenting styles. Parenting styles, according to Kimberly Kopko, are \"based upon two aspects of parenting behavior; control and warmth. Parental control refers to the degree to which parents manage their children's behavior. Parental warmth refers to the degree to which parents are accepting and responsive to their children's behavior.\"[157] The following parenting styles have been described in the child development literature: Parenting research has traditionally focused on mothers, but recent studies highlight the important role of fathers in child development. Children as young as 15 months benefit significantly from substantial engagement with their father.[162][163] In particular, a study in the U.S. and New Zealand found the presence of the natural father was the most significant factor in reducing rates of early sexual activity and rates of teenage pregnancy in girls.[164] However, neither a mother nor a father is actually essential in successful parenting, and both single parents as well as homosexual couples can support positive child outcomes.[165] Children need at least one consistently responsible adult with whom they can form a positive emotional bond. Having multiple such figures further increases the likelihood of positive outcomes.[165] Recent research also suggests that the way parents interact with infants can influence early brain development. Parents who guide their baby's attention during play by shifting their gaze between a toy and the child tend to have infants with more complex brain activity. This attention-guiding behavior helps infants process social cues more effectively.[166] Another parental factor often debated in terms of its effects on child development is divorce. Divorce in itself is not a determining factor of negative child outcomes. In fact, the majority of children from divorcing families fall into the normal range on measures of psychological and cognitive functioning.[167] A number of mediating factors play a role in determining the effects divorce has on a child, for example, divorcing families with young children often face harsher consequences in terms of demographic, social, and economic changes than do families with older children.[167] Positive coparenting after divorce is part of a pattern associated with positive child coping, while hostile parenting behaviors lead to a destructive pattern leaving children at risk.[167] Additionally, direct parental relationship with the child also affects the development of a child after a divorce. Overall, protective factors facilitating positive child development after a divorce are maternal warmth, positive father-child relationship, and cooperation between parents.[167] A way to improve developmental psychology is a representation of cross-cultural studies. The psychology field in general assumes that \"basic\" human developments are represented in any population, specifically the Western-Educated-Industrialized-Rich and Democratic (W.E.I.R.D.) subjects that are relied on for a majority of their studies. Previous research generalizes the findings done with W.E.I.R.D. samples because many in the Psychological field assume certain aspects of development are exempted from or are not affected by life experiences. However, many of the assumptions have been proven incorrect or are not supported by empirical research. For example, according to Kohlberg, moral reasoning is dependent on cognitive abilities. While both analytical and holistic cognitive systems do have the potential to develop in any adult, the West is still on the extreme end of analytical thinking, and the non-West tend to use holistic processes. Furthermore, moral reasoning in the West only considers aspects that support autonomy and the individual, whereas non-Western adults emphasize moral behaviors supporting the community and maintaining an image of holiness or divinity. Not all aspects of human development are universal and we can learn a lot from observing different regions and subjects.[168] An example of a non-Western model for development stages is the Indian model, focusing a large amount of its psychological research on morality and interpersonal progress. The developmental stages in Indian models are founded by Hinduism, which primarily teaches stages of life in the process of someone discovering their fate or Dharma.[169] This cross-cultural model can add another perspective to psychological development in which the West behavioral sciences have not emphasized kinship, ethnicity, or religion.[168] Indian psychologists study the relevance of attentive families during the early stages of life. The early life stages conceptualize a different parenting style from the West because it does not try to rush children out of dependency. The family is meant to help the child grow into the next developmental stage at a particular age. This way, when children finally integrate into society, they are interconnected with those around them and reach renunciation when they are older. Children are raised in joint families so that in early childhood (ages 6 months to 2 years) the other family members help gradually wean the child from its mother. During ages 2 to 5, the parents do not rush toilet training. Instead of training the child to perform this behavior, the child learns to do it as they mature at their own pace. This model of early human development encourages dependency, unlike Western models that value autonomy and independence. By being attentive and not forcing the child to become independent, they are confident and have a sense of belonging by late childhood and adolescence. This stage in life (5\u201315 years) is also when children start education and increase their knowledge of Dharma.[170] It is within early and middle adulthood that we see moral development progress. Early, middle, and late adulthood are all concerned with caring for others and fulfilling Dharma. The main distinction between early adulthood to middle or late adulthood is how far their influence reaches. Early adulthood emphasizes the importance of fulfilling the immediate family needs, until later adulthood when they broaden their responsibilities to the general public. The old-age life stage development reaches renunciation or a complete understanding of Dharma.[169] The current mainstream views in the psychological field are against the Indian model for human development. The criticism against such models is that the parenting style is overly protective and encourages too much dependency. It focuses on interpersonal instead of individual goals. Also, there are some overlaps and similarities between Erikson's stages of human development and the Indian model but both of them still have major differences. The West prefers Erickson's ideas over the Indian model because they are supported by scientific studies. The life cycles based on Hinduism are not as favored, because it is not supported with research and it focuses on the ideal human development.[169] [1][2]",
      "ground_truth_chunk_ids": [
        "165_fixed_chunk1"
      ],
      "source_ids": [
        "S165"
      ],
      "category": "factual",
      "id": 86
    },
    {
      "question": "What is Compound of five great dodecahedra?",
      "ground_truth": "This uniform polyhedron compound is a composition of 5 great dodecahedra, in the same arrangement as in the compound of 5 icosahedra. It is one of only five polyhedral compounds (along with the compound of six tetrahedra, the compound of two great dodecahedra, the compound of two small stellated dodecahedra, and the compound of five small stellated dodecahedra) which is vertex-transitive and face-transitive but not edge-transitive. This polyhedron-related article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "This uniform polyhedron compound is a composition of 5 great dodecahedra, in the same arrangement as in the compound of 5 icosahedra. It is one of only five polyhedral compounds (along with the compound of six tetrahedra, the compound of two great dodecahedra, the compound of two small stellated dodecahedra, and the compound of five small stellated dodecahedra) which is vertex-transitive and face-transitive but not edge-transitive. This polyhedron-related article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "184_random_chunk1"
      ],
      "source_ids": [
        "S384"
      ],
      "category": "factual",
      "id": 87
    },
    {
      "question": "What is Database?",
      "ground_truth": "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data became widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file. Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use",
      "expected_answer": "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data became widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file. Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages. Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. Because of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it. Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1] Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups: Both a database and its DBMS conform to the principles of a particular database model.[5] \"Database system\" refers collectively to the database model, database management system, and database.[6] Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed] Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.[7] Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security. The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational. The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another. The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed] Object databases were developed in the 1980s to overcome the inconvenience of object\u2013relational impedance mismatch, which led to the coining of the term \"post-relational\" and also the development of hybrid object\u2013relational databases. The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key\u2013value stores and document-oriented databases. A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs. The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense.[10] As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market. The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods: Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications. IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11] Edgar F. Codd worked at IBM in San Jose, California, in an office primarily involved in the development of hard disk systems.[12] He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[13] The paper described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of \"tables\", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each \"fact\" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated. Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based. The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit. In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys. For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided. As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic. Codd's paper inspired teams at various universities to research the subject, including one at University of California, Berkeley[12] led by Eugene Wong and Michael Stonebraker, who started INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard. IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs. In 1970, the University of Michigan began development of the MICRO Information Management System[14] based on D.L. Childs' Set-Theoretic Data model.[15][16][17] The university in 1974 hosted a debate between Codd and Bachman which Bruce Lindsay of IBM later described as \"throwing lightning bolts at each other!\".[12] MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System.[18] The system remained in production until 1998. In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine. Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata). IBM formed a team led by Codd that started working on a prototype system, System R despite opposition from others at the company.[12] The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language \u2013 SQL[citation needed] \u2013 had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2). Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.[19] Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions). In Sweden, Codd's paper was also read and Mimer SQL was developed in the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise. Another data model, the entity\u2013relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity\u2013relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.[citation needed] Besides IBM and various software companies such as Sybase and Informix Corporation, most large computer hardware vendors by the 1980s had their own database systems such as DEC's VAX Rdb/VMS.[20] The decade ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\"[21] dBASE was one of the top selling software titles in the 1980s and early 1990s. By the start of the decade databases had become a billion-dollar industry in about ten years.[20] The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields.[22] The term \"object\u2013relational impedance mismatch\" described the inconvenience of translating between programmed objects and database tables. Object databases and object\u2013relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object\u2013relational mappings (ORMs) attempt to solve the same problem. Database sales grew rapidly during the dotcom bubble and, after its end, the rise of ecommerce. The popularity of open source databases such as MySQL has grown since 2000, to the extent that Ken Jacobs of Oracle said in 2005 that perhaps \"these guys are doing to us what we did to IBM\".[20] XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records. NoSQL databases are often very fast,[23][24] do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally. In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency. NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system. Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software). Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database. One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases. Connolly and Begg define database management system (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database.\"[28] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access. The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object\u2013relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems. The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:[29] It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[30] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine. Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount. The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime.[a] Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client\u2013server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.[32] A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email. External interaction with the database will be via an application program that interfaces with the DBMS.[33] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information. A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET. Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages: Database languages are specific to a particular data model. Notable examples include: A database language may also incorporate features like: Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, \"data about the data\", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[37] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database). Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database. Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases. Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy. Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated. With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[38] Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program). Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces. This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases. Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption). Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.[39] Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability. A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs. After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.). When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation. After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc. Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state. Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[40] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc. Other DBMS features might include: Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as \"DevOps for database\".[41] The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity\u2013relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes. Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data. Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design). The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency. The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS. Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself. A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format. Common logical data models for databases include: An object\u2013relational database combines the two related structures. Physical data models include: Other models include: Specialized models are optimized for particular types of data: A database management system provides three views of the database data: While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database. The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[43] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance. The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types. Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more. The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).",
      "ground_truth_chunk_ids": [
        "46_fixed_chunk1"
      ],
      "source_ids": [
        "S046"
      ],
      "category": "factual",
      "id": 88
    },
    {
      "question": "What is Information security?",
      "ground_truth": "Information security (infosec) is the practice of protecting information by mitigating information risks. It is part of information risk management.[1] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[2][3] Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (known as the CIA triad, unrelated to the US government organization)[4] while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[5] This is largely achieved through a structured risk management process.[6] To standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on passwords, antivirus software, firewalls, encryption software, legal liability, security awareness and training, and so forth.[7] This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred, and destroyed.[8] While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized,[9][10] with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses.[11] They are responsible for keeping all of the technology within the company secure from malicious attacks that often attempt to acquire critical private information or gain control of the internal systems.[12][13] There are many specialist roles in Information Security including securing networks and",
      "expected_answer": "Information security (infosec) is the practice of protecting information by mitigating information risks. It is part of information risk management.[1] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[2][3] Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (known as the CIA triad, unrelated to the US government organization)[4] while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[5] This is largely achieved through a structured risk management process.[6] To standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on passwords, antivirus software, firewalls, encryption software, legal liability, security awareness and training, and so forth.[7] This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred, and destroyed.[8] While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized,[9][10] with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses.[11] They are responsible for keeping all of the technology within the company secure from malicious attacks that often attempt to acquire critical private information or gain control of the internal systems.[12][13] There are many specialist roles in Information Security including securing networks and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning, electronic record discovery, and digital forensics.[14] Information security standards are techniques generally outlined in published materials that attempt to protect the information of a user or organization.[15] This environment includes users themselves, networks, devices, all software, processes, information in storage or transit, applications, services, and systems that can be connected directly or indirectly to networks. The principal objective is to reduce the risks, including preventing or mitigating attacks. These published materials consist of tools, policies, security concepts, security safeguards, guidelines, risk management approaches, actions, training, best practices, assurance and technologies. Various definitions of information security are suggested below, summarized from different sources: Information security threats come in many different forms.[26] Some of the most common threats today are software attacks, theft of intellectual property, theft of identity, theft of equipment or information, sabotage, and information extortion.[27][28] Viruses,[29] worms, phishing attacks, and Trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses.[30] Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information through social engineering.[31][32] Sabotage usually consists of the destruction of an organization's website in an attempt to cause loss of confidence on the part of its customers.[33] Information extortion consists of theft of a company's property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner, as with ransomware.[34] One of the most functional precautions against these attacks is to conduct periodical user awareness.[35] Governments, military, corporations, financial institutions, hospitals, non-profit organizations, and private businesses amass a great deal of confidential information about their employees, customers, products, research, and financial status.[36] Should confidential information about a business's customers or finances or new product line fall into the hands of a competitor or hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation.[37] From a business perspective, information security must be balanced against cost; the Gordon-Loeb Model provides a mathematical economic approach for addressing this concern.[38] For the individual, information security has a significant effect on privacy, which is viewed very differently in various cultures.[39] Since the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering.[40] Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands.[41] However, for the most part protection was achieved through the application of procedural handling controls.[42][43] Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box.[44] As postal services expanded, governments created official organizations to intercept, decipher, read, and reseal letters (e.g., the U.K.'s Secret Office, founded in 1653[45]). In the mid-nineteenth century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity.[46] For example, the British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889.[47] Section 1 of the law concerned espionage and unlawful disclosures of information, while Section 2 dealt with breaches of official trust.[48] A public interest defense was soon added to defend disclosures in the interest of the state.[49] A similar law was passed in India in 1889, The Indian Official Secrets Act, which was associated with the British colonial era and used to crack down on newspapers that opposed the Raj's policies.[50] A newer version was passed in 1923 that extended to all matters of confidential or secret information for governance.[51]  By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters.[52] Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information.[53] The establishment of computer security inaugurated the history of information security. The need for such appeared during World War II.[54] The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls.[55] An arcane range of markings evolved to indicate who could handle documents (usually officers rather than enlisted troops) and where they should be stored as increasingly complex safes and storage facilities were developed.[56] The Enigma Machine, which was employed by the Germans to encrypt the data of warfare and was successfully decrypted by Alan Turing, can be regarded as a striking example of creating and using secured information.[57] Procedures evolved to ensure documents were destroyed properly, and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g., the capture of U-570[57]). Various mainframe computers were connected online during the Cold War to complete more sophisticated tasks, in a communication process easier than mailing magnetic tapes back and forth by computer centers. As such, the Advanced Research Projects Agency (ARPA), of the United States Department of Defense, started researching the feasibility of a networked system of communication to trade information within the United States Armed Forces. In 1968, the ARPANET project was formulated by Larry Roberts, which would later evolve into what is known as the internet.[58] In 1973, important elements of ARPANET security were found by internet pioneer Robert Metcalfe to have many flaws such as the: \"vulnerability of password structure and formats; lack of safety procedures for dial-up connections; and nonexistent user identification and authorizations\", aside from the lack of controls and safeguards to keep data safe from unauthorized access. Hackers had effortless access to ARPANET, as phone numbers were known by the public.[59] Due to these problems, coupled with the constant violation of computer security, as well as the exponential increase in the number of hosts and users of the system, \"network security\" was often alluded to as \"network insecurity\".[59] The end of the twentieth century and the early years of the twenty-first century saw rapid advancements in telecommunications, computing hardware and software, and data encryption.[60] The availability of smaller, more powerful, and less expensive computing equipment made electronic data processing within the reach of small business and home users.[61] The establishment of Transfer Control Protocol/Internetwork Protocol (TCP/IP) in the early 1980s enabled different types of computers to communicate.[62] These computers quickly became interconnected through the internet.[63] The rapid growth and widespread use of electronic data processing and electronic business conducted through the internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process, and transmit.[64] The academic disciplines of computer security and information assurance emerged along with numerous professional organizations, all sharing the common goals of ensuring the security and reliability of information systems.[65] The \"CIA triad\" of confidentiality, integrity, and availability is at the heart of information security.[66] The concept was introduced in the Anderson Report in 1972 and later repeated in The Protection of Information in Computer Systems. The abbreviation was coined by Steve Lipner around 1986.[67] Debate continues about whether or not this triad is sufficient to address rapidly changing technology and business requirements, with recommendations to consider expanding on the intersections between availability and confidentiality, as well as the relationship between security and privacy.[4] Other principles such as \"accountability\" have sometimes been proposed; it has been pointed out that issues such as non-repudiation do not fit well within the three core concepts.[68] In information security, confidentiality \"is the property, that information is not made available or disclosed to unauthorized individuals, entities, or processes.\"[69] While similar to \"privacy\", the two words are not interchangeable. Rather, confidentiality is a component of privacy that implements to protect our data from unauthorized viewers.[70] Examples of confidentiality of electronic data being compromised include laptop theft, password theft, or sensitive emails being sent to the incorrect individuals.[71] In IT security, data integrity means maintaining and assuring the accuracy and completeness of data over its entire lifecycle.[72] This means that data cannot be modified in an unauthorized or undetected manner.[73] This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing.[74] Information security systems typically incorporate controls to ensure their own integrity, in particular protecting the kernel or core functions against both deliberate and accidental threats.[75] Multi-purpose and multi-user computer systems aim to compartmentalize the data and processing such that no user or process can adversely impact another: the controls may not succeed however, as we see in incidents such as malware infections, hacks, data theft, fraud, and privacy breaches.[76] More broadly, integrity is an information security principle that involves human/social, process, and commercial integrity, as well as data integrity. As such it touches on aspects such as credibility, consistency, truthfulness, completeness, accuracy, timeliness, and assurance.[77] For any information system to serve its purpose, the information must be available when it is needed.[78] This means the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly.[79] High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades.[80] Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system, essentially forcing it to shut down.[81] In the realm of information security, availability can often be viewed as one of the most important parts of a successful information security program.[citation needed] Ultimately end-users need to be able to perform job functions; by ensuring availability an organization is able to perform to the standards that an organization's stakeholders expect.[82] This can involve topics such as proxy configurations, outside web access, the ability to access shared drives and the ability to send emails.[83] Executives oftentimes do not understand the technical side of information security and look at availability as an easy fix, but this often requires collaboration from many different organizational teams, such as network operations, development operations, incident response, and policy/change management.[84] A successful information security team involves many different key roles to mesh and align for the \"CIA\" triad to be provided effectively.[85] In addition to the classic CIA triad of security goals, some organisations may want to include security goals like authenticity, accountability, non-repudiation, and reliability. In law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction, nor can the other party deny having sent a transaction.[86] It is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology.[87] It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message, and nobody else could have altered it in transit (data integrity).[88] The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised.[89] The fault for these violations may or may not lie with the sender, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity. As such, the sender may repudiate the message (because authenticity and integrity are pre-requisites for non-repudiation).[90] In 1992 and revised in 2002, the OECD's Guidelines for the Security of Information Systems and Networks[91] proposed the nine generally accepted principles: awareness, responsibility, response, ethics, democracy, risk assessment, security design and implementation, security management, and reassessment.[92] Building upon those, in 2004 the NIST's Engineering Principles for Information Technology Security[68] proposed 33 principles. In 1998, Donn Parker proposed an alternative model for the classic \"CIA\" triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian Hexad are a subject of debate amongst security professionals.[93] In 2011, The Open Group published the information security management standard O-ISM3.[94] This standard proposed an operational definition of the key concepts of security, with elements called \"security objectives\", related to access control (9), availability (3), data quality (1), compliance, and technical (4). Risk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset).[95] A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm.[96] The likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact.[97] In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property).[98] The Certified Information Systems Auditor (CISA) Review Manual 2006 defines risk management as \"the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures,[99] if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization.\"[100] There are two things in this definition that may need some clarification. First, the process of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day.[101] Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.[102] Furthermore, these processes have limitations as security breaches are generally rare and emerge in a specific context which may not be easily duplicated.[103] Thus, any process and countermeasure should itself be evaluated for vulnerabilities.[104] It is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called \"residual risk\".[105] A risk assessment is carried out by a team of people who have knowledge of specific areas of the business.[106] Membership of the team may vary over time as different parts of the business are assessed.[107] The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis. Research has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human.[108] The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment: In broad terms, the risk management process consists of:[109][110] For any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business.[117] Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business.[118] The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.[119] Selecting and implementing proper security controls will initially help an organization bring down risk to acceptable levels.[120] Control selection should follow and should be based on the risk assessment.[121] Controls can vary in nature, but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. ISO/IEC 27001 has defined controls in different areas.[122] Organizations can implement additional controls according to requirement of the organization.[123] ISO/IEC 27002 offers a guideline for organizational information security standards.[124] Defense in depth is a fundamental security philosophy that relies on overlapping security systems designed to maintain protection even if individual components fail. Rather than depending on a single security measure, it combines multiple layers of security controls both in the cloud and at network endpoints. This approach includes combinations like firewalls with intrusion-detection systems, email filtering services with desktop anti-virus, and cloud-based security alongside traditional network defenses.[125]\nThe concept can be implemented through three distinct layers of administrative, logical, and physical controls,[126] or visualized as an onion model with data at the core, surrounded by people, network security, host-based security, and application security layers.[127] The strategy emphasizes that security involves not just technology, but also people and processes working together, with real-time monitoring and response being crucial components.[125] An important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information.[128] Not all information is equal and so not all information requires the same degree of protection.[129] This requires information to be assigned a security classification.[130] The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy.[131] The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.[132] Some factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete.[133] Laws and other regulatory requirements are also important considerations when classifying information.[134] The Information Systems Audit and Control Association (ISACA) and its Business Model for Information Security also serves as a tool for security professionals to examine security from a systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.[135] The type of information security classification labels selected and used will depend on the nature of the organization, with examples being:[132] All employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification.[138] The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.[139] Access to protected information must be restricted to people who are authorized to access the information.[140] The computer programs, and in many cases the computers that process the information, must also be authorized.[141] This requires that mechanisms be in place to control the access to protected information.[141] The sophistication of the access control mechanisms should be in parity with the value of the information being protected; the more sensitive or valuable the information the stronger the control mechanisms need to be.[142] The foundation on which access control mechanisms are built start with identification and authentication.[143] Access control is generally considered in three steps: identification, authentication, and authorization.[144][71] Identification is an assertion of who someone is or what something is. If a person makes the statement \"Hello, my name is John Doe\" they are making a claim of who they are.[145] However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe.[146] Typically the claim is in the form of a username. By entering that username you are claiming \"I am the person the username belongs to\".[147] Authentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe, a claim of identity.[148] The bank teller asks to see a photo ID, so he hands the teller his driver's license.[149] The bank teller checks the license to make sure it has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe.[150] If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly, by entering the correct password, the user is providing evidence that he/she is the person the username belongs to.[151] There are three different types of information that can be used for authentication:[152][153] Strong authentication requires providing more than one type of authentication information (two-factor authentication).[159] The username is the most common form of identification on computer systems today and the password is the most common form of authentication.[160] Usernames and passwords have served their purpose, but they are increasingly inadequate.[161] Usernames and passwords are slowly being replaced or supplemented with more sophisticated authentication mechanisms such as time-based one-time password algorithms.[162] After a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change).[163] This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures.[164] The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies.[165] Different computing systems are equipped with different kinds of access control mechanisms. Some may even offer a choice of different access control mechanisms.[166] The access control mechanism a system offers will be based upon one of three approaches to access control, or it may be derived from a combination of the three approaches.[71] The non-discretionary approach consolidates all access control under a centralized administration.[167] The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform.[168][169] The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources.[167] In the mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.[140] Examples of common access control mechanisms in use today include role-based access control, available in many advanced database management systems; simple file permissions provided in the UNIX and Windows operating systems;[170] Group Policy Objects provided in Windows network systems; and Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.[171] To be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions.[172] The U.S. Treasury's guidelines for systems processing sensitive or proprietary information, for example, states that all failed and successful authentication and access attempts must be logged, and all access to information must leave some type of audit trail.[173] Also, the need-to-know principle needs to be in effect when talking about access control. This principle gives access rights to a person to perform their job functions.[174] This principle is used in the government when dealing with difference clearances.[175] Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee the least amount of privilege to prevent employees from accessing more than what they are supposed to.[176] Need-to-know helps to enforce the confidentiality-integrity-availability triad. Need-to-know directly impacts the confidential area of the triad.[177] Information security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption.[178] Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user who possesses the cryptographic key, through the process of decryption.[179] Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.[71] Cryptography provides information security with other useful applications as well, including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications.[180] Older, less secure applications such as Telnet and File Transfer Protocol (FTP) are slowly being replaced with more secure applications such as Secure Shell (SSH) that use encrypted network communications.[181] Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU\u2011T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange.[182] Software applications such as GnuPG or PGP can be used to encrypt data files and email.[183] Cryptography can introduce security problems when it is not implemented correctly.[184] Cryptographic solutions need to be implemented using industry-accepted solutions that have undergone rigorous peer review by independent experts in cryptography.[185] The length and strength of the encryption key is also an important consideration.[186] A key that is weak or too short will produce weak encryption.[186] The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information.[187] They must be protected from unauthorized disclosure and destruction, and they must be available when needed.[citation needed] Public key infrastructure (PKI) solutions address many of the problems that surround key management.[71] U.S. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.[188] In the field of information security, Harris[189]\noffers the following definitions of due care and due diligence: \"Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees[190].\" And, [Due diligence are the] \"continual activities that make sure the protection mechanisms are continually maintained and operational.\"[191] Attention should be made to two important points in these definitions.[192][193] First, in due care, steps are taken to show; this means that the steps can be verified, measured, or even produce tangible artifacts.[194][195] Second, in due diligence, there are continual activities; this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.[196] Organizations have a responsibility with practicing duty of care when applying information security. The Duty of Care Risk Analysis Standard (DoCRA)[197] provides principles and practices for evaluating risk.[198] It considers all parties that could be affected by those risks.[199] DoCRA helps evaluate safeguards if they are appropriate in protecting others from harm while presenting a reasonable burden.[200] With increased data breach litigation, companies must balance security controls, compliance, and its mission.[201] Computer security incident management is a specialized form of incident management focused on monitoring, detecting, and responding to security events on computers and networks in a predictable way.[202] Organizations implement this through incident response plans (IRPs) that are activated when security breaches are detected.[203] These plans typically involve an incident response team (IRT) with specialized skills in areas like penetration testing, computer forensics, and network security.[204] Change management is a formal process for directing and controlling alterations to the information processing environment.[205][206] This includes alterations to desktop computers, the network, servers, and software.[207] The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the processing environment as changes are made.[208] It is not the objective of change management to prevent or hinder necessary changes from being implemented.[209][210] Any change to the information processing environment introduces an element of risk.[211] Even apparently simple changes can have unexpected effects.[212] One of management's many responsibilities is the management of risk.[213][214] Change management is a tool for managing the risks introduced by changes to the information processing environment.[215] Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.[216] Not every change needs to be managed.[217][218] Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment.[219] Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management.[220] However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity.[221] The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.[222] Change management is usually overseen by a change review board composed of representatives from key business areas,[223] security, networking, systems administrators, database administration, application developers, desktop support, and the help desk.[224] The tasks of the change review board can be facilitated with the use of automated work flow application.[225] The responsibility of the change review board is to ensure the organization's documented change management procedures are followed.[226] The change management process is as follows[227] Change management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment.[259] Good change management procedures improve the overall quality and success of changes as they are implemented.[260] This is accomplished through planning, peer review, documentation, and communication.[261] ISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps[262] (Full book summary),[263] and ITIL all provide valuable guidance on implementing an efficient and effective change management program information security.[264] Business continuity management (BCM) concerns arrangements aiming to protect an organization's critical business functions from interruption due to incidents, or at least minimize the effects.[265][266] BCM is essential to any organization to keep technology and business in line with current threats to the continuation of business as usual.[267] The BCM should be included in an organizations risk analysis plan to ensure that all of the necessary business functions have what they need to keep going in the event of any type of threat to any business function.[268] It encompasses: Whereas BCM takes a broad approach to minimizing disaster-related risks by reducing both the probability and the severity of incidents, a disaster recovery plan (DRP) focuses specifically on resuming business operations as quickly as possible after a disaster.[278] A disaster recovery plan, invoked soon after a disaster occurs, lays out the steps necessary to recover critical information and communications technology (ICT) infrastructure.[279] Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.[280] Below is a partial listing of governmental laws and regulations in various parts of the world that have, had, or will have, a significant effect on data processing and information security.[281][282] Important industry sector regulations have also been included when they have a significant impact on information security.[281] The US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.[317] Describing more than simply how security aware employees are, information security culture is the ideas, customs, and social behaviors of an organization that impact information security in both positive and negative ways.[318] Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization. The way employees think and feel about security and the actions they take can have a big impact on information security in organizations. Roer & Petric (2017) identify seven core dimensions of information security culture in organizations:[319] Andersson and Reimers (2014) found that employees often do not see themselves as part of the organization Information Security \"effort\" and often take actions that ignore organizational information security best interests.[321] Research shows information security culture needs to be improved continuously. In Information Security Culture from Analysis to Change, authors commented, \"It's a never ending process, a cycle of evaluation and change or maintenance.\" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.[322]",
      "ground_truth_chunk_ids": [
        "157_fixed_chunk1"
      ],
      "source_ids": [
        "S157"
      ],
      "category": "factual",
      "id": 89
    },
    {
      "question": "What is Energy?",
      "ground_truth": "Energy (from Ancient Greek \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 (en\u00e9rgeia) 'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J). Forms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass. These are not mutually exclusive. All living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the Sun.[6] The total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the object's components \u2013 while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself.[7] While these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to",
      "expected_answer": "Energy (from Ancient Greek  \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 (en\u00e9rgeia)\u00a0'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J). Forms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass. These are not mutually exclusive. All living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the Sun.[6] The total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the object's components \u2013 while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself.[7] While these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to the combined potentials within an atomic nucleus from either the nuclear force or the weak force, among other examples.[8] The word energy derives from the Ancient Greek: \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1, romanized:\u00a0energeia, lit.\u2009'activity, operation',[11] which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.[12] In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the motions of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two.[13] Writing in the early 18th century, \u00c9milie du Ch\u00e2telet proposed the concept of conservation of energy in the marginalia of her French language translation of Newton's Principia Mathematica, which represented the first formulation of a conserved measurable quantity that was distinct from momentum, and which would later be called \"energy\".[14] In 1807, Thomas Young was possibly the first to use the term \"energy\" instead of vis viva, in its modern sense.[15] Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense,[16] and in 1853, William Rankine coined the term \"potential energy\".[17] The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system.[18] It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.[19] These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics.[20] Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, Walther Nernst, and others.[21] It also led to a mathematical formulation of the concept of entropy by Clausius[22] and to the introduction of laws of radiant energy by Jo\u017eef Stefan.[23] According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time.[24] Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.[25] Albert Einstein's 1905 theory of special relativity showed that rest mass corresponds to an equivalent amount of rest energy. This means that rest mass can be converted to or from equivalent amounts of (non-material) forms of energy, for example, kinetic energy, potential energy, and electromagnetic radiant energy. When this happens, rest mass is not conserved, unlike the total mass or total energy. All forms of energy contribute to the total mass and total energy. Thus, conservation of energy (total, including material or rest energy) and conservation of mass (total, not just rest) are one (equivalent) law. In the 18th century, these had appeared as two seemingly-distinct laws.[26][27] The first evidence of quantization in atoms was the observation of spectral lines in light from the sun in the early 1800s by Joseph von Fraunhofer and William Hyde Wollaston. The notion of quantized energy levels was proposed in 1913 by Danish physicist Niels Bohr in the Bohr theory of the atom. The modern quantum mechanical theory giving an explanation of these energy levels in terms of the Schr\u00f6dinger equation was advanced by Erwin Schr\u00f6dinger and Werner Heisenberg in 1926.[28] Noether's theorem shows that the symmetry of this equation is equivalent to a conservation of probability.[29] At the quantum level, mass-energy interactions are all subject to this principle.[30] During wave function collapse, the conservation of energy does not hold at the local level, although statistically the principle holds on average for sufficiently large numbers of collapses.[31] Conservation of energy does apply during wave function collapse in H. Everett's many-worlds interpretation of quantum mechanics.[32] In dimensional analysis, the base units of energy are given by: Work = Force \u00d7 Distance = M L2 T\u22122, with the fundamental dimensions of Mass M, Length L, and time T.[5] In the International System of Units (SI), the unit of energy is the joule. It is a derived unit that is equal to the energy expended, or work done, in applying a force of one newton through a distance of one metre.[1] The SI unit of power, defined as energy per unit of time, is the watt, which is one joule per second.[3] Thus, a kilowatt-hour (kWh), which can be realized as the energy delivered by one kilowatt of power for an hour, is equal to 3.6 million joules.[33] The CGS energy unit is the erg and the imperial and US customary unit is the foot-pound.[34] Other energy units such as the electronvolt, food calorie, thermodynamic kilocalorie and BTU are used in specific areas of science and commerce.[35][2] In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept. Work, a function of energy, is force times distance.[36] This says that the work (\n\n\n\nW\n\n\n{\\displaystyle W}\n\n) is equal to the line integral of the force F along a path C; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.[37] The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems.[38] These classical equations have direct analogs in nonrelativistic quantum mechanics.[39] Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).[40] Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian;[41] for example, dissipative systems with continuous symmetries need not have a corresponding conservation law. In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular, or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is usually accompanied by a decrease, and sometimes an increase, of the total energy of the substances involved. Some energy may be transferred between the surroundings and the reactants in the form of heat or light; thus the products of a reaction have sometimes more but usually less energy than the reactants. A reaction is said to be exothermic or exergonic if the final state is lower on the energy scale than the initial state; in the less common case of endothermic reactions the situation is the reverse.[42] Chemical reactions are usually not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at a given temperature\u00a0T) is related to the activation energy\u00a0E by the Boltzmann population factor\u00a0e\u2212E/kT; that is, the probability of a molecule to have energy greater than or equal to\u00a0E at a given temperature\u00a0T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction can be provided in the form of thermal energy.[43] In biology, energy is an attribute of all biological systems, from the biosphere to the smallest living organism. It enables the growth, development, and functioning of a biological cell or organelle in an organism. All living creatures rely on an external source of energy to be able to grow and reproduce \u2013 radiant energy from the Sun in the case of green plants and chemical energy (in some form) in the case of animals. Energy provided through cellular respiration is stored in nutrients such as carbohydrates (including sugars), lipids, and proteins by cells.[44] Sunlight's radiant energy is captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into carbohydrates, lipids, proteins, and oxygen.[45] Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark in a forest fire, or it may be made available more slowly for animal or human metabolism when organic molecules are ingested and catabolism is triggered by enzyme action.[46] The basal metabolism rate measures the food energy expenditure per unit time by endothermic animals at rest.[47] In other words it is the energy required by body organs to perform normally. For humans, metabolic equivalent of task (MET) compares the energy expenditure per unit mass while performing a physical activity, relative to a baseline. By convention, this baseline is 3.5\u00a0mL of oxygen consumed per kg per minute, which is the energy consumed by a typical individual when sitting quietly.[48] In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, using as a standard an average human energy expenditure of 6,900\u00a0kJ per day and a basal metabolic rate of 80 watts.[citation needed] For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 \u00f7 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum.[49] The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.[50] The daily 1,600\u20133,000 calories (7\u201313\u00a0MJ) recommended for a human adult are taken as food molecules,[51] mostly carbohydrates and fats. Only a tiny fraction of the original chemical energy is used for work:[note 1] It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy); most machines manage higher efficiencies.[citation needed] In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism's tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\").[note 2] Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology. As an example, to take just the first step in the food chain: of the estimated 124.7\u00a0Pg/a of carbon that is fixed by photosynthesis, 64.3\u00a0Pg/a (52%) are used for the metabolism of green plants,[52] i.e. reconverted into carbon dioxide and heat. Multicellular organisms such as humans have cell forms that are classified as Eukaryote. These cells include an organelle called the mitochondria that generates chemical energy for the rest of the hosting cell. Ninety percent of the oxygen intake by humans is utilized by the mitochondria, especially for nutrient processing.[53] The molecule adenosine triphosphate (ATP) is the primary energy transporter in living cells, providing an energy source for cellular processes. It is continually being broken down and synthesized as a component of cellular respiration.[54] Two examples of nutrients consumed by animals are glucose (C6H12O6) and stearin (C57H110O6). These food molecules are oxidized to carbon dioxide and water in the mitochondria:[55]\n\n\n\n\n\n\nC\n\n6\n\n\n\n\n\n\nH\n\n12\n\n\n\n\n\n\nO\n\n6\n\n\n\n\n\n+\n6\n\n\nO\n\n2\n\n\n\n\n\n\u27f6\n6\n\n\nCO\n\n2\n\n\n\n\n\n+\n6\n\n\nH\n\n2\n\n\n\n\n\nO\n\n\n\n{\\displaystyle {\\ce {C6H12O6 + 6O2 -> 6CO2 + 6H2O}}}\n\n\n\n\n\n\n\n\nC\n\n57\n\n\n\n\n\n\nH\n\n110\n\n\n\n\n\n\nO\n\n6\n\n\n\n\n\n+\n\n(\n81\n\n\n\n1\n2\n\n\n)\n\n\nO\n\n2\n\n\n\n\n\n\u27f6\n57\n\n\nCO\n\n2\n\n\n\n\n\n+\n55\n\n\nH\n\n2\n\n\n\n\n\nO\n\n\n\n{\\displaystyle {\\ce {C57H110O6 + (81 1/2) O2 -> 57CO2 + 55H2O}}}\n\n\nand some of the energy is used to convert ADP into ATP:[56][53] The rest of the chemical energy of the nutrients are converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior,[57] while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes, and hurricanes are all a result of energy transformations in our atmosphere brought about by solar energy. Sunlight is the main input to Earth's energy budget which accounts for its temperature and climate stability, after accounting for interaction with the atmosphere.[58] Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example when) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity).[59]  An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, suddenly give up some of their thermal energy to power a few days of violent air movement.[60] In a slower process, radioactive decay of atoms in the core of the Earth releases heat, which supplies more than half of the planet's internal heat budget.[61] In the present day, this radiogenic heat production was primarily driven by the decay of Uranium-235, Potassium-40, and Thorium-232 some time in the past.[62] This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may later be transformed into active kinetic energy during landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks.[63] Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars (which created these atoms).[64] Early in a planet's history, the accretion process provides impact energy that can partially or completely melt the body. This allows a planet to become differentiated by chemical element. Chemical phase changes of minerals during formation provide additional internal heating. Over time the internal heat is brought to the surface then radiated away into space, cooling the body. Accreted radiogenic heat sources settle toward the core, providing thermal energy to the planet on a geologic time scale.[65] Ongoing sedimentation provides a persistent internal energy source for gas giant planets like Jupiter and Saturn.[66] In cosmology and astronomy the phenomena of stars, nova, supernova, quasars, and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen).[67] The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.[68] The accretion of matter onto a compact object is a very efficient means of generating energy from gravitational potential. This behavior is responsible for some of the universe's brightest persistent energy sources.[69] The Penrose process is a theoretical method by which energy could be extracted from a rotating black hole.[70] Hawking radiation is the emission of black-body radiation from a black hole, which results in a steady loss of mass and rotational energy. As the object evaporates, the temperature of this radiation is predicted to increase, speeding up the process.[71] In quantum mechanics, energy is defined in terms of the energy operator\n(Hamiltonian) as a time derivative of the wave function. The Schr\u00f6dinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schr\u00f6dinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta.[72] In the solution of the Schr\u00f6dinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by the Planck relation: \n\n\n\nE\n=\nh\n\u03bd\n\n\n{\\displaystyle E=h\\nu }\n\n, where \n\n\n\nh\n\n\n{\\displaystyle h}\n\n is the Planck constant and \n\n\n\n\u03bd\n\n\n{\\displaystyle \\nu }\n\n the frequency. In the case of an electromagnetic wave these energy states are called quanta of light or photons. For matter waves, the de Broglie relation yields \n\n\n\np\n=\nh\n\u03bd\n\n\n{\\displaystyle p=h\\nu }\n\n, where \n\n\n\np\n\n\n{\\displaystyle p}\n\n is the momentum.[73] When calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically \u2013 using Lorentz transformations instead of Newtonian mechanics \u2013 Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:[74] E\n\n0\n\n\n=\n\nm\n\n0\n\n\n\nc\n\n2\n\n\n,\n\n\n{\\displaystyle E_{0}=m_{0}c^{2},}\n\n\nwhere For example, consider electron\u2013positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process \u2013 the inverse process is called pair creation \u2013 in which the rest mass of the particles is created from a sufficiently energetic photon near a nucleus.[75] In general relativity, the stress\u2013energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.[76][page\u00a0needed] Energy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"),[77] and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws. In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy\u2013momentum 4-vector).[76][page\u00a0needed] In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of spacetime (= boosts). Energy may be transformed between different forms at various efficiencies. Devices that usefully transform between these forms are called transducers. Examples of transducers include a battery (from chemical energy to electric energy), a dam (from gravitational potential energy to the kinetic energy of water spinning the blades of a turbine, and ultimately to electric energy through an electric generator), and a heat engine (from heat to work).[78][79] Examples of energy transformation include generating electric energy from heat energy via a steam turbine,[79] or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground.[80] The Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that itself (since it still contains the same total energy even in different forms) but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.[81] There are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics.[82] However, some energy transformations can be quite efficient.[83] The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a sufficiently small scale, but certain larger transformations are highly improbable because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.[84] Energy transformations in the universe over time are characterized by various kinds of potential energy, that has been available since the Big Bang, being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available.[85] Familiar examples of such processes include nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae to \"store\" energy in the creation of heavy isotopes (such as uranium and thorium), and nuclear decay, a process in which energy is released that was originally stored in these heavy elements, before they were incorporated into the Solar System and the Earth.[86] This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic and thermal energy in a very short time.[87] Yet another example of energy transformation is that of a simple gravity pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at its maximum. At its lowest point the kinetic energy is at its maximum and is equal to the decrease in potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever. Energy is transferred from potential energy (\n\n\n\n\nE\n\np\n\n\n\n\n{\\displaystyle E_{p}}\n\n) to kinetic energy (\n\n\n\n\nE\n\nk\n\n\n\n\n{\\displaystyle E_{k}}\n\n) and then back to potential energy constantly. This is referred to as conservation of energy. In this isolated system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following: The equation can then be simplified further since \n\n\n\n\nE\n\np\n\n\n=\nm\ng\nh\n\n\n{\\displaystyle E_{p}=mgh}\n\n (mass times acceleration due to gravity times the height) and \n\n\n\n\nE\n\nk\n\n\n=\n\n\n1\n2\n\n\nm\n\nv\n\n2\n\n\n\n\n{\\textstyle E_{k}={\\frac {1}{2}}mv^{2}}\n\n (half\u00a0mass times velocity squared). Then the total amount of energy can be found by adding \n\n\n\n\nE\n\np\n\n\n+\n\nE\n\nk\n\n\n=\n\nE\n\ntotal\n\n\n\n\n{\\displaystyle E_{p}+E_{k}=E_{\\text{total}}}\n\n.[88] Within a gravitational field, both mass and energy give rise to a measureable weight when trapped in a system with zero momentum. The formula E\u00a0=\u00a0mc2, derived by Albert Einstein (1905) quantifies this mass\u2013energy equivalence between relativistic mass and energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincar\u00e9 (1900), Friedrich Hasen\u00f6hrl (1904), and others (see Mass\u2013energy equivalence#History for further information). Part of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since \n\n\n\n\nc\n\n2\n\n\n\n\n{\\displaystyle c^{2}}\n\n is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy, as can be seen in nuclear reactors and nuclear weapons.[89] For example, 1\u00a0kg of rest mass equals 9\u00d71016\u00a0joules, equivalent to 21.5 megatonnes of TNT.[90] Conversely, the mass equivalent of an everyday amount energy is minuscule. Examples of large-scale transformations between the rest energy of matter and other forms of energy are found in nuclear physics and particle physics. The complete conversion of matter, such as atoms, to non-matter, such as photons, occurs during interaction with antimatter.[91] Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another is reversible, as in the pendulum system described above.[92] At the atomic scale, thermal energy is present in the form of motion and vibrations of individual atoms and molecules. When heat is generated, radiation excites lower energy states of these atoms and their surrounding fields. This heating process acts as a reservoir for part of the applied energy, from which it cannot be converted with 100% efficiency into other forms of energy.[93] According to the second law of thermodynamics, this heat can only be completely recovered as usable energy at the price of an increase in some other kind of heat-like disorder in quantum states. As the universe evolves with time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or as other kinds of increases in disorder). This has led to the hypothesis of the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), continues to decrease.[94] The fact that energy can be neither created nor destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out as work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.[95] While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations.[96] The total energy of a system can be calculated by adding up all forms of energy in the system. Richard Feynman said during a 1961 lecture:[97] There is a fact, or if you wish, a law, governing all natural phenomena that are known to date. There is no known exception to this law \u2013 it is exact so far as we know. The law is called the conservation of energy. It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same. \u2014\u200aThe Feynman Lectures on Physics Most kinds of energy (with gravitational energy being a notable exception)[98] are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.[96][97] This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time,[99] a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle \u2013 it is impossible to define the exact amount of energy during any definite time interval (though this is practically significant only for very short time intervals). The uncertainty principle should not be confused with energy conservation \u2013 rather it provides mathematical limits to which energy can in principle be defined and measured. Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appear as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.[100] In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scale, the uncertainty in the energy is given by which is similar in form to the Heisenberg Uncertainty Principle,[101] but not really mathematically equivalent thereto, since E and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics.[102] In particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum.[102] The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions).[103]:\u200a101\u200a Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law),[103]:\u200a336\u200a for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force,[104] for the Van der Waals force,[105] and some other observable phenomena.[106] Energy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat.[note 3] Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy,[note 4] tidal interactions,[107] and the conductive transfer of thermal energy.[108] Energy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:[note 5][108] where \n\n\n\nE\n\n\n{\\displaystyle E}\n\n is the amount of energy transferred, \n\n\n\nW\n\n\n{\\displaystyle W}\n\n\u00a0 represents the work done on or by the system, and \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n represents the heat flow into or out of the system. As a simplification, the heat term, \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n, can sometimes be ignored, especially for fast processes involving gases, which are poor conductors of heat, or when the thermal efficiency of the transfer is high. For such adiabatic processes, This simplified equation is the one used to define the joule, for example. Beyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (this process is illustrated by injection of an air-fuel mixture into a car engine, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by \n\n\n\n\nE\n\nmatter\n\n\n\n\n{\\displaystyle E_{\\text{matter}}}\n\n, one may write:[109] Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.[110] The first law of thermodynamics asserts that the total energy of a system and its surroundings (but not necessarily thermodynamic free energy) is always conserved[111] and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as:[112] where the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and its change dS is positive when heat is added to the system), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system). This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and PV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form by:[108] where \n\n\n\n\u03b4\nQ\n\n\n{\\displaystyle \\delta Q}\n\n is the heat supplied to the system and \n\n\n\n\u03b4\nW\n\n\n{\\displaystyle \\delta W}\n\n is the work applied to the system. The energy of a mechanical harmonic oscillator (a mass on a spring) is alternately kinetic and potential energy. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential.[88] Over a whole cycle, or over many cycles, average energy is equally split between kinetic and potential. This is an example of the equipartition principle: the total energy of a system with many degrees of freedom is equally split among all available degrees of freedom, on average.[113] This principle is vitally important to understanding the behavior of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is part of the second law of thermodynamics. The second law of thermodynamics is simple only for systems which are near or in a physical equilibrium state. For non-equilibrium systems, the laws governing the systems' behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production.[114][115] It states that nonequilibrium systems behave in such a way as to maximize their entropy production.[116]",
      "ground_truth_chunk_ids": [
        "54_fixed_chunk1"
      ],
      "source_ids": [
        "S054"
      ],
      "category": "factual",
      "id": 90
    },
    {
      "question": "What is Volcano?",
      "ground_truth": "A volcano is commonly defined as a vent or fissure in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface.[1] On Earth, volcanoes are most often found where tectonic plates are diverging or converging, and because most of Earth's plate boundaries are underwater, most volcanoes are found underwater. For example, a mid-ocean ridge, such as the Mid-Atlantic Ridge, has volcanoes caused by divergent tectonic plates whereas the Pacific Ring of Fire has volcanoes caused by convergent tectonic plates. Volcanoes resulting from divergent tectonic activity are usually non-explosive whereas those resulting from convergent tectonic activity cause violent eruptions.[2][3] Volcanoes can also form where there is stretching and thinning of the crust's plates, such as in the East African Rift, the Wells Gray-Clearwater volcanic field, and the Rio Grande rift in North America. Volcanism away from plate boundaries most likely arises from upwelling diapirs from the core\u2013mantle boundary called mantle plumes, 3,000 kilometres (1,900 mi) deep within Earth. This results in hotspot volcanism or intraplate volcanism, in which the plume may cause thinning of the crust and result in a volcanic island chain due to the continuous movement of the tectonic plate, of which the Hawaiian hotspot is an example.[4] Volcanoes are usually not created at transform tectonic boundaries where two tectonic plates slide past one another. Volcanoes, based on their frequency of eruption or volcanism, are referred to as either active, dormant, or extinct.[5] Active volcanoes have a history of volcanism and are likely to erupt again, while extinct ones are not capable of eruption at all as they have no magma source. \"Dormant\" volcanoes have not erupted in a long time \u2013 generally accepted as since the start of the Holocene, about 12,000",
      "expected_answer": "A volcano is commonly defined as a vent or fissure in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface.[1] On Earth, volcanoes are most often found where tectonic plates are diverging or converging, and because most of Earth's plate boundaries are underwater, most volcanoes are found underwater. For example, a mid-ocean ridge, such as the Mid-Atlantic Ridge, has volcanoes caused by divergent tectonic plates whereas the Pacific Ring of Fire has volcanoes caused by convergent tectonic plates. Volcanoes resulting from divergent tectonic activity are usually non-explosive whereas those resulting from convergent tectonic activity cause violent eruptions.[2][3] Volcanoes can also form where there is stretching and thinning of the crust's plates, such as in the East African Rift, the Wells Gray-Clearwater volcanic field, and the Rio Grande rift in North America. Volcanism away from plate boundaries most likely arises from upwelling diapirs from the core\u2013mantle boundary called mantle plumes, 3,000 kilometres (1,900\u00a0mi) deep within Earth. This results in hotspot volcanism or intraplate volcanism, in which the plume may cause thinning of the crust and result in a volcanic island chain due to the continuous movement of the tectonic plate, of which the Hawaiian hotspot is an example.[4] Volcanoes are usually not created at transform tectonic boundaries where two tectonic plates slide past one another. Volcanoes, based on their frequency of eruption or volcanism, are referred to as either active, dormant, or extinct.[5] Active volcanoes have a history of volcanism and are likely to erupt again, while extinct ones are not capable of eruption at all as they have no magma source. \"Dormant\" volcanoes have not erupted in a long time \u2013 generally accepted as since the start of the Holocene, about 12,000 years ago \u2013 but may erupt again. However, dormant volcanoes are technically considered to be seismically \"active\".[5] These categories aren't entirely uniform; they may overlap for certain examples.[2][6][7] Large eruptions can affect atmospheric temperature as ash and droplets of sulfuric acid obscure the Sun and cool Earth's troposphere. Historically, large volcanic eruptions have been followed by volcanic winters which have caused catastrophic famines.[8] Other planets besides Earth have volcanoes. For example, volcanoes are very numerous on Venus.[9] Mars has significant volcanoes.[10] In 2009, a paper was published suggesting a new definition for the word 'volcano' that includes processes such as cryovolcanism. It suggested that a volcano be defined as 'an opening on a planet or moon's surface from which magma, as defined for that body, and/or magmatic gas is erupted.'[11] This article mainly covers volcanoes on Earth. See \u00a7\u00a0Volcanoes on other celestial bodies and cryovolcano for more information. The word volcano (UK: /v\u0252l\u02c8ke\u026an\u0259\u028a/; US: /v\u0251\u02d0l\u02c8ke\u026ano\u028a/) originates from the early 17th century, derived from the Italian name Vulcano, a volcanic island in the Aeolian Islands of Italy, which in turn comes from the Latin name Volc\u0101nus or Vulc\u0101nus, referring to Vulcan, the god of fire in Roman mythology.[12][13] The set of processes and phenomena involved in volcanic activity is called volcanism [early 19th century: from volcano + -ism]. The study of volcanism and volcanoes is called volcanology [mid-19th century: from volcano + -logy], sometimes spelled vulcanology.[12] According to the theory of plate tectonics, Earth's lithosphere, its rigid outer shell, is broken into sixteen larger and several smaller plates. These move continuously at a slow pace, due to convection in the underlying ductile mantle, and most volcanic activity on Earth takes place along plate boundaries, where plates are converging (and lithosphere is being destroyed) or are diverging (and new lithosphere is being created).[14] During the development of geological theory, certain concepts that allowed the grouping of volcanoes in time, place, structure and composition have developed that ultimately have had to be explained in the theory of plate tectonics. For example, some volcanoes are polygenetic with more than one period of activity during their history; other volcanoes that become extinct after erupting exactly once are monogenetic (meaning \"one life\") and such volcanoes are often grouped together in a geographical region.[15] At the mid-ocean ridges, two tectonic plates diverge from one another as hot mantle rock creeps upwards beneath the thinned oceanic crust. The decrease of pressure in the rising mantle rock leads to adiabatic expansion and the partial melting of the rock, causing volcanism and creating new oceanic crust. Most divergent plate boundaries are at the bottom of the oceans, and so most volcanic activity on Earth is submarine, forming new seafloor. Black smokers (also known as deep sea vents) are evidence of this kind of volcanic activity. Where the mid-oceanic ridge is above sea level, volcanic islands are formed, such as Iceland.[16][3] Subduction zones are places where two plates, usually an oceanic plate and a continental plate, collide. The oceanic plate subducts (dives beneath the continental plate), forming a deep ocean trench just offshore. In a process called flux melting, water released from the subducting plate lowers the melting temperature of the overlying mantle wedge, thus creating magma. This magma tends to be extremely viscous because of its high silica content, so it often does not reach the surface but cools and solidifies at depth. When it does reach the surface, however, a volcano is formed. Thus subduction zones are bordered by chains of volcanoes called volcanic arcs. Typical examples are the volcanoes in the Pacific Ring of Fire, such as the Cascade Volcanoes or the Japanese Archipelago, or the eastern islands of Indonesia.[17][2] Hotspots are volcanic areas thought to be formed by mantle plumes, which are hypothesized to be columns of hot material rising from the core-mantle boundary. As with mid-ocean ridges, the rising mantle rock experiences decompression melting which generates large volumes of magma. Because tectonic plates move across mantle plumes, each volcano becomes inactive as it drifts off the plume, and new volcanoes are created where the plate advances over the plume. The Hawaiian Islands are thought to have been formed in such a manner, as has the Snake River Plain, with the Yellowstone Caldera being part of the North American plate currently above the Yellowstone hotspot.[18][4] However, the mantle plume hypothesis has been questioned.[19] Sustained upwelling of hot mantle rock can develop under the interior of a continent and lead to rifting. Early stages of rifting are characterized by flood basalts and may progress to the point where a tectonic plate is completely split.[20][21] A divergent plate boundary then develops between the two halves of the split plate. However, rifting often fails to completely split the continental lithosphere (such as in an aulacogen), and failed rifts are characterized by volcanoes that erupt unusual alkali lava or carbonatites. Examples include the volcanoes of the East African Rift.[22] A volcano needs a reservoir of molten magma (e.g. a magma chamber), a conduit to allow magma to rise through the crust, and a vent to allow the magma to escape above the surface as lava. The erupted volcanic material (lava and tephra) that is deposited around the vent is known as a volcanic edifice, typically a volcanic cone or mountain.[2][23] The most common perception of a volcano is of a conical mountain, spewing lava and poisonous gases from a crater at its summit; however, this describes just one of the many types of volcano. The features of volcanoes are varied. The structure and behaviour of volcanoes depend on several factors. Some volcanoes have rugged peaks formed by lava domes rather than a summit crater while others have landscape features such as massive plateaus. Vents that issue volcanic material (including lava and ash) and gases (mainly steam and magmatic gases) can develop anywhere on the landform and may give rise to smaller cones such as Pu\u02bbu \u02bb\u014c\u02bb\u014d on a flank of K\u012blauea in Hawaii. Volcanic craters are not always at the top of a mountain or hill and may be filled with lakes such as with Lake Taup\u014d in New Zealand. Some volcanoes can be low-relief landform features, with the potential to be hard to recognize as such and be obscured by geological processes.[2][24][25] Other types of volcano include mud volcanoes, which are structures often not associated with known magmatic activity; and cryovolcanoes (or ice volcanoes), particularly on some moons of Jupiter, Saturn, and Neptune. Active mud volcanoes tend to involve temperatures much lower than those of igneous volcanoes except when the mud volcano is actually a vent of an igneous volcano. Volcanic fissure vents are generally found at diverging plate boundaries, they are flat, linear fractures through which basaltic lava emerges. These kinds of volcanoes are non-explosive and the basaltic lava tends to have a low viscosity and solidifies slowly leading to a gentle sloping basaltic lava plateau. They often relate or constitute shield volcanoes[2][26] Shield volcanoes, so named for their broad, shield-like profiles, are formed by the eruption of low-viscosity basaltic or andesitic lava that can flow a great distance from a vent. They generally do not explode catastrophically but are characterized by relatively gentle effusive eruptions.[2] Since low-viscosity magma is typically low in silica, shield volcanoes are more common in oceanic than continental settings. The Hawaiian volcanic chain is a series of shield cones, and they are common in Iceland, as well.[26] Olympus Mons, an extinct martian shield volcano is the largest known volcano in the Solar System.[27] Lava domes, also called dome volcanoes, have steep convex sides built by slow eruptions of highly viscous lava, for example, rhyolite.[2] They are sometimes formed within the crater of a previous volcanic eruption, as in the case of Mount St. Helens, but can also form independently, as in the case of Lassen Peak. Like stratovolcanoes, they can produce violent, explosive eruptions, but the lava generally does not flow far from the originating vent. Cryptodomes are formed when viscous lava is forced upward causing the surface to bulge. The 1980 eruption of Mount St. Helens was an example; lava beneath the surface of the mountain created an upward bulge, which later collapsed down the north side of the mountain. Cinder cones result from eruptions of mostly small pieces of scoria and pyroclastics (both resemble cinders, hence the name of this volcano type) that build up around the vent. These can be relatively short-lived eruptions that produce a cone-shaped hill perhaps 30 to 400 metres (100 to 1,300\u00a0ft) high. Most cinder cones erupt only once and some may be found in monogenetic volcanic fields that may include other features that form when magma comes into contact with water such as maar explosion craters and tuff rings.[28] Cinder cones may form as flank vents on larger volcanoes, or occur on their own. Par\u00edcutin in Mexico and Sunset Crater in Arizona are examples of cinder cones. In New Mexico, Caja del Rio is a volcanic field of over 60 cinder cones. Based on satellite images, it has been suggested that cinder cones might occur on other terrestrial bodies in the Solar system too; on the surface of Mars and the Moon.[29][30][31][32] Stratovolcanoes are tall conical mountains composed of lava flows and tephra in alternate layers, the strata that gives rise to the name. They are also known as composite volcanoes because they are created from multiple structures during different kinds of eruptions; the main conduit bringing magma to the surface branches into multiple secondary conduits and occasional laccoliths or sills, the branching conduits may form parasitic cones on the flanks of the main cone.[2] Classic examples include Mount Fuji in Japan, Mayon Volcano in the Philippines, and Mount Vesuvius and Stromboli in Italy. Ash produced by the explosive eruption of stratovolcanoes has historically posed the greatest volcanic hazard to civilizations. The lavas of stratovolcanoes are higher in silica, and therefore much more viscous, than lavas from shield volcanoes. High-silica lavas also tend to contain more dissolved gas. The combination is deadly, promoting explosive eruptions that produce great quantities of ash, as well as pyroclastic surges like the one that destroyed the city of Saint-Pierre in Martinique in 1902. They are also steeper than shield volcanoes, with slopes of 30\u201335\u00b0 compared to slopes of generally 5\u201310\u00b0, and their loose tephra are material for dangerous lahars.[33] Large pieces of tephra are called volcanic bombs. Big bombs can measure more than 1.2 metres (4\u00a0ft) across and weigh several tons.[34] A supervolcano is defined as a volcano that has experienced one or more eruptions that produced over 1,000 cubic kilometres (240\u00a0cu\u00a0mi) of volcanic deposits in a single explosive event.[35] Such eruptions occur when a very large magma chamber full of gas-rich, silicic magma is emptied in a catastrophic caldera-forming eruption. Ash flow tuffs emplaced by such eruptions are the only volcanic product with volumes rivalling those of flood basalts.[36] Supervolcano eruptions, while the most dangerous type, are very rare; four are known from the last million years, and about 60 historical VEI 8 eruptions have been identified in the geologic record over millions of years. A supervolcano can produce devastation on a continental scale, and severely cool global temperatures for many years after the eruption due to the huge volumes of sulfur and ash released into the atmosphere. Because of the enormous area they cover, and subsequent concealment under vegetation and glacial deposits, supervolcanoes can be difficult to identify in the geologic record without careful geological mapping.[37] Known examples include Yellowstone Caldera in Yellowstone National Park and Valles Caldera in New Mexico (both western United States); Lake Taup\u014d in New Zealand; Lake Toba in Sumatra, Indonesia; and Ngorongoro Crater in Tanzania. Volcanoes that, though large, are not large enough to be called supervolcanoes, may also form calderas (collapsed crater) in the same way. There may be active or dormant cones inside of the caldera or even a lake, such lakes are called Volcanogenic lakes, or simply, volcanic lakes.[38][2] Submarine volcanoes are common features of the ocean floor. Volcanic activity during the Holocene Epoch has been documented at only 119 submarine volcanoes, but there may be more than one million geologically young submarine volcanoes on the ocean floor.[39][40] In shallow water, active volcanoes disclose their presence by blasting steam and rocky debris high above the ocean's surface. In the deep ocean basins, the tremendous weight of the water prevents the explosive release of steam and gases; however, submarine eruptions can be detected by hydrophones and by the discoloration of water because of volcanic gases. Pillow lava is a common eruptive product of submarine volcanoes and is characterized by thick sequences of discontinuous pillow-shaped masses which form underwater. Even large submarine eruptions may not disturb the ocean surface, due to the rapid cooling effect and increased buoyancy in water (as compared to air), which often causes volcanic vents to form steep pillars on the ocean floor. Hydrothermal vents are common near these volcanoes, and some support peculiar ecosystems based on chemotrophs feeding on dissolved minerals. Over time, the formations created by submarine volcanoes may become so large that they break the ocean surface as new islands or floating pumice rafts. In May and June 2018, a multitude of seismic signals were detected by earthquake monitoring agencies all over the world. They took the form of unusual humming sounds, and some of the signals detected in November of that year had a duration of up to 20 minutes. An oceanographic research campaign in May 2019 showed that the previously mysterious humming noises were caused by the formation of a submarine volcano off the coast of Mayotte.[41] Subglacial volcanoes develop underneath ice caps. They are made up of lava plateaus capping extensive pillow lavas and palagonite. These volcanoes are also called table mountains, tuyas,[42] or (in Iceland) mobergs.[43] Very good examples of this type of volcano can be seen in Iceland and in British Columbia. The origin of the term comes from Tuya Butte, which is one of the several tuyas in the area of the Tuya River and Tuya Range in northern British Columbia. Tuya Butte was the first such landform analysed and so its name has entered the geological literature for this kind of volcanic formation.[44] The Tuya Mountains Provincial Park was recently established to protect this unusual landscape, which lies north of Tuya Lake and south of the Jennings River near the boundary with the Yukon Territory. Hydrothermal features, for example geysers, fumaroles, mud pools, mud volcanoes, hot springs and acidic hot springs involve water as well as geothermal or magmatic activity. Such features are common around volcanoes and are often indicative of volcanism.[2][45] Mud volcanoes or mud domes are conical structures created by eruption of liquids and gases, particularly mud (slurries), water and gases, although several activities may contribute. The largest mud volcanoes are 10 kilometres (6.2\u00a0mi) in diameter and reach 700 metres (2,300\u00a0ft) high.[46][47] Mud volcanoes can be seen off the shore of Indonesia, on the island of Baratang, in Balochistan and in central Asia. Fumaroles are vents on the surface from which hot steam and volcanic gases erupt due to the presence of superheated groundwater, these may indicate volcanic activity. Fumaroles erupting sulfurous gases are also often called solfataras.[48][2] Geysers are springs which will occasionally erupt and discharge hot water and steam. Geysers may indicate ongoing magmatism, water underground is heated by hot rocks and steam pressure builds up before being released along with a jet of hot water. Almost half of all active geysers are present in Yellowstone National Park, US.[2][49] The material that is expelled in a volcanic eruption can be classified into three types: The concentrations of different volcanic gases can vary considerably from one volcano to the next. Water vapour is typically the most abundant volcanic gas, followed by carbon dioxide[53] and sulfur dioxide. Other principal volcanic gases include hydrogen sulfide, hydrogen chloride, and hydrogen fluoride. A large number of minor and trace gases are also found in volcanic emissions, for example hydrogen, carbon monoxide, halocarbons, organic compounds, and volatile metal chlorides. The form and style of an eruption of a volcano is largely determined by the composition of the lava it erupts. The viscosity (how fluid the lava is) and the amount of dissolved gas are the most important characteristics of magma, and both are largely determined by the amount of silica in the magma. Magma rich in silica is much more viscous than silica-poor magma, and silica-rich magma also tends to contain more dissolved gases. Lava can be broadly classified into four different compositions:[54] Mafic lava flows show two varieties of surface texture: \u02bbA\u02bba (pronounced [\u02c8\u0294a\u0294a]) and p\u0101hoehoe ([pa\u02d0\u02c8ho.e\u02c8ho.e]), both Hawaiian words. \u02bbA\u02bba is characterized by a rough, clinkery surface and is the typical texture of cooler basalt lava flows. P\u0101hoehoe is characterized by its smooth and often ropey or wrinkly surface and is generally formed from more fluid lava flows. P\u0101hoehoe flows are sometimes observed to transition to \u02bba\u02bba flows as they move away from the vent, but never the reverse.[68] More silicic lava flows take the form of block lava, where the flow is covered with angular, vesicle-poor blocks. Rhyolitic flows typically consist largely of obsidian.[69] Tephra is made when magma inside the volcano is blown apart by the rapid expansion of hot volcanic gases. Magma commonly explodes as the gas dissolved in it comes out of solution as the pressure decreases when it flows to the surface. These violent explosions produce particles of material that can then fly from the volcano. Solid particles smaller than 2\u00a0mm in diameter (sand-sized or smaller) are called volcanic ash.[51][52] Tephra and other volcaniclastics (shattered volcanic material) make up more of the volume of many volcanoes than do lava flows. Volcaniclastics may have contributed as much as a third of all sedimentation in the geologic record. The production of large volumes of tephra is characteristic of explosive volcanism.[70] Through natural processes, mainly erosion, so much of the solidified erupted material that makes up the mantle of a volcano may be stripped away that its inner anatomy becomes apparent. Using the metaphor of biological anatomy, such a process is called \"dissection\".[71] When the volcano is extinct, a plug forms on its vent, over time due to erosion, the volcanic cone slowly erodes away leaving the resistant lava plug intact.[2] Cinder Hill, a feature of Mount Bird on Ross Island, Antarctica, is a prominent example of a dissected volcano. Volcanoes that were, on a geological timescale, recently active, such as for example Mount Kaimon in southern Ky\u016bsh\u016b, Japan, tend to be undissected. Devils Tower in Wyoming is a famous example of exposed volcanic plug. As of December\u00a02022[update], the Smithsonian Institution's Global Volcanism Program database of volcanic eruptions in the Holocene Epoch (the last 11,700 years) lists 9,901 confirmed eruptions from 859 volcanoes. The database also lists 1,113 uncertain eruptions and 168 discredited eruptions for the same time interval.[72][73] Eruption styles are broadly divided into magmatic, phreatomagmatic (hydrovolcanic), and phreatic eruptions.[74] The intensity of explosive volcanism is expressed using the volcanic explosivity index (VEI), which ranges from 0 for Hawaiian-type eruptions to 8 for supervolcanic eruptions:[75][76] Volcanoes vary greatly in their level of activity, with individual volcanic systems having an eruption recurrence ranging from several times a year to once in tens of thousands of years.[77] Volcanoes are informally described as erupting, active, dormant, or extinct, but the definitions of these terms are not entirely uniform among volcanologists. The level of activity of most volcanoes falls upon a graduated spectrum, with much overlap between categories, and does not always fit neatly into only one of these three separate categories.[6] The USGS defines a volcano as \"erupting\" whenever the ejection of magma from any point on the volcano is visible, including visible magma still contained within the walls of the summit crater. While there is no international consensus among volcanologists on how to define an active volcano, the USGS defines a volcano as active whenever subterranean indicators, such as earthquake swarms, ground inflation, or unusually high levels of carbon dioxide or sulfur dioxide are present.[78][79] The USGS defines a dormant volcano as any volcano that is not showing any signs of unrest such as earthquake swarms, ground swelling, or excessive noxious gas emissions, but which shows signs that it could yet become active again.[79] Many dormant volcanoes have not erupted for thousands of years, but have still shown signs that they may be likely to erupt again in the future.[80][81] Technically, any volcano that is dormant is also considered to be geologically \"active\".[5] In an article justifying the re-classification of Alaska's Mount Edgecumbe volcano from \"dormant\" to \"active\", volcanologists at the Alaska Volcano Observatory pointed out that the term \"dormant\" in reference to volcanoes has been deprecated over the past few decades and that \"[t]he term \"dormant volcano\" is so little used and undefined in modern volcanology that the Encyclopedia of Volcanoes (2000) does not contain it in the glossaries or index\",[82] however the USGS still widely employs the term. Previously a volcano was often considered to be extinct if there were no written records of its activity. Such a generalization is inconsistent with observation and deeper study, as has occurred recently with the unexpected eruption of the Chait\u00e9n volcano in 2008.[83] Modern volcanic activity monitoring techniques, and improvements in the modelling of the factors that produce eruptions, have helped the understanding of why volcanoes may remain dormant for a long time, and then become unexpectedly active again. The potential for eruptions, and their style, depend mainly upon the state of the magma storage system under the volcano, the eruption trigger mechanism and its timescale.[84]:\u200a95\u200a For example, the Yellowstone volcano has a repose/recharge period of around 700,000 years, and Toba of around 380,000 years.[85] Vesuvius was described by Roman writers as having been covered with gardens and vineyards before its unexpected eruption of 79 CE, which destroyed the towns of Herculaneum and Pompeii. Accordingly, it can sometimes be difficult to distinguish between an extinct volcano and a dormant (inactive) one. Long volcano dormancy is known to decrease awareness.[84]:\u200a96\u200a Pinatubo was an inconspicuous volcano, unknown to most people in the surrounding areas, and initially not seismically monitored before its unanticipated and catastrophic eruption of 1991. Two other examples of volcanoes that were once thought to be extinct, before springing back into eruptive activity were the long-dormant Soufri\u00e8re Hills volcano on the island of Montserrat, thought to be extinct until activity resumed in 1995 (turning its capital Plymouth into a ghost town) and Fourpeaked Mountain in Alaska, which, before its September 2006 eruption, had not erupted since before 8000 BCE. Another example is the Taftan volcano in southwestern Iran. This volcano was long thought by volcanologists to be extinct, with its last eruption having occurred an estimated 710,000 years ago. However, beginning around June 2023, the volcano began experiencing uplifting near its summit, suggesting that the volcano was dormant.[86] Extinct volcanoes are those that scientists consider unlikely to erupt again because the volcano no longer has a magma supply. Examples of extinct volcanoes are many volcanoes on the Hawaiian\u2013Emperor seamount chain in the Pacific Ocean (although some volcanoes at the eastern end of the chain are active), Hohentwiel in Germany, Shiprock in New Mexico, U.S., Capulin in New Mexico, U.S., Zuidwal volcano in the Netherlands, and many volcanoes in Italy such as Monte Vulture. Edinburgh Castle in Scotland is located atop an extinct volcano, which forms Castle Rock. Whether a volcano is truly extinct is often difficult to determine. Since \"supervolcano\" calderas can have eruptive lifespans sometimes measured in millions of years, a caldera that has not produced an eruption in tens of thousands of years may be considered dormant instead of extinct. An individual volcano in a monogenetic volcanic field can be extinct, but that does not mean a completely new volcano might not erupt close by with little or no warning, as its field may have an active magma supply. The three common popular classifications of volcanoes can be subjective and some volcanoes thought to have been extinct have erupted again. To help prevent people from falsely believing they are not at risk when living on or near a volcano, countries have adopted new classifications to describe the various levels and stages of volcanic activity.[87] Some alert systems use different numbers or colours to designate the different stages. Other systems use colours and words. Some systems use a combination of both. The Decade Volcanoes are 16 volcanoes identified by the International Association of Volcanology and Chemistry of the Earth's Interior (IAVCEI) as being worthy of particular study in light of their history of large, destructive eruptions and proximity to populated areas. They are named Decade Volcanoes because the project was initiated as part of the United Nations-sponsored International Decade for Natural Disaster Reduction (the 1990s). The 16 current Decade Volcanoes are: The Deep Earth Carbon Degassing Project, an initiative of the Deep Carbon Observatory, monitors nine volcanoes, two of which are Decade volcanoes. The focus of the Deep Earth Carbon Degassing Project is to use Multi-Component Gas Analyzer System instruments to measure CO2/SO2 ratios in real-time and in high-resolution to allow detection of the pre-eruptive degassing of rising magmas, improving prediction of volcanic activity.[88] Volcanic eruptions pose a significant threat to human civilization. However, volcanic activity has also provided humans with important resources. There are many different types of volcanic eruptions and associated activity: phreatic eruptions (steam-generated eruptions), explosive eruptions of high-silica lava (e.g., rhyolite), effusive eruptions of low-silica lava (e.g., basalt), sector collapses, pyroclastic flows, lahars (debris flows) and volcanic gas emissions. These can pose a hazard to humans. Earthquakes, hot springs, fumaroles, mud pots and geysers often accompany volcanic activity. Volcanic gases can reach the stratosphere, where they form sulfuric acid aerosols that can reflect solar radiation and lower surface temperatures significantly.[89] Sulfur dioxide from the eruption of Huaynaputina may have caused the Russian famine of 1601\u20131603.[90] Chemical reactions of sulfate aerosols in the stratosphere can also damage the ozone layer, and acids such as hydrogen chloride (HCl) and hydrogen fluoride (HF) can fall to the ground as acid rain. Excessive fluoride salts from eruptions have poisoned livestock in Iceland on multiple occasions.[91]:\u200a39\u201358\u200a Explosive volcanic eruptions release the greenhouse gas carbon dioxide and thus provide a deep source of carbon for biogeochemical cycles.[92] Ash thrown into the air by eruptions can present a hazard to aircraft, especially jet aircraft where the particles can be melted by the high operating temperature; the melted particles then adhere to the turbine blades and alter their shape, disrupting the operation of the turbine. This can cause major disruptions to air travel. A volcanic winter is thought to have taken place around 70,000 years ago after the supereruption of Lake Toba on Sumatra island in Indonesia.[93] This may have created a population bottleneck that affected the genetic inheritance of all humans today.[94] Volcanic eruptions may have contributed to major extinction events, such as the End-Ordovician, Permian-Triassic, and Late Devonian mass extinctions.[95] The 1815 eruption of Mount Tambora created global climate anomalies that became known as the \"Year Without a Summer\" because of the effect on North American and European weather.[96] The freezing winter of 1740\u201341, which led to widespread famine in northern Europe, may also owe its origins to a volcanic eruption.[97] Although volcanic eruptions pose considerable hazards to humans, past volcanic activity has created important economic resources. Tuff formed from volcanic ash is a relatively soft rock, and it has been used for construction since ancient times.[98][99] The Romans often used tuff, which is abundant in Italy, for construction.[100] The Rapa Nui people used tuff to make most of the moai statues in Easter Island.[101] Volcanic ash and weathered basalt produce some of the most fertile soil in the world, rich in nutrients such as iron, magnesium, potassium, calcium, and phosphorus.[102] Volcanic activity is responsible for emplacing valuable mineral resources, such as metal ores.[102] It is accompanied by high rates of heat flow from Earth's interior. These can be tapped as geothermal power.[102] Tourism associated with volcanoes is also a worldwide industry.[103] Many volcanoes near human settlements are heavily monitored with the aim of providing adequate advance warnings of imminent eruptions to nearby populations. Also, a better modern-day understanding of volcanology has led to some better informed governmental and public responses to unanticipated volcanic activities. While the science of volcanology may not yet be capable of predicting the exact times and dates of eruptions far into the future, on suitably monitored volcanoes the monitoring of ongoing volcanic indicators is often capable of predicting imminent eruptions with advance warnings minimally of hours, and usually of days prior to any eruptions.[104] The diversity of volcanoes and their complexities mean that eruption forecasts for the foreseeable future will be based on probability, and the application of risk management. Even then, some eruptions will have no useful warning. An example of this occurred in March 2017, when a tourist group was witnessing a presumed to be predictable Mount Etna eruption and the flowing lava came in contact with a snow accumulation causing a situational phreatic explosion causing injury to ten persons.[103] Other types of significant eruptions are known to give useful warnings of only hours at the most by seismic monitoring.[83] The recent demonstration of a magma chamber with repose times of tens of thousands of years, with potential for rapid recharge so potentially decreasing warning times, under the youngest volcano in central Europe,[84] does not tell us if more careful monitoring will be useful. Scientists are known to perceive risk, with its social elements, differently from local populations and those that undertake social risk assessments on their behalf, so that both disruptive false alarms and retrospective blame, when disasters occur, will continue to happen.[105]:\u200a1\u20133 Thus in many cases, while volcanic eruptions may still cause major property destruction, the periodic large-scale loss of human life that was once associated with many volcanic eruptions, has recently been significantly reduced in areas where volcanoes are adequately monitored. This life-saving ability is derived via such volcanic-activity monitoring programs, through the greater abilities of local officials to facilitate timely evacuations based upon the greater modern-day knowledge of volcanism that is now available, and upon improved communications technologies such as cell phones. Such operations tend to provide enough time for humans to escape at least with their lives before a pending eruption. One example of such a recent successful volcanic evacuation was the Mount Pinatubo evacuation of 1991. This evacuation is believed to have saved 20,000 lives.[106] In the case of Mount Etna, a 2021 review found 77 deaths due to eruptions since 1536 but none since 1987.[103] Citizens who may be concerned about their own exposure to risk from nearby volcanic activity should familiarize themselves with the types of, and quality of, volcano monitoring and public notification procedures being employed by governmental authorities in their areas.[107] Earth's Moon has no large volcanoes and no current volcanic activity, although recent evidence suggests it may still possess a partially molten core.[108] However, the Moon does have many volcanic features such as maria[109] (the darker patches seen on the Moon), rilles[110] and domes.[111] The planet Venus has a surface that is 90% basalt, indicating that volcanism played a major role in shaping its surface. The planet may have had a major global resurfacing event about 500 million years ago,[112] from what scientists can tell from the density of impact craters on the surface. Lava flows are widespread and forms of volcanism not present on Earth occur as well. Changes in the planet's atmosphere and observations of lightning have been attributed to ongoing volcanic eruptions, although there is no confirmation of whether or not Venus is still volcanically active. However, radar sounding by the Magellan probe revealed evidence for comparatively recent volcanic activity at Venus's highest volcano Maat Mons, in the form of ash flows near the summit and on the northern flank.[113] However, the interpretation of the flows as ash flows has been questioned.[114] There are several extinct volcanoes on Mars, four of which are vast shield volcanoes far bigger than any on Earth. They include Arsia Mons, Ascraeus Mons, Hecates Tholus, Olympus Mons, and Pavonis Mons. These volcanoes have been extinct for many millions of years,[115] but the European Mars Express spacecraft has found evidence that volcanic activity may have occurred on Mars in the recent past as well.[115] Jupiter's moon Io is the most volcanically active object in the Solar System because of tidal interaction with Jupiter. It is covered with volcanoes that erupt sulfur, sulfur dioxide and silicate rock, and as a result, Io is constantly being resurfaced. Its lavas are the hottest known anywhere in the Solar System, with temperatures exceeding 1,800 K (1,500\u00a0\u00b0C). In February 2001, the largest recorded volcanic eruptions in the Solar System occurred on Io.[116] Europa, the smallest of Jupiter's Galilean moons, also appears to have an active volcanic system, except that its volcanic activity is entirely in the form of water, which freezes into ice on the frigid surface. This process is known as cryovolcanism, and is apparently most common on the moons of the outer planets of the Solar System.[117] In 1989, the Voyager 2 spacecraft observed cryovolcanoes (ice volcanoes) on Triton, a moon of Neptune, and in 2005 the Cassini\u2013Huygens probe photographed fountains of frozen particles erupting from Enceladus, a moon of Saturn.[118][119] The ejecta may be composed of water, liquid nitrogen, ammonia, dust, or methane compounds. Cassini\u2013Huygens also found evidence of a methane-spewing cryovolcano on the Saturnian moon Titan, which is believed to be a significant source of the methane found in its atmosphere.[120] It is theorized that cryovolcanism may also be present on the Kuiper Belt Object Quaoar. A 2010 study of the exoplanet COROT-7b, which was detected by transit in 2009, suggested that tidal heating from the host star very close to the planet and neighbouring planets could generate intense volcanic activity similar to that found on Io.[121] Volcanoes are not distributed evenly over the Earth's surface but active ones with significant impact were encountered early in human history, evidenced by footprints of hominina found in East African volcanic ash dated at 3.66 million years old.[122]:\u200a104\u200a The association of volcanoes with fire and disaster is found in many oral traditions and had religious and thus social significance before the first written record of concepts related to volcanoes. Examples are: (1) the stories in the Athabascan subcultures about humans living inside mountains and a woman who uses fire to escape from a mountain,[123]:\u200a135\u200a (2) Pele's migration through the Hawarian island chain, ability to destroy forests and manifestations of the god's temper,[124] and (3) the association in Javanese folklore of a king resident in Mount Merapi volcano and a queen resident at a beach 50\u00a0km (31\u00a0mi) away on what is now known to be an earthquake fault that interacts with that volcano.[125] Many ancient accounts ascribe volcanic eruptions to supernatural causes, such as the actions of gods or demigods. The earliest known such example is a neolithic goddess at \u00c7atalh\u00f6y\u00fck.[126]:\u200a203\u200a The Ancient Greek god Hephaistos and the concepts of the underworld are aligned to volcanoes in that Greek culture.[103] However, others proposed more natural (but still incorrect) causes of volcanic activity. In the fifth century BC, Anaxagoras proposed eruptions were caused by a great wind.[127] By 65\u00a0CE, Seneca the Younger proposed combustion as the cause,[127] an idea also adopted by the Jesuit Athanasius Kircher (1602\u20131680), who witnessed eruptions of Mount Etna and Stromboli, then visited the crater of Vesuvius and published his view of an Earth in Mundus Subterraneus with a central fire connected to numerous others depicting volcanoes as a type of safety valve.[128] Edward Jorden, in his work on mineral waters, challenged this view; in 1632 he proposed sulfur \"fermentation\" as a heat source within Earth,[127] Astronomer Johannes Kepler (1571\u20131630) believed volcanoes were ducts for Earth's tears.[129][better\u00a0source\u00a0needed] In 1650, Ren\u00e9 Descartes proposed the core of Earth was incandescent and, by 1785, the works of Decartes and others were synthesized into geology by James Hutton in his writings about igneous intrusions of magma.[127] Lazzaro Spallanzani had demonstrated by 1794 that steam explosions could cause explosive eruptions and many geologists held this as the universal cause of explosive eruptions up to the 1886 eruption of Mount Tarawera which allowed in one event differentiation of the concurrent phreatomagmatic and hydrothermal eruptions from dry explosive eruption, of, as it turned out, a basalt dyke.[130]:\u200a16\u201318\u200a[131]:\u200a4\u200a Alfred Lacroix built upon his other knowledge with his studies on the 1902 eruption of Mount Pel\u00e9e,[127] and by 1928 Arthur Holmes work had brought together the concepts of radioactive generation of heat, Earth's mantle structure, partial decompression melting of magma, and magma convection.[127] This eventually led to the acceptance of plate tectonics.[132]",
      "ground_truth_chunk_ids": [
        "73_fixed_chunk1"
      ],
      "source_ids": [
        "S073"
      ],
      "category": "factual",
      "id": 91
    },
    {
      "question": "What is List of storms named Brenda?",
      "ground_truth": "The name Brenda has been used for nine tropical cyclones worldwide, including five in the Atlantic Ocean. In the Atlantic: In the Western Pacific Ocean: In the South-West Indian: In the Australian region:",
      "expected_answer": "The name Brenda has been used for nine tropical cyclones worldwide, including five in the Atlantic Ocean. In the Atlantic: In the Western Pacific Ocean: In the South-West Indian: In the Australian region:",
      "ground_truth_chunk_ids": [
        "116_random_chunk1"
      ],
      "source_ids": [
        "S316"
      ],
      "category": "factual",
      "id": 92
    },
    {
      "question": "What is The Konstantinos Staikos' book collection?",
      "ground_truth": "The book collection of Konstantinos Staikos is now part of the Alexander S. Onassis Public Benefit Foundation Library[1][2] It is centered on the intellectual, printing and publishing activity of the Greeks from the Fall of Constantinople in 1453 to the late 19th century. The aim of its creation was to collect and present relevant material from that time period. The genesis of the book collection dates from the 1970s. The bibliophilic interests of Konstantinos Staikos changed radically. In those years also, the Hellenic Bibliophile Society was established [3] under the Honorary Presidency of Constantinos Tsatsos. The exhibitions of books of the Society (1975) with travellers' accounts: 'Travellers in Greece from the fifteenth century to 1821', or with printed material regarding the chronicle of Greek typography: 'Outset of Greek typography' (1976) radically altered Konstantinos Staikos interests as collector and from then on he consciously turned to the study and research of the pioneers of Greek printing and the relations they cultivated with the world of books in Venice and elsewhere. His acquaintance with Georgios Ladas, who was profoundly conscious of the role played by printed books during the Ottoman domination and who collected and documented the bibliographic identity of an enormous number of books that came into his hands, empowered Konstantinos Staikos' intention to explore the chronicle of Greek typography in greater depth. The initial approach was to record printers' marks and emblems characterizing printed Greek books, resulting in the planning of the Charta of Greek Printing. At the same time the collection began to take shape, with the purchase of books entirely compatible with the terms regulating the Hellenic Bibliography as recorded by \u00c9. Legrand, printed material, that is to say, testifying to the pains and labours of the printing workshops. From 1986 the most representative body of the Konstantinos",
      "expected_answer": "The book collection of Konstantinos Staikos is now part of the Alexander S. Onassis Public Benefit Foundation Library[1][2] It is centered on the intellectual, printing and publishing activity of the Greeks from the Fall of Constantinople in 1453 to the late 19th century. The aim of its creation was to collect and present relevant material from that time period. The genesis of the book collection dates from the 1970s. The bibliophilic interests of Konstantinos Staikos changed radically. In those years also, the Hellenic Bibliophile Society was established [3] under the Honorary Presidency of Constantinos Tsatsos. The exhibitions of books of the Society (1975) with travellers' accounts: 'Travellers in Greece from the fifteenth century to 1821', or with printed material regarding the chronicle of Greek typography: 'Outset of Greek typography' (1976) radically altered Konstantinos Staikos interests as collector and from then on he consciously turned to the study and research of the pioneers of Greek printing and the relations they cultivated with the world of books in Venice and elsewhere. His acquaintance with Georgios Ladas, who was profoundly conscious of the role played by printed books during the Ottoman domination and who collected and documented the bibliographic identity of an enormous number of books that came into his hands, empowered Konstantinos Staikos' intention to explore the chronicle of Greek typography in greater depth. The initial approach was to record printers' marks and emblems characterizing printed Greek books, resulting in the planning of the Charta of Greek Printing. At the same time the collection began to take shape, with the purchase of books entirely compatible with the terms regulating the Hellenic Bibliography as recorded by \u00c9. Legrand, printed material, that is to say, testifying to the pains and labours of the printing workshops. From 1986 the most representative body of the Konstantinos Staikos collection, covering the works and the days of Greek scholars and printers active in the period of the Italian Renaissance (late fourteenth \u2013 mid-sixteenth centuries) became the object of exhibitions for the promotion of their work. First editions by Manuel Chrysoloras, George of Trebizond, Cardinal Bessarion, Theodoros Gazis, Zacharias Kallierges, Nikolaos Vlastos and numerous others were presented successively in Florence (1986); the Benaki Museum (1987); Geneva University (1988); Strasburg (1989) and elsewhere. These exhibitions were accompanied by detailed bilingual catalogues, compiled in collaboration with M.I. Manoussakas, with introductory notes and extensive commentaries for each book. The ultimate goal of these exhibitions was the promotion of the inestimable and decisive contribution of the Greek scholars of the period to the diffusion of Greek letters and to demonstrate: the relations they cultivated with the supreme Humanists of Italy, many of whom had been their pupils. Examples from the collection were exhibited at the Hellenic Institute of Byzantine and Post-Byzantine Studies in Venice in 1993, with landmark editions by Aldus Manutius, the products of literary editors by renowned Greek scholars such as Marcus Musurus and Ioannes Gregoropoulos. In Austria, at Vienna's Imperial Library nearly all the Greek books published/printed there (1749\u20131800) were exhibited, which were the most significant examples of the Neohellenic Enlightenment. In celebration of the Five Hundred Years since the establishment of the first Greek printing press (Venice 1499), the Greek Parliament Foundation assigned to Triantafyllos Sklavenitis and Konstantinos Staikos the organization of an exhibition of the most important material of the whole period: a considerable number of incunables and printed material deriving for the greater part from his library. In 2010 the Collection was acquired by the Onassis Foundation in order to be preserved as perpetual property of the Greek Nation.",
      "ground_truth_chunk_ids": [
        "10_random_chunk1"
      ],
      "source_ids": [
        "S210"
      ],
      "category": "factual",
      "id": 93
    },
    {
      "question": "What is Mineral?",
      "ground_truth": "In geology and mineralogy, a mineral or mineral species is, broadly speaking, a solid substance with a fairly well-defined chemical composition and a specific crystal structure that occurs naturally in pure form.[1][2] The geological definition of mineral normally excludes compounds that occur only in living organisms. However, some minerals are often biogenic (such as calcite) or chemically organic compounds (such as mellite). Moreover, living organisms often synthesize inorganic minerals (such as hydroxylapatite) that also occur in rocks. The concept of mineral is distinct from rock, which is any bulk solid geologic material that is relatively homogeneous at a large enough scale. A rock may consist of one type of mineral or may be an aggregate of two or more different types of minerals, spacially segregated into distinct phases.[3] Some natural solid substances without a definite crystalline structure, such as opal or obsidian, are more properly called mineraloids.[4] If a chemical compound occurs naturally with different crystal structures, each structure is considered a different mineral species. Thus, for example, quartz and stishovite are two different minerals consisting of the same compound, silicon dioxide. The International Mineralogical Association (IMA) is the generally recognized standard body for the definition and nomenclature of mineral species. As of May 2025[update], the IMA recognizes 6,145 official mineral species.[5] The chemical composition of a named mineral species may vary somewhat because the inclusion of small amounts of impurities. Specific varieties of a species sometimes have conventional or official names of their own.[6] For example, amethyst is a purple variety of the mineral species quartz. Some mineral species can have variable proportions of two or more chemical elements that occupy equivalent positions in the mineral's structure; for example, the formula of mackinawite is given as (Fe,Ni)9S8, meaning FexNi9-xS8, where x is a variable number between 0 and 9.",
      "expected_answer": "In geology and mineralogy, a mineral or mineral species is, broadly speaking, a solid substance with a fairly well-defined chemical composition and a specific crystal structure that occurs naturally in pure form.[1][2] The geological definition of mineral normally excludes compounds that occur only in living organisms. However, some minerals are often biogenic (such as calcite) or chemically organic compounds (such as mellite). Moreover, living organisms often synthesize inorganic minerals (such as hydroxylapatite) that also occur in rocks. The concept of mineral is distinct from rock, which is any bulk solid geologic material that is relatively homogeneous at a large enough scale. A rock may consist of one type of mineral or may be an aggregate of two or more different types of minerals, spacially segregated into distinct phases.[3] Some natural solid substances without a definite crystalline structure, such as opal or obsidian, are more properly called mineraloids.[4] If a chemical compound occurs naturally with different crystal structures, each structure is considered a different mineral species. Thus, for example, quartz and stishovite are two different minerals consisting of the same compound, silicon dioxide. The International Mineralogical Association (IMA) is the generally recognized standard body for the definition and nomenclature of mineral species. As of May\u00a02025[update], the IMA recognizes 6,145 official mineral species.[5] The chemical composition of a named mineral species may vary somewhat because the inclusion of small amounts of impurities.  Specific varieties of a species sometimes have conventional or official names of their own.[6] For example, amethyst is a purple variety of the mineral species quartz.  Some mineral species can have variable proportions of two or more chemical elements that occupy equivalent positions in the mineral's structure; for example, the formula of mackinawite is given as (Fe,Ni)9S8, meaning FexNi9-xS8, where x is a variable number between 0 and 9.  Sometimes a mineral with variable composition is split into separate species, more or less arbitrarily, forming a mineral group; that is the case of the silicates CaxMgyFe2-x-ySiO4, the olivine group. Besides the essential chemical composition and crystal structure, the description of a mineral species usually includes its common physical properties such as  habit, hardness, lustre, diaphaneity, colour, streak, tenacity, cleavage, fracture, system, zoning, parting, specific gravity, magnetism, fluorescence, radioactivity, as well as its taste or smell and its reaction to acid.[7][8] Minerals are classified by key chemical constituents; the two dominant systems are the Dana classification and the Strunz classification. Silicate minerals comprise approximately 90% of the Earth's crust.[9][10] Other important mineral groups include the native elements (made up of a single pure element) and compounds (combinations of multiple elements) namely sulfides (e.g. Galena PbS), oxides (e.g. quartz SiO2), halides (e.g. rock salt NaCl), carbonates (e.g. calcite CaCO3), sulfates (e.g. gypsum CaSO4\u00b72H2O), silicates (e.g. orthoclase KAlSi3O8), molybdates (e.g. wulfenite PbMoO4) and phosphates (e.g. pyromorphite Pb5(PO4)3Cl).[7] The International Mineralogical Association has established the following requirements for a substance to be considered a distinct mineral:[11][12] The details of these rules are somewhat controversial.[15] For instance, there have been several recent proposals to classify amorphous substances as minerals, but they have not been accepted by the IMA. The IMA is also reluctant to accept minerals that occur naturally only in the form of nanoparticles a few hundred atoms across, but has not defined a minimum crystal size.[11] Some authors require the material to be a stable or metastable solid at room temperature (25\u00a0\u00b0C).[15]  However, the IMA only requires that the substance be stable enough for its structure and composition to be well-determined. For example, it recognizes meridianiite (a naturally occurring hydrate of magnesium sulfate) as a mineral, even though it is formed and stable only below 2\u00a0\u00b0C. As of May\u00a02025[update], 6,145 mineral species are approved by the IMA.[5] They are most commonly named after a person, followed by discovery location; names based on chemical composition or physical properties are the two other major groups of mineral name etymologies.[18][19]  Most names end in \"-ite\"; the exceptions are usually names that were well-established before the organization of mineralogy as a discipline, for example galena and diamond. A topic of contention among geologists and mineralogists has been the IMA's decision to exclude biogenic crystalline substances. For example, Lowenstam (1981) stated that \"organisms are capable of forming a diverse array of minerals, some of which cannot be formed inorganically in the biosphere.\"[20] Skinner (2005) views all solids as potential minerals and includes biominerals in the mineral kingdom, which are those that are created by the metabolic activities of organisms. Skinner expanded the previous definition of a mineral to classify \"element or compound, amorphous or crystalline, formed through biogeochemical  processes,\" as a mineral.[21] Recent advances in high-resolution genetics and X-ray absorption spectroscopy are providing revelations on the biogeochemical relations between microorganisms and minerals that may shed new light on this question.[12][21] For example, the IMA-commissioned \"Working Group on Environmental Mineralogy and Geochemistry \" deals with minerals in the hydrosphere, atmosphere, and biosphere.[22] The group's scope includes mineral-forming microorganisms, which exist on nearly every rock, soil, and particle surface spanning the globe to depths of at least 1600 metres below the sea floor and 70 kilometres into the stratosphere (possibly entering the mesosphere).[23][24][25] Biogeochemical cycles have contributed to the formation of minerals for billions of years. Microorganisms can precipitate metals from solution, contributing to the formation of ore deposits. They can also catalyze the dissolution of minerals.[26][27][28] Prior to the International Mineralogical Association's listing, over 60 biominerals had been discovered, named, and published.[29] These minerals (a sub-set tabulated in Lowenstam (1981)[20]) are considered minerals proper according to Skinner's (2005) definition.[21] These biominerals are not listed in the International Mineral Association official list of mineral names;[30]  however, many of these biomineral representatives are distributed amongst the 78 mineral classes listed in the Dana classification scheme.[21] Skinner's (2005) definition of a mineral takes this matter into account by stating that a mineral can be crystalline or amorphous.[21] Although biominerals are not the most common form of minerals,[31] they help to define the limits of what constitutes a mineral proper. Nickel's (1995) formal definition explicitly mentioned crystallinity as a key to defining a substance as a mineral. A 2011 article defined icosahedrite, an aluminium-iron-copper alloy, as a mineral; named for its unique natural icosahedral symmetry, it is a quasicrystal. Unlike a true crystal, quasicrystals are ordered but not periodic.[32][33] A mineral assemblage is defined by Mindat.org as \"Any set of minerals in a rock, whether in [chemical] equilibrium or not\",[34] while Encyclopaedia Britannica says \"The term assemblage is frequently applied to all minerals included in a rock but more appropriately should be used for those minerals that are in equilibrium (and are known more specifically as the equilibrium assemblage)\".[35] The term is often prefixed by other terms that describe its formation.[34] A rock is an aggregate of one or more minerals[36] or mineraloids. Some rocks, such as limestone or quartzite, are composed primarily of one mineral\u00a0\u2013 calcite or aragonite in the case of limestone, and quartz in the latter case.[37][38] Other rocks can be defined by relative abundances of key (essential) minerals; a granite is defined by proportions of quartz, alkali feldspar, and plagioclase feldspar.[39] The other minerals in the rock are termed accessory minerals, and do not greatly affect the bulk composition of the rock. Rocks can also be composed entirely of non-mineral material; coal is a sedimentary rock composed primarily of organically derived carbon.[36][40] In rocks, some mineral species and groups are much more abundant than others; these are termed the rock-forming minerals. The major examples of these are quartz, the feldspars, the micas, the amphiboles, the pyroxenes, the olivines, and calcite; except for the last one, all of these minerals are silicates.[41] Overall, around 150 minerals are considered particularly important, whether in terms of their abundance or aesthetic value in terms of collecting.[42] Commercially valuable minerals and rocks, other than gemstones, metal ores, or mineral fuels, are referred to as industrial minerals.[43] For example, muscovite, a white mica, can be used for windows (sometimes referred to as isinglass), as a filler, or as an insulator.[44] Ores are minerals that have a high concentration of a certain element, typically a metal. Examples are cinnabar (HgS), an ore of mercury; sphalerite (ZnS), an ore of zinc; cassiterite (SnO2), an ore of tin; and colemanite, an ore of boron. Gems are minerals with an ornamental value, and are distinguished from non-gems by their beauty, durability, and usually, rarity. There are about 20 mineral species that qualify as gem minerals, which constitute about 35 of the most common gemstones. Gem minerals are often present in several varieties, and so one mineral can account for several different gemstones; for example, ruby and sapphire are both corundum, Al2O3.[45] The first known use of the word \"mineral\" in the English language (Middle English) was the 15th century.  The word came from Medieval Latin: minerale, from minera, mine, ore.[46] The word \"species\" comes from the Latin species, \"a particular sort, kind, or type with distinct look, or appearance\".[47] The abundance and diversity of minerals is controlled directly by their chemistry, in turn dependent on elemental abundances in the Earth. The majority of minerals observed are derived from the Earth's crust. Eight elements account for most of the key components of minerals, due to their abundance in the crust. These eight elements, summing to over 98% of the crust by weight, are, in order of decreasing abundance: oxygen, silicon, aluminium, iron, magnesium, calcium, sodium and potassium. Oxygen and silicon are by far the two most important\u00a0\u2013 oxygen composes 47% of the crust by weight, and silicon accounts for 28%.[48] The minerals that form are those that are most stable at the temperature and pressure of formation, within the limits imposed by the bulk chemistry of the parent body.[49] For example, in most igneous rocks, the aluminium and alkali metals (sodium and potassium) that are present are  primarily found in combination with oxygen, silicon, and calcium as feldspar minerals. However, if the rock is unusually rich in alkali metals, there will not be enough aluminium to combine with all the sodium as feldspar, and the excess sodium will form sodic amphiboles such as riebeckite. If the aluminium abundance is unusually high, the excess aluminium will form muscovite or other aluminium-rich minerals.[50] If silicon is deficient, part of the feldspar will be replaced by feldspathoid minerals.[51] Precise predictions of which minerals will be present in a rock of a particular composition formed at a particular temperature and pressure requires complex thermodynamic calculations. However, approximate estimates may be made using relatively simple rules of thumb, such as the CIPW norm, which gives reasonable estimates for volcanic rock formed from dry magma.[52] The chemical composition may vary between end member species of a solid solution series. For example, the plagioclase feldspars comprise a continuous series from sodium-rich end member albite (NaAlSi3O8) to calcium-rich anorthite (CaAl2Si2O8) with four recognized intermediate varieties between them (given in order from sodium- to calcium-rich): oligoclase, andesine, labradorite, and bytownite.[53] Other examples of series include the olivine series of magnesium-rich forsterite and iron-rich fayalite, and the wolframite series of manganese-rich h\u00fcbnerite and iron-rich ferberite.[54] Chemical substitution and coordination polyhedra explain this common feature of minerals. In nature, minerals are not pure substances, and are contaminated by whatever other elements are present in the given chemical system. As a result, it is possible for one element to be substituted for another.[55] Chemical substitution will occur between ions of a similar size and charge; for example, K+ will not substitute for Si4+ because of chemical and structural incompatibilities caused by a big difference in size and charge. A common example of chemical substitution is that of Si4+ by Al3+, which are close in charge, size, and abundance in the crust. In the example of plagioclase, there are three cases of substitution. Feldspars are all framework silicates, which have a silicon-oxygen ratio of 2:1, and the space for other elements is given by the substitution of Si4+ by Al3+ to give a base unit of [AlSi3O8]\u2212; without the substitution, the formula would be charge-balanced as SiO2, giving quartz.[56] The significance of this structural property will be explained further by coordination polyhedra. The second substitution occurs between Na+ and Ca2+; however, the difference in charge has to accounted for by making a second substitution of Si4+ by Al3+.[57] Coordination polyhedra are geometric representations of how a cation is surrounded by an anion. In mineralogy, coordination polyhedra are usually considered in terms of oxygen, due its abundance in the crust. The base unit of silicate minerals is the silica tetrahedron\u00a0\u2013 one Si4+ surrounded by four O2\u2212. An alternate way of describing the coordination of the silicate is by a number: in the case of the silica tetrahedron, the silicon is said to have a coordination number of 4. Various cations have a specific range of possible coordination numbers; for silicon, it is almost always 4, except for very high-pressure minerals where the compound is compressed such that silicon is in six-fold (octahedral) coordination with oxygen. Bigger cations have a bigger coordination numbers because of the increase in relative size as compared to oxygen (the last orbital subshell of heavier atoms is different too). Changes in coordination numbers leads to physical and mineralogical differences; for example, at high pressure, such as in the mantle, many minerals, especially silicates such as olivine and garnet, will change to a perovskite structure, where silicon is in octahedral coordination. Other examples are the aluminosilicates kyanite, andalusite, and sillimanite (polymorphs, since they share the formula Al2SiO5), which differ by the coordination number of the Al3+; these minerals transition from one another as a response to changes in pressure and temperature.[48] In the case of silicate materials, the substitution of Si4+ by Al3+ allows for a variety of minerals because of the need to balance charges.[58] Because the eight most common elements make up over 98% of the Earth's crust, the small quantities of the other elements that are typically present are substituted into the common rock-forming minerals. The distinctive minerals of most elements are quite rare, being found only where these elements have been concentrated by geological processes, such as hydrothermal circulation, to the point where they can no longer be accommodated in common minerals.[59] Changes in temperature and pressure and composition alter the mineralogy of a rock sample. Changes in composition can be caused by processes such as weathering or metasomatism (hydrothermal alteration). Changes in temperature and pressure occur when the host rock undergoes tectonic or magmatic movement into differing physical regimes. Changes in thermodynamic conditions make it favourable for mineral assemblages to react with each other to produce new minerals; as such, it is possible for two rocks to have an identical or a very similar bulk rock chemistry without having a similar mineralogy. This process of mineralogical alteration is related to the rock cycle. An example of a series of mineral reactions is illustrated as follows.[60] Orthoclase feldspar (KAlSi3O8) is a mineral commonly found in granite, a plutonic igneous rock. When exposed to weathering, it reacts to form kaolinite (Al2Si2O5(OH)4, a sedimentary mineral, and silicic acid): Under low-grade metamorphic conditions, kaolinite reacts with quartz to form pyrophyllite (Al2Si4O10(OH)2): As metamorphic grade increases, the pyrophyllite reacts to form kyanite and quartz: Alternatively, a mineral may change its crystal structure as a consequence of changes in temperature and pressure without reacting. For example, quartz will change into a variety of its SiO2 polymorphs, such as tridymite and cristobalite at high temperatures, and coesite at high pressures.[61] Classifying minerals ranges from simple to difficult. A mineral can be identified by several physical properties, some of them being sufficient for full identification without equivocation. In other cases, minerals can only be classified by more complex optical, chemical or X-ray diffraction analysis; these methods, however, can be costly and time-consuming.  Physical properties applied for classification include crystal structure and habit, hardness, lustre, diaphaneity, colour, streak, cleavage and fracture, and specific gravity. Other less general tests include fluorescence, phosphorescence, magnetism, radioactivity, tenacity (response to mechanical induced changes of shape or form), piezoelectricity and reactivity to dilute acids.[62] Crystal structure results from the orderly geometric spatial arrangement of atoms in the internal structure of a mineral. This crystal structure is based on regular internal atomic or ionic arrangement that is often expressed in the geometric form that the crystal takes. Even when the mineral grains are too small to see or are irregularly shaped, the underlying crystal structure is always periodic and can be determined by X-ray diffraction.[15] Minerals are typically described by their symmetry content. Crystals are restricted to 32 point groups, which differ by their symmetry. These groups are classified in turn into more broad categories, the most encompassing of these being the six crystal families.[63] These families can be described by the relative lengths of the three crystallographic axes, and the angles between them; these relationships correspond to the symmetry operations that define the narrower point groups. They are summarized below; a, b, and c represent the axes, and \u03b1, \u03b2, \u03b3 represent the angle opposite the respective crystallographic axis (e.g. \u03b1 is the angle opposite the a-axis, viz. the angle between the b and c axes):[63] The hexagonal crystal family is also split into two crystal systems\u00a0\u2013 the trigonal, which has a three-fold axis of symmetry, and the hexagonal, which has a six-fold axis of symmetry. Chemistry and crystal structure together define a mineral. With a restriction to 32 point groups, minerals of different chemistry may have identical crystal structure. For example, halite (NaCl), galena (PbS), and periclase (MgO) all belong to the hexaoctahedral point group (isometric family), as they have a similar stoichiometry between their different constituent elements. In contrast, polymorphs are groupings of minerals that share a chemical formula but have a different structure. For example, pyrite and marcasite, both iron sulfides, have the formula FeS2; however, the former is isometric while the latter is orthorhombic. This polymorphism extends to other sulfides with the generic AX2 formula; these two groups are collectively known as the pyrite and marcasite groups.[64] Polymorphism can extend beyond pure symmetry content. The aluminosilicates are a group of three minerals\u00a0\u2013 kyanite, andalusite, and sillimanite\u00a0\u2013 which share the chemical formula Al2SiO5. Kyanite is triclinic, while andalusite and sillimanite are both orthorhombic and belong to the dipyramidal point group. These differences arise corresponding to how aluminium is coordinated within the crystal structure. In all minerals, one aluminium ion is always in six-fold coordination with oxygen. Silicon, as a general rule, is in four-fold coordination in all minerals; an exception is a case like stishovite (SiO2, an ultra-high pressure quartz polymorph with rutile structure).[65] In kyanite, the second aluminium is in six-fold coordination; its chemical formula can be expressed as Al[6]Al[6]SiO5, to reflect its crystal structure. Andalusite has the second aluminium in five-fold coordination (Al[6]Al[5]SiO5) and sillimanite has it in four-fold coordination (Al[6]Al[4]SiO5).[66] Differences in crystal structure and chemistry greatly influence other physical properties of the mineral. The carbon allotropes diamond and graphite have vastly different properties; diamond is the hardest natural substance, has an adamantine lustre, and belongs to the isometric crystal family, whereas graphite is very soft, has a greasy lustre, and crystallises in the hexagonal family. This difference is accounted for by differences in bonding. In diamond, the carbons are in sp3 hybrid orbitals, which means they form a framework where each carbon is covalently bonded to four neighbours in a tetrahedral fashion; on the other hand, graphite is composed of sheets of carbons in sp2 hybrid orbitals, where each carbon is bonded covalently to only three others. These sheets are held together by much weaker van der Waals forces, and this discrepancy translates to large macroscopic differences.[67] Twinning is the intergrowth of two or more crystals of a single mineral species. The geometry of the twinning is controlled by the mineral's symmetry. As a result, there are several types of twins, including contact twins, reticulated twins, geniculated twins, penetration twins, cyclic twins, and polysynthetic twins. Contact, or simple twins, consist of two crystals joined at a plane; this type of twinning is common in spinel. Reticulated twins, common in rutile, are interlocking crystals resembling netting. Geniculated twins have a bend in the middle that is caused by start of the twin. Penetration twins consist of two single crystals that have grown into each other; examples of this twinning include cross-shaped staurolite twins and Carlsbad twinning in orthoclase. Cyclic twins are caused by repeated twinning around a rotation axis. This type of twinning occurs around three, four, five, six, or eight-fold axes, and the corresponding patterns are called threelings, fourlings, fivelings, sixlings, and eightlings. Sixlings are common in aragonite. Polysynthetic twins are similar to cyclic twins through the presence of repetitive twinning; however, instead of occurring around a rotational axis, polysynthetic twinning occurs along parallel planes, usually on a microscopic scale.[68][69] Crystal habit refers to the overall shape of the aggregate crystal of any mineral. Several terms are used to describe this property. Common habits include acicular, which describes needle-like crystals as in natrolite; dendritic (tree-pattern) is common in native copper or native gold with a groundmass (matrix); equant, which is typical of garnet; prismatic (elongated in one direction) as seen in kunzite or stibnite; botryoidal (like a bunch of grapes) seen in chalcedony; fibrous, which has fibre-like crystals as seen in wollastonite; tabular, which differs from bladed habit in that the former is platy whereas the latter has a defined elongation as seen in muscovite; and massive, which has no definite shape as seen in carnallite.[7] Related to crystal form, the quality of crystal faces is diagnostic of some minerals, especially with a petrographic microscope. Euhedral crystals have a defined external shape, while anhedral crystals do not; those intermediate forms are termed subhedral.[70][71] The hardness of a mineral defines how much it can resist scratching or indentation. This physical property is controlled by the chemical composition and crystalline structure of a mineral. The most commonly used scale of measurement is the ordinal Mohs hardness scale, which measures resistance to scratching. Defined by ten indicators, a mineral with a higher index scratches those below it. The scale ranges from talc, a phyllosilicate, to diamond, a carbon polymorph that is the hardest natural material. The scale is provided below:[72][7] A mineral's hardness is a function of its structure. Hardness is not necessarily constant for all crystallographic directions; crystallographic weakness renders some directions softer than others.[72] An example of this hardness variability exists in kyanite, which has a Mohs hardness of 51\u20442 parallel to [001] but 7 parallel to [100].[73] Other scales include these;[74] Lustre indicates how light reflects from the mineral's surface, with regard to its quality and intensity. There are numerous qualitative terms used to describe this property, which are split into metallic and non-metallic categories. Metallic and sub-metallic minerals have high reflectivity like metal; examples of minerals with this lustre are galena and pyrite. Non-metallic lustres include: adamantine, such as in diamond; vitreous, which is a glassy lustre very common in silicate minerals; pearly, such as in talc and apophyllite; resinous, such as members of the garnet group; silky which is common in fibrous minerals such as asbestiform chrysotile.[76] The diaphaneity of a mineral describes the ability of light to pass through it. Transparent minerals do not diminish the intensity of light passing through them. An example of a transparent mineral is muscovite (potassium mica); some varieties are sufficiently clear to have been used for windows. Translucent minerals allow some light to pass, but less than those that are transparent. Jadeite and nephrite (mineral forms of jade are examples of minerals with this property). Minerals that do not allow light to pass are called opaque.[77][78] The diaphaneity of a mineral depends on the thickness of the sample. When a mineral is sufficiently thin (e.g., in a thin section for petrography), it may become transparent even if that property is not seen in a hand sample. In contrast, some minerals, such as hematite or pyrite, are opaque even in thin-section.[78] Colour is the most obvious property of a mineral, but it is often non-diagnostic.[79] It is caused by electromagnetic radiation interacting with electrons (except in the case of incandescence, which does not apply to minerals).[80] Two broad classes of elements (idiochromatic and allochromatic) are defined with regard to their contribution to a mineral's colour: Idiochromatic elements are essential to a mineral's composition; their contribution to a mineral's colour is diagnostic.[77][81] Examples of such minerals are malachite (green) and azurite (blue). In contrast, allochromatic elements in minerals are present in trace amounts as impurities. An example of such a mineral would be the ruby and sapphire varieties of the mineral corundum.[81]\nThe colours of pseudochromatic minerals are the result of interference of light waves. Examples include labradorite and bornite. In addition to simple body colour, minerals can have various other distinctive optical properties, such as play of colours, asterism, chatoyancy, iridescence, tarnish, and pleochroism. Several of these properties involve variability in colour. Play of colour, such as in opal, results in the sample reflecting different colours as it is turned, while pleochroism describes the change in colour as light passes through a mineral in a different orientation. Iridescence is a variety of the play of colours where light scatters off a coating on the surface of crystal, cleavage planes, or off layers having minor gradations in chemistry.[82] In contrast, the play of colours in opal is caused by light refracting from ordered microscopic silica spheres within its physical structure.[83] Chatoyancy (\"cat's eye\") is the wavy banding of colour that is observed as the sample is rotated; asterism, a variety of chatoyancy, gives the appearance of a star on the mineral grain. The latter property is particularly common in gem-quality corundum.[82][83] The streak of a mineral refers to the colour of a mineral in powdered form, which may or may not be identical to its body colour.[81] The most common way of testing this property is done with a streak plate, which is made out of porcelain and coloured either white or black. The streak of a mineral is independent of trace elements[77] or any weathering surface.[81] A common example of this property is illustrated with hematite, which is coloured black, silver or red in hand sample, but has a cherry-red[77] to reddish-brown streak;[81][7] or with chalcopyrite, which is brassy golden in colour and leaves a black streak.[7] Streak is more often distinctive for metallic minerals, in contrast to non-metallic minerals whose body colour is created by allochromatic elements.[77] Streak testing is constrained by the hardness of the mineral, as those harder than 7 powder the streak plate instead.[81] By definition, minerals have a characteristic atomic arrangement. Weakness in this crystalline structure causes planes of weakness, and the breakage of a mineral along such planes is termed cleavage. The quality of cleavage can be described based on how cleanly and easily the mineral breaks; common descriptors, in order of decreasing quality, are \"perfect\", \"good\", \"distinct\", and \"poor\". In particularly transparent minerals, or in thin-section, cleavage can be seen as a series of parallel lines marking the planar surfaces when viewed from the side. Cleavage is not a universal property among minerals; for example, quartz, consisting of extensively interconnected silica tetrahedra, does not have a crystallographic weakness which would allow it to cleave. In contrast, micas, which have perfect basal cleavage, consist of sheets of silica tetrahedra which are very weakly held together.[84][85] As cleavage is a function of crystallography, there are a variety of cleavage types. Cleavage occurs typically in either one, two, three, four, or six directions. Basal cleavage in one direction is a distinctive property of the micas. Two-directional cleavage is described as prismatic, and occurs in minerals such as the amphiboles and pyroxenes. Minerals such as galena or halite have cubic (or isometric) cleavage in three directions, at 90\u00b0; when three directions of cleavage are present, but not at 90\u00b0, such as in calcite or rhodochrosite, it is termed rhombohedral cleavage. Octahedral cleavage (four directions) is present in fluorite and diamond, and sphalerite has six-directional dodecahedral cleavage.[84][85] Minerals with many cleavages might not break equally well in all of the directions; for example, calcite has good cleavage in three directions, but gypsum has perfect cleavage in one direction, and poor cleavage in two other directions. Angles between cleavage planes vary between minerals. For example, as the amphiboles are double-chain silicates and the pyroxenes are single-chain silicates, the angle between their cleavage planes is different. The pyroxenes cleave in two directions at approximately 90\u00b0, whereas the amphiboles distinctively cleave in two directions separated by approximately 120\u00b0 and 60\u00b0. The cleavage angles can be measured with a contact goniometer, which is similar to a protractor.[84][85] Parting, sometimes called \"false cleavage\", is similar in appearance to cleavage but is instead produced by structural defects in the mineral, as opposed to systematic weakness. Parting varies from crystal to crystal of a mineral, whereas all crystals of a given mineral will cleave if the atomic structure allows for that property. In general, parting is caused by some stress applied to a crystal. The sources of the stresses include deformation (e.g. an increase in pressure), exsolution, or twinning. Minerals that often display parting include the pyroxenes, hematite, magnetite, and corundum.[84][86] When a mineral is broken in a direction that does not correspond to a plane of cleavage, it is termed to have been fractured. There are several types of uneven fracture. The classic example is conchoidal fracture, like that of quartz; rounded surfaces are created, which are marked by smooth curved lines. This type of fracture occurs only in very homogeneous minerals. Other types of fracture are fibrous, splintery, and hackly. The latter describes a break along a rough, jagged surface; an example of this property is found in native copper.[87] Tenacity is related to both cleavage and fracture. Whereas fracture and cleavage describes the surfaces that are created when a mineral is broken, tenacity describes how resistant a mineral is to such breaking. Minerals can be described as brittle, ductile, malleable, sectile, flexible, or elastic.[88] Specific gravity numerically describes the density of a mineral. The dimensions of density are mass divided by volume with units: kg/m3 or g/cm3. Specific gravity is defined as the density of the mineral divided by the density of water at 4\u00a0\u00b0C and thus is a dimensionless quantity, identical in all unit systems.[89] It can be measured as the quotient of the mass of the sample and difference between the weight of the sample in air and its corresponding weight in water. Among most minerals, this property is not diagnostic. Rock forming minerals\u00a0\u2013 typically silicates or occasionally carbonates\u00a0\u2013 have a specific gravity of 2.5\u20133.5.[90] High specific gravity is a diagnostic property of a mineral. A variation in chemistry (and consequently, mineral class) correlates to a change in specific gravity. Among more common minerals, oxides and sulfides tend to have a higher specific gravity as they include elements with higher atomic mass. A generalization is that minerals with metallic or adamantine lustre tend to have higher specific gravities than those having a non-metallic to dull lustre. For example, hematite, Fe2O3, has a specific gravity of 5.26[91] while galena, PbS, has a specific gravity of 7.2\u20137.6,[92] which is a result of their high iron and lead content, respectively. A very high specific gravity is characteristic of native metals; for example, kamacite, an iron-nickel alloy common in iron meteorites has a specific gravity of 7.9,[93] and gold has an observed specific gravity between 15 and 19.3.[90][94] Other properties can be used to diagnose minerals. These are less general, and apply to specific minerals. Dropping dilute acid (often 10% HCl) onto a mineral aids in distinguishing carbonates from other mineral classes. The acid reacts with the carbonate ([CO3]2\u2212) group, which causes the affected area to effervesce, giving off carbon dioxide gas. This test can be further expanded to test the mineral in its original crystal form or powdered form. An example of this test is done when distinguishing calcite from dolomite, especially within the rocks (limestone and dolomite respectively). Calcite immediately effervesces in acid, whereas acid must be applied to powdered dolomite (often to a scratched surface in a rock), for it to effervesce.[95] Zeolite minerals will not effervesce in acid; instead, they become frosted after 5\u201310 minutes, and if left in acid for a day, they dissolve or become a silica gel.[96] Magnetism is a very conspicuous property of a few minerals. Among common minerals, magnetite exhibits this property strongly, and magnetism is also present, albeit not as strongly, in pyrrhotite and ilmenite.[95] Some minerals exhibit electrical properties \u2013 for example, quartz is piezoelectric \u2013 but electrical properties are rarely used as diagnostic criteria for minerals because of incomplete data and natural variation.[97] Minerals can also be tested for taste or smell. Halite, NaCl, is table salt; its potassium-bearing counterpart, sylvite, has a pronounced bitter taste. Sulfides have a characteristic smell, especially as samples are fractured, reacting, or powdered.[95] Radioactivity is a rare property found in minerals containing radioactive elements. The radioactive elements could be a defining constituent, such as uranium in uraninite, autunite, and carnotite, or present as trace impurities, as in zircon. The decay of a radioactive element damages the mineral crystal structure rendering it locally amorphous (metamict state); the optical result, termed a radioactive halo or pleochroic halo, is observable with various techniques, such as thin-section petrography.[95] In 315 BCE, Theophrastus presented his classification of minerals in his treatise On Stones. His classification was influenced by the ideas of his teachers Plato  and Aristotle. Theophrastus classified minerals as stones, earths or metals.[98] Georgius Agricola's classification of minerals in his book De Natura Fossilium, published in 1546, divided minerals into three types of substance: simple (stones, earths, metals, and congealed juices), compound (intimately mixed) and composite (separable).[98] An early classification of minerals was given by Carl Linnaeus in his seminal 1735 book Systema Naturae. He divided the natural world into three kingdoms\u00a0\u2013 plants, animals, and minerals\u00a0\u2013 and classified each with the same hierarchy.[99] In descending order, these were Phylum, Class, Order, Family, Tribe, Genus, and Species. However, while his system was justified by Charles Darwin's theory of species formation and has been largely adopted and expanded by biologists in the following centuries (who still use his Greek- and Latin-based binomial naming scheme), it had little success among mineralogists (although each distinct mineral is still formally referred to as a mineral species). Minerals are classified by variety, species, series and group, in order of increasing generality. The basic level of definition is that of mineral species, each of which is distinguished from the others by unique chemical and physical properties. For example, quartz is defined by its formula, SiO2, and a specific crystalline structure that distinguishes it from other minerals with the same chemical formula (termed polymorphs). When there exists a range of composition between two minerals species, a mineral series is defined. For example, the biotite series is represented by variable amounts of the endmembers phlogopite, siderophyllite, annite, and eastonite. In contrast, a mineral group is a grouping of mineral species with some common chemical properties that share a crystal structure. The pyroxene group has a common  formula of XY(Si,Al)2O6, where X and Y are both cations, with X typically bigger than Y; the pyroxenes are single-chain silicates that crystallize in either the orthorhombic or monoclinic crystal systems. Finally, a mineral variety is a specific type of mineral species that differs by some physical characteristic, such as colour or crystal habit. An example is amethyst, which is a purple variety of quartz.[18] Two common classifications, Dana and Strunz, are used for minerals; both rely on composition, specifically with regard to important chemical groups, and structure. James Dwight Dana, a leading geologist of his time, first published his System of Mineralogy in 1837; as of 1997[update], it is in its eighth edition. The Dana classification assigns a four-part number to a mineral species. Its class number is based on important compositional groups; the type gives the ratio of cations to anions in the mineral, and the last two numbers group minerals by structural similarity within a given type or class. The less commonly used Strunz classification, named for German mineralogist Karl Hugo Strunz, is based on the Dana system, but combines both chemical and structural criteria, the latter with regard to distribution of chemical bonds.[100] As the composition of the Earth's crust is dominated by silicon and oxygen, silicates are by far the most important class of minerals in terms of rock formation and diversity. However, non-silicate minerals are of great economic importance, especially as ores.[101][102] Non-silicate minerals are subdivided into several other classes by their dominant chemistry, which includes native elements, sulfides, halides, oxides and hydroxides, carbonates and nitrates, borates, sulfates, phosphates, and organic compounds. Most non-silicate mineral species are rare (constituting in total 8% of the Earth's crust), although some are relatively common, such as calcite, pyrite, magnetite, and hematite. There are two major structural styles observed in non-silicates: close-packing and silicate-like linked tetrahedra. Close-packed structures are a way to densely pack atoms while minimizing interstitial space. Hexagonal close-packing involves stacking layers where every other layer is the same (\"ababab\"), whereas cubic close-packing involves stacking groups of three layers (\"abcabcabc\"). Analogues to linked silica tetrahedra include SO4\u22124 (sulfate), PO4\u22124 (phosphate), AsO4\u22124 (arsenate), and VO4\u22124 (vanadate) structures. The non-silicates have great economic importance, as they concentrate elements more than the silicate minerals do.[103] The largest grouping of minerals by far are the silicates; most rocks are composed of greater than 95% silicate minerals, and over 90% of the Earth's crust is composed of these minerals.[104] The two main constituents of silicates are silicon and oxygen, which are the two most abundant elements in the Earth's crust. Other common elements in silicate minerals correspond to other common elements in the Earth's crust, such as aluminium, magnesium, iron, calcium, sodium, and potassium.[105] Some important rock-forming silicates include the feldspars, quartz, olivines, pyroxenes, amphiboles, garnets, and micas. The base unit of a silicate mineral is the [SiO4]4\u2212 tetrahedron. In the vast majority of cases, silicon is in four-fold or tetrahedral coordination with oxygen. In very high-pressure situations, silicon will be in six-fold or octahedral coordination, such as in the perovskite structure or the quartz polymorph stishovite (SiO2). In the latter case, the mineral no longer has a silicate structure, but that of rutile (TiO2), and its associated group, which are simple oxides. These silica tetrahedra are then polymerized to some degree to create various structures, such as one-dimensional chains, two-dimensional sheets, and three-dimensional frameworks. The basic silicate mineral where no polymerization of the tetrahedra has occurred requires other elements to balance out the base 4- charge. In other silicate structures, different combinations of elements are required to balance out the resultant negative charge. It is common for the Si4+ to be substituted by  Al3+ because of similarity in ionic radius and charge; in those cases, the [AlO4]5\u2212 tetrahedra form the same structures as do the unsubstituted tetrahedra, but their charge-balancing requirements are different.[106] The degree of polymerization can be described by both the structure formed and how many tetrahedral corners (or coordinating oxygens) are shared (for aluminium and silicon in tetrahedral sites):[107][108] The silicate subclasses are described below in order of decreasing polymerization. Tectosilicates, also known as framework silicates, have the highest degree of polymerization. With all corners of a tetrahedra shared, the silicon:oxygen ratio becomes 1:2. Examples are quartz, the feldspars, feldspathoids, and the zeolites. Framework silicates tend to be particularly chemically stable as a result of strong covalent bonds.[109] Forming 12% of the Earth's crust, quartz (SiO2) is the most abundant mineral species. It is characterized by its high chemical and physical resistivity. Quartz has several polymorphs, including tridymite and cristobalite at high temperatures, high-pressure coesite, and ultra-high pressure stishovite. The latter mineral can only be formed on Earth by meteorite impacts, and its structure has been compressed so much that it has changed from a silicate structure to that of rutile (TiO2). The silica polymorph that is most stable at the Earth's surface is \u03b1-quartz. Its counterpart, \u03b2-quartz, is present only at high temperatures and pressures (changes to \u03b1-quartz below 573\u00a0\u00b0C at 1 bar). These two polymorphs differ by a \"kinking\" of bonds; this change in structure gives \u03b2-quartz greater symmetry than \u03b1-quartz, and they are thus also called high quartz (\u03b2) and low quartz (\u03b1).[104][110] Feldspars are the most abundant group in the Earth's crust, at about 50%. In the feldspars, Al3+ substitutes for Si4+, which creates a charge imbalance that must be accounted for by the addition of cations. The base structure becomes either [AlSi3O8]\u2212 or [Al2Si2O8]2\u2212  There are 22 mineral species of feldspars, subdivided into two major subgroups \u2013 alkali and plagioclase \u2013 and two less common groups \u2013 celsian and banalsite. The alkali feldspars are most commonly in a series between potassium-rich orthoclase and sodium-rich albite; in the case of plagioclase, the most common series ranges from albite to calcium-rich anorthite. Crystal twinning is common in feldspars, especially polysynthetic twins in plagioclase and Carlsbad twins in alkali feldspars. If the latter subgroup cools slowly from a melt, it forms exsolution lamellae because the two components \u2013 orthoclase and albite \u2013 are unstable in solid solution. Exsolution can be on a scale from microscopic to readily observable in hand-sample; perthitic texture forms when Na-rich feldspar exsolve in a K-rich host. The opposite texture (antiperthitic), where K-rich feldspar exsolves in a Na-rich host, is very rare.[111] Feldspathoids are structurally similar to feldspar, but differ in that they form in Si-deficient conditions, which allows for further substitution by Al3+. As a result, feldspathoids are almost never found in association with quartz. A common example of a feldspathoid is nepheline ((Na, K)AlSiO4); compared to alkali feldspar, nepheline has an Al2O3:SiO2 ratio of 1:2, as opposed to 1:6 in alkali feldspar.[112] Zeolites often have distinctive crystal habits, occurring in needles, plates, or blocky masses. They form in the presence of water at low temperatures and pressures, and have channels and voids in their structure. Zeolites have several industrial applications, especially in waste water treatment.[113] Phyllosilicates consist of sheets of polymerized tetrahedra. They are bound at three oxygen sites, which gives a characteristic silicon:oxygen ratio of 2:5. Important examples include the mica, chlorite, and the kaolinite-serpentine groups. In addition to the tetrahedra, phyllosilicates have a sheet of octahedra (elements in six-fold coordination by oxygen) that balance out the basic tetrahedra, which have a negative charge (e.g. [Si4O10]4\u2212) These tetrahedra (T) and octahedra (O) sheets are stacked in a variety of combinations to create phyllosilicate layers. Within an octahedral sheet, there are three octahedral sites in a unit structure; however, not all of the sites may be occupied. In that case, the mineral is termed dioctahedral, whereas in other case it is termed trioctahedral.[114] The layers are weakly bound by van der Waals forces, hydrogen bonds, or sparse ionic bonds, which causes a crystallographic weakness, in turn leading to a prominent basal cleavage among the phyllosilicates.[115] The kaolinite-serpentine group consists of T-O stacks (the 1:1 clay minerals); their hardness ranges from 2 to 4, as the sheets are held by hydrogen bonds. The 2:1 clay minerals (pyrophyllite-talc) consist of T-O-T stacks, but they are softer (hardness from 1 to 2), as they are instead held together by van der Waals forces. These two groups of minerals are subgrouped by octahedral occupation; specifically, kaolinite and pyrophyllite are dioctahedral whereas serpentine and talc trioctahedral.[116] Micas are also T-O-T-stacked phyllosilicates, but differ from the other T-O-T and T-O-stacked subclass members in that they incorporate aluminium into the tetrahedral sheets (clay minerals have Al3+ in octahedral sites). Common examples of micas are muscovite, and the biotite series. Mica T-O-T layers are bonded together by metal ions, giving them a greater hardness than other phyllosilicate minerals, though they retain perfect basal cleavage.[117] The chlorite group is related to mica group, but a brucite-like (Mg(OH)2) layer between the T-O-T stacks.[118] Because of their chemical structure, phyllosilicates typically have flexible, elastic, transparent layers that are electrical insulators and can be split into very thin flakes. Micas can be used in electronics as insulators, in construction, as optical filler, or even cosmetics. Chrysotile, a species of serpentine, is the most common mineral species in industrial asbestos, as it is less dangerous in terms of health than the amphibole asbestos.[119] Inosilicates consist of tetrahedra repeatedly bonded in chains. These chains can be single, where a tetrahedron is bound to two others to form a continuous chain; alternatively, two chains can be merged to create double-chain silicates. Single-chain silicates have a silicon:oxygen ratio of 1:3 (e.g. [Si2O6]4\u2212), whereas the double-chain variety has a ratio of 4:11, e.g. [Si8O22]12\u2212. Inosilicates contain two important rock-forming mineral groups; single-chain silicates are most commonly pyroxenes, while double-chain silicates are often amphiboles.[120] Higher-order chains exist (e.g. three-member, four-member, five-member chains, etc.) but they are rare.[121] The pyroxene group consists of 21 mineral species.[122] Pyroxenes have a general structure formula of XY(Si2O6), where X is an octahedral site, while Y can vary in coordination number from six to eight. Most varieties of pyroxene consist of permutations of Ca2+, Fe2+ and Mg2+ to balance the negative charge on the backbone. Pyroxenes are common in the Earth's crust (about 10%) and are a key constituent of mafic igneous rocks.[123] Amphiboles have great variability in chemistry, described variously as a \"mineralogical garbage can\" or a \"mineralogical shark swimming a sea of elements\". The backbone of the amphiboles is the [Si8O22]12\u2212; it is balanced by cations in three possible positions, although the third position is not always used, and one element can occupy both remaining ones. Finally, the amphiboles are usually hydrated, that is, they have a hydroxyl group ([OH]\u2212), although it can be replaced by a fluoride, a chloride, or an oxide ion.[124] Because of the variable chemistry, there are over 80 species of amphibole, although variations, as in the pyroxenes, most commonly involve mixtures of Ca2+, Fe2+ and Mg2+.[122] Several amphibole mineral species can have an asbestiform crystal habit. These asbestos minerals form long, thin, flexible, and strong fibres, which are electrical insulators, chemically inert and heat-resistant; as such, they have several applications, especially in construction materials. However, asbestos are known carcinogens, and cause various other illnesses, such as asbestosis; amphibole asbestos (anthophyllite, tremolite, actinolite, grunerite, and riebeckite) are considered more dangerous than chrysotile serpentine asbestos.[125] Cyclosilicates, or ring silicates, have a ratio of silicon to oxygen of 1:3. Six-member rings are most common, with a base structure of [Si6O18]12\u2212; examples include the tourmaline group and beryl. Other ring structures exist, with 3, 4, 8, 9, 12 having been described.[126]  Cyclosilicates tend to be strong, with elongated, striated crystals.[127] Tourmalines have a very complex chemistry that can be described by a general formula XY3Z6(BO3)3T6O18V3W. The T6O18 is the basic ring structure, where T is usually Si4+, but substitutable by Al3+ or B3+. Tourmalines can be subgrouped by the occupancy of the X site, and from there further subdivided by the chemistry of the W site. The Y and Z sites can accommodate a variety of cations, especially various transition metals; this variability in structural transition metal content gives the tourmaline group greater variability in colour. Other cyclosilicates include beryl, Al2Be3Si6O18, whose varieties include the gemstones emerald (green) and aquamarine (bluish). Cordierite is structurally similar to beryl, and is a common metamorphic mineral.[128] Sorosilicates, also termed disilicates, have tetrahedron-tetrahedron bonding at one oxygen, which results in a 2:7 ratio of silicon to oxygen. The resultant common structural element is the [Si2O7]6\u2212 group. The most common disilicates by far are members of the epidote group. Epidotes are found in variety of geologic settings, ranging from mid-ocean ridge to granites to metapelites. Epidotes are built around the structure [(SiO4)(Si2O7)]10\u2212 structure; for example, the mineral species epidote has calcium, aluminium, and ferric iron to charge balance: Ca2Al2(Fe3+, Al)(SiO4)(Si2O7)O(OH). The presence of iron as Fe3+ and Fe2+ helps buffer oxygen fugacity, which in turn is a significant factor in petrogenesis.[129] Other examples of sorosilicates include lawsonite, a  metamorphic mineral forming in the blueschist facies (subduction zone setting with low temperature and high pressure), vesuvianite, which takes up a significant amount of calcium in its chemical structure.[129][130] Orthosilicates consist of isolated tetrahedra that are charge-balanced by other cations.[131] Also termed nesosilicates, this type of silicate has a silicon:oxygen ratio of 1:4 (e.g. SiO4). Typical orthosilicates tend to form blocky equant crystals, and are fairly hard.[132] Several rock-forming minerals are part of this subclass, such as the aluminosilicates, the olivine group, and the garnet group. The aluminosilicates \u2013bkyanite, andalusite, and sillimanite, all Al2SiO5 \u2013 are structurally composed of one [SiO4]4\u2212 tetrahedron, and one Al3+ in octahedral coordination. The remaining Al3+ can be in six-fold coordination (kyanite), five-fold (andalusite) or four-fold (sillimanite); which mineral forms in a given environment is depend on pressure and temperature conditions. In the olivine structure, the main olivine series of (Mg, Fe)2SiO4 consist of magnesium-rich forsterite and iron-rich fayalite. Both iron and magnesium are in octahedral by oxygen. Other mineral species having this structure exist, such as tephroite, Mn2SiO4.[133] The garnet group has a general formula of X3Y2(SiO4)3, where X is a large eight-fold coordinated cation, and Y is a smaller six-fold coordinated cation. There are six ideal endmembers of garnet, split into two group. The pyralspite garnets have Al3+ in the Y position: pyrope (Mg3Al2(SiO4)3), almandine (Fe3Al2(SiO4)3), and spessartine (Mn3Al2(SiO4)3). The ugrandite garnets have Ca2+ in the X position: uvarovite (Ca3Cr2(SiO4)3), grossular (Ca3Al2(SiO4)3) and andradite (Ca3Fe2(SiO4)3). While there are two subgroups of garnet, solid solutions exist between all six end-members.[131] Other orthosilicates include zircon, staurolite, and topaz. Zircon (ZrSiO4) is useful in geochronology as U6+ can substitute for Zr4+; furthermore, because of its very resistant structure, it is difficult to reset it as a chronometer. Staurolite is a common metamorphic intermediate-grade index mineral. It has a particularly complicated crystal structure that was only fully described in 1986. Topaz (Al2SiO4(F, OH)2, often found in granitic pegmatites associated with tourmaline, is a common gemstone mineral.[134] Native elements are those that are not chemically bonded to other elements. This mineral group includes native metals, semi-metals, and non-metals, and various alloys and solid solutions. The metals are held together by metallic bonding, which confers distinctive physical properties such as their shiny metallic lustre, ductility and malleability, and electrical conductivity. Native elements are subdivided into groups by their structure or chemical attributes. The gold group, with a cubic close-packed structure, includes metals such as gold, silver, and copper. The platinum group is similar in structure to the gold group. The iron-nickel group is characterized by several iron-nickel alloy species. Two examples are kamacite and taenite, which are found in iron meteorites; these species differ by the amount of Ni in the alloy; kamacite has less than 5\u20137% nickel and is a variety of native iron, whereas the nickel content of taenite ranges from 7\u201337%. Arsenic group minerals consist of semi-metals, which have only some metallic traits; for example, they lack the malleability of metals. Native carbon occurs in two allotropes, graphite and diamond; the latter forms at very high pressure in the mantle, which gives it a much stronger structure than graphite.[135] The sulfide minerals are chemical compounds of one or more metals or semimetals with a chalcogen or pnictogen, of which sulfur is most common. Tellurium, arsenic, or selenium can substitute for the sulfur. Sulfides tend to be soft, brittle minerals with a high specific gravity. Many powdered sulfides, such as pyrite, have a sulfurous smell when powdered. Sulfides are susceptible to weathering, and many readily dissolve in water; these dissolved minerals can be later redeposited, which creates enriched secondary ore deposits.[136] Sulfides are classified by the ratio of the metal or semimetal to the sulfur, such as M:S equal to 2:1, or 1:1.[137] Many sulfide minerals are economically important as metal ores; examples include sphalerite (ZnS), an ore of zinc, galena (PbS), an ore of lead, cinnabar (HgS), an ore of mercury, and molybdenite (MoS2, an ore of molybdenum.[138] Pyrite (FeS2), is the most commonly occurring sulfide, and can be found in most geological environments. It is not, however, an ore of iron, but can be instead oxidized to produce sulfuric acid.[139] Related to the sulfides are the rare sulfosalts, in which a metallic element is bonded to sulfur and a semimetal such as antimony, arsenic, or bismuth. Like the sulfides, sulfosalts are typically soft, heavy, and brittle minerals.[140] Oxide minerals are divided into three categories: simple oxides, hydroxides, and multiple oxides. Simple oxides are characterized by O2\u2212 as the main anion and primarily ionic bonding. They can be further subdivided by the ratio of oxygen to the cations. The periclase group consists of minerals with a 1:1 ratio. Oxides with a 2:1 ratio include cuprite (Cu2O) and water ice. Corundum group minerals have a 2:3 ratio, and includes minerals such as corundum (Al2O3), and hematite (Fe2O3). Rutile group minerals have a ratio of 1:2; the eponymous species, rutile (TiO2) is the chief ore of titanium; other examples include cassiterite (SnO2; ore of tin), and pyrolusite (MnO2; ore of manganese).[141][142]  In hydroxides, the dominant anion is the hydroxyl ion, OH\u2212. Bauxites are the chief aluminium ore, and are a heterogeneous mixture of the hydroxide minerals diaspore, gibbsite, and bohmite; they form in areas with a very high rate of chemical weathering (mainly tropical conditions).[143]  Finally, multiple oxides are compounds of two metals with oxygen. A major group within this class are the spinels, with a general formula of X2+Y3+2O4. Examples of species include spinel (MgAl2O4), chromite (FeCr2O4), and magnetite (Fe3O4). The latter is readily distinguishable by its strong magnetism, which occurs as it has iron in two oxidation states (Fe2+Fe3+2O4), which makes it a multiple oxide instead of a single oxide.[144] The halide minerals are compounds in which a halogen (fluorine, chlorine, iodine, or bromine) is the main anion. These minerals tend to be soft, weak, brittle, and water-soluble. Common examples of halides include halite (NaCl, table salt), sylvite (KCl), and fluorite (CaF2). Halite and sylvite commonly form as evaporites, and can be dominant minerals in chemical sedimentary rocks. Cryolite, Na3AlF6, is a key mineral in the extraction of aluminium from bauxites; however, as the only significant occurrence at Ivittuut, Greenland, in a granitic pegmatite, was depleted, synthetic cryolite can be made from fluorite.[145] The carbonate minerals are those in which the main anionic group is carbonate, [CO3]2\u2212. Carbonates tend to be brittle, many have rhombohedral cleavage, and all react with acid.[146] Due to the last characteristic, field geologists often carry dilute hydrochloric acid to distinguish carbonates from non-carbonates. The reaction of acid with carbonates, most commonly found as the polymorph calcite and aragonite (CaCO3), relates to the dissolution and precipitation of the mineral, which is a key in the formation of limestone caves, features within them such as stalactite and stalagmites, and karst landforms. Carbonates are most often formed as biogenic or chemical sediments in marine environments. The carbonate group is structurally a triangle, where a central C4+ cation is surrounded by three O2\u2212 anions; different groups of minerals form from different arrangements of these triangles.[147] The most common carbonate mineral is calcite, which is the primary constituent of sedimentary limestone and metamorphic marble. Calcite, CaCO3, can have a significant percentage of magnesium substituting for calcium. Under high-Mg conditions, its polymorph aragonite will form instead; the marine geochemistry in this regard can be described as an aragonite or calcite sea, depending on which mineral preferentially forms. Dolomite is a double carbonate, with the formula CaMg(CO3)2. Secondary dolomitization of limestone is common, in which calcite or aragonite are converted to dolomite; this reaction increases pore space (the unit cell volume of dolomite is 88% that of calcite), which can create a reservoir for oil and gas. These two mineral species are members of eponymous mineral groups: the calcite group includes carbonates with the general formula XCO3, and the dolomite group constitutes minerals with the general formula XY(CO3)2.[148] The sulfate minerals all contain the sulfate anion, [SO4]2\u2212. They tend to be transparent to translucent, soft, and many are fragile.[149] Sulfate minerals commonly form as evaporites, where they precipitate out of evaporating saline waters. Sulfates can also be found in hydrothermal vein systems associated with sulfides,[150] or as oxidation products of sulfides.[151] Sulfates can be subdivided into anhydrous and hydrous minerals. The most common hydrous sulfate by far is gypsum, CaSO4\u22c52H2O. It forms as an evaporite, and is associated with other evaporites such as calcite and halite; if it incorporates sand grains as it crystallizes, gypsum can form desert roses. Gypsum has very low thermal conductivity and maintains a low temperature when heated as it loses that heat by dehydrating; as such, gypsum is used as an insulator in materials such as plaster and drywall. The anhydrous equivalent of gypsum is anhydrite; it can form directly from seawater in highly arid conditions. The barite group has the general formula XSO4, where the X is a large 12-coordinated cation. Examples include barite (BaSO4), celestine (SrSO4), and anglesite (PbSO4); anhydrite is not part of the barite group, as the smaller Ca2+ is only in eight-fold coordination.[152] The phosphate minerals are characterized by the tetrahedral [PO4]3\u2212 unit, although the structure can be generalized, and phosphorus is replaced by antimony, arsenic, or vanadium. The most common phosphate is the apatite group; common species within this group are fluorapatite (Ca5(PO4)3F), chlorapatite (Ca5(PO4)3Cl) and hydroxylapatite (Ca5(PO4)3(OH)). Minerals in this group are the main crystalline constituents of teeth and bones in vertebrates. The relatively abundant monazite group has a general structure of ATO4, where T is phosphorus or arsenic, and A is often a rare-earth element (REE). Monazite is important in two ways: first, as a REE \"sink\", it can sufficiently concentrate these elements to become an ore; secondly, monazite group elements can incorporate relatively large amounts of uranium and thorium, which can be used in monazite geochronology to date the rock based on the decay of the U and Th to lead.[153] The Strunz classification includes a class for organic minerals. These rare compounds contain organic carbon, but can be formed by a geologic process. For example, whewellite, CaC2O4\u22c5H2O is an oxalate that can be deposited in hydrothermal ore veins. While hydrated calcium oxalate can be found in coal seams and other sedimentary deposits involving organic matter, the hydrothermal occurrence is not considered to be related to biological activity.[102] Mineral classification schemes and their definitions are evolving to match recent advances in mineral science. Recent changes have included the addition of an organic class, in both the new Dana and the Strunz classification schemes.[154][155] The organic class includes a very rare group of minerals with hydrocarbons. The IMA Commission on New Minerals and Mineral Names adopted in 2009 a hierarchical scheme for the naming and classification of mineral groups and group names and established seven commissions and four working groups to review and classify minerals into an official listing of their published names.[156][157]  According to these new rules, \"mineral species can be grouped in a number of different ways, on the basis of chemistry, crystal structure, occurrence, association, genetic history, or resource, for example, depending on the purpose to be served by the classification.\"[156] It has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on Mars.  Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.[158] In January 2014, NASA reported that studies by the Curiosity and Opportunity rovers on Mars would search for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable.[159][160][161][162] The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars became a primary NASA objective.[159][160]",
      "ground_truth_chunk_ids": [
        "75_fixed_chunk1"
      ],
      "source_ids": [
        "S075"
      ],
      "category": "factual",
      "id": 94
    },
    {
      "question": "What is Funeral?",
      "ground_truth": "A funeral is a ceremony connected with the final disposition of a corpse, such as a burial, entombment or cremation with the attendant observances.[1] Funerary customs comprise the complex of beliefs and practices used by a culture to remember and respect the dead, from interment, to various monuments, prayers, and rituals undertaken in their honour. Customs vary between cultures and religious groups. Funerals have both normative and legal components. Common secular motivations for funerals include mourning the deceased, celebrating their life, and offering support and sympathy to the bereaved; additionally, funerals may have religious aspects that are intended to help the soul of the deceased reach the afterlife, resurrection or reincarnation. The funeral usually includes a ritual through which the corpse receives a final disposition.[2] Depending on culture and religion, these can involve either the destruction of the body (for example, by cremation, sky burial, decomposition, disintegration or dissolution) or its preservation (for example, by mummification). Differing beliefs about cleanliness and the relationship between body and soul are reflected in funerary practices. A memorial service (service of remembrance or celebration of life) is a funerary ceremony that is performed without the remains of the deceased person.[3] In both a closed casket funeral[4] and a memorial service, photos of the deceased representing stages of life would be displayed on an altar. Relatives or friends would give out eulogies in both services as well.[5] The word funeral comes from the Latin funus, which had a variety of meanings, including the corpse and the funerary rites themselves. Funerary art is art produced in connection with burials, including many kinds of tombs, and objects specially made for burial like flowers with a corpse. Funeral rites pre-date modern Homo sapiens and dated to at least 300,000 years ago.[6] For example, in the Shanidar Cave in",
      "expected_answer": "A funeral is a ceremony connected with the final disposition of a corpse, such as a burial, entombment or cremation with the attendant observances.[1] Funerary customs comprise the complex of beliefs and practices used by a culture to remember and respect the dead, from interment, to various monuments, prayers, and rituals undertaken in their honour. Customs vary between cultures and religious groups.  Funerals have both normative and legal components. Common secular motivations for funerals include mourning the deceased, celebrating their life, and offering support and sympathy to the bereaved; additionally, funerals may have religious aspects that are intended to help the soul of the deceased reach the afterlife, resurrection or reincarnation. The funeral usually includes a ritual through which the corpse receives a final disposition.[2] Depending on culture and religion, these can involve either the destruction of the body (for example, by cremation, sky burial, decomposition, disintegration or dissolution) or its preservation (for example, by mummification). Differing beliefs about cleanliness and the relationship between body and soul are reflected in funerary practices. A memorial service (service of remembrance or celebration of life) is a funerary ceremony that is performed without the remains of the deceased person.[3] In both a closed casket funeral[4] and a memorial service, photos of the deceased representing stages of life would be displayed on an altar. Relatives or friends would give out eulogies in both services as well.[5] The word funeral comes from the Latin funus, which had a variety of meanings, including the corpse and the funerary rites themselves. Funerary art is art produced in connection with burials, including many kinds of tombs, and objects specially made for burial like flowers with a corpse. Funeral rites pre-date modern Homo sapiens and dated to at least 300,000 years ago.[6] For example, in the Shanidar Cave in Iraq, in Pontnewydd Cave in Wales and at other sites across Europe and the Near East,[6] Archaeologists have discovered Neanderthal skeletons with a characteristic layer of flower pollen. This deliberate burial and reverence given to the dead has been interpreted as suggesting that Neanderthals had religious beliefs,[6] although the evidence is not unequivocal \u2013 while the dead were apparently buried deliberately, burrowing rodents could have introduced the flowers.[7] Substantial cross-cultural and historical research document funeral customs as a highly predictable, stable force in communities.[8][9] Funeral customs tend to be characterized by five \"anchors\": significant symbols, gathered community, ritual action, cultural heritage, and transition of the dead body (corpse).[2] The most common venues for funeral services would be in a place of worship (synagogue or church) or a funeral home. However, a cemetery's chapel features a reflecting serene intimacy as well as a respectful environment for clergy, mourning families and friends. Graveside services are a less common option for these rituals. A mausoleum's chapel mostly intends to be for entombment after the funeral itself. These two funerary chapels both generously accommodate open or closed-casket services prior to a traditional burial within the cemetery. If a funeral is subsequently followed by cremation, the service would be in a crematorium. In the Bah\u00e1\u02bc\u00ed Faith, burial law prescribes both the location of burial and burial practices and precludes cremation of the dead. It is forbidden to carry the body for more than one hour's journey from the place of death. Before interment the body should be wrapped in a shroud of silk or cotton, and a ring should be placed on its finger bearing the inscription \"I came forth from God, and return unto Him, detached from all save Him, holding fast to His Name, the Merciful, the Compassionate\". The coffin should be of crystal, stone or hard fine wood. Also, before interment, a specific Prayer for the Dead[10] is ordained. The body should be placed with the feet facing the Qiblih. The formal prayer and the ring are meant to be used for those who have reached 15 years of age. Since there are no Bah\u00e1'\u00ed clergy, services are usually conducted under the guidance, or with the assistance of, a Local Spiritual Assembly.[11][12][13] A Buddhist funeral marks the transition from one life to the next for the deceased. It also reminds the living of their own mortality. Cremation is the preferred choice,[14] although burial is also allowed. Buddhists in Tibet perform sky burials where the body is exposed to be eaten by vultures. The body is dissected with a blade on the mountain top before the exposure. Crying and wailing is discouraged and the rogyapas (body breakers who perform the ritual) laugh as if they are doing farm work. Tibetan Buddhists believe that a lighthearted atmosphere during the funeral helps the soul of the dead to get a better afterlife. After the vultures consume all the flesh the rogpyas smash the bones into pieces and mix them with tsampa to feed to the vultures.[15] Congregations of varied denominations perform different funeral ceremonies, but most involve offering prayers, scripture reading from the Bible, a sermon, homily, or eulogy, and music.[2][16] One issue of concern as the 21st century began was with the use of secular music at Christian funerals, a custom generally forbidden by the Catholic Church.[17] Christian burials have traditionally occurred on consecrated ground such as in churchyards. There are many funeral norms in Christianity.[18] Burial, rather than a destructive process such as cremation, was the traditional practice amongst Christians, because of the belief in the resurrection of the body. Cremations later came into widespread use, although some denominations forbid them. The US Conference of Catholic Bishops said \"The Church earnestly recommends that the pious custom of burying the bodies of the deceased be observed; nevertheless, the Church does not prohibit cremation unless it was chosen for reasons contrary to Christian doctrine\" (canon 1176.3).[19][20] Antyesti, literally 'last rites' or 'last sacrifice', refers to the rite-of-passage rituals associated with a funeral in Hinduism.[21] It is sometimes referred to as Antima Samskaram, Antya-kriya, Anvarohanyya, or Vahni Sanskara. A dead adult Hindu is cremated, while a dead child is typically buried.[22][23] The rite of passage is said to be performed in harmony with the sacred premise that the microcosm of all living beings is a reflection of a macrocosm of the universe.[24] The soul (Atman, Brahman) is believed to be the immortal essence that is released at the Antyeshti ritual, but both the body and the universe are vehicles and transitory in various schools of Hinduism. They consist of five elements: air, water, fire, earth and space.[24] The last rite of passage returns the body to the five elements and origins.[22][24] The roots of this belief are found in the Vedas, for example in the hymns of Rigveda in section 10.16, as follows: Burn him not up, nor quite consume him, Agni: let not his body or his skin be scattered,\nO all possessing Fire, when thou hast matured him, then send him on his way unto the Fathers.\nWhen thou hast made him ready, all possessing Fire, then do thou give him over to the Fathers,\nWhen he attains unto the life that waits him, he shall become subject to the will of gods.\nThe Sun receive thine eye, the Wind thy Prana (life-principle, breathe); go, as thy merit is, to earth or heaven.\nGo, if it be thy lot, unto the waters; go, make thine home in plants with all thy members. \u2014\u200aRigveda 10.16[25] The final rites of a burial, in case of untimely death of a child, is rooted in Rigveda's section 10.18, where the hymns mourn the death of the child, praying to deity Mrityu to \"neither harm our girls nor our boys\", and pleads the earth to cover, protect the deceased child as a soft wool.[26][27] Among Hindus, the dead body is usually cremated within a day of death. In Hindu tradition, the body is usually kept at home with the family until its time for cremation. A typical Hindu funeral includes three main stages: a gathering or wake in the home, the cremation itself\u2014referred to as mukhagni\u2014and a follow-up ritual called the shraddha ceremony.[28] The body is washed, wrapped in white cloth for a man or a widow, red for a married woman,[23] the two toes tied together with a string, a Tilak (red mark) placed on the forehead.[22] The dead adult's body is carried to the cremation ground near a river or water, by family and friends, and placed on a pyre with feet facing south.[23] The eldest son, or a male mourner, or a priest then bathes before leading the cremation ceremonial function.[22][29] He circumambulates the dry wood pyre with the body, says a eulogy or recites a hymn in some cases, places sesame seed in the dead person's mouth, sprinkles the body and the pyre with ghee (clarified butter), then draws three lines signifying Yama (deity of the dead), Kala (time, deity of cremation) and the dead.[22] The pyre is then set ablaze, while the mourners mourn. The ash from the cremation is consecrated to the nearest river or sea.[29] After the cremation, a period of mourning is observed for 10 to 12 days after which the immediate male relatives or the sons of the deceased shave their head, trim their nails, recites prayers with the help of priest or Brahmin and invite all relatives, kins, friends and neighbours to eat a simple meal together in remembrance of the deceased.\nDuring the mourning period, sleeping arrangements in the home change too. Mattresses are taken off the beds and placed on the floor, and for twelve days, everyone in the household sleeps on the floor as part of the funeral customs.[30] This day, in some communities, also marks a day when the poor and needy are offered food in memory of the dead.[31] In most Hindu communities the last day of the mourning is called as Terahveen (the thirteenth day), and on this day items of basic needs along with some favourite items of the deceased are donated to the priests. Also on the same day the eldest son of the family is ceremonially crowned (called Pagdi Rasm) for he is now the head of the family. A feast is also organised for Brahmins, family members, and friends.[32] The belief that bodies are infested by Nasu upon death greatly influenced Zoroastrian burial ceremonies and funeral rites. Burial and cremation of corpses was prohibited, as such acts would defile the sacred creations of earth and fire respectively.[33] Burial of corpses was so looked down upon that the exhumation of \"buried corpses was regarded as meritorious.\" For these reasons, \"Towers of Silence\" were developed\u2014open air, amphitheater like structures in which corpses were placed so carrion-eating birds could feed on them. Sagd\u012bd, meaning 'seen by a dog,' is a ritual that must be performed as promptly after death as possible. The dog is able to calculate the degree of evil within the corpse, and entraps the contamination so it may not spread further, expelling Nasu from the body.[34] Nasu remains within the corpse until it has been seen by a dog, or until it has been consumed by a dog or a carrion-eating bird.[35] According to chapter 31 of the Denkard, the reasoning for the required consumption of corpses is that the evil influences of Nasu are contained within the corpse until, upon being digested, the body is changed from the form of nasa into nourishment for animals. The corpse is thereby delivered over to the animals, changing from the state of corrupted nasa to that of hixr, which is \"dry dead matter,\" considered to be less polluting. A path through which a funeral procession has traveled must not be passed again, as Nasu haunts the area thereafter, until the proper rites of banishment are performed.[36] Nasu is expelled from the area only after \"a yellow dog with four eyes, or a white dog with yellow ears\" is walked through the path three times.[37] If the dog goes unwillingly down the path, it must be walked back and forth up to nine times to ensure that Nasu has been driven off.[38] Zoroastrian ritual exposure of the dead is first known of from the writings of the mid-5th century BCE Herodotus, who observed the custom amongst Iranian expatriates in Asia Minor. In Herodotus' account (Histories i.140), the rites are said to have been \"secret\", but were first performed after the body had been dragged around by a bird or dog. The corpse was then embalmed with wax and laid in a trench. While the discovery of ossuaries in both eastern and western Iran dating to the 5th and 4th centuries BCE indicates that bones were isolated, that this separation occurred through ritual exposure cannot be assumed: burial mounds,[39] where the bodies were wrapped in wax, have also been discovered. The tombs of the Achaemenid emperors at Naqsh-e Rustam and Pasargadae likewise suggest non-exposure, at least until the bones could be collected. According to legend (incorporated by Ferdowsi into his Shahnameh), Zoroaster is himself interred in a tomb at Balkh (in present-day Afghanistan). Writing on the culture of the Persians, Herodotus reports on the Persian burial customs performed by the Magi, which are kept secret. However, he writes that he knows they expose the body of male dead to dogs and birds of prey, then they cover the corpse in wax, and then it is buried.[40] The Achaemenid custom is recorded for the dead in the regions of Bactria, Sogdia, and Hyrcania, but not in Western Iran. The Byzantine historian Agathias has described the burial of the Sasanian general Mihr-Mihroe: \"the attendants of Mermeroes took up his body and removed it to a place outside the city and laid it there as it was, alone and uncovered according to their traditional custom, as refuse for dogs and horrible carrion\". Towers are a much later invention and are first documented in the early 9th century CE. The ritual customs surrounding that practice appear to date to the Sassanid era (3rd\u20137th century CE). They are known in detail from the supplement to the Sh\u0101yest n\u0113 Sh\u0101yest, the two Revayats collections, and the two Saddars. Funerals in Islam (called Janazah in Arabic) follow fairly specific rites. In all cases, however, sharia (Islamic religious law) calls for burial of the body, preceded by a simple ritual involving bathing and shrouding the body, followed by salat (prayer). Burial rituals should normally take place as soon as possible and include: The mourning period is 40 days long.[44] In Judaism, funerals follow fairly specific rites, though they are subject to variation in custom. Halakha calls for preparatory rituals involving bathing and shrouding the body accompanied by prayers and readings from the Hebrew Bible, and then a funeral service marked by eulogies and brief prayers, and then the lowering of the body into the grave and the filling of the grave. Traditional law and practice forbid cremation of the body; the Reform Jewish movement generally discourages cremation but does not outright forbid it.[45][46] Burial rites should normally take place as soon as possible and include: In Sikhism death is considered a natural process, an event that has absolute certainty and only happens as a direct result of God's Will or Hukam.[48] In Sikhism, birth and death are closely associated, as they are part of the cycle of human life of \"coming and going\" (Punjabi: \u0a06\u0a35\u0a23\u0a41 \u0a1c\u0a3e\u0a23\u0a3e, romanized:\u00a0Aana Jaana) which is seen as a transient stage towards Liberation (\u0a2e\u0a4b\u0a16\u0a41 \u0a26\u0a41\u0a06\u0a30\u0a41, Mokh Du-aar), understood as completely in unity with God. Sikhs believe in reincarnation. Death is only the progression of the soul on its journey from God, through the created universe and back to God again. In life a Sikh is expected to constantly remember death so that they may be sufficiently prayerful, detached and righteous to break the cycle of birth and death and return to God. The public display of grief by wailing or crying out loud at the funeral (called Antam Sanskar) is discouraged and should be kept to a minimum. Cremation is the preferred method of disposal, burial and burial at sea are also allowed if by necessity or by the will of the person. Markers such as gravestones, monuments, etc. are not allowed, because the body is considered to be just the shell and the person's soul is their real self.[49] On the day of the cremation, the body is washed and dressed and then taken to the Gurdwara or home where hymns (Shabadads) from Sri Guru Granth Sahib Ji, the Sikh Scriptures are recited by the congregation. Kirtan may also be performed by Ragis while the relatives of the deceased recite \"Waheguru\" sitting near the coffin. This service normally takes from 30 to 60 minutes. At the conclusion of the service, an Ardas is said before the coffin is taken to the cremation site. At the point of cremation, a few more Shabadads may be sung and final speeches are made about the deceased person. The eldest son or a close relative generally lights the fire. This service usually lasts about 30 to 60 minutes. The ashes are later collected and disposed of by immersing them in a river, preferably one of the five rivers in the state of Punjab, India. The ceremony in which the Sidharan Paath is begun after the cremation ceremony, may be held when convenient, wherever the Sri Guru Granth Sahib Ji is present. Hymns are sung from Sri Guru Granth Sahib Ji; the first five and final verses of \"Anand Sahib,\" the \"Song of Bliss,\" are recited or sung. The first five verses of Sikhism's morning prayer, \"Japji Sahib\", are read aloud to begin the Sidharan paath. A hukam, or random verse, is then read from Sri Guru Granth Sahib Ji. Ardas, a prayer, is offered, and Prashad, a sacred sweet, is distributed. Langar, a meal, is then served to guests. While the Sidharan paath is being read, the family may also sing hymns daily. Reading may take as long as needed to complete the paath. This ceremony is followed by Sahaj Paath Bhog, Kirtan Sohila, night time prayer is recited for one week, and finally Ardas called the \"Antim Ardas\" (\"Final Prayer\") is offered the last week.[50] It was custom for an officiant to walk in front of the coffin with a horse's skull; this tradition was still observed by Welsh peasants up until the 19th century.[51] The Greek word for funeral \u2013 k\u0113de\u00eda (\u03ba\u03b7\u03b4\u03b5\u03af\u03b1) \u2013 derives from the verb k\u0113domai (\u03ba\u03ae\u03b4\u03bf\u03bc\u03b1\u03b9), that means attend to, take care of someone. Derivative words are also k\u0113dem\u00f3n (\u03ba\u03b7\u03b4\u03b5\u03bc\u03ce\u03bd, \"guardian\") and k\u0113demon\u00eda (\u03ba\u03b7\u03b4\u03b5\u03bc\u03bf\u03bd\u03af\u03b1, \"guardianship\"). From the Cycladic civilization in 3000 BCE until the Hypo-Mycenaean era in 1200\u20131100 BCE the main practice of burial is interment. The cremation of the dead that appears around the 11th century BCE constitutes a new practice of burial and is probably an influence from the East. Until the Christian era, when interment becomes again the only burial practice, both cremation and interment had been practiced depending on the area.[52] The ancient Greek funeral since the Homeric era included the pr\u00f3thesis (\u03c0\u03c1\u03cc\u03b8\u03b5\u03c3\u03b9\u03c2), the ekphor\u00e1 (\u1f10\u03ba\u03c6\u03bf\u03c1\u03ac), the burial and the per\u00eddeipnon (\u03c0\u03b5\u03c1\u03af\u03b4\u03b5\u03b9\u03c0\u03bd\u03bf\u03bd). In most cases, this process is followed faithfully in Greece until today.[53] Pr\u00f3thesis is the deposition of the body of the deceased on the funeral bed and the threnody of his relatives. Today the body is placed in the casket, that is always open in Greek funerals. This part takes place in the house where the deceased had lived. An important part of the Greek tradition is the epicedium, the mournful songs that are sung by the family of the deceased along with professional mourners (who are extinct in the modern era). The deceased was watched over by his beloved the entire night before the burial, an obligatory ritual in popular thought, which is maintained still. Ekphor\u00e1 is the process of transport of the mortal remains of the deceased from his residence to the church, nowadays, and afterward to the place of burial. The procession in the ancient times, according to the law, should have passed silently through the streets of the city. Usually certain favourite objects of the deceased were placed in the coffin in order to \"go along with him\". In certain regions, coins to pay Charon, who ferries the dead to the underworld, are also placed inside the casket. A last kiss is given to the beloved dead by the family before the coffin is closed. The Roman orator Cicero[54] describes the habit of planting flowers around the tomb as an effort to guarantee the repose of the deceased and the purification of the ground, a custom that is maintained until today. After the ceremony, the mourners return to the house of the deceased for the per\u00eddeipnon, the dinner after the burial. According to archaeological findings \u2013 traces of ash, bones of animals, shards of crockery, dishes and basins \u2013 the dinner during the classical era was also organized at the burial spot. Taking into consideration the written sources, however, the dinner could also be served in the houses.[55] The Necrodeipnon (\u039d\u03b5\u03ba\u03c1\u03cc\u03b4\u03b5\u03b9\u03c0\u03bd\u03bf\u03bd) was the funeral banquet which was given at the house of the nearest relative.[56][57] Two days after the burial, a ceremony called \"the thirds\" was held. Eight days after the burial the relatives and the friends of the deceased assembled at the burial spot, where \"the ninths\" would take place, a custom still kept. In addition to this, in the modern era, memorial services take place 40 days, 3 months, 6 months, 9 months, 1 year after the death and from then on every year on the anniversary of the death. The relatives of the deceased, for an unspecified length of time that depends on them, are in mourning, during which women wear black clothes and men a black armband.[clarification needed] Nekysia (\u039d\u03b5\u03ba\u03cd\u03c3\u03b9\u03b1), meaning the day of the dead, and Genesia (\u0393\u03b5\u03bd\u03ad\u03c3\u03b9\u03b1), meaning the day of the forefathers (ancestors), were yearly feasts in honour of the dead.[58][59] Nemesia (\u039d\u03b5\u03bc\u03ad\u03c3\u03b9\u03b1) or Nemeseia (N\u03b5\u03bc\u03ad\u03c3\u03b5\u03b9\u03b1) was also a yearly feast in honour of the dead, most probably intended for averting the anger of the dead.[60][61] In ancient Rome, the eldest surviving male of the household, the pater familias, was summoned to the death-bed, where he attempted to catch and inhale the last breath of the decedent. Funerals of the socially prominent usually were undertaken by professional undertakers called libitinarii. No direct description has been passed down of Roman funeral rites. These rites usually included a public procession to the tomb or pyre where the body was to be cremated. The surviving relations bore masks bearing the images of the family's deceased ancestors. The right to carry the masks in public eventually was restricted to families prominent enough to have held curule magistracies. Mimes, dancers, and musicians hired by the undertakers, and professional female mourners, took part in these processions. Less well-to-do Romans could join benevolent funerary societies (collegia funeraticia) that undertook these rites on their behalf. Nine days after the disposal of the body, by burial or cremation, a feast was given (cena novendialis) and a libation poured over the grave or the ashes. Since most Romans were cremated, the ashes typically were collected in an urn and placed in a niche in a collective tomb called a columbarium (literally, \"dovecote\").[62] During this nine-day period, the house was considered to be tainted, funesta, and was hung with Taxus baccata or Mediterranean Cypress branches to warn passersby. At the end of the period, the house was swept out to symbolically purge it of the taint of death. Several Roman holidays commemorated a family's dead ancestors, including the Parentalia, held February 13 through 21, to honor the family's ancestors; and the Feast of the Lemures, held on May 9, 11, and 13, in which ghosts (larvae) were feared to be active, and the pater familias sought to appease them with offerings of beans. The Romans prohibited cremation or inhumation within the sacred boundary of the city (pomerium), for both religious and civil reasons, so that the priests might not be contaminated by touching a dead body, and that houses would not be endangered by funeral fires. Restrictions on the length, ostentation, expense of, and behaviour during funerals and mourning gradually were enacted by a variety of lawmakers. Often the pomp and length of rites could be politically or socially motivated to advertise or aggrandise a particular kin group in Roman society. This was seen as deleterious to society and conditions for grieving were set. For instance, under some laws, women were prohibited from loud wailing or lacerating their faces and limits were introduced for expenditure on tombs and burial clothes. The Romans commonly built tombs for themselves during their lifetime. Hence these words frequently occur in ancient inscriptions, V.F. Vivus Facit, V.S.P. Vivus Sibi Posuit. The tombs of the rich usually were constructed of marble, the ground enclosed with walls, and planted around with trees. But common sepulchres usually were built below ground, and called hypogea. There were niches cut out of the walls, in which the urns were placed; these, from their resemblance to the niche of a pigeon-house, were called columbaria. Within the United States and Canada, in most cultural groups and regions, the funeral rituals can be divided into three parts: visitation, funeral, and the burial service. A home funeral (services prepared and conducted by the family, with little or no involvement from professionals) is legal in nearly every part of North America, but in the 21st century, they are uncommon in the US.[63] At the visitation (also called a \"viewing\", \"wake\" or \"calling hours\"), in Christian or secular Western custom, the body of the deceased person (or decedent) is placed on display in the casket (also called a coffin, however almost all body containers are caskets). The viewing often takes place on one or two evenings before the funeral. In the past, it was common practice to place the casket in the decedent's home or that of a relative for viewing. This practice continues in many areas of Ireland and Scotland. The body is traditionally dressed in the decedent's best clothes. In recent times there has been more variation in what the decedent is dressed in \u2013 some people choose to be dressed in clothing more reflective of how they dressed in life. The body will often be adorned with common jewelry, such as watches, necklaces, brooches, etc. The jewelry may be taken off and given to the family of the deceased prior to burial or be buried with the deceased. Jewelry has to be removed before cremation in order to prevent damage to the crematory. The body may or may not be embalmed, depending upon such factors as the amount of time since the death has occurred, religious practices, or requirements of the place of burial. The most commonly prescribed aspects of this gathering are that the attendees sign a book kept by the deceased's survivors to record who attended. In addition, a family may choose to display photographs taken of the deceased person during his/her life (often, formal portraits with other family members and candid pictures to show \"happy times\"), prized possessions and other items representing his/her hobbies and/or accomplishments. A more recent trend[when?] is to create a DVD with pictures and video of the deceased, accompanied by music, and play this DVD continuously during the visitation. The viewing is either \"open casket\", in which the embalmed body of the deceased has been clothed and treated with cosmetics for display; or \"closed casket\", in which the coffin is closed. The coffin may be closed if the body was too badly damaged because of an accident or fire or other trauma, deformed from illness, if someone in the group is emotionally unable to cope with viewing the corpse, or if the deceased did not wish to be viewed. In cases such as these, a picture of the deceased, usually a formal photo, is placed atop the casket. However, this step is foreign to Judaism; Jewish funerals are held soon after death (preferably within a day or two, unless more time is needed for relatives to come), and the corpse is never displayed. Torah law forbids embalming.[64] Traditionally flowers (and music) are not sent to a grieving Jewish family as it is a reminder of the life that is now lost. The Jewish shiva tradition discourages family members from cooking, so food is brought by friends and neighbors.[44] (See also Jewish bereavement.) The decedent's closest friends and relatives who are unable to attend frequently send flowers to the viewing, with the exception of a Jewish funeral,[65] where flowers would not be appropriate (donations are often given to a charity instead). Obituaries sometimes contain a request that attendees do not send flowers (e.g. \"In lieu of flowers\"). The use of these phrases has been on the rise for the past century. In the US in 1927, only 6% of the obituaries included the directive, with only 2% of those mentioned charitable contributions instead. By the middle of the century, they had grown to 15%, with over 54% of those noting a charitable contribution as the preferred method of expressing sympathy.[66] The deceased is usually transported from the funeral home to a church in a hearse, a specialized vehicle designed to carry casketed remains. The deceased is often transported in a procession (also called a funeral cort\u00e8ge), with the hearse, funeral service vehicles, and private automobiles traveling in a procession to the church or other location where the services will be held. In a number of jurisdictions, special laws cover funeral processions \u2013 such as requiring most other vehicles to give right-of-way to a funeral procession. Funeral service vehicles may be equipped with light bars and special flashers to increase their visibility on the roads. They may also all have their headlights on, to identify which vehicles are part of the cortege, although the practice also has roots in ancient Roman customs.[67] After the funeral service, if the deceased is to be buried the funeral procession will proceed to a cemetery if not already there. If the deceased is to be cremated, the funeral procession may then proceed to the crematorium. Funeral customs vary from country to country. In the United States, any type of noise other than quiet whispering or mourning is considered disrespectful. A burial tends to cost more than a cremation.[68] At a religious burial service, conducted at the side of the grave, tomb, mausoleum or cremation, the body of the decedent is buried or cremated at the conclusion. Sometimes, the burial service will immediately follow the funeral, in which case a funeral procession travels from the site of the funeral to the burial site. In some other cases, the burial service is the funeral, in which case the procession might travel from the cemetery office to the grave site. Other times, the burial service takes place at a later time, when the final resting place is ready, if the death occurred in the middle of winter. If the decedent served in a branch of the Armed forces, military rites are often accorded at the burial service.[69] In many religious traditions, pallbearers, usually males who are relatives or friends of the decedent, will carry the casket from the chapel (of a funeral home or church) to the hearse, and from the hearse to the site of the burial service.[70] Most religions expect coffins to be kept closed during the burial ceremony. In Eastern Orthodox funerals, the coffins are reopened just before burial to allow mourners to look at the deceased one last time and give their final farewells. Greek funerals are an exception as the coffin is open during the whole procedure unless the state of the body does not allow it. Morticians may ensure that all jewelry, including wristwatch, that were displayed at the wake are in the casket before it is buried or entombed. Custom requires that everything goes into the ground; however this is not true for Jewish services. Jewish tradition stipulates that nothing of value is buried with the deceased. In the case of cremation such items are usually removed before the body goes into the furnace. Pacemakers are removed prior to cremation \u2013 if left in they could explode. Funerals for indigenous people, like many other cultures, are a method to remember, commemorate and respect the dead through their own cultural practices and traditions. In the past, there has been scrutiny when the topic of indigenous funeral sites was approached. Thus the federal government deemed it necessary to include a series of acts that would protect and accurately affiliate some of these burials with their correct native individuals or groups. This was enacted through the Native American Graves Protection and Repatriation Act. Furthermore, in 2001 California created the California Native American Graves Protection and Repatriation Act that would \"require all state agencies and museums that receive state funding and that have possession or control over collections of humans remains or cultural items to provide a process for identification and repatriates of these items to appropriate tribes.\" In 2020, it was amended to include tribes that were beyond State and Federal knowledge. In the Ipai, Tipai, Paipai, and Kiliwa regions funeral practices are similar in their social and power dynamics. The way that these funeral sites were created was based on previous habitation. Meaning, these were sites were their peoples may have died or if they had been a temporary home for some of these groups.[71] Additionally, these individual burials were characterized by grave markers and/or grave offerings. The markers included inverted metates, fractured pieces of metates as well as cairns. As for offerings, food, shell and stone beads were often found in burial mounds along with portions human remains. The state of the human remains found at the site can vary, data suggests[71] that cremations are recent in prehistory compared to just burials. Ranging from the middle Holocene era to the Late Prehistoric Period. Additionally, the position these people were placed in plays a role in how the afterlife was viewed. With recent ethnographic evidence coming from the Yuman people, it is believed that the spirits of the dead could potentially harm the living. so, they would often layer the markers or offerings above the body so that they would be unable to \"leave\" their graves and enact harm. In the Los Angeles Basin, researchers discovered communal mourning features at West Bluffs and Landing Hill. These communal mourning rituals were estimated to have taken place during the Intermediate Period (3,000-1,000 B.P.). Archaeologists have found fragmented pieces of a large schist pestle which was deliberately broken in a methodical way. Other fragmented vessels show signs of uneven burning on the interior surface presumed to have been caused by burning combustible material. In the West Bluffs and Landing Hill assemblages there are many instances of artifacts that were dyed in red ochre pigment after being broken. The tradition of intentionally breaking objects has been a custom in the region for thousands of  years for the purpose of releasing the spirit within the object, reducing harm to the community, or as an expression of grief. Pigmentation of grave goods also has many interpretations, the Chumash associate the color red with both earth and fire. While some researchers consider the usage of the red pigment as an important transitional moment in the adult life cycle.[72] A memorial service[73] or memorial gathering is one given for the deceased, often without the body present. The service takes place after cremation or burial at sea, after an entombment in a mausoleum's crypt, after donation of the body to an academic or research institution, after a traditional burial in a cemetery plot (remains either in a coffin or an urn) or after the ashes have been scattered someplace. It is also significant when the person is missing and presumed dead, or known to be deceased though the body is not recoverable. These services often take place at a funeral home;[74] however, they can be held in a home, cemetery chapel, university, town hall, country club, restaurant, beach, community center, workplace, place of worship, hospital chapel, health club, performing arts center, wedding chapel, national park, townhouse, civic center, hotel, museum, sports field, pub, urban park or other location of some significance.[75][76] A memorial service may include speeches (eulogies), prayers, poems, or songs (most particularly hymns) to commemorate the deceased. Pictures of the deceased and flowers with sometimes an urn are usually placed where the coffin would normally be placed. After the sudden deaths of important public officials, public memorial services have been held by communities, including those without any specific connection to the deceased. For examples, community memorial services were held after the assassinations of US presidents James A. Garfield and William McKinley. In Finland, religious funerals (hautajaiset) are quite ascetic and typically follow Lutheran traditions.[77] The local priest or minister says prayers and blesses the deceased in their house. The mourners (saattov\u00e4ki) traditionally bring food to the mourners' house. Common current practice has the deceased placed into the coffin in the place where they died. The undertaker will pick up the coffin and place it in the hearse and drive it to the funeral home, while the closest relatives or friends of the deceased will follow the hearse in a funeral procession in their own cars. The coffin will be held at the funeral home until the day of the funeral. The funeral services may be divided into two parts. First is the church service (siunaustilaisuus) in a cemetery chapel or local church, then the burial.[78] The majority of Italians are Roman Catholic and follow Catholic funeral traditions. Historically, mourners would walk in a funeral procession to the gravesite; today vehicles are used. Greek funerals are generally held in churches, including a Trisagion service. There is usually a 40-day mourning period, and the end of which, a memorial service is held. Every year following, a similar service takes place, to mark the anniversary of the death.[79][80] In Poland, in urban areas, there are usually two, or just one \"stop\". The body, brought by a hearse from the mortuary, may be taken to a church or to a cemetery chapel. There is then a funeral mass or service at the cemetery chapel. Following the mass or Service the casket is carried in procession (usually on foot) by hearse to the grave. Once at the grave-site, the priest will commence the graveside committal service and the casket is lowered. The mass or service usually takes place at the cemetery. In some traditional rural areas, the wake (czuwanie) takes place in the house of the deceased or their relatives. The body lies in state for three days in the house. The funeral usually takes place on the third day. Family, neighbors and friends gather and pray during the day and night on those three days and nights. There are usually three stages in the funeral ceremony (ceremonia pogrzebowa, pogrzeb): the wake (czuwanie), then the body is carried by procession (usually on foot) or people drive in their own cars to the church or cemetery chapel for mass, and another procession by foot to the gravesite. After the funeral, families gather for a post-funeral get-together (stypa). It can be at the family home, or at a function hall. In Poland cremation is less popular because the Catholic Church in Poland prefers traditional burials (though cremation is allowed). Cremation is more popular among non-religious people and Protestants in Poland. An old funeral rite from the Scottish Highlands involved burying the deceased with a wooden plate resting on his chest. On the plate were placed a small amount of earth and salt, to represent the future of the deceased. The earth hinted that the body would decay and become one with the earth, while the salt represented the soul, which does not decay. This rite was known as \"earth laid upon a corpse\". This practice was also carried out in Ireland, as well as in parts of England, particularly in Leicestershire, although in England the salt was intended to prevent air from distending the corpse.[81] In Spain, a burial or cremation may occur very soon after a death. Most Spaniards are Roman Catholics and follow Catholic funeral traditions. First, family and friends sit with the deceased during the wake until the burial. Wakes are a social event and a time to laugh and honor the dead. Following the wake comes the funeral mass (Tanatorio) at the church or cemetery chapel. Following the mass is the burial. The coffin is then moved from the church to the local cemetery, often with a procession of locals walking behind the hearse. The first Swedish evangelical order of burial was given in Olaus Petri's handbook of 1529. From the medieval order, it had only kept burial and cremation.[82] The funeral where the priest blessed the recently deceased, which after the Reformation came to be called a reading, was forbidden in the church order of 1686, but was taken over by lay people instead. It was then followed by the wake, which was banned by the church law in 1686, when it was often considered degenerate to do dancing and games where beer and brandy were served.[83] It came however, to live on in the custom of \"singing out corpses\". In older times, the grave was often shoveled closed during the hymn singing. During the 17th century, homilies became common, they were later replaced by grift speeches, which, however, never became mandatory. In 1686, it was decided that those who had lived a Christian life should be honestly and properly buried in a grave. It also determined that the burial would be performed by a priest in the Church of Sweden (later some religious communities were given the right to bury their dead themselves). Burial could only take place at a burial site intended for the purpose. Loss of honorable burial became a punishment. A distinction was made between silent burial (for some serious criminals) and quiet burial without singing and bell ringing and with abbreviated ritual (for some criminals, unbaptized children and for those who committed suicide). Church burial was compulsory for members of the Church of Sweden until 1926, when the possibility was opened for civil burial.[82] In the UK, funerals are commonly held at a church, crematorium or cemetery chapel.[84] Historically, it was customary to bury the dead, but since the 1960s, cremation has been more common.[85] While there is no visitation ceremony like in North America, relatives may view the body beforehand at the funeral home. A room for viewing is usually called a chapel of rest.[86] Funerals typically last about half an hour.[87] They are sometimes split into two ceremonies: a main funeral and a shorter committal ceremony. In the latter, the coffin is either handed over to a crematorium[87] or buried in a cemetery.[88] This allows the funeral to be held at a place without cremation or burial facilities. Alternatively, the entire funeral may be held in the chapel of the crematorium or cemetery. It is not customary to view a cremation; instead, the coffin may be removed from the chapel or hidden with curtains towards the end of the funeral.[87] After the funeral, it is common for the mourners to gather for refreshments. This is sometimes called a wake, though this is different from how the term is used in other countries, where a wake is a ceremony before the funeral.[84] Traditionally, a good funeral (as they were called) had one draw the curtains for a period of time; at the wake, when new visitors arrived, they would enter from the front door and leave through the back door. The women stayed at home whilst the men attended the funeral, the village priest would then visit the family at their home to talk about the deceased and to console them.[89] The first child of William Price, a Welsh Neo-Druidic priest, died in 1884. Believing that it was wrong to bury a corpse, and thereby pollute the earth, Price decided to cremate his son's body, a practice which had been common in Celtic societies.\nThe police arrested him for the illegal disposal of a corpse.[90] Price successfully argued in court that while the law did not state that cremation was legal, it also did not state that it was illegal. The case set a precedent that, together with the activities of the newly founded Cremation Society of Great Britain, led to the Cremation Act 1902.[91] The Act imposed procedural requirements before a cremation could occur and restricted the practice to authorised places.[92] A growing number of families choose to hold a life celebration or celebration of life[93][94] event for the deceased. Like memorial services, this ceremony is held after burial, entombment or cremation of the deceased. An urn can be on display with flowers and photos on the altar after cremation like in a memorial service. Unlike funerals, the focus of the ceremony is on the life that was lived.[95] Such ceremonies may be held outside the funeral home or place of worship; country clubs, cemetery chapels, restaurants, beaches, performing arts centers, wedding chapels, urban parks, sports fields, hotels, civic centers, museums, hospital chapels, community centers, town halls, pubs and sporting facilities are popular choices based on the specific interests of the deceased. Celebrations of life focus on including the person's best qualities, interests, achievements and impact, rather than mourning a death.[93] Some events are portrayed as joyous parties, instead of a traditional somber funeral. Taking on happy and hopeful tones, celebrations of life discourage wearing black and focus on the deceased's individuality.[93] An extreme example might have \"a fully stocked open bar, catered food, and even favors.\"[94] Notable recent celebrations of life ceremonies include those for Ren\u00e9 Ang\u00e9lil[96] and Maya Angelou.[97] In Australia, funerary customs continue to evolve in response to cultural diversity and environmental awareness; see Funeral rituals and trends in Australia for details of current practices. Originating in New Orleans, Louisiana, U.S., alongside the emergence of jazz music in late 19th and early 20th centuries, the jazz funeral is a traditionally African-American burial ceremony and celebration of life unique to New Orleans that involves a parading funeral procession accompanied by a brass band playing somber hymns followed by upbeat jazz music. Traditional jazz funerals begin with a processional led by the funeral director, family, friends, and the brass band, i.e., the \"main line\", who march from the funeral service to the burial site while the band plays slow dirges and Christian hymns. After the body is buried, or \"cut loose\", the band begins to play up-tempo, joyful jazz numbers, as the main line parades through the streets and crowds of \"second liners\" join in and begin dancing and marching along, transforming the funeral into a street festival.[98] The terms \"green burial\" and \"natural burial\", used interchangeably, apply to ceremonies that aim to return the body with the earth with little to no use of artificial, non-biodegradable materials. As a concept, the idea of uniting an individual with the natural world after they die appears as old as human death itself, being widespread before the rise of the funeral industry. Holding environmentally-friendly ceremonies as a modern concept first attracted widespread attention in the 1990s. In terms of North America, the opening of the first explicitly \"green\" burial cemetery in the U.S. took place in the state of South Carolina. However, the Green Burial Council, which came into being in 2005, has based its operations out of California. The institution works to officially certify burial practices for funeral homes and cemeteries, making sure that appropriate materials are used.[99] Religiously, some adherents of the Roman Catholic Church often have particular interest in \"green\" funerals given the faith's preference to full burial of the body as well as the theological commitments to care for the environment stated in Catholic social teaching.[99] Those with concerns about the effects on the environment of traditional burial or cremation may be placed into a natural bio-degradable green burial shroud. That, in turn, sometimes gets placed into a simple coffin made of cardboard or other easily biodegradable material. Furthermore, individuals may choose their final resting place to be in a specially designed park or woodland, sometimes known as an \"ecocemetery\", and may have a tree or other item of greenery planted over their grave both as a contribution to the environment and a symbol of remembrance. Humanists UK organises a network of humanist funeral celebrants or officiants across England and Wales, Northern Ireland, and the Channel Islands[100] and a similar network is organised by the Humanist Society Scotland. Humanist officiants are trained and experienced in devising and conducting suitable ceremonies for non-religious individuals.[101] Humanist funerals recognise no \"afterlife\", but celebrate the life of the person who has died.[100] In the twenty-first century, humanist funerals were held for well-known people including Claire Rayner,[102] Keith Floyd,[103][104] Linda Smith,[105] and Ronnie Barker.[106] In areas outside of the United Kingdom, Ireland has featured an increasing number of non-religious funeral arrangements according to publications such as Dublin Live. This has occurred in parallel with a trend of increasing numbers of people carefully scripting their own funerals before they die, writing the details of their own ceremonies. The Irish Association of Funeral Directors has reported that funerals without a religious focus occur mainly in more urbanized areas in contrast to rural territories.[107] Notably, humanist funerals have started to become more prominent in other nations such as the Republic of Malta, in which civil rights activist and humanist Ramon Casha had a large scale event at the Radisson Blu Golden Sands resort devoted to laying him to rest. Although such non-religious ceremonies are \"a rare scene in Maltese society\" due to the large role of the Roman Catholic Church within that country's culture, according to Lovin Malta, \"more and more Maltese people want to know about alternative forms of burial... without any religion being involved\".[108][109] Actual events during non-religious funerals vary, but they frequently reflect upon the interests and personality of the deceased. For example, the humanist ceremony for the aforementioned Keith Floyd, a restaurateur and television personality, included a reading of Rudyard Kipling's poetic work \"If\u2014\" and a performance by musician Bill Padley.[103] Organizations such as the Irish Institute of Celebrants have stated that more and more regular individuals request training for administering funeral ceremonies, instead of leaving things to other individuals.[107] More recently, some commercial organisations offer civil funerals that can integrate traditionally religious content.[110] Funerals specifically for fallen members of fire or police services are common in United States and Canada. These funerals involve honour guards from police forces and/or fire services from across the country and sometimes from overseas.[111] A parade of officers often precedes or follows the hearse carrying the fallen comrade.[111] A traditional fire department funeral consists of two raised aerial ladders.[112] The firefighters travel under the aerials on their ride, on the fire apparatus, to the cemetery. Once there, the grave service includes the playing of bagpipes. The pipes have come to be a distinguishing feature of a fallen hero's funeral. Also a \"Last Alarm Bell\" is rung. A portable fire department bell is tolled at the conclusion of the ceremony. A Masonic funeral is held at the request of a departed Mason or family member. The service may be held in any of the usual places or a Lodge room with committal at graveside, or the complete service can be performed at any of the aforementioned places without a separate committal. Freemasonry does not require a Masonic funeral. There is no single convention for a Masonic funeral service. Some Grand Lodges have a prescribed service (as it is a worldwide organisation). Some of the customs include the presiding officer wearing a hat while doing his part in the service, the Lodge members placing sprigs of evergreen on the casket, and a small white leather apron may being placed in or on the casket. The hat may be worn because it is Masonic custom (in some places in the world) for the presiding officer to have his head covered while officiating. To Masons, the sprig of evergreen is a symbol of immortality. A Mason wears a white leather apron, called a \"lambskin\", on becoming a Mason, and he may continue to wear it even in death.[113][114] In most East Asian, South Asian and many Southeast Asian cultures, the wearing of white is symbolic of death. In these societies, white or off-white robes are traditionally worn to symbolize that someone has died and can be seen worn among relatives of the deceased during a funeral ceremony. In Chinese culture, red is strictly forbidden as it is a traditionally symbolic color of happiness. Exceptions are sometimes made if the deceased has reached an advanced age such as 85, in which case the funeral is considered a celebration, where wearing white with some red is acceptable. Contemporary Western influence however has meant that dark-colored or black attire is now often also acceptable for mourners to wear (particularly for those outside the family). In such cases, mourners wearing dark colors at times may also wear a white or off-white armband or white robe. Contemporary South Korean funerals typically mix western culture with traditional Korean culture, largely depending on socio-economic status, region, and religion. In almost all cases, all related males in the family wear woven armbands representing seniority and lineage in relation to the deceased, and must grieve next to the deceased for a period of three days before burying the body. During this period of time, it is customary for the males in the family to personally greet all who come to show respect. While burials have been preferred historically, recent trends show a dramatic increase in cremations due to shortages of proper burial sites and difficulties in maintaining a traditional grave. The ashes of the cremated corpse are commonly stored in columbaria. Most Japanese funerals are conducted with Buddhist and/or Shinto rites.[115] Many ritually bestow a new name on the deceased; funerary names typically use obsolete or archaic kanji and words, to avoid the likelihood of the name being used in ordinary speech or writing. The new names are typically chosen by a Buddhist priest, after consulting the family of the deceased. Religious thought among the Japanese people is generally a blend of Shint\u014d and Buddhist beliefs. In modern practice, specific rites concerning an individual's passage through life are generally ascribed to one of these two faiths. Funerals and follow-up memorial services fall under the purview of Buddhist ritual, and 90% Japanese funerals are conducted in a Buddhist manner[?]. Aside from the religious aspect, a Japanese funeral usually includes a wake, the cremation of the deceased, and inclusion within the family grave. Follow-up services are then performed by a Buddhist priest on specific anniversaries after death. According to an estimate in 2005, 99% of all deceased Japanese are cremated.[116] In most cases the cremated remains are placed in an urn and then deposited in a family grave. In recent years however, alternative methods of disposal have become more popular, including scattering of the ashes, burial in outer space, and conversion of the cremated remains into a diamond that can be set in jewelry. Funeral practices and burial customs in the Philippines encompass a wide range of personal, cultural, and traditional beliefs and practices which Filipinos observe in relation to death, bereavement, and the proper honoring, interment, and remembrance of the dead. These practices have been vastly shaped by the variety of religions and cultures that entered the Philippines throughout its complex history. Most if not all present-day Filipinos, like their ancestors, believe in some form of an afterlife and give considerable attention to honouring the dead.[117] Except amongst Filipino Muslims (who are obliged to bury a corpse less than 24 hours after death), a wake is generally held from three days to a week.[118] Wakes in rural areas are usually held in the home, while in urban settings the dead is typically displayed in a funeral home. Friends and neighbors bring food to the family, such as pancit noodles and bibingka cake; any leftovers are never taken home by guests, because of a superstition against it.[44] Apart from spreading the news about someone's death verbally,[118] obituaries are also published in newspapers. Although the majority of the Filipino people are Christians,[119] they have retained some traditional indigenous beliefs concerning death.[120][121] In Korea, funerals are typically held for three days and different things are done in each day. The first day: on the day a person dies, the body is moved to a funeral hall. They prepare clothes for the body and put them into a chapel of rest. Then food is prepared for the deceased. It is made up of three bowls of rice and three kinds of Korean side dishes. Also, there has to be three coins and three straw shoes. This can be cancelled if the family of the dead person have a particular religion.[122] On the second day the funeral director washes the body and shrouding is done. Then, a family member of the dead person puts uncooked rice in the mouth of the body. This step does not have to be done if the family has a certain religion. After putting the rice in the mouth, the body is moved into a coffin. Family members, including close relatives, of the dead person will wear mourning clothing. Typically, mourning for a woman includes Korean traditional clothes, Hanbok, and mourning for man includes a suit. The color has to be black. The ritual ceremony begins when they are done with changing clothes and preparing foods for the dead person. The ritual ceremony is different depending on their religion. After the ritual ceremony family members will start to greet guests.[123] On the third day, the family decides whether to bury the body in the ground or cremate the body. In the case of burial, three family members sprinkle dirt on the coffin three times. In the case of cremation, there is no specific ritual; the only requirement is a jar to store burned bones and a place to keep the jar. Other than these facts, in Korea, people who come to the funeral bring condolence money. Also, a food called Yukgaejang is served to guests, oftentimes with the Korean distilled drink called soju.[124] In Mongolia, like many other cultures, funeral practices are considered extremely important.[citation needed], possessing significant elements of both native Mongolian rituals and Buddhist tradition.[125] For Mongolians who are very strict about tradition, families choose from three different ways of burial: open-air burial which is most common, cremation, and embalming. Many factors go into deciding which funeral practice to do. These consisted of the family's social standing, the cause of death, and the place of death. Embalming was mainly chosen by members of the Lamaistic Church; by choosing this practice, they are usually buried in a sitting position. This would show that they would always be in the position of prayer. Also, more important people such as nobles would be buried with weapons, horses and food in their coffins to help them prepare for the next world.[126] The coffin is designed and built by three to four relatives, mainly men. The builders bring planks to the hut where the dead is located and put together the box and the lid. The same people who build the coffin also decorate the funeral. Most of this work is done after dusk. With specific instruction, they work on decorations inside the youngest daughter's house. The reason for this is so the deceased is not disturbed at night.[127] In Vietnam, Buddhism is the most commonly practiced religion, however, most burial methods do not coincide with the Buddhist belief of cremation.[128] The body of the deceased is moved to a loved one's house and placed in an expensive coffin. The body usually stays there for about three days, allowing time for people to visit and place gifts in the mouth.[128] This stems from the Vietnamese belief that the dead should be surrounded by their family. This belief goes so far as to include superstition as well. If somebody is dying in Vietnamese culture, they are rushed home from the hospital so they can die there, because if they die away from home it is believed to be bad luck to take a corpse home.[129] Many services are also held in the Vietnamese burial practices. One is held before moving the coffin from the home and the other is held at the burial site.[130] After the burial of the loved one, incense is burned at the gravesite and respect is paid to all the nearby graves. Following this, the family and friends return to the home and enjoy a feast to celebrate the life of the recently departed.[130] Even after the deceased has been buried, the respect and honor continues. For the first 49 days after the burying, the family holds a memorial service every 7 days, where the family and friends come back together to celebrate the life of their loved one. After this, they meet again on the 100th day after the death, then 265 days after the death, and finally they meet on the anniversary of the death of their loved one, a whole year later, to continue to celebrate the glorious life of their recently departed.[131] The Vietnamese funeral, or \u0111\u00e1m gi\u1ed7, is a less somber occasion than most traditional Western funerals. The \u0111\u00e1m gi\u1ed7 is a celebration of the deceased's life and is centered around the deceased's family.[132] Family members might wear a traditional garment called a mourning headband to signify their relationship with the deceased. Typical mourning headbands are thin strips of fabric that are wrapped around the wearer's head. Traditionally, the deceased's closest family members, such as children, siblings, spouses, and parents will wear white mourning headbands. More distant family members' headband colors may vary. In some cultures, the deceased's nieces, nephews, or grandchildren may be required to wear white headbands with red dots. Other societies may encourage grandchildren to wear white headbands with blue dots. Fourth generation grandchildren often wear yellow mourning headbands. The use of mourning headbands emphasizes the importance of personal and familial roles in Vietnamese society. It also allows funeral attendants to carefully choose their interactions and offer condolences to those closest to the deceased.[133] Traditionally, attendants of a Vietnamese funeral service are encouraged to wear the color white. In many East Asian cultures, white is viewed as a sign of loss and mourning. In Vietnam, members of the Caodaist faith believe that white represents purity and the ability to communicate beyond spiritual worlds.[134] African funerals are usually open to many visitors. The custom of burying the dead in the floor of dwelling-houses has been to some degree prevalent on the Gold Coast of Africa. The ceremony depends on the traditions of the ethnicity the deceased belonged to. The funeral may last for as much as a week. Another custom, a kind of memorial, frequently takes place seven years after the person's death. These funerals and especially the memorials may be extremely expensive for the family in question. Cattle, sheep, goats, and poultry, may be offered and then consumed. The Ashanti and Akan ethnic groups in Ghana typically wear red and black during funerals. For special family members, there is typically a funeral celebration with singing and dancing to honor the life of the deceased. Afterwards, the Akan hold a sombre funeral procession and burial with intense displays of sorrow. Other funerals in Ghana are held with the deceased put in elaborate Fantasy coffins colored and shaped after a certain object, such as a fish, crab, boat, and even airplanes.[131] The Kane Kwei Carpentry Workshop in Teshie, named after Seth Kane Kwei who invented this new style of coffin, has become an international reference for this form of art. Evidence of Africa's earliest funeral was found in Kenya in 2021. A 78,000 year old Middle Stone Age grave of a three-year-old child was discovered in Panga ya Saidi cave complex, Kenya. Researchers said the child's head appeared to have been laid on a pillow. The body had been laid in a fetal position.[135][136] In Kenya funerals are an expensive undertaking. Keeping bodies in morgues to allow for fund raising is a common occurrence more so in urban areas. Some families opt to bury their dead in the countryside homes instead of urban cemeteries, thus spending more money on transporting the dead. The first emperor of the Qin dynasty, Qin Shi Huang's mausoleum is located in the Lintong District of Xi'an, Shaanxi Province. Qin Shi Huang's tomb is one of the World Heritage sites in China. Its remarkable feature and size have been known as one of the most important historical sites in China.[137] Qin Shi Huang is the first emperor who united China for the first time. The mausoleum was built in 247 BCE after he became the emperor of the Qin dynasty. Ancient Chinese mausoleums have unique characteristics compared to other cultures[citation?]. Ancient Chinese thought that the soul remains even after death, (immortal soul) regarded funeral practices as an important tradition.[138] From their long history, the construction of mausoleums has developed over time, creating monumental and massive ancient emperor's tomb. Archeologists have found more than 8,000 life-sized figures resembling an army surrounding the emperor's tomb.[139] The primary purpose of the placement of Terracotta Army is to protect the emperor's tomb. The figures were composed of clay and fragments of pottery. The Terracotta Army represents soldiers, horses, government officials, and even musicians. The arrangement and the weapons they are carrying accurately represent the real formations and weapons of the time. Furthermore, facial features aren't identical, each sculpture bearing a unique look. The Imperial Tombs of the Ming and Qing Dynasties are included as World Heritage Sites. The three Imperial Tombs of the Qin dynasty were added in 2000 and 2003.[140] The three tombs were all built in the 17th century. The tombs have been constructed to memorialize the emperors of the Qing dynasty and their ancestors. In tradition, Chinese have followed Feng Shui to build and decorate the interior. All of the tombs are strictly made following the superstition of Feng Shui. The Imperial Tombs of the Ming and Qing Dynasties clearly show the cultural and architectural tradition that has existed in the area for more than 500 years[citation?]. In Chinese culture, the tombs were considered as a portal between the world of the living and the dead[citation?]. Chinese believed that the portal would divide the soul into two parts. The half of the soul would go to heaven, and the other half would remain within the physical body.[141] From about 1600 to 1914 Europe had two professions that have almost entirely disappeared. The mute appears in art quite frequently, but in literature is probably best known from Dickens's Oliver Twist (1837\u20131839). Oliver is working for Mr Sowerberry when characterised thus: \"There's an expression of melancholy in his face, my dear... which is very interesting. He would make a delightful mute, my love.\" And in Martin Chuzzlewit (1842\u20131844), Moult, the undertaker, states: \"This promises to be one of the most impressive funerals,...no limitation of expense...I have orders to put on my whole establishment of mutes, and mutes come very dear, Mr Pecksniff\". The main function of a funeral mute was to stand around at funerals with a sad, pathetic face. A symbolic protector of the deceased, the mute would usually stand near the door of the home or church. In Victorian times, mutes would wear somber clothing including black cloaks, top hats with trailing hatbands, and gloves.[142] The professional mourner, generally a woman, would shriek and wail (often while clawing her face and tearing at her clothing), to encourage others to weep. Records document forms of professional mourning from Ancient Greece.[143][144] The 2003 award-winning Philippine comedy Crying Ladies revolves around the lives of three women who are part-time professional mourners for the Chinese-Filipino community in Manila's Chinatown. According to the film, the Chinese use professional mourners to help expedite the entry of a deceased loved one's soul into heaven by giving the impression that he or she was a good and loving person, well-loved by many. High-ranking national figures such as heads of state, prominent politicians, military figures, national heroes and eminent cultural figures may be offered state funerals. Common methods of disposal are: Some people choose to make their funeral arrangements in advance so that at the time of their death, their wishes are known to their family. However, the extent to which decisions regarding the disposition of a decedent's remains (including funeral arrangements) can be controlled by the decedent while still alive vary from one jurisdiction to another. In the United States, there are states which allow one to make these decisions for oneself if desired, for example by appointing an agent to carry out one's wishes; in other states, the law allows the decedent's next-of-kin to make the final decisions about the funeral without taking the wishes of the decedent into account.[150] The decedent may, in most U.S. jurisdictions, provide instructions as to the funeral by means of a last will and testament. These instructions can be given some legal effect if bequests are made contingent on the heirs carrying them out, with alternative gifts if they are not followed. This requires the will to become available in time; aspects of the disposition of the remains of US President Franklin Delano Roosevelt ran contrary to a number of his stated wishes, which were found in a safe that was not opened until after the funeral. Some people donate their bodies to a medical school for use in research or education. Medical students frequently study anatomy from donated cadavers; they are also useful in forensic research.[151] Some medical conditions, such as amputations or various surgeries can make the cadaver unsuitable for these purposes; in other cases the bodies of people who had certain medical conditions are useful for research into those conditions. Many medical schools rely on the donation of cadavers for the teaching of anatomy.[152] It is also possible to arrange for donate organs and tissue after death for treating the sick, or even whole cadavers for forensic research at body farms.",
      "ground_truth_chunk_ids": [
        "134_random_chunk1"
      ],
      "source_ids": [
        "S334"
      ],
      "category": "factual",
      "id": 95
    },
    {
      "question": "What is Dartmouth Jack-O-Lantern?",
      "ground_truth": "The Dartmouth Jack-O-Lantern (also known as the Jacko)[1] is a college humor magazine, founded at Dartmouth College in 1908. One of the magazine's oldest traditions is \"Stockman's Dogs\". In the October 1934 issue, F.C. Stockman (class of 1935) drew a single-panel cartoon of two dogs talking to each other. That same cartoon has appeared in virtually every issue published since, always with a different caption.[2] The magazine is alluded to in the opening lines of F. Scott Fitzgerald's short story \"The Lost Decade\", which was first published in Esquire in 1939.[3] Jack-O-Lantern writers Nic Duquette and Chris Plehal invented the unofficial Dartmouth mascot Keggy the Keg in the fall of 2003.[4] From 1972 to 1974 the Editor in chief was playwright Robert DeKanter '74. Among the first Dartmouth women on the staff was Barbara Donnelly, '77, later a writer for the Wall Street Journal. DeKanter was succeeded by the team Brad Brinegar and Maxwell Anderson, both '77. One evening in July, 1975, cartoonists Brian \"Hojo\" Hansen '76 and Mike Mosher '77 slipped in and painted a cubist rendition of bibulous alumni in translucent acrylic washes upon the wall. When this was eradicated the following week, Hansen and Mosher replaced it with a Renaissance-style \"pittura infamante\" (topic of an art history lecture in Carpenter Hall) called Allegory of the Evisceration of Humor, depicting Brinegar and Anderson abusing a Jack-O-Lantern figure. \"This was the perfect crime\" enthused Hansen, \"for to paint it over would prove our point: that they have no sense of humor.\" From 1976 to 1978 the Editor was N. Brooks Clark, who published a Jack-O-Lantern calendar during his tenure. Clark wrote a parody of the controversial college-issued sex guide, which he called Thrilling Contraception Comics and Stories, illustrated by Mosher and featuring a wisecracking spermatozoic guide, Snappy Sammy Sperm.",
      "expected_answer": "The Dartmouth Jack-O-Lantern (also known as the Jacko)[1] is a college humor magazine, founded at Dartmouth College in 1908. One of the magazine's oldest traditions is \"Stockman's Dogs\". In the October 1934 issue, F.C. Stockman (class of 1935) drew a single-panel cartoon of two dogs talking to each other. That same cartoon has appeared in virtually every issue published since, always with a different caption.[2] The magazine is alluded to in the opening lines of F. Scott Fitzgerald's short story \"The Lost Decade\", which was first published in Esquire in 1939.[3] Jack-O-Lantern writers Nic Duquette and Chris Plehal invented the unofficial Dartmouth mascot Keggy the Keg in the fall of 2003.[4] From 1972 to 1974 the Editor in chief was playwright Robert DeKanter '74.  Among the first Dartmouth women on the staff was Barbara Donnelly, '77, later a writer for the Wall Street Journal. DeKanter was succeeded by the team Brad Brinegar and Maxwell Anderson, both '77.  One evening in July, 1975, cartoonists Brian \"Hojo\" Hansen '76 and Mike Mosher '77 slipped in and painted a cubist rendition of bibulous alumni in translucent acrylic washes upon the wall.  When this was eradicated the following week, Hansen and Mosher replaced it with a Renaissance-style \"pittura infamante\" (topic of an art history lecture in Carpenter Hall) called Allegory of the Evisceration of Humor, depicting Brinegar and Anderson abusing a Jack-O-Lantern figure.  \"This was the perfect crime\" enthused Hansen, \"for to paint it over would prove our point: that they have no sense of humor.\" From 1976 to 1978 the Editor was N. Brooks Clark, who published a Jack-O-Lantern calendar during his tenure.  Clark wrote a parody of the controversial college-issued sex guide, which he called Thrilling Contraception Comics and Stories, illustrated by Mosher and featuring a wisecracking spermatozoic guide, Snappy Sammy Sperm.  It was reprinted in the 1982 Holt paperback collection of 1970s college humor,[5] whose lead editor Joey Green was the founding editor of the Cornell Lunatic. A 2006 video prank by the Jack-O-Lantern on a Dartmouth College tour group entitled \"Drinkin' Time\" was featured in an article by the Chronicle of Higher Education,[6] posted by AOL on the Online Video Blog,[7] and was mentioned by The Volokh Conspiracy.[8] As of November\u00a02013[update], the video has garnered over 585,000 views on YouTube.[9] The Jacko publishes print issues approximately four times a year, as well as regularly updated online content and occasional video productions. The magazine devotes one publication cycle each year to a parody of the campus newspaper, The Dartmouth.[1] Some notable writers, artists, comedians and politicians began their careers at the Jacko, including:[10]",
      "ground_truth_chunk_ids": [
        "204_random_chunk1"
      ],
      "source_ids": [
        "S404"
      ],
      "category": "factual",
      "id": 96
    },
    {
      "question": "What is Chelsea F.C.\u2013Liverpool F.C. rivalry?",
      "ground_truth": "The Chelsea F.C.\u2013Liverpool F.C. rivalry is an inter-city rivalry between English professional football clubs Chelsea and Liverpool. Chelsea play their home games at Stamford Bridge, while Liverpool play their home games at Anfield. Though both clubs have frequently competed in the same division for over a century, the modern rivalry between Chelsea and Liverpool began in the early 2000s, when the two clubs clashed repeatedly in cup competitions, particularly in the FA Cup, the League Cup, and the UEFA Champions League. The clubs have competed in seven major cup finals: the 2005 League Cup final, which Chelsea won 3\u20132 after extra time, the 2006 Community Shield, which Liverpool won 2\u20131, the 2012 FA Cup final, which Chelsea won 2\u20131, the 2019 UEFA Super Cup, which Liverpool won 5\u20134 on penalties, the 2022 EFL Cup and FA Cup finals, both of which saw Liverpool win on penalties after two goalless affairs, and the 2024 EFL Cup final, which Liverpool won 1\u20130 after extra time. The two clubs also met in five consecutive Champions League campaigns; in the group stage of the 2005\u201306 season, where both legs finished as goalless draws, in the quarter-finals of the 2008\u201309 season, which Chelsea won 7\u20135 on aggregate, and in the semi-finals of the 2004\u201305, 2006\u201307 and 2007\u201308 seasons, with Liverpool winning the former two and Chelsea winning the latter one.[1][2] Overall, Liverpool have won more of the meetings, defeating Chelsea 87 times to their 67 wins, and a further 46 games ended in draws, as of their latest clash in October 2025. Chelsea's record win over the Reds was a 6\u20131 thrashing at Stamford Bridge in August 1937, whereas Liverpool's biggest win was a 6\u20130 home win in April 1935. In 1904, Gus Mears acquired the Stamford Bridge athletics stadium in Fulham with the aim",
      "expected_answer": "The Chelsea F.C.\u2013Liverpool F.C. rivalry is an inter-city rivalry between English professional football clubs Chelsea and Liverpool. Chelsea play their home games at Stamford Bridge, while Liverpool play their home games at Anfield. Though both clubs have frequently competed in the same division for over a century, the modern rivalry between Chelsea and Liverpool began in the early 2000s, when the two clubs clashed repeatedly in cup competitions, particularly in the FA Cup, the League Cup, and the UEFA Champions League. The clubs have competed in seven major cup finals: the 2005 League Cup final, which Chelsea won 3\u20132 after extra time, the 2006 Community Shield, which Liverpool won 2\u20131, the 2012 FA Cup final, which Chelsea won 2\u20131, the 2019 UEFA Super Cup, which Liverpool won 5\u20134 on penalties, the 2022 EFL Cup and FA Cup finals, both of which saw Liverpool win on penalties after two goalless affairs, and the 2024 EFL Cup final, which Liverpool won 1\u20130 after extra time. The two clubs also met in five consecutive Champions League campaigns; in the group stage of the 2005\u201306 season, where both legs finished as goalless draws, in the quarter-finals of the 2008\u201309 season, which Chelsea won 7\u20135 on aggregate, and in the semi-finals of the 2004\u201305, 2006\u201307 and 2007\u201308 seasons, with Liverpool winning the former two and Chelsea winning the latter one.[1][2] Overall, Liverpool have won more of the meetings, defeating Chelsea 87 times to their 67 wins, and a further 46 games ended in draws, as of their latest clash in October 2025. Chelsea's record win over the Reds was a 6\u20131 thrashing at Stamford Bridge in August 1937, whereas Liverpool's biggest win was a 6\u20130 home win in April 1935. In 1904, Gus Mears acquired the Stamford Bridge athletics stadium in Fulham with the aim of turning it into a football ground. An offer to lease it to nearby Fulham F.C. was turned down, so Mears opted to found his own club to use the stadium. As there was already a team named Fulham in the borough, the name of the adjacent borough of Chelsea was chosen for the new club; names like Kensington FC, Stamford Bridge FC and London FC were also considered.[3] Chelsea Football Club was founded on 10 March 1905 at The Rising Sun pub (now The Butcher's Hook),[4][5] opposite the present-day main entrance to the ground on Fulham Road, and were elected to the Football League shortly afterwards. Chelsea won promotion to the First Division in their second season, and yo-yoed between the First and Second Divisions in its early years. The team reached the 1915 FA Cup final, where they lost to Sheffield United at Old Trafford, and finished third in the First Division in 1920, the club's best league campaign to that point.[6] Chelsea had a reputation for signing star players[7] and attracted large crowds. The club had the highest average attendance in English football in ten separate seasons[8] including 1907\u201308,[9] 1909\u201310,[10] 1911\u201312,[11] 1912\u201313,[12] 1913\u201314[13] and 1919\u201320.[14][15] They were FA Cup semi-finalists in 1920 and 1932, and remained in the First Division throughout the 1930s, but success eluded the club in the inter-war years. Liverpool Football Club was founded following a dispute between the Everton committee and John Houlding, club president and owner of the land at Anfield. After eight years at the stadium, Everton relocated to Goodison Park in 1892 and Houlding founded Liverpool F.C. to play at Anfield.[16] Originally named \"Everton F.C. and Athletic Grounds Ltd\" (Everton Athletic for short), the club became Liverpool F.C. in March 1892 and gained official recognition three months later, after The Football Association refused to recognise the club as Everton.[17] Liverpool played their first match on 1 September 1892, a pre-season friendly match against Rotherham Town, which they won 7\u20131. The team Liverpool fielded against Rotherham was composed entirely of Scottish players\u2014the players who came from Scotland to play in England in those days were known as the Scotch Professors. Manager John McKenna had recruited the players after a scouting trip to Scotland\u2014so they became known as the \"team of Macs\".[18] The team won the Lancashire League in its debut season and joined the Football League Second Division at the start of the 1893\u201394 season. After the club was promoted to the First Division in 1896, Tom Watson was appointed manager. He led Liverpool to its first league title in 1901, before winning it again in 1906.[19] Chelsea and Liverpool were not traditional rivals, meeting first for the first time on 25 December 1907 at Anfield in the Football League First Division, which ended in a 4\u20131 win for Chelsea. However, for the next 96 years, Chelsea would only manage one single league title, which came in 1955, whereas Liverpool (who were already two-time champions) would go on to win the First Division title sixteen more times, cementing the Reds' status as one of the biggest clubs in England and in Europe, along with major rivals Manchester United, whereas Chelsea were considered to be a mid-table club, and their rivalry with Liverpool was non-existent during the years leading up to the 21st century. We were the new kids on the block who had a few quid and signed a load of players. Jos\u00e9 Mourinho puffed his chest out and then we kept playing each other. It was a clash of two ideals. \u2014\u200aFrank Lampard on Chelsea's sudden rise to success[20] The seeds of the Chelsea vs. Liverpool rivalry were beginning to be sowed in May 2003. The first major meeting that would spark this feud was on the final day of the 2002\u201303 Premier League season, where fourth-placed Chelsea were to play fifth-placed Liverpool at Stamford Bridge in a clash for UEFA Champions League football. Both teams were level on 64 points, with the Blues having a +8 superior goal difference. The three teams that were above them, Manchester United in 1st, Arsenal in 2nd and Newcastle United in 3rd had already accumulated enough points to qualify for next season's Champions League, and sixth-placed Blackburn Rovers were unable to qualify, meaning Liverpool had to defeat Chelsea otherwise they would miss out on Champions League football next season. A goal from Sami Hyypi\u00e4 in the 11th minute put the Reds 1\u20130 up, but Chelsea equalised just two minutes later through Marcel Desailly. Fourteen minutes later, Chelsea found themselves ahead via a Jesper Gr\u00f8nkj\u00e6r strike. Steven Gerrard was dismissed two minutes from full-time, as Chelsea won 2\u20131 and ensured their place in the Champions League next season, with Liverpool having to settle for UEFA Cup (now Europa League) football instead. In July 2003, long-time chairman of Chelsea Ken Bates sold the club to Russian billionaire Roman Abramovich for \u00a3140,000,000. Chelsea spent \u00a3103,000,000 on transfers in the summer of 2003, which included the signings of Joe Cole from West Ham United and Hern\u00e1n Crespo from Inter Milan. Unlike the previous years, Chelsea under Abramovich had now become serious title contenders, threatening the likes of Manchester United and Arsenal, who combined had won ten of the first eleven Premier League titles. The first meeting between Chelsea and Liverpool after the Abramovich takeover was on the first matchday of the new campaign, at Anfield. Chelsea won 2\u20131, courtesy of goals from Juan Sebasti\u00e1n Ver\u00f3n and Jimmy Floyd Hasselbaink. Liverpool would get revenge in the reverse fixture at Stamford Bridge in January 2004, which saw Bruno Cheyrou condemn Chelsea to a 1\u20130 home defeat. However, despite the mass spend of Chelsea, they would still be unable to win the league, finishing as runners-up to the undefeated Arsenal. Meanwhile, Liverpool finished in fourth place, nineteen points behind Chelsea, but still qualifying for the Champions League. In the summer of 2004, Chelsea and Liverpool had respectively appointed managers Jos\u00e9 Mourinho and Rafael Ben\u00edtez, which was the beginning of a vicious rivalry between the pair. In their first season as rivals, they clashed five times, including two Premier League victories for Mourinho, both of which finished 1\u20130 to Chelsea and both of those goals being scored by Joe Cole. In the year 2005 alone, Chelsea and Liverpool met seven times. On 27 February 2005, Liverpool faced Chelsea in the final of the Football League Cup. This was Liverpool's tenth appearance in a Football League Cup final, having won seven of them (1981, 1982, 1983, 1984, 1995, 2001, 2003) and losing twice (1978, 1987). For Chelsea, this was their fourth appearance in the final, winning the cup final in 1965 and 1998, and losing in 1972. Liverpool had defeated Millwall, Middlesbrough, Tottenham Hotspur and Watford en route to the final, whereas Chelsea got past West Ham United, Newcastle United, West London rivals Fulham and Manchester United. A crowd of 78,000 at the Millennium Stadium in Cardiff saw John Arne Riise score a volley inside the first minute to put Liverpool ahead. The score remained 1-0 to the Reds for 79 minutes, until Steven Gerrard headed into his own net from a Chelsea free kick to give the Blues a lifeline. Jos\u00e9 Mourinho was also made to watch from the stands after making a gesture to the Liverpool fans. The score was 1\u20131 at full-time, taking the game to extra-time. Goals from Didier Drogba and Mateja Ke\u017eman put Chelsea 3\u20131 up. A goal from Antonio N\u00fa\u00f1ez a minute later reduced the deficit for Liverpool, but the Blues would triumph 3\u20132 and win the League Cup for the third time. Following the match, Mourinho defended the gesture that saw him dismissed, claiming that it had been intended for the media and not Liverpool fans: \"The signal of close your mouth was not for them but for the press, they speak too much and in my opinion they try to do everything to disturb Chelsea. Wait, don't speak too soon. We lost two matches and in my opinion you (the media) try to take confidence from us and put pressure on us.\" Mourinho was happy that Chelsea had won, but said the victory was not special: \"It's just one more. I had a few before this, I'm very happy to win. It's important for the fans, for the club and especially for the players.\"[21] Just two months after the League Cup final, the two clubs met yet again in the semi-finals of the Champions League. The first leg at Stamford Bridge ended in a goalless stalemate, however, in the second leg at Anfield, Liverpool controversially won 1\u20130, thanks to a goal scored by Luis Garc\u00eda, which despite Chelsea's attempts to clear the ball off the line, the goal was given. It was dubbed as a \"ghost goal\" by Jos\u00e9 Mourinho, which popularised the term for other future incidents. This would seal Liverpool's place in the Champions League final, where they would take on AC Milan, famously coming back from 3\u20130 down and winning the Champions League on penalties. Prior to the match, Chelsea were in hot pursuit of Steven Gerrard. Liverpool had rejected a \u00a332,000,000 bid from Chelsea, however, in a shocking turn of events, Gerrard had rejected a \u00a3100,000-a-week contract offer and had submitted a transfer request, just six weeks after inspiring the comeback to help Liverpool win their fifth Champions League. He eventually changed his mind, soon after signing a new four-year deal and later stating that he would rather win one Premier League title at Liverpool than multiple at Chelsea, as it would mean more to him. Chelsea's failed signing of Liverpool's elite poster boy resulted in yet more bad blood developing between the two sets of supporters. As for the Premier League season, Chelsea were runaway winners, winning their first Premier League title (second overall), finishing twelve points clear of second-placed Arsenal. They amassed a then record-setting 95 points, also winning 29 games, a record Chelsea themselves broke in 2016-17 with 30 wins[22][circular reference] (both broken by Manchester City in 2017\u201318) and conceding 15 goals, a record that still stands to this day as the best defensive record in Premier League history. Liverpool, meanwhile, finished fifth (a regression from the previous season), behind their Merseyside derby rivals Everton, who finished fourth, and 37 points behind Chelsea. However, this also meant that despite being the winners of the 2004\u201305 Champions League, they were not guaranteed a place in next season's edition, as they had finished outside of the top four of the Premier League. On 10 June 2005, UEFA decided to grant Liverpool special dispensation to defend their title, however, they would have to enter in the first qualifying round, and were denied country protection, which meant they could face any English team at any stage of the competition.[23][24][25] Liverpool would go on to defeat The New Saints, FBK Kaunas and CSKA Sofia in the Champions League first, second and third qualifying rounds, respectively, to advance to the group stage, where they were drawn in Group G, along with Chelsea, Anderlecht, and Real Betis, although both of their matches against Chelsea were 0\u20130 draws.[26] Liverpool would finish top of the group with 12 points, with Chelsea finishing second, just behind the Reds with 11. Mourinho's Chelsea would manage to get the better of Liverpool in their Premier League clashes, defeating them 4\u20131 away at Anfield in October 2005, which made them the first Premier League opposition team to score four goals at Anfield[a] and also beating them 2\u20130 at Stamford Bridge in February 2006. However, Ben\u00edtez's Liverpool were victorious in their semi-final encounter in the FA Cup, winning 2\u20131, ending Chelsea's hopes for their first ever double and progressing to the FA Cup final, where Steven Gerrard would score an equaliser in added time to help Liverpool defeat West Ham United 3\u20131 on penalties, in what became known as The Gerrard Final. After the match, Mourinho refused to shake Ben\u00edtez's hand and claimed that the best team had lost, pointing to his side's superior league position, stating: \"Did the best team win? I don't think so. In a one-off game, maybe they will surprise me and they can do it. In the Premiership, the distance between the teams is 45 points over two seasons.\" Chelsea would win the Premier League for a second consecutive season, finishing on 91 points, whereas Liverpool, who were also title contenders throughout the season as well, finished third on 82 points, a point behind second-placed Manchester United, and 9 points behind Chelsea. As Chelsea and Liverpool were the respective winners of the 2005\u201306 Premier League and the 2005\u201306 FA Cup, this meant that they would be playing each other in the 2006 FA Community Shield on 13 August 2006, at the Millennium Stadium in Cardiff, the same venue that hosted the 2005 Football League Cup final a year and a half prior, which saw Chelsea beat Liverpool 3\u20132. Chelsea were also the defending champions, having beaten their London rivals Arsenal the previous year. The Blues were making their sixth appearance in the Community Shield, having previously won in 1955 and 2000, and losing in 1970 and 1997. Liverpool, on the other hand, were appearing for the 21st time, emerging outright victorious eight times (1966, 1976, 1979, 1980, 1982, 1988, 1989, 2001), sharing the shield six times (1964, 1965, 1974, 1977, 1986, 1990) and losing it six times (1922, 1971, 1983, 1984, 1992, 2002). In the match, John Arne Riise opened the scoring for Liverpool early in the first half, only for Chelsea's recently signed forward Andriy Shevchenko to equalise shortly before half-time. Both sides had chances to win the match in the second half, but a Peter Crouch goal late in the half ensured Liverpool won the match 2\u20131, and won their 15th Community Shield. The two teams were again drawn against each other in the Champions League, squaring off in the semi-finals of the competition. Chelsea would win the first leg at Stamford Bridge 1\u20130 courtesy of a goal from Joe Cole, but Liverpool won the second leg 1\u20130 as well at Anfield, with Daniel Agger ensuring the tie finished 1\u20131 on aggregate. The team that would progress was decided in a penalty shootout. Liverpool would win the penalty shootout 4\u20131, sending them to their second Champions League final in three years, which would be a rematch of the 2005 edition, which saw AC Milan get their revenge on Liverpool and defeat them 2\u20131. I'll be honest. I couldn't stand Chelsea as a club. It surpassed Everton and Manchester United as our rivalry for a period. The following season saw Jos\u00e9 Mourinho depart Chelsea in September 2007 by mutual consent, and would replaced by Avram Grant, but they would still defeat Liverpool 2\u20130 in the quarter-finals of the League Cup, with goals from Frank Lampard and Andriy Shevchenko sending the Reds crashing out of the competition. Chelsea and Liverpool were drawn against each other yet again in the semi-finals of the Champions League. In the first leg at Anfield, a Dirk Kuyt goal two minutes before half-time put Liverpool ahead, and the scoreline would remain unchanged until the 95th minute, which saw John Arne Riise scored own goal to give Chelsea an advantage, with the match finishing 1\u20131 and the Blues heading into the second leg at Stamford Bridge with a crucial away goal. Chelsea would defeat Liverpool 3\u20132, with a brace from Didier Drogba and an emotional penalty from Frank Lampard seeing Chelsea finally get the better of Liverpool in the Champions League, and sending them to their first ever Champions League final, which they would go on to lose 6\u20135 on penalties to Manchester United. On 26 October 2008, Chelsea hosted Liverpool at Stamford Bridge in the ninth gameweek of the new Premier League campaign. At this point, Chelsea were top of the Premier League, and Liverpool were second, with both teams having 20 points and Chelsea having a +9 superior goal difference. Chelsea hadn't lost a home match in the Premier League in over four and a half years, last losing at Stamford Bridge to Arsenal in February 2004, and were looking to extend their lead at the top of the table and their home unbeaten run to 87 games. In surprising fashion, however, Liverpool would defeat Chelsea 1\u20130, with a 10th-minute strike from Xabi Alonso that deflected off Chelsea defender Jos\u00e9 Bosingwa sending the Reds to the top of the Premier League and ending the Blues' record-setting 86-game home unbeaten run, their first home league defeat in over four years, which is still the record for the most home games unbeaten in the Premier League.[29] In the reverse fixture at Anfield in February 2009, Liverpool defeated Chelsea again, this time winning 2\u20130, with both goals coming from Fernando Torres late in the game. This was the first season in Premier League history that Liverpool had completed a Premier League double over Chelsea. They would also finish second, above Chelsea who finished third, making it the first Premier League season since 2001\u201302 where Liverpool finished above Chelsea in the Premier League table. In the quarter finals of the Champions League, Liverpool and Chelsea were drawn against each other again, marking the fifth consecutive season in which they played together in the Champions League, the most in Champions League history. In the first leg at Anfield, Chelsea emphatically won 3\u20131, with two goals from defender Branislav Ivanovi\u0107 and a goal from Didier Drogba giving Chelsea an advantage in the second leg at Stamford Bridge, which saw both teams play out a thrilling 4\u20134 draw, with Chelsea winning 7\u20135 on aggregate and progressing to the semi-finals of the Champions League. From 2004 to 2009, Chelsea and Liverpool met a staggering 24 times.[30][31][32] After Rafael Ben\u00edtez departed from Liverpool in June 2010, the club struggled greatly under new manager Roy Hodgson, which saw them nine out of their first twenty matches in the Premier League and sitting 12th in the table, and one of the players who struggled was elite Spanish striker Fernando Torres. Despite Torres having a successful three and a half seasons at Liverpool, which saw him score 81 goals in nearly 150 appearances, he failed to win a single trophy at the club. Chelsea had previously expressed interest in signing Torres in 2008, but Torres responded by saying it would be \"many years\" before he left Liverpool.[33][34] On 27 January 2011, Liverpool rejected a \u00a340,000,000 bid from Chelsea for Torres, which was followed by Torres handing in a transfer request the next day, which was also rejected. Chelsea finally completed the signing of Torres on 31 January 2011, for \u00a350,000,000, a then British transfer record and making Torres the sixth most expensive player in football history at the time, with the signing enraging Liverpool fans and boiling the blood in the rivalry even further. Ironically, Torres made his Chelsea debut against Liverpool at Stamford Bridge on 6 February, where was he was greeted with flags and signs held up by Liverpool fans labelling him as a \"traitor\". Liverpool would go on to beat Chelsea 1\u20130, with a 69th-minute goal from Raul Meireles putting Fernando Torres' Chelsea debut in vain. The next season's edition of the Premier League saw both Chelsea and Liverpool underperform, with both teams finishing outside of the top four, which in normal circumstances would have saw them both absent from Europe entirely next season, with Chelsea, who finished sixth, qualifying for the Champions League as the Champions League winners, which also put their fierce London rivals Tottenham Hotspur, who finished fourth, in the nightmare scenario of finishing in the top four and still not qualifying for the Champions League, who had to settle for Europa League football instead. Meanwhile, Liverpool, who finished 8th, qualified for the third qualifying round of the Europa League as the runners-up of the FA Cup. However, Liverpool still managed to do a Premier League double over Chelsea, defeating them both home and away, which included a 4\u20131 humiliation at Anfield in May 2012. On 5 May 2012, Chelsea and Liverpool faced off in the final of the FA Cup for the very first time, at Wembley Stadium. Chelsea were looking to win their first trophy of the season, being managed by interim manager Roberto Di Matteo, who was prosperous about being appointed as Chelsea manager on a permanent basis. Meanwhile, Liverpool, who were being managed by club legend Kenny Dalglish, had already won the League Cup by beating Cardiff City on penalties in the final, also defeating Chelsea 2\u20130 in the fifth round en route to the final, and were aiming for a double. For Chelsea, this was their 11th appearance in a FA Cup final, having won on six occasions (1970, 1997, 2000, 2007, 2009, 2010) and lost on four occasions (1915, 1967, 1994, 2002). As for Liverpool, this was their 14th FA Cup final, winning the trophy seven times (1965, 1974, 1986, 1989, 1992, 2001, 2006) and being beaten six times (1914, 1950, 1971, 1977, 1988, 1996). On their way to the final, Chelsea defeated Portsmouth, West London rivals Queens Park Rangers, Birmingham City, Leicester City, and London rivals Tottenham Hotspur, whereas Liverpool defeated Oldham Athletic, rivals Manchester United, Brighton & Hove Albion, Stoke City, and Merseyside rivals Everton to earn their place in the final. In the match, Ramires put Chelsea in front in the 11th minute after he dispossessed Liverpool midfielder Jay Spearing and beat Pepe Reina in the Liverpool area. Chelsea extended their lead in the 52nd minute when striker Didier Drogba scored. Liverpool substitute Andy Carroll scored in the 64th minute to reduce the deficit to one goal. Carroll thought he had scored a second in the 81st minute, but his header was saved on the line by Chelsea goalkeeper Petr \u010cech. Carroll ran off celebrating, thinking he had equalised and the ball had crossed the line, but referee Phil Dowd did not award a goal (unlike the Luis Garc\u00eda \"ghost goal\" seven years prior), and Chelsea held on to win the match 2\u20131 and the FA Cup for the seventh time. On 21 April 2013, during Liverpool's 2\u20132 draw with Chelsea in a Premier League match at Anfield, Liverpool striker Luis Su\u00e1rez bit Chelsea defender Branislav Ivanovi\u0107. This was not the first time that something like this had happened; it was the second time that Su\u00e1rez had bitten an opponent.[35] It was not noticed by the officials, and Su\u00e1rez scored an equalizer in injury time.[36] The bite prompted UK Prime Minister David Cameron to call on the FA to take a hard line with Su\u00e1rez: the FA charged him with violent conduct and he was fined an undisclosed sum by his club.[37] Contrary to claims from Su\u00e1rez, Ivanovi\u0107 did not accept an apology.[37] Su\u00e1rez accepted the violent conduct charge but denied the FA's claim the standard punishment of three matches was clearly insufficient for his offence.[38] A three-man independent panel appointed by the FA decided on a ten-game ban for Su\u00e1rez, who did not appeal the ban; the panel criticized Su\u00e1rez for not appreciating \"the seriousness\" of the incident when he argued against a long ban. The panel also wanted to send a \"strong message that such deplorable behaviours do not have a place in football\", while noting that \"all players in the higher level of the game are seen as role models, have the duty to act professionally and responsibly, and set the highest example of good conduct to the rest of the game \u2013 especially to young players\".[39] The 2013\u201314 Premier League season saw Chelsea, Liverpool, Arsenal and Manchester City battle it out in a four-way title race, which eventually boiled down to Chelsea, Liverpool, and City. Chelsea did the Premier League double over both Liverpool and Manchester City, but inconsistent form and losses against the low-block teams saw them fail to win the Premier League. On 27 April 2014, Liverpool, now managed by Brendan Rodgers, welcomed Chelsea, now managed by a returning Jos\u00e9 Mourinho, to Anfield. At this point, the high-flying Reds were top of the Premier League on 80 points with just three games to go, five points clear of second-placed Chelsea and six of third-placed Manchester City (who had a game in hand). They had also scored nearly a century of Premier League goals, and were on course to win their first ever Premier League title, which would have happened if they were to win their last three games, which were Chelsea at home, Crystal Palace away, and Newcastle United at home. Additionally, a win against Chelsea would have seen the Blues be unable to catch Liverpool, as they would have been eight points behind them with two games left had they have won. The match saw Steven Gerrard infamously slip while receiving a pass in first half injury-time, which allowed Demba Ba to score for Chelsea and put them 1\u20130 up at Anfield. Liverpool ultimately were unable to equalise, as a goal from Willian in the dying moments of the game saw Chelsea run out 2\u20130 winners, with Liverpool only now being two points clear of their rivals from London and three points of clear of Manchester City, who had a game in hand and had +8 superior goal difference. Liverpool followed this up by throwing away a 3\u20130 lead at Crystal Palace and only managing to come out with a 3\u20133 draw, all but confirming Manchester City's Premier League victory. The next season, Chelsea hosted Liverpool at Stamford Bridge on 10 May 2015, who at this point had been top of the Premier League for every single matchday. Liverpool provided a guard of honour for Chelsea before kick off. The match finished 1\u20131, with the goals coming from John Terry and Steven Gerrard. In the match, Gerrard, who had confirmed a few months prior that he would be departing from Liverpool at the end of the season, received a standing ovation from both the Liverpool and Chelsea fans as he was being substituted off. In a post-match interview, Gerrard had mixed feelings about being clapped off the pitch by Chelsea fans, stating: \"I was more happy with the ovation from the Liverpool fans. I think Chelsea fans have showed respect for a couple of seconds for me, but they've slaughtered me all game, so I'm not going to get drawn into wishing the Chelsea fans very well. It was nice of them to turn up for once today. But yeah, you know when you get a standing ovation at a stadium, it's nice, but what's important to me is the support from the Liverpool fans and they've been with me since day one.\" Since then, the rivalry has cooled down a little bit, though fans of both clubs still hold a dislike for each other. Liverpool, then under J\u00fcrgen Klopp, would beat Chelsea in four successive major finals during this period: the 2019 UEFA Super Cup, the 2022 EFL Cup final and the 2022 FA Cup final (all on penalties), as well as finally winning their first Premier League title in the 2019\u201320 season. The two clubs met again in the final of the 2024 EFL Cup, with Liverpool winning 1\u20130 in extra-time thanks to a header from Reds captain Virgil van Dijk.[41] Chelsea also gave Liverpool a guard of honour when the latter won the 2019\u201320 and 2024\u201325 titles. Below are the players who have played for both Chelsea and Liverpool.[42][43] In 2024, former City academy player Rio Ngumoha joined Liverpool, having never played for Chelsea's first team.",
      "ground_truth_chunk_ids": [
        "216_random_chunk1"
      ],
      "source_ids": [
        "S416"
      ],
      "category": "factual",
      "id": 97
    },
    {
      "question": "What is Photography?",
      "ground_truth": "This is an accepted version of this page Photography is the art, application, and practice of creating images by recording light, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. It is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.[1] A person who operates a camera to capture or take photographs is called a photographer, while the captured image, also known as a photograph, is the result produced by the camera. Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive, depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing. Before the emergence of digital photography, photographs that utilized film had to be developed to produce negatives or projectable slides, and negatives had to be printed as positive images, usually in enlarged form. This was typically done by photographic laboratories, but many amateur photographers, students, and photographic artists did their own processing. The word \"photography\" was created from the Greek roots \u03c6\u03c9\u03c4\u03cc\u03c2 (ph\u014dt\u00f3s), genitive of \u03c6\u1ff6\u03c2 (ph\u014ds), \"light\"[2] and \u03b3\u03c1\u03b1\u03c6\u03ae (graph\u00e9)",
      "expected_answer": "Photography is the art, application, and practice of creating images by recording light, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. It is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.[1] A person who operates a camera to capture or take photographs is called a photographer, while the captured image, also known as a photograph, is the result produced by the camera. Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive, depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing. Before the emergence of digital photography, photographs that utilized film had to be developed to produce negatives or projectable slides, and negatives had to be printed as positive images, usually in enlarged form. This was typically done by photographic laboratories, but many amateur photographers, students, and photographic artists did their own processing. The word \"photography\" was created from the Greek roots \u03c6\u03c9\u03c4\u03cc\u03c2 (ph\u014dt\u00f3s), genitive of \u03c6\u1ff6\u03c2 (ph\u014ds), \"light\"[2] and \u03b3\u03c1\u03b1\u03c6\u03ae (graph\u00e9) \"representation by means of lines\" or \"drawing\",[3] together meaning \"drawing with light\".[4] Several people may have coined the same new term from these roots independently. H\u00e9rcules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, photographie, in private notes which a Brazilian historian believes were written in 1834.[5] This claim is widely reported but is not yet largely recognized internationally. The first use of the word by Florence became widely known after the research of Boris Kossoy in 1980.[6] On 25 February 1839, the German newspaper Vossische Zeitung published an article titled Photographie, discussing several priority claims, especially that of Henry Fox Talbot's, in relation to Daguerre's claim of invention.[7] The article is the earliest known occurrence of the word in public print.[8] It was signed \"J.M.\", believed to have been Berlin astronomer Johann von Maedler.[9] The astronomer John Herschel is also credited with coining the word, independent of Talbot, in 1839.[10] The inventors Nic\u00e9phore Ni\u00e9pce, Talbot, and Louis Daguerre seem not to have known or used the word \"photography\", but referred to their processes as \"Heliography\" (Ni\u00e9pce), \"Photogenic Drawing\"/\"Talbotype\"/\"Calotype\" (Talbot), and \"Daguerreotype\" (Daguerre).[9] Photography is the result of combining several technical discoveries relating to seeing an image and capturing the image. The discovery of the camera obscura (\"dark chamber\" in Latin) that provides an image of a scene dates back to ancient China. Greek mathematicians Aristotle and Euclid independently described a camera obscura in the 5th and 4th centuries BCE.[11][12] In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments.[13] The Arab physicist Ibn al-Haytham (Alhazen) (965\u20131040) also invented a camera obscura as well as the first true pinhole camera.[12][14][15] The invention of the camera has been traced back to the work of Ibn al-Haytham.[16] While the effects of a single light passing through a pinhole had been described earlier,[16] Ibn al-Haytham gave the first correct analysis of the camera obscura,[17] including the first geometrical and quantitative descriptions of the phenomenon,[18] and was the first to use a screen in a dark room so that an image from one side of a hole in the surface could be projected onto a screen on the other side.[19] He also first understood the relationship between the focal point and the pinhole,[20] and performed early experiments with afterimages, laying the foundations for the invention of photography in the 19th century.[15] Leonardo da Vinci mentions natural camerae obscurae that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. Renaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western art. It is a box with a small hole in one side, which allows specific light rays to enter, projecting an inverted image onto a viewing screen or paper. The birth of photography was then concerned with inventing means to capture and keep the image produced by the camera obscura. Albertus Magnus (1193\u20131280) discovered silver nitrate,[21] and Georg Fabricius (1516\u20131571) discovered silver chloride.[22] Daniele Barbaro described a diaphragm in 1566.[23] Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694.[24] Around 1717, Johann Heinrich Schulze used a light-sensitive slurry to capture images of cut-out letters on a bottle and on that basis many German sources and some international ones credit Schulze as the inventor of photography.[25][26]\nThe fiction book Giphantie, published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.[23] In June 1802, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance.[27] He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that \"the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver.\" The shadow images eventually darkened all over.[28] The first permanent photoetching was an image produced in 1822 by the French inventor Nic\u00e9phore Ni\u00e9pce, but it was destroyed in a later attempt to make prints from it.[29] Ni\u00e9pce was successful again in 1825. In 1826 he made the View from the Window at Le Gras, the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).[30] Because Ni\u00e9pce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy. Ni\u00e9pce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Ni\u00e9pce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements\u2014a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and \"fixed\" with hot saturated salt water\u2014were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait. In Brazil, Hercules Florence had started working out a silver-salt-based paper process in 1832, later naming it photographia, at least four years before John Herschel coined the English word photography. In 1834, having settled on silver nitrate on paper, a combination which had been the subject of experiments by Thomas Wedgwood around the year 1800, Florence's notebooks indicate that he eventually succeeded in creating light-fast, durable images.[31] Partly because he never published his invention adequately, partly because he was an obscure inventor living in a remote and undeveloped province, H\u00e9rcules Florence died, in Brazil, unrecognized internationally as one of the inventors of photography during his lifetime.[32][33][34] Meanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834[35] but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method in a paper to the Royal Society[35] and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as daguerreotypes could only be replicated by rephotographing them with a camera.[36] Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.[37][38] In March 1837,[39] Franz von Kobell, used silver chloride and a cardboard camera to make pictures in negative of the Frauenkirche and other buildings in Munich, then taking another picture of the negative to get a positive, the actual black and white reproduction of a view on the object. In 1839, Kobell, together with Carl August von Steinheil, reported on their experiments to the Bavarian Academy of Sciences. The pictures produced were round with a diameter of 4\u00a0cm, the method was later named the \"Steinheil method\". In France, Hippolyte Bayard invented his own process for producing direct positive paper prints and claimed to have invented photography earlier than Daguerre or Talbot.[40] British chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the \"blueprint\". He was the first to use the terms \"photography\", \"negative\" and \"positive\". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to \"fix\" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839. In the March 1851 issue of The Chemist, Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper. Many advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908. Glass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 21st century. Hurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised. The first flexible photographic roll film was marketed by George Eastman, founder of Kodak in 1885, but this original \"film\" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose known as nitrate film. Although cellulose acetate or \"safety film\" had been introduced by Kodak in 1908,[42] at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16\u00a0mm and 8\u00a0mm home movies, nitrate film remained standard for theatrical 35\u00a0mm motion pictures until it was finally discontinued in 1951. Films remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats.[43] Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive \"look\" of film based photographs compared to digital images is likely due to a combination of factors, including (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors),[44] (2) resolution, and (3) continuity of tone.[45] Originally, all photography was monochrome, or black-and-white. Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost, chemical stability, and its \"classic\" photographic look. The tones and contrast between light and dark areas define black-and-white photography.[46] Monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process, publicly revealed in 1847, produces brownish tones. Many photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color. Color photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not \"fix\" the photograph to prevent the color from quickly fading when exposed to white light. The first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855.[47][48] The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters.[47][48] This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s. Russian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color \"fringes\" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images. Implementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability. Autochrome, the first commercially successful color process, was introduced by the Lumi\u00e8re brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s. Kodachrome, the first modern \"integral tripack\" (or \"monopack\") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure. Agfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product. Instant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963. Color photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995\u20132005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive \"look\". In 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. The first digital camera to both record and save images in a digital format was the Fujix DS-1P created by Fujifilm in 1988.[49] In 1991, Kodak unveiled the DCS 100, the first commercially available digital single-lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born. Digital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film.[50] An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications. Digital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones. A large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; dual photography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques. The camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.[51] Photographers control the camera and lens to \"expose\" the light recording material to the required amount of light to form a \"latent image\" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal\u2013oxide\u2013semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on paper. The camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera). As soon as photographic materials became \"fast\" (sensitive) enough for taking candid or surreptitious pictures, small \"detective\" cameras were made, some actually disguised as a book or handbag or pocket watch (the Ticka camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens. The movie camera is a type of photographic camera that takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a \"frame\". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the \"frame rate\" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures to create the illusion of motion.[52] Photographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion.[53] While known colloquially as \"3-D\" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras). Dualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.[54] Ultraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions. Modified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350\u00a0nm to 1000\u00a0nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400\u00a0nm to 700\u00a0nm.[55] Replacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters). Uses of full spectrum photography are for fine art photography, geology, forensics and law enforcement. Layering is a photographic composition technique that manipulates the foreground, subject or middle-ground, and background layers in a way that they all work together to tell a story through the image.[56] Layers may be incorporated by altering the focal length, distorting the perspective by positioning the camera in a certain spot.[57] People, movement, light and a variety of objects can be used in layering.[58] Digital methods of image capture and display processing have enabled the new technology of \"light field photography\" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected after the photograph has been captured.[59] As explained by Michael Faraday in 1846, the \"light field\" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point. These additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm. Besides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures. Amateur photographers take photos for personal use, as a hobby or out of casual interest, rather than as a business or job. The quality of amateur work can be comparable to that of many professionals. Amateurs can fill a gap in subjects or topics that might not otherwise be photographed if they are not commercially useful or salable. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera.[60] Twenty-first century social media and near-ubiquitous camera phones have made photographic and video recording pervasive in everyday life. In the mid-2010s smartphone cameras added numerous automatic assistance features like color management, autofocus face detection and image stabilization that significantly decreased skill and effort needed to take high quality images.[61] Commercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. The commercial photographic world could include: During the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.\nAt first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else. The aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images \"written with light\"; Nic\u00e9phore Ni\u00e9pce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art. Clive Bell in his classic essay Art states that only \"significant form\" can distinguish art from what is not art. There must be some one quality without which a work of art cannot exist; possessing which, in the least degree, no work is altogether worthless. What is this quality? What quality is shared by all objects that provoke our aesthetic emotions? What quality is common to Sta. Sophia and the windows at Chartres, Mexican sculpture, a Persian bowl, Chinese carpets, Giotto's frescoes at Padua, and the masterpieces of Poussin, Piero della Francesca, and Cezanne? Only one answer seems possible\u00a0\u2013 significant form. In each, lines and colors combined in a particular way, certain forms and relations of forms, stir our aesthetic emotions.[62] On 7 February 2007, Sotheby's London sold the 2001 photograph 99 Cent II Diptychon for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.[63] Conceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract. In parallel to this development, the then largely separate interface between painting and photography was closed in the second half of the 20th century with the chemigram of Pierre Cordier and the chemogram of Josef H. Neumann.[64] In 1974 the chemograms by Josef H. Neumann concluded the separation of the painterly background and the photographic layer by showing the picture elements in a symbiosis that had never existed before, as an unmistakable unique specimen, in a simultaneous painterly and at the same time real photographic perspective, using lenses, within a photographic layer, united in colors and shapes. This Neumann chemogram from the 1970s thus differs from the beginning of the previously created cameraless chemigrams of a Pierre Cordier and the photogram Man Ray or L\u00e1szl\u00f3 Moholy-Nagy of the previous decades. These works of art were almost simultaneous with the invention of photography by various important artists who characterized Hippolyte Bayard, Thomas Wedgwood, William Henry Fox Talbot in their early stages, and later Man Ray and L\u00e1szl\u00f3 Moholy-Nagy in the twenties and by the painter in the thirties Edmund Kesting and Christian Schad by draping objects directly onto appropriately sensitized photo paper and using a light source without a camera.\n[65] Photojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining, including sports photography. The camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are usually taken from three vantage points: overview, mid-range, and close-up.[66] In 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24-hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century.[67][68] Charles Brooke a little later developed similar instruments for the Greenwich Observatory.[69] Science regularly uses image technology that has derived from the design of the pinhole camera to avoid distortions that can be caused by lenses. X-ray machines are similar in design to pinhole cameras, with high-grade filters and laser radiation.[70]\nPhotography has become universal in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.[71] The first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an \"Ion\" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.[72] Wildlife photography involves capturing images of various forms of wildlife. Unlike other forms of photography such as product or food photography, successful wildlife photography requires a photographer to choose the right place and right time when specific wildlife are present and active. It often requires great patience and considerable skill and command of the right photographic equipment.[73] There are many ongoing questions about different aspects of photography. In her On Photography (1977), Susan Sontag dismisses the objectivity of photography. This is a highly debated subject within the photographic community.[74] Sontag argues, \"To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power.\"[75] Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation. Modern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's Rear Window (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.[75] The camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate \u2013 all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.[75] Digital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make \"photomontages\", passing them as \"real\" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography. Photography is one of the new media forms that changes perception and changes the structure of society.[76] Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that \"to photograph is to turn people into objects that can be symbolically possessed\". Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.[75] One of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a \"tourist gaze\"[77] in which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a \"reverse gaze\"[78] through which indigenous photographees can position the tourist photographer as a shallow consumer of images. Photography is both restricted and protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view.[79] In the UK, the Counter-Terrorism Act (2008) has increased the power of the police to prevent people, even press photographers, from taking pictures in public places.[80] In South Africa, any person may photograph any other person, without their permission, in public spaces and the only specific restriction placed on what may not be photographed by government is related to anything classed as national security. Each country has different laws.",
      "ground_truth_chunk_ids": [
        "93_fixed_chunk1"
      ],
      "source_ids": [
        "S093"
      ],
      "category": "factual",
      "id": 98
    },
    {
      "question": "What is Ferrari 166 S?",
      "ground_truth": "The Ferrari 166 S is a sports car built by Ferrari between 1948 and 1953, as a evolution of its Colombo V12-powered 125 S racer. It was adapted into a sports car for the street in the form of the 166 Inter. Only 12 Ferrari 166 S were produced, nine of them with cycle-fenders as the Spyder Corsa. It was soon followed by the updated and highly successful Ferrari 166 MM (Mille Miglia), of which 47 were made from 1948 to 1953. Its early victories in the Targa Florio and Mille Miglia and others in international competition made the manufacturer a serious competitor in the racing industry.[4] Both were later replaced by the 2.3 L 195 S. The 166 shared its Aurelio Lampredi-designed tube frame[5] and double wishbone/live axle suspension with the 125. Like the 125, the wheelbase was 2420 mm long. Nine 166 Spyder Corsas and three 166 Sports were built. The first two 166 S models were coachbuilt by Carrozzeria Allemano and the last one by Carlo Anderloni at Carrozzeria Touring. Majority of the 166 MM cars were bodied at Touring in a barchetta form. The 1.5 L Gioacchino Colombo-designed V12 engine of the 125 was changed, however, with single overhead camshafts specified and a larger 2.0 L (1995 cc/121 in\u00b3) displacement. This was achieved with both a bore and stroke increase, to 60 by 58.8 mm respectively. Output was 110 PS (81 kW) at 5,600 rpm to 130 PS (96 kW) at 6,500 rpm with three carburetors, giving top speed of 170\u2013215 km/h (106\u2013134 mph).[6][7] For the 166 MM power output rose to 140 PS (103 kW) at 6,600 rpm and top speed to 220 km/h (137 mph).[8] Motor Trend Classic named the 166 MM Barchetta as number six in their list of the ten \"Greatest Ferraris",
      "expected_answer": "The Ferrari 166 S is a sports car built by Ferrari between 1948 and 1953, as a evolution of its Colombo V12-powered 125 S racer. It was adapted into a sports car for the street in the form of the 166 Inter. Only 12 Ferrari 166 S were produced, nine of them with cycle-fenders as the Spyder Corsa. It was soon followed by the updated and highly successful Ferrari 166 MM (Mille Miglia), of which 47 were made from 1948 to 1953. Its early victories in the Targa Florio and Mille Miglia and others in international competition made the manufacturer a serious competitor in the racing industry.[4] Both were later replaced by the 2.3\u00a0L 195 S. The 166 shared its Aurelio Lampredi-designed tube frame[5] and double wishbone/live axle suspension with the 125. Like the 125, the wheelbase was 2420\u00a0mm long. Nine 166 Spyder Corsas and three 166 Sports were built. The first two 166 S models were coachbuilt by Carrozzeria Allemano and the last one by Carlo Anderloni at Carrozzeria Touring. Majority of the 166 MM cars were bodied at Touring in a barchetta form. The 1.5\u00a0L Gioacchino Colombo-designed V12 engine of the 125 was changed, however, with single overhead camshafts specified and a larger 2.0\u00a0L (1995\u00a0cc/121\u00a0in\u00b3) displacement. This was achieved with both a bore and stroke increase, to 60 by 58.8\u00a0mm respectively. Output was 110\u00a0PS (81\u00a0kW) at 5,600\u00a0rpm to 130\u00a0PS (96\u00a0kW) at 6,500\u00a0rpm with three carburetors, giving top speed of 170\u2013215\u00a0km/h (106\u2013134\u00a0mph).[6][7] For the 166 MM power output rose to 140\u00a0PS (103\u00a0kW) at 6,600\u00a0rpm and top speed to 220\u00a0km/h (137\u00a0mph).[8] Motor Trend Classic named the 166 MM Barchetta as number six in their list of the ten \"Greatest Ferraris of all time\".[9] The Ferrari 166 S won Targa Florio with Clemente Biondetti and Igor Troubetzkoy in 1948. In 1949, Biondetti also won in the 166 SC with Benedetti as co-driver. The 166 S won 1948 Mille Miglia, also driven by Biondetti, this time with Giuseppe Navone.[10] In 1949 Mille Miglia, the Ferrari 166 MM Barchettas scored 1-2 victory with Biondetti/Salani and Bonetto/Carpani respectively.[11] In 1949, the 166 MM also won the 24 Hours of Le Mans in the hands of Luigi Chinetti and Lord Selsdon, and so the \n166 was the only car ever to win all three races.[12] Another 166 won the 1949 Spa 24 Hours. A 166 chassis, this time with the bigger 195 S engine, won the Mille Miglia again in 1950 with drivers Giannino Marzotto and Marco Crosara. The oldest Ferrari car with an undisputed pedigree[citation needed] is s/n 002C, a 166 Spider Corsa which was originally a 159 and is currently owned and driven by James Glickenhaus. S/n 0052M, a 1950 166 MM Touring Barchetta was uncovered in a barn and was shown in public for the first time since 1959 in the August 2006 issue of Cavallino magazine. One 166 MM, 1949 s/n 0018M, was bodied by Zagato in 'Panoramica' style, very similar to their one-off Maserati A6 1500, also designed by Vieri Rapi. It is considered as first Ferrari coachbuilt by Zagato. A year later it was rebodied as Zagato Spyder.[13] The original car was recreated in 2007 as part of Zagato's Sanction Lost programme.[14]",
      "ground_truth_chunk_ids": [
        "16_random_chunk1"
      ],
      "source_ids": [
        "S216"
      ],
      "category": "factual",
      "id": 99
    },
    {
      "question": "What is Jack Keeney?",
      "ground_truth": "John Christopher \"Jack\" Keeney (February 19, 1922 \u2013 November 19, 2011) was an American prosecutor who retired in 2010 as U.S. deputy United States Assistant Attorney General. At age 88, he was at the time the DOJ's oldest employee, and one of the longest-serving career employees in the history of the United States government. Upon his retirement, Keeney was the longest-serving federal prosecutor in American history.[1] Keeney spent decades in the United States Department of Justice Criminal Division, starting in 1951. On numerous occasions, Keeney served as Acting Assistant Attorney General. Keeney was born in Ashley, Pennsylvania, on February 19, 1922.[1] Keeney was a pilot in the Army Air Corps during World War II, and was held by German forces as a prisoner of war. Keeney graduated from the University of Scranton in 1947.[1] He received law degrees from Dickinson School of Law in 1949 and from George Washington University Law School in 1953.[1] In 2000, the Justice Department named one of its buildings (1301 New York Avenue, N.W., Washington, D.C.) after Keeney, an honor rarely bestowed on a living person.[1] In the month following his death, the Justice Department created the John C. Keeney Award for Exceptional Integrity and Professionalism. The John C. Keeney Award recognizes a Justice Department employee who has demonstrated outstanding professionalism and integrity over a sustained period of time or an employee who has displayed extraordinary strength of character in a unique situation, as Mr. Keeney displayed during his years of service to the federal government.[2] Keeney died on November 19, 2011, at his home in Kensington, Maryland, aged 89.[3] This American law\u2013related biographical article is a stub. You can help Wikipedia by adding missing information.",
      "expected_answer": "John Christopher \"Jack\" Keeney (February 19, 1922 \u2013 November 19, 2011) was an American prosecutor who retired in 2010 as U.S. deputy United States Assistant Attorney General. At age 88, he was at the time the DOJ's oldest employee, and one of the longest-serving career employees in the history of the United States government. Upon his retirement, Keeney was the longest-serving federal prosecutor in American history.[1] Keeney spent decades in the United States Department of Justice Criminal Division, starting in 1951. On numerous occasions, Keeney served as Acting Assistant Attorney General. Keeney was born in Ashley, Pennsylvania, on February 19, 1922.[1] Keeney was a pilot in the Army Air Corps during World War II, and was held by German forces as a prisoner of war. Keeney graduated from the University of Scranton in 1947.[1] He received law degrees from Dickinson School of Law in 1949 and from George Washington University Law School in 1953.[1] In 2000, the Justice Department named one of its buildings (1301 New York Avenue, N.W., Washington, D.C.) after Keeney, an honor rarely bestowed on a living person.[1] In the month following his death, the Justice Department created the John C. Keeney Award for Exceptional Integrity and Professionalism. The John C. Keeney Award recognizes a Justice Department employee who has demonstrated outstanding professionalism and integrity over a sustained period of time or an employee who has displayed extraordinary strength of character in a unique situation, as Mr. Keeney displayed during his years of service to the federal government.[2] Keeney died on November 19, 2011, at his home in Kensington, Maryland, aged 89.[3] This American law\u2013related biographical article is a stub. You can help Wikipedia by adding missing information.",
      "ground_truth_chunk_ids": [
        "280_random_chunk1"
      ],
      "source_ids": [
        "S480"
      ],
      "category": "factual",
      "id": 100
    }
  ]
}